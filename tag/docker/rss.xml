<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>docker on blog.oddbit.com</title><link>https://blog.oddbit.com/tag/docker/</link><description>Recent content in docker on blog.oddbit.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Lars Kellogg-Stedman</copyright><lastBuildDate>Mon, 20 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.oddbit.com/tag/docker/rss.xml" rel="self" type="application/rss+xml"/><item><title>Directing different ports to different containers with Traefik</title><link>https://blog.oddbit.com/post/2022-06-20-traefik-multiple-listeners/</link><pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2022-06-20-traefik-multiple-listeners/</guid><description>This post is mostly for myself: I find the Traefik documentation hard to navigate, so having figured this out in response to a question on Stack Overflow, I&amp;rsquo;m putting it here to help it stick in my head.
The question asks essentially how to perform port-based routing of requests to containers, so that a request for http://example.com goes to one container while a request for http://example.com:9090 goes to a different container.</description><content>&lt;p>This post is mostly for myself: I find the &lt;a href="https://traefik.io">Traefik&lt;/a> documentation hard to navigate, so having figured this out in response to &lt;a href="https://stackoverflow.com/a/72694677/147356">a question on Stack Overflow&lt;/a>, I&amp;rsquo;m putting it here to help it stick in my head.&lt;/p>
&lt;p>The question asks essentially how to perform port-based routing of requests to containers, so that a request for &lt;code>http://example.com&lt;/code> goes to one container while a request for &lt;code>http://example.com:9090&lt;/code> goes to a different container.&lt;/p>
&lt;h2 id="creating-entrypoints">Creating entrypoints&lt;/h2>
&lt;p>A default Traefik configuration will already have a listener on port 80, but if we want to accept connections on port 9090 we need to create a new listener: what Traefik calls an &lt;a href="https://doc.traefik.io/traefik/routing/entrypoints/">entrypoint&lt;/a>. We do this using the &lt;code>--entrypoints.&amp;lt;name&amp;gt;.address&lt;/code> option. For example, &lt;code>--entrypoints.ep1.address=80&lt;/code> creates an entrypoint named &lt;code>ep1&lt;/code> on port 80, while &lt;code>--entrypoints.ep2.address=9090&lt;/code> creates an entrypoint named &lt;code>ep2&lt;/code> on port 9090. Those names are important because we&amp;rsquo;ll use them for mapping containers to the appropriate listener later on.&lt;/p>
&lt;p>This gives us a Traefik configuration that looks something like:&lt;/p>
&lt;pre tabindex="0">&lt;code> proxy:
image: traefik:latest
command:
- --api.insecure=true
- --providers.docker
- --entrypoints.ep1.address=:80
- --entrypoints.ep2.address=:9090
ports:
- &amp;#34;80:80&amp;#34;
- &amp;#34;127.0.0.1:8080:8080&amp;#34;
- &amp;#34;9090:9090&amp;#34;
volumes:
- /var/run/docker.sock:/var/run/docker.sock
&lt;/code>&lt;/pre>&lt;p>We need to publish ports &lt;code>80&lt;/code> and &lt;code>9090&lt;/code> on the host in order to accept connections. Port 8080 is by default the Traefik dashboard; in this configuration I have it bound to &lt;code>localhost&lt;/code> because I don&amp;rsquo;t want to provide external access to the dashboard.&lt;/p>
&lt;h2 id="routing-services">Routing services&lt;/h2>
&lt;p>Now we need to configure our services so that connections on ports 80 and 9090 will get routed to the appropriate containers. We do this using the &lt;code>traefik.http.routers.&amp;lt;name&amp;gt;.entrypoints&lt;/code> label. Here&amp;rsquo;s a simple example:&lt;/p>
&lt;pre tabindex="0">&lt;code>app1:
image: docker.io/alpinelinux/darkhttpd:latest
labels:
- traefik.http.routers.app1.entrypoints=ep1
- traefik.http.routers.app1.rule=Host(`example.com`)
&lt;/code>&lt;/pre>&lt;p>In the above configuration, we&amp;rsquo;re using the following labels:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>traefik.http.routers.app1.entrypoints=ep1&lt;/code>&lt;/p>
&lt;p>This binds our &lt;code>app1&lt;/code> container to the &lt;code>ep1&lt;/code> entrypoint.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>traefik.http.routers.app1.rule=Host(`example.com`)&lt;/code>&lt;/p>
&lt;p>This matches requests with &lt;code>Host: example.com&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>So in combination, these two rules say that any request on port 80 for &lt;code>Host: example.com&lt;/code> will be routed to the &lt;code>app1&lt;/code> container.&lt;/p>
&lt;p>To get port &lt;code>9090&lt;/code> routed to a second container, we add:&lt;/p>
&lt;pre tabindex="0">&lt;code>app2:
image: docker.io/alpinelinux/darkhttpd:latest
labels:
- traefik.http.routers.app2.rule=Host(`example.com`)
- traefik.http.routers.app2.entrypoints=ep2
&lt;/code>&lt;/pre>&lt;p>This is the same thing, except we use entrypoint &lt;code>ep2&lt;/code>.&lt;/p>
&lt;p>With everything running, we can watch the logs from &lt;code>docker-compose up&lt;/code> and see that a request on port 80:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -H &amp;#39;host: example.com&amp;#39; localhost
&lt;/code>&lt;/pre>&lt;p>Is serviced by &lt;code>app1&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>app1_1 | 172.20.0.2 - - [21/Jun/2022:02:44:11 +0000] &amp;#34;GET / HTTP/1.1&amp;#34; 200 354 &amp;#34;&amp;#34; &amp;#34;curl/7.76.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>And that request on port 9090:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -H &amp;#39;host: example.com&amp;#39; localhost:9090
&lt;/code>&lt;/pre>&lt;p>Is serviced by &lt;code>app2&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>app2_1 | 172.20.0.2 - - [21/Jun/2022:02:44:39 +0000] &amp;#34;GET / HTTP/1.1&amp;#34; 200 354 &amp;#34;&amp;#34; &amp;#34;curl/7.76.1&amp;#34;
&lt;/code>&lt;/pre>&lt;hr>
&lt;p>The complete &lt;code>docker-compose.yaml&lt;/code> file from this post looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: &amp;#34;3&amp;#34;
services:
proxy:
image: traefik:latest
command:
- --api.insecure=true
- --providers.docker
- --entrypoints.ep1.address=:80
- --entrypoints.ep2.address=:9090
ports:
- &amp;#34;80:80&amp;#34;
- &amp;#34;8080:8080&amp;#34;
- &amp;#34;9090:9090&amp;#34;
volumes:
- /var/run/docker.sock:/var/run/docker.sock
app1:
image: docker.io/alpinelinux/darkhttpd:latest
labels:
- traefik.http.routers.app1.rule=Host(`example.com`)
- traefik.http.routers.app1.entrypoints=ep1
app2:
image: docker.io/alpinelinux/darkhttpd:latest
labels:
- traefik.http.routers.app2.rule=Host(`example.com`)
- traefik.http.routers.app2.entrypoints=ep2
&lt;/code>&lt;/pre></content></item><item><title>Building multi-architecture images with GitHub Actions</title><link>https://blog.oddbit.com/post/2020-09-25-building-multi-architecture-im/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-09-25-building-multi-architecture-im/</guid><description>At work we have a cluster of IBM Power 9 systems running OpenShift. The problem with this environment is that nobody runs Power 9 on their desktop, and Docker Hub only offers automatic build support for the x86 architecture. This means there&amp;rsquo;s no convenient options for building Power 9 Docker images&amp;hellip;or so I thought.
It turns out that Docker provides GitHub actions that make the process of producing multi-architecture images quite simple.</description><content>&lt;p>At work we have a cluster of IBM Power 9 systems running OpenShift. The
problem with this environment is that nobody runs Power 9 on their desktop,
and Docker Hub only offers automatic build support for the x86
architecture. This means there&amp;rsquo;s no convenient options for building Power 9
Docker images&amp;hellip;or so I thought.&lt;/p>
&lt;p>It turns out that &lt;a href="https://github.com/docker">Docker&lt;/a> provides &lt;a href="https://github.com/features/actions">GitHub actions&lt;/a> that make the process
of producing multi-architecture images quite simple.&lt;/p>
&lt;p>The code demonstrated in this post can be found in my &lt;a href="https://github.com/larsks/hello-flask">hello-flask&lt;/a>
GitHub repository.&lt;/p>
&lt;h2 id="configuring-secrets">Configuring secrets&lt;/h2>
&lt;p>There is some information we need to provide to our workflow that we don&amp;rsquo;t
want to hardcode into configuration files, both for reasons of security (we
don&amp;rsquo;t want to expose passwords in the repository) and convenience (we want
other people to be able to fork this repository and run the workflow
without needing to make any changes to the code).&lt;/p>
&lt;p>We can do this by configuring &amp;ldquo;secrets&amp;rdquo; in the repository on GitHub. You
can configure secrets by visiting the &amp;ldquo;Secrets&amp;rdquo; tab in your repository
settings (&lt;code>https://github.com/&amp;lt;USERNAME&amp;gt;/&amp;lt;REPOSITORY&amp;gt;/settings/secrets&lt;/code>),&lt;/p>
&lt;p>For this workflow, we&amp;rsquo;re going to need two secrets:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>DOCKER_USERNAME&lt;/code> &amp;ndash; this is our Docker Hub username; we&amp;rsquo;ll need this
both for authentication and to set the namespace for the images we&amp;rsquo;re
building.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>DOCKER_PASSWORD&lt;/code> &amp;ndash; this is our Docker Hub password, used for
authentication.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Within a workflow, we can refer to these secrets using syntax like &lt;code>${{ secrets.DOCKER_USERNAME }}&lt;/code> (you&amp;rsquo;ll see example of this later on).&lt;/p>
&lt;h2 id="creating-a-workflow">Creating a workflow&lt;/h2>
&lt;p>In the repository containing your &lt;code>Dockerfile&lt;/code>, create a
&lt;code>.github/workflows&lt;/code> directory. This is where we will place the files that
configure GitHub actions. In this directory, create a file called
&lt;code>build_images.yml&lt;/code> (the particular name isn&amp;rsquo;t important, but it&amp;rsquo;s nice to
make names descriptive).&lt;/p>
&lt;p>We&amp;rsquo;ll first give this workflow a name and configure it to run for pushes on
our &lt;code>master&lt;/code> branch by adding the following to our &lt;code>build_images.yml&lt;/code> file:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
name: &amp;#39;build images&amp;#39;
on:
push:
branches:
- master
&lt;/code>&lt;/pre>&lt;h2 id="setting-up-jobs">Setting up jobs&lt;/h2>
&lt;p>With that boilerplate out of the way, we can start configuring the jobs
that will comprise our workflow. Jobs are defined in the &lt;code>jobs&lt;/code> section of
the configuration file, which is a dictionary that maps job names to their
definition. A job can have multiple actions. For this example, we&amp;rsquo;re going
to set up a &lt;code>docker&lt;/code> job that will perform the following steps:&lt;/p>
&lt;ul>
&lt;li>check out the repository&lt;/li>
&lt;li>prepare some parameters&lt;/li>
&lt;li>set up qemu, which is used to provide emulated environments for
building on architecture other than the host arch&lt;/li>
&lt;li>configure the docker builders&lt;/li>
&lt;li>authenticate to docker hub&lt;/li>
&lt;li>build and push the images to docker hub&lt;/li>
&lt;/ul>
&lt;p>We start by providing a name for our job and configuring the machine on
which the jobs will run. In this example, we&amp;rsquo;re using &lt;code>ubuntu-latest&lt;/code>;
other options include some other Ubuntu variants, Windows, and MacOS (and
you are able to host your own custom builders, but that&amp;rsquo;s outside the scope
of this article).&lt;/p>
&lt;pre tabindex="0">&lt;code>jobs:
docker:
runs-on: ubuntu-latest
steps:
&lt;/code>&lt;/pre>&lt;h3 id="checking-out-the-repository">Checking out the repository&lt;/h3>
&lt;p>In our first step, we use the standard &lt;a href="https://github.com/actions/checkout">actions/checkout&lt;/a>
action to check out the repository:&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Checkout
uses: actions/checkout@v2
&lt;/code>&lt;/pre>&lt;h3 id="preparing-parameters">Preparing parameters&lt;/h3>
&lt;p>The next step is a simple shell script that sets some output parameters we
will be able to consume in subsequent steps. A script can set parameters by
generating output in the form:&lt;/p>
&lt;pre tabindex="0">&lt;code>::set-output name=&amp;lt;name&amp;gt;::&amp;lt;value&amp;gt;
&lt;/code>&lt;/pre>&lt;p>In other steps, we can refer to these parameters using the syntax
&lt;code>${{ steps.&amp;lt;step_name&amp;gt;.output.&amp;lt;name&amp;gt; }}&lt;/code> (e.g. &lt;code>${{ steps.prep.output.tags }}&lt;/code>).&lt;/p>
&lt;p>We&amp;rsquo;re going to use this step to set things like the image name (using our
&lt;code>DOCKER_USERNAME&lt;/code> secret to set the namespace), and to set up several tags
for the image:&lt;/p>
&lt;ul>
&lt;li>By default, we tag it &lt;code>latest&lt;/code>&lt;/li>
&lt;li>If we&amp;rsquo;re building from a git tag, use the tag name instead of &lt;code>latest&lt;/code>.
Note that here we&amp;rsquo;re assuming that git tags are of the form &lt;code>v1.0&lt;/code>, so we
strip off that initial &lt;code>v&lt;/code> to get a Docker tag that is just the version
number.&lt;/li>
&lt;li>We also tag the image with the short commit id&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> - name: Prepare
id: prep
run: |
DOCKER_IMAGE=${{ secrets.DOCKER_USERNAME }}/${GITHUB_REPOSITORY#*/}
VERSION=latest
SHORTREF=${GITHUB_SHA::8}
# If this is git tag, use the tag name as a docker tag
if [[ $GITHUB_REF == refs/tags/* ]]; then
VERSION=${GITHUB_REF#refs/tags/v}
fi
TAGS=&amp;#34;${DOCKER_IMAGE}:${VERSION},${DOCKER_IMAGE}:${SHORTREF}&amp;#34;
# If the VERSION looks like a version number, assume that
# this is the most recent version of the image and also
# tag it &amp;#39;latest&amp;#39;.
if [[ $VERSION =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]]; then
TAGS=&amp;#34;$TAGS,${DOCKER_IMAGE}:latest&amp;#34;
fi
# Set output parameters.
echo ::set-output name=tags::${TAGS}
echo ::set-output name=docker_image::${DOCKER_IMAGE}
&lt;/code>&lt;/pre>&lt;h3 id="set-up-qemu">Set up QEMU&lt;/h3>
&lt;p>The &lt;a href="https://github.com/docker/setup-qemu-action">docker/setup-qemu&lt;/a> action installs QEMU &lt;a href="https://wiki.debian.org/QemuUserEmulation">static binaries&lt;/a>, which
are used to run builders for architectures other than the host.&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Set up QEMU
uses: docker/setup-qemu-action@master
with:
platforms: all
&lt;/code>&lt;/pre>&lt;h3 id="set-up-docker-builders">Set up Docker builders&lt;/h3>
&lt;p>The &lt;a href="https://github.com/docker/setup-buildx-action">docker/setup-buildx&lt;/a> action configures &lt;a href="https://github.com/docker/buildx">buildx&lt;/a>, which is a Docker
CLI plugin that provides enhanced build capabilities. This is the
infrastructure that the following step will use for actually building
images.&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Set up Docker Buildx
id: buildx
uses: docker/setup-buildx-action@master
&lt;/code>&lt;/pre>&lt;h3 id="authenticate-to-docker-hub">Authenticate to Docker Hub&lt;/h3>
&lt;p>In order to push images to Docker Hub, we use the &lt;a href="https://github.com/docker/login-action">docker/login-action&lt;/a>
action to authenticate. This uses the &lt;code>DOCKER_USERNAME&lt;/code> and
&lt;code>DOCKER_PASSWORD&lt;/code> secrets we created earlier in order to establish
credentials for use in subsequent steps.&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Login to DockerHub
if: github.event_name != &amp;#39;pull_request&amp;#39;
uses: docker/login-action@v1
with:
username: ${{ secrets.DOCKER_USERNAME }}
password: ${{ secrets.DOCKER_PASSWORD }}
&lt;/code>&lt;/pre>&lt;h3 id="build-and-push-the-images">Build and push the images&lt;/h3>
&lt;p>This final step uses the [docker/build-push-action][] to build the images
and push them to Docker Hub using the tags we defined in the &lt;code>prep&lt;/code> step.
In this example, we&amp;rsquo;re building images for &lt;code>amd64&lt;/code>, &lt;code>arm64&lt;/code>, and &lt;code>ppc64le&lt;/code>
architectures.&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Build
uses: docker/build-push-action@v2
with:
builder: ${{ steps.buildx.outputs.name }}
context: .
file: ./Dockerfile
platforms: linux/amd64,linux/arm64,linux/ppc64le
push: true
tags: ${{ steps.prep.outputs.tags }}
&lt;/code>&lt;/pre>&lt;h2 id="the-complete-workflow">The complete workflow&lt;/h2>
&lt;p>When we put all of the above together, we get:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
name: &amp;#39;build images&amp;#39;
on:
push:
branches:
- master
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Checkout
uses: actions/checkout@v2
- name: Prepare
id: prep
run: |
DOCKER_IMAGE=${{ secrets.DOCKER_USERNAME }}/${GITHUB_REPOSITORY#*/}
VERSION=latest
SHORTREF=${GITHUB_SHA::8}
# If this is git tag, use the tag name as a docker tag
if [[ $GITHUB_REF == refs/tags/* ]]; then
VERSION=${GITHUB_REF#refs/tags/v}
fi
TAGS=&amp;#34;${DOCKER_IMAGE}:${VERSION},${DOCKER_IMAGE}:${SHORTREF}&amp;#34;
# If the VERSION looks like a version number, assume that
# this is the most recent version of the image and also
# tag it &amp;#39;latest&amp;#39;.
if [[ $VERSION =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]]; then
TAGS=&amp;#34;$TAGS,${DOCKER_IMAGE}:latest&amp;#34;
fi
# Set output parameters.
echo ::set-output name=tags::${TAGS}
echo ::set-output name=docker_image::${DOCKER_IMAGE}
- name: Set up QEMU
uses: docker/setup-qemu-action@master
with:
platforms: all
- name: Set up Docker Buildx
id: buildx
uses: docker/setup-buildx-action@master
- name: Login to DockerHub
if: github.event_name != &amp;#39;pull_request&amp;#39;
uses: docker/login-action@v1
with:
username: ${{ secrets.DOCKER_USERNAME }}
password: ${{ secrets.DOCKER_PASSWORD }}
- name: Build
uses: docker/build-push-action@v2
with:
builder: ${{ steps.buildx.outputs.name }}
context: .
file: ./Dockerfile
platforms: linux/amd64,linux/arm64,linux/ppc64le
push: true
tags: ${{ steps.prep.outputs.tags }}
&lt;/code>&lt;/pre>&lt;p>You can grab the &lt;a href="https://github.com/larsks/hello-flask">hello-flask&lt;/a> repository and try this out yourself.
You&amp;rsquo;ll need to set up the secrets described earlier in this article, but
then for each commit to the &lt;code>master&lt;/code> branch you will end up a new image,
tagged both as &lt;code>latest&lt;/code> and with the short git commit id.&lt;/p>
&lt;h2 id="the-results">The results&lt;/h2>
&lt;p>We can use the &lt;code>docker manifest inspect&lt;/code> command to inspect the output of
the build step. In the output below, you can see the images build for our
three target architectures:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ docker manifest inspect !$
docker manifest inspect larsks/hello-flask
{
&amp;#34;schemaVersion&amp;#34;: 2,
&amp;#34;mediaType&amp;#34;: &amp;#34;application/vnd.docker.distribution.manifest.list.v2+json&amp;#34;,
&amp;#34;manifests&amp;#34;: [
{
&amp;#34;mediaType&amp;#34;: &amp;#34;application/vnd.docker.distribution.manifest.v2+json&amp;#34;,
&amp;#34;size&amp;#34;: 3261,
&amp;#34;digest&amp;#34;: &amp;#34;sha256:c6bab778a9fd0dc7bf167a5a49281bcd5ebc5e762ceeb06791aff8f0fbd15325&amp;#34;,
&amp;#34;platform&amp;#34;: {
&amp;#34;architecture&amp;#34;: &amp;#34;amd64&amp;#34;,
&amp;#34;os&amp;#34;: &amp;#34;linux&amp;#34;
}
},
{
&amp;#34;mediaType&amp;#34;: &amp;#34;application/vnd.docker.distribution.manifest.v2+json&amp;#34;,
&amp;#34;size&amp;#34;: 3261,
&amp;#34;digest&amp;#34;: &amp;#34;sha256:3c02f36562fcf8718a369a78054750382aba5706e1e9164b76bdc214591024c4&amp;#34;,
&amp;#34;platform&amp;#34;: {
&amp;#34;architecture&amp;#34;: &amp;#34;arm64&amp;#34;,
&amp;#34;os&amp;#34;: &amp;#34;linux&amp;#34;
}
},
{
&amp;#34;mediaType&amp;#34;: &amp;#34;application/vnd.docker.distribution.manifest.v2+json&amp;#34;,
&amp;#34;size&amp;#34;: 3262,
&amp;#34;digest&amp;#34;: &amp;#34;sha256:192fc9acd658edd6b7f2726f921cba2582fb1101d929800dff7fb53de951dd76&amp;#34;,
&amp;#34;platform&amp;#34;: {
&amp;#34;architecture&amp;#34;: &amp;#34;ppc64le&amp;#34;,
&amp;#34;os&amp;#34;: &amp;#34;linux&amp;#34;
}
}
]
}
&lt;/code>&lt;/pre>&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>This process assumes, of course, that your base image of choice is available for your selected architectures. &lt;a href="https://docs.docker.com/docker-for-mac/multi-arch/">According to Docker&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Most of the official images on Docker Hub provide a variety of architectures.
For example, the busybox image supports amd64, arm32v5, arm32v6, arm32v7,
arm64v8, i386, ppc64le, and s390x.&lt;/p>
&lt;/blockquote>
&lt;p>So if you are starting from one of the official images, you&amp;rsquo;ll probably be in good shape. On the other hand, if you&amp;rsquo;re attempting to use a community image as a starting point, you might find that it&amp;rsquo;s only available for a single architecture.&lt;/p></content></item><item><title>Running Keystone with Docker Compose</title><link>https://blog.oddbit.com/post/2019-06-07-running-keystone-with-docker-c/</link><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-06-07-running-keystone-with-docker-c/</guid><description>In this article, we will look at what is necessary to run OpenStack&amp;rsquo;s Keystone service (and the requisite database server) in containers using Docker Compose.
Running MariaDB The standard mariadb docker image can be configured via a number of environment variables. It also benefits from persistent volume storage, since in most situations you don&amp;rsquo;t want to lose your data when you remove a container. A simple docker command line for starting MariaDB might look something like:</description><content>&lt;p>In this article, we will look at what is necessary to run OpenStack&amp;rsquo;s &lt;a href="https://docs.openstack.org/keystone/latest/">Keystone&lt;/a> service (and the requisite database server) in containers using &lt;a href="https://docs.docker.com/compose/">Docker Compose&lt;/a>.&lt;/p>
&lt;h2 id="running-mariadb">Running MariaDB&lt;/h2>
&lt;p>The standard &lt;a href="https://hub.docker.com/_/mariadb/">mariadb docker image&lt;/a> can be configured via a number of environment variables. It also benefits from persistent volume storage, since in most situations you don&amp;rsquo;t want to lose your data when you remove a container. A simple &lt;code>docker&lt;/code> command line for starting MariaDB might look something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run \
-v mariadb_data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=secret.password
mariadb
&lt;/code>&lt;/pre>&lt;p>The above assumes that we have previously created a Docker volume named &lt;code>mariadb_data&lt;/code> by running:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker volume create mariadb_data
&lt;/code>&lt;/pre>&lt;p>An equivalent &lt;code>docker-compose.yml&lt;/code> would look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb
environment:
MYSQL_ROOT_PASSWORD: secret.password
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;p>Now, rather than typing a long &lt;code>docker run&lt;/code> command line (and possibly forgetting something), you can simply run:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker-compose up
&lt;/code>&lt;/pre>&lt;h3 id="pre-creating-a-database">Pre-creating a database&lt;/h3>
&lt;p>For the purposes of setting up Keystone in Docker, we will need to make a few changes. In particular, we will need to have the &lt;code>mariadb&lt;/code> container create the &lt;code>keystone&lt;/code> database (and user) for us, and as a matter of best practice we will want to specify an explicit tag for the &lt;code>mariadb&lt;/code> image rather than relying on the default &lt;code>latest&lt;/code>.&lt;/p>
&lt;p>We can have the &lt;code>mariadb&lt;/code> image create a database for us at startup by setting the &lt;code>MYSQL_DATABASE&lt;/code>, &lt;code>MYSQL_USER&lt;/code>, and &lt;code>MYSQL_PASSWORD&lt;/code> environment variables:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb:10.4.5-bionic
environment:
MYSQL_ROOT_PASSWORD: secret.password
MYSQL_USER: keystone
MYSQL_PASSWORD: another.password
MYSQL_DATABASE: keystone
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;p>When the &lt;code>database&lt;/code> service starts up, it will create a &lt;code>keystone&lt;/code> database accessible by the &lt;code>keystone&lt;/code> user.&lt;/p>
&lt;h3 id="parameterize-all-the-things">Parameterize all the things&lt;/h3>
&lt;p>The above example is pretty much what we want, but there is one problem: we&amp;rsquo;ve hardcoded our passwords (and database name) into the &lt;code>docker-compose.yml&lt;/code> file, which makes it hard to share: it would be unsuitable for hosting on a public git repository, because anybody who wanted to use it would need to modify the file first, which would make it difficult to contribute changes or bring in new changes from the upstream repository. We can solve that problem by using environment variables in our &lt;code>docker-compose.yml&lt;/code>. Much like the shell, &lt;code>docker-compose&lt;/code> will replace an expression of the form &lt;code>${MY_VARIABLE}&lt;/code> with the value of the &lt;code>MY_VARIABLE&lt;/code> environment variable. It is possible to provide a fallback value in the event that an environment variable is undefined by writing &lt;code>${MY_VARIABLE:-some_default_value}&lt;/code>.&lt;/p>
&lt;p>You have a couple options for providing values for this variables. You can of course simply set them in the environment, either like this:&lt;/p>
&lt;pre>&lt;code>export MY_VARIABLE=some_value
&lt;/code>&lt;/pre>
&lt;p>Or as part of the &lt;code>docker-compose&lt;/code> command line, like this:&lt;/p>
&lt;pre>&lt;code>MY_VARIABLE=some_value docker-compose up
&lt;/code>&lt;/pre>
&lt;p>Alternatively, you can also set them in a &lt;code>.env&lt;/code> file in the same directory as your &lt;code>docker-compose.yml&lt;/code> file; &lt;code>docker-compose&lt;/code> reads this file automatically when it runs. A &lt;code>.env&lt;/code> file looks like this:&lt;/p>
&lt;pre>&lt;code>MY_VARIABLE=some_value
&lt;/code>&lt;/pre>
&lt;p>With the above in mind, we can restructure our example &lt;code>docker-compose.yml&lt;/code> so that it looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb:${MARIADB_IMAGE_TAG:-10.4.5-bionic}
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
MYSQL_USER: ${KEYSTONE_DB_USER:-keystone}
MYSQL_PASSWORD: ${KEYSTONE_DB_PASSWORD}
MYSQL_DATABASE: ${KEYSTONE_DB_NAME:-keystone}
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;h2 id="running-keystone">Running Keystone&lt;/h2>
&lt;h3 id="selecting-a-base-image">Selecting a base image&lt;/h3>
&lt;p>While there is an official MariaDB image available in Docker Hub, there is no such thing as an official Keystone image. A search for &lt;code>keystone&lt;/code> yields over 300 results. I have elected to use the Keystone image produced as part of the &lt;a href="https://wiki.openstack.org/wiki/TripleO">TripleO&lt;/a> project, &lt;a href="https://hub.docker.com/r/tripleomaster/centos-binary-keystone">tripleo-master/centos-binary-keystone&lt;/a>. The &lt;code>current-rdo&lt;/code> tag follows the head of the Keystone repository, and the images are produced automatically as part of the CI process. Unlike the MariaDB image, which is designed to pretty much be &amp;ldquo;plug and play&amp;rdquo;, the Keystone image is going to require some configuration before it provides us with a useful service.&lt;/p>
&lt;p>Using the &lt;code>centos-binary-keystone&lt;/code> image, there are two required configuration tasks we will have to complete when starting the container:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>We will need to inject an appropriate configuration file to run Keystone as a &lt;a href="https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface">WSGI&lt;/a> binary under Apache &lt;a href="http://httpd.apache.org/">httpd&lt;/a>. This is certainly not the only way to run Keystone, but the &lt;code>centos-binary-keystone&lt;/code> image has both &lt;code>httpd&lt;/code> and &lt;code>mod_wsgi&lt;/code> installed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We will need to inject a minimal configuration for Keystone (for example, we will need to provide Keystone with connection information for the database instance).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="keystone-wsgi-configuration">Keystone WSGI configuration&lt;/h3>
&lt;p>We need to configure Keystone as a WSGI service running on port 5000. We will do this with the following configuration file:&lt;/p>
&lt;pre tabindex="0">&lt;code>Listen 5000
ErrorLog &amp;#34;/dev/stderr&amp;#34;
CustomLog &amp;#34;/dev/stderr&amp;#34; combined
&amp;lt;VirtualHost *:5000&amp;gt;
ServerName keystone
ServerSignature Off
DocumentRoot &amp;#34;/var/www/cgi-bin/keystone&amp;#34;
&amp;lt;Directory &amp;#34;/var/www/cgi-bin/keystone&amp;#34;&amp;gt;
Options Indexes FollowSymLinks MultiViews
AllowOverride None
Require all granted
&amp;lt;/Directory&amp;gt;
WSGIApplicationGroup %{GLOBAL}
WSGIDaemonProcess keystone_main display-name=keystone-main \
processes=12 threads=1 user=keystone group=keystone
WSGIProcessGroup keystone_main
WSGIScriptAlias / &amp;#34;/var/www/cgi-bin/keystone/main&amp;#34;
WSGIPassAuthorization On
&amp;lt;/VirtualHost&amp;gt;
&lt;/code>&lt;/pre>&lt;p>The easiest way to inject this custom configuration is to bake it into a custom image. Using the &lt;code>tripleomaster/centos-binary-keystone&lt;/code> base image identified earlier, we can start with a custom &lt;code>Dockerfile&lt;/code> that looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>ARG KEYSTONE_IMAGE_TAG=current-tripleo
FROM tripleomaster/centos-binary-keystone:${KEYSTONE_IMAGE_TAG}
COPY keystone-wsgi-main.conf /etc/httpd/conf.d/keystone-wsgi-main.conf
&lt;/code>&lt;/pre>&lt;p>The &lt;code>ARG&lt;/code> directive permits us to select an image tag via a build argument (but defaults to &lt;code>current-tripleo&lt;/code>).&lt;/p>
&lt;p>We can ask &lt;code>docker-compose&lt;/code> to build our custom image for us when we run &lt;code>docker-compose up&lt;/code>. Instead of specifying an &lt;code>image&lt;/code> as we did with the MariaDB container, we use the &lt;code>build&lt;/code> directive:&lt;/p>
&lt;pre tabindex="0">&lt;code>[...]
keystone:
build:
context: .
args:
KEYSTONE_IMAGE_TAG: current-tripleo
[...]
&lt;/code>&lt;/pre>&lt;p>This tells &lt;code>docker-compose&lt;/code> to use the &lt;code>Dockerfile&lt;/code> in the current directory (and to set the &lt;code>KEYSTONE_IMAGE_TAG&lt;/code> build argument to &lt;code>current-tripleo&lt;/code>). Note that &lt;code>docker-compose&lt;/code> will only build this image for us by default if it doesn&amp;rsquo;t already exist; we can ask &lt;code>docker-compose&lt;/code> to build it explicitly by running &lt;code>docker-compose build&lt;/code>, or by providing the &lt;code>--build&lt;/code> option to &lt;code>docker-compose up&lt;/code>.&lt;/p>
&lt;h3 id="configuring-at-build-time-vs-run-time">Configuring at build time vs run time&lt;/h3>
&lt;p>In the previous section, we used a &lt;code>Dockerfile&lt;/code> to build on a selected base image by adding custom content. Other sorts of configuration must happen when the container starts up (for example, we probably want to be able to set passwords at runtime). One way of solving this problem is to embed some scripts into our custom image and then run them when the container starts in order to perform any necessary initialization.&lt;/p>
&lt;p>I have placed some custom scripts and templates into the &lt;code>runtime&lt;/code> directory and arranged to copy that directory into the custom image like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>ARG KEYSTONE_IMAGE_TAG=current-tripleo
FROM tripleomaster/centos-binary-keystone:${KEYSTONE_IMAGE_TAG}
COPY keystone-wsgi-main.conf /etc/httpd/conf.d/keystone-wsgi-main.conf
COPY runtime /runtime
CMD [&amp;#34;/bin/sh&amp;#34;, &amp;#34;/runtime/startup.sh&amp;#34;]
&lt;/code>&lt;/pre>&lt;p>The &lt;code>runtime&lt;/code> directory contains the following files:&lt;/p>
&lt;ul>
&lt;li>&lt;code>runtime/dtu.py&lt;/code> &amp;ndash; a short Python script for generating files from templates.&lt;/li>
&lt;li>&lt;code>runtime/startup.sh&lt;/code> &amp;ndash; a shell script that performs all the necessary initialization tasks before starting Keystone&lt;/li>
&lt;li>&lt;code>runtime/keystone.j2.conf&lt;/code> &amp;ndash; template for the Keystone configuration file&lt;/li>
&lt;li>&lt;code>runtime/clouds.j2.yaml&lt;/code> &amp;ndash; template for a &lt;code>clouds.yaml&lt;/code> for use by the &lt;code>openshift&lt;/code> command line client.&lt;/li>
&lt;/ul>
&lt;h3 id="starting-up">Starting up&lt;/h3>
&lt;p>The &lt;code>startup.sh&lt;/code> script performs the following actions:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Generates &lt;code>/etc/keystone/keystone.conf&lt;/code> from &lt;code>/runtime/keystone.j2.conf&lt;/code>.&lt;/p>
&lt;p>The file &lt;code>/runtime/keystone.j2.conf&lt;/code> is a minimal Keystone configuration template. It ensures that Keystone logs to &lt;code>stderr&lt;/code> (by setting &lt;code>log_file&lt;/code> to an empty value) and configures the database connection using values from the environment.&lt;/p>
&lt;pre tabindex="0">&lt;code>[DEFAULT]
debug = {{ environ.KEYSTONE_DEBUG|default(&amp;#39;false&amp;#39;) }}
log_file =
[database]
{% set keystone_db_user = environ.KEYSTONE_DB_USER|default(&amp;#39;keystone&amp;#39;) %}
{% set keystone_db_host = environ.KEYSTONE_DB_HOST|default(&amp;#39;localhost&amp;#39;) %}
{% set keystone_db_port = environ.KEYSTONE_DB_PORT|default(&amp;#39;3306&amp;#39;) %}
{% set keystone_db_name = environ.KEYSTONE_DB_NAME|default(&amp;#39;keystone&amp;#39;) %}
{% set keystone_db_pass = environ.KEYSTONE_DB_PASSWORD|default(&amp;#39;insert-password-here&amp;#39;) %}
connection = mysql+pymysql://{{ keystone_db_user }}:{{ keystone_db_pass }}@{{ keystone_db_host }}:{{ keystone_db_port }}/{{ keystone_db_name }}
[token]
provider = fernet
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Generates &lt;code>/root/clouds.yaml&lt;/code> from &lt;code>/runtime/clouds.j2.yaml&lt;/code>.&lt;/p>
&lt;p>The &lt;code>clouds.yaml&lt;/code> file can be used with to provide authentication information to the &lt;code>openshift&lt;/code> command line client (and other applications that use the OpenStack Python SDK). We&amp;rsquo;ll see an example of this further on in this article.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Initializes Keystone&amp;rsquo;s fernet token mechanism by running &lt;code>keystone-manage fernet_setup&lt;/code>.&lt;/p>
&lt;p>Keystone supports various token generation mechanisms. Fernet tokens provide some advantages over the older UUID token mechanism. From the &lt;a href="fernet-faq">FAQ&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Even though fernet tokens operate very similarly to UUID tokens, they do not require persistence or leverage the configured token persistence driver in any way. The keystone token database no longer suffers bloat as a side effect of authentication. Pruning expired tokens from the token database is no longer required when using fernet tokens. Because fernet tokens do not require persistence, they do not have to be replicated. As long as each keystone node shares the same key repository, fernet tokens can be created and validated instantly across nodes.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>Initializes the Keystone database schema by running &lt;code>keystone-manage db_sync&lt;/code>.&lt;/p>
&lt;p>The &lt;code>db_sync&lt;/code> command creates the database tables that Keystone requires to operate.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Creates the Keystone &lt;code>admin&lt;/code> user and initial service catalog entries by running &lt;code>keystone-manage bootstrap&lt;/code>&lt;/p>
&lt;p>Before we can authenticate to Keystone, there needs to exist a user with administrative privileges (so that we can create other users, projects, and so forth).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Starts &lt;code>httpd&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="avoiding-race-conditions">Avoiding race conditions&lt;/h3>
&lt;p>When we run &lt;code>docker-compose up&lt;/code>, it will bring up both the &lt;code>keystone&lt;/code> container and the &lt;code>database&lt;/code> container in parallel. This is going to cause problems if we try to initialize the Keystone database schema before the database server is actually up and running. There is a &lt;code>depends_on&lt;/code> keyword that can be used to order the startup of containers in your &lt;code>docker-compose.yml&lt;/code> file, but this isn&amp;rsquo;t useful to us: this only delays the startup of the dependent container until the indicated container is &lt;em>running&lt;/em>. It doesn&amp;rsquo;t know anything about application startup, and so it would not wait for the database to be ready.&lt;/p>
&lt;p>We need to explicitly wait until we can successfully connect to the database before we can complete initializing the Keystone service. It turns out the easiest solution to this problem is to imply run the database schema initialization in a loop until it is successful, like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>echo &amp;#34;* initializing database schema&amp;#34;
while ! keystone-manage db_sync; do
echo &amp;#34;! database schema initialization failed; retrying in 5 seconds...&amp;#34;
sleep 5
done
&lt;/code>&lt;/pre>&lt;p>This will attempt the &lt;code>db_sync&lt;/code> command every five seconds until it is sucessful.&lt;/p>
&lt;h2 id="the-final-docker-compose-file">The final docker-compose file&lt;/h2>
&lt;p>Taking all of the above into account, this is what the final &lt;code>docker-compose.yml&lt;/code> file looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
version: &amp;#34;3&amp;#34;
services:
database:
image: mariadb:10.4.5-bionic
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
MYSQL_USER: ${KEYSTONE_DB_USER:-keystone}
MYSQL_PASSWORD: ${KEYSTONE_DB_PASSWORD}
MYSQL_DATABASE: ${KEYSTONE_DB_NAME:-keystone}
volumes:
- mysql:/var/lib/mysql
keystone:
build:
context: .
args:
KEYSTONE_IMAGE_TAG: current-tripleo
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
KEYSTONE_ADMIN_PASSWORD: ${KEYSTONE_ADMIN_PASSWORD}
KEYSTONE_DB_PASSWORD: ${KEYSTONE_DB_PASSWORD}
KEYSTONE_DB_USER: ${KEYSTONE_DB_USER:-keystone}
KEYSTONE_DB_NAME: ${KEYSTONE_DB_NAME:-keystone}
KEYSTONE_DEBUG: ${KEYSTONE_DEBUG:-&amp;#34;false&amp;#34;}
ports:
- &amp;#34;127.0.0.1:5000:5000&amp;#34;
volumes:
mysql:
&lt;/code>&lt;/pre>&lt;h2 id="interacting-with-keystone">Interacting with Keystone&lt;/h2>
&lt;p>Once Keystone is up and running, we can grab the generated &lt;code>clouds.yaml&lt;/code> file like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker-compose exec keystone cat /root/clouds.yaml &amp;gt; clouds.yaml
&lt;/code>&lt;/pre>&lt;p>Now we can run the &lt;code>openstack&lt;/code> command line client:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ export OS_CLOUD=openstack-public
$ openstack catalog list
+----------+----------+-----------------------------------+
| Name | Type | Endpoints |
+----------+----------+-----------------------------------+
| keystone | identity | RegionOne |
| | | internal: http://localhost:5000 |
| | | RegionOne |
| | | public: http://localhost:5000 |
| | | |
+----------+----------+-----------------------------------+
$ openstack user list
+----------------------------------+-------+
| ID | Name |
+----------------------------------+-------+
| e8f460619a854c849feaf278b8d68e2c | admin |
+----------------------------------+-------+
&lt;/code>&lt;/pre>&lt;h2 id="project-sources">Project sources&lt;/h2>
&lt;p>You can find everything reference in this article in the &lt;a href="https://github.com/cci-moc/flocx-keystone-dev">flocx-keystone-dev&lt;/a> repository.&lt;/p></content></item><item><title>Docker build learns about secrets and ssh agent forwarding</title><link>https://blog.oddbit.com/post/2019-02-24-docker-build-learns-about-secr/</link><pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-02-24-docker-build-learns-about-secr/</guid><description>A common problem for folks working with Docker is accessing resources which require authentication during the image build step. A particularly common use case is getting access to private git repositories using ssh key-based authentication. Until recently there hasn&amp;rsquo;t been a great solution:
you can embed secrets in your image, but now you can&amp;rsquo;t share the image with anybody. you can use build arguments, but this requires passing in an unenecrypted private key on the docker build command line, which is suboptimal for a number of reasons you can perform all the steps requiring authentication at runtime, but this can needlessly complicate your container startup process.</description><content>&lt;p>A common problem for folks working with Docker is accessing resources which require authentication during the image build step. A particularly common use case is getting access to private git repositories using ssh key-based authentication. Until recently there hasn&amp;rsquo;t been a great solution:&lt;/p>
&lt;ul>
&lt;li>you can embed secrets in your image, but now you can&amp;rsquo;t share the image with anybody.&lt;/li>
&lt;li>you can use build arguments, but this requires passing in an unenecrypted private key on the &lt;code>docker build&lt;/code> command line, which is suboptimal for a number of reasons&lt;/li>
&lt;li>you can perform all the steps requiring authentication at runtime, but this can needlessly complicate your container startup process.&lt;/li>
&lt;/ul>
&lt;p>With Docker 18.09, there are some experimental features available that makes this much easier. You can read the official announcement &lt;a href="https://docs.docker.com/develop/develop-images/build_enhancements/">here&lt;/a>, but I wanted to highlight the support for ssh agent forwarding and private keys.&lt;/p>
&lt;h1 id="prerequisites">Prerequisites&lt;/h1>
&lt;p>In order to use the new features, you first need to explicitly enable BuildKit support by setting &lt;code>DOCKER_BUILDKIT=1&lt;/code> in your environment:&lt;/p>
&lt;pre>&lt;code>export DOCKER_BUILDKIT=1
&lt;/code>&lt;/pre>
&lt;p>And to utilize the new &lt;code>Dockerfile&lt;/code> syntax, you need to start your &lt;code>Dockerfile&lt;/code> with this directive:&lt;/p>
&lt;pre>&lt;code># syntax=docker/dockerfile:1.0.0-experimental
&lt;/code>&lt;/pre>
&lt;p>That instructs Docker to use the named image (&lt;code>docker/dockerfile:1.0.0-experimental&lt;/code>) to handle the image build process.&lt;/p>
&lt;h2 id="a-simple-example">A simple example&lt;/h2>
&lt;p>The most common use case will probably be forwarding access to your local ssh agent. In order for the build process to get access to your agent, two things must happen:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The &lt;code>RUN&lt;/code> command that requires credentials must specify &lt;code>--mount=type=ssh&lt;/code> in order to have access to the forwarded agent connection, and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You must pass an appropriate &lt;code>--ssh&lt;/code> option on the &lt;code>docker build&lt;/code> command line. This is to prevent a Dockerfile from unexpectedly gaining access to your ssh credentials.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>We can see this in action if we start with the following &lt;code>Dockerfile&lt;/code>:&lt;/p>
&lt;pre>&lt;code># syntax=docker/dockerfile:1.0.0-experimental
FROM alpine
RUN apk add --update git openssh
# This is necessary to prevent the &amp;quot;git clone&amp;quot; operation from failing
# with an &amp;quot;unknown host key&amp;quot; error.
RUN mkdir -m 700 /root/.ssh; \
touch -m 600 /root/.ssh/known_hosts; \
ssh-keyscan github.com &amp;gt; /root/.ssh/known_hosts
# This command will have access to the forwarded agent (if one is
# available)
RUN --mount=type=ssh git clone git@github.com:moby/buildkit
&lt;/code>&lt;/pre>
&lt;p>If we run build the image like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>export DOCKER_BUILDKIT=1
docker build --ssh default -t buildtest .
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;then our &lt;code>git clone&lt;/code> operation will successfully authenticate with github using our ssh private key, assuming that we had one loaded into our local ssh agent.&lt;/p>
&lt;h2 id="but-wait-theres-more">But wait, there&amp;rsquo;s more&lt;/h2>
&lt;p>In the previous example line, the &lt;code>--ssh default&lt;/code> option requests &lt;code>docker build&lt;/code> to forward your default ssh agent. There may be situations in which this isn&amp;rsquo;t appropriate (for example, maybe you need to use a key that isn&amp;rsquo;t loaded into your default agent). You can provide the &lt;code>--ssh&lt;/code> option with one or more paths to ssh agent sockets or (unencrypted) private key files. Let&amp;rsquo;s say you have two service-specific private keys:&lt;/p>
&lt;ul>
&lt;li>For GitHub, you need to use &lt;code>$HOME/.ssh/github_rsa&lt;/code>&lt;/li>
&lt;li>For BitBucket, you need to use &lt;code>$HOME/.ssh/bitbucket_rsa&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>You can provide the keys on the &lt;code>docker build&lt;/code> command line like this:&lt;/p>
&lt;pre>&lt;code>docker build --ssh github=$HOME/.ssh/github_rsa,bitbucket=$HOME/.ssh/bitbucket_rsa -t buildtest .
&lt;/code>&lt;/pre>
&lt;p>Then inside your &lt;code>Dockerfile&lt;/code>, you can use the &lt;code>id=&amp;lt;name&amp;gt;&lt;/code> parameter to the &lt;code>--mount&lt;/code> option to specify which key should be available to the &lt;code>RUN&lt;/code> command:&lt;/p>
&lt;pre>&lt;code># syntax=docker/dockerfile:1.0.0-experimental
FROM alpine
RUN apk add --update git openssh
# This is necessary to prevent the &amp;quot;git clone&amp;quot; operation from failing
# with an &amp;quot;unknown host key&amp;quot; error.
RUN mkdir -m 700 /root/.ssh; \
touch -m 600 /root/.ssh/known_hosts; \
ssh-keyscan github.com bitbucket.com &amp;gt; /root/.ssh/known_hosts
# This command has access to the &amp;quot;github&amp;quot; key
RUN --mount=type=ssh,id=github git clone git@github.com:some/project
# This command has access to the &amp;quot;bitbucket&amp;quot; key
RUN --mount=type=ssh,id=bitbucket git clone git@bitbucket.com:other/project
&lt;/code>&lt;/pre>
&lt;h2 id="other-secrets">Other secrets&lt;/h2>
&lt;p>In this post I&amp;rsquo;ve looked specfically at providing &lt;code>docker build&lt;/code> with access to your ssh keys. Docker 18.09 also introduces support for exposing other secrets to the build process; see the official announcement (linked above) for details.&lt;/p></content></item><item><title>Using Docker macvlan networks</title><link>https://blog.oddbit.com/post/2018-03-12-using-docker-macvlan-networks/</link><pubDate>Mon, 12 Mar 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-03-12-using-docker-macvlan-networks/</guid><description>A question that crops up regularly on #docker is &amp;ldquo;How do I attach a container directly to my local network?&amp;rdquo; One possible answer to that question is the macvlan network type, which lets you create &amp;ldquo;clones&amp;rdquo; of a physical interface on your host and use that to attach containers directly to your local network. For the most part it works great, but it does come with some minor caveats and limitations.</description><content>&lt;p>A question that crops up regularly on &lt;a href="https://docs.docker.com/opensource/ways/#docker-users">#docker&lt;/a> is &amp;ldquo;How do I attach
a container directly to my local network?&amp;rdquo; One possible answer to that
question is the &lt;a href="https://docs.docker.com/network/macvlan/">macvlan&lt;/a> network type, which lets you create
&amp;ldquo;clones&amp;rdquo; of a physical interface on your host and use that to attach
containers directly to your local network. For the most part it works
great, but it does come with some minor caveats and limitations. I
would like to explore those here.&lt;/p>
&lt;p>For the purpose of this example, let&amp;rsquo;s say we have a host interface
&lt;code>eno1&lt;/code> that looks like this:&lt;/p>
&lt;pre>&lt;code>2: eno1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
link/ether 64:00:6a:7d:06:1a brd ff:ff:ff:ff:ff:ff
inet 192.168.1.24/24 brd 192.168.1.255 scope global dynamic eno1
valid_lft 73303sec preferred_lft 73303sec
inet6 fe80::b2c9:3793:303:2a55/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>To create a macvlan network named &lt;code>mynet&lt;/code> attached to that interface,
you might run something like this:&lt;/p>
&lt;pre>&lt;code>docker network create -d macvlan -o parent=eno1 \
--subnet 192.168.1.0/24 \
--gateway 192.168.1.1 \
mynet
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;but don&amp;rsquo;t do that.&lt;/p>
&lt;h2 id="address-assignment">Address assignment&lt;/h2>
&lt;p>When you create a container attached to your macvlan network, Docker
will select an address from the subnet range and assign it to your
container. This leads to the potential for conflicts: if Docker picks
an address that has already been assigned to another host on your
network, you have a problem!&lt;/p>
&lt;p>You can avoid this by reserving a portion of the subnet range for use
by Docker. There are two parts to this solution:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You must configure any DHCP service on your network such that it
will not assign addresses in a given range.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You must tell Docker about that reserved range of addresses.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>How you accomplish the former depends entirely on your local network
infrastructure and is beyond the scope of this document. The latter
task is accomplished with the &lt;code>--ip-range&lt;/code> option to &lt;code>docker network create&lt;/code>.&lt;/p>
&lt;p>On my local network, my DHCP server will not assign any addresses
above &lt;code>192.168.1.190&lt;/code>. I have decided to assign to Docker the subset
&lt;code>192.168.1.192/27&lt;/code>, which is a range of 32 address starting at
192.168.1.192 and ending at 192.168.1.223. The corresponding &lt;code>docker network create&lt;/code> command would be:&lt;/p>
&lt;pre>&lt;code>docker network create -d macvlan -o parent=eno1 \
--subnet 192.168.1.0/24 \
--gateway 192.168.1.1 \
--ip-range 192.168.1.192/27 \
mynet
&lt;/code>&lt;/pre>
&lt;p>Now it is possible to create containers attached to my local network
without worrying about the possibility of ip address conflicts.&lt;/p>
&lt;h2 id="host-access">Host access&lt;/h2>
&lt;p>With a container attached to a macvlan network, you will find that
while it can contact other systems on your local network without a
problem, the container will not be able to connect to your host (and
your host will not be able to connect to your container). This is a
limitation of macvlan interfaces: without special support from a
network switch, your host is unable to send packets to its own macvlan
interfaces.&lt;/p>
&lt;p>Fortunately, there is a workaround for this problem: you can create
another macvlan interface on your host, and use that to communicate
with containers on the macvlan network.&lt;/p>
&lt;p>First, I&amp;rsquo;m going to reserve an address from our network range for use
by the host interface by using the &lt;code>--aux-address&lt;/code> option to &lt;code>docker network create&lt;/code>. That makes our final command line look like:&lt;/p>
&lt;pre>&lt;code>docker network create -d macvlan -o parent=eno1 \
--subnet 192.168.1.0/24 \
--gateway 192.168.1.1 \
--ip-range 192.168.1.192/27 \
--aux-address 'host=192.168.1.223' \
mynet
&lt;/code>&lt;/pre>
&lt;p>This will prevent Docker from assigning that address to a container.&lt;/p>
&lt;p>Next, we create a new macvlan interface on the host. You can call it
whatever you want, but I&amp;rsquo;m calling this one &lt;code>mynet-shim&lt;/code>:&lt;/p>
&lt;pre>&lt;code>ip link add mynet-shim link eno1 type macvlan mode bridge
&lt;/code>&lt;/pre>
&lt;p>Now we need to configure the interface with the address we reserved
and bring it up:&lt;/p>
&lt;pre>&lt;code>ip addr add 192.168.1.223/32 dev mynet-shim
ip link set mynet-shim up
&lt;/code>&lt;/pre>
&lt;p>The last thing we need to do is to tell our host to use that interface
when communicating with the containers. This is relatively easy
because we have restricted our containers to a particular CIDR subset
of the local network; we just add a route to that range like this:&lt;/p>
&lt;pre>&lt;code>ip route add 192.168.1.192/27 dev mynet-shim
&lt;/code>&lt;/pre>
&lt;p>With that route in place, your host will automatically use ths
&lt;code>mynet-shim&lt;/code> interface when communicating with containers on the
&lt;code>mynet&lt;/code> network.&lt;/p>
&lt;p>Note that the interface and routing configuration presented here is
not persistent &amp;ndash; you will lose if if you were to reboot your host.
How to make it persistent is distribution dependent.&lt;/p></content></item><item><title>Ansible 2.0: The Docker connection driver</title><link>https://blog.oddbit.com/post/2015-10-13-ansible-20-the-docker-connecti/</link><pubDate>Tue, 13 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-13-ansible-20-the-docker-connecti/</guid><description>As the release of Ansible 2.0 draws closer, I&amp;rsquo;d like to take a look at some of the new features that are coming down the pipe. In this post, we&amp;rsquo;ll look at the docker connection driver.
A &amp;ldquo;connection driver&amp;rdquo; is the mechanism by which Ansible connects to your target hosts. These days it uses ssh by default (which relies on the OpenSSH command line client for connectivity), and it also offers the Paramiko library as an alternative ssh implementation (this was in fact the default driver in earlier versions of Ansible).</description><content>&lt;p>As the release of &lt;a href="http://ansible.com/">Ansible&lt;/a> 2.0 draws closer, I&amp;rsquo;d like to take a
look at some of the new features that are coming down the pipe. In
this post, we&amp;rsquo;ll look at the &lt;code>docker&lt;/code> connection driver.&lt;/p>
&lt;p>A &amp;ldquo;connection driver&amp;rdquo; is the mechanism by which Ansible connects to
your target hosts. These days it uses &lt;code>ssh&lt;/code> by default (which relies
on the OpenSSH command line client for connectivity), and it also
offers the &lt;a href="http://www.paramiko.org/">Paramiko&lt;/a> library as an alternative ssh implementation
(this was in fact the default driver in earlier versions of Ansible).
Alternative drivers offered by recent versions of ansible included the
&lt;code>winrm&lt;/code> driver, for accessing Windows hosts, the &lt;code>fireball&lt;/code> driver, a
(deprecated) driver that used &lt;a href="http://zeromq.org/">0mq&lt;/a> for communication, and &lt;code>jail&lt;/code>, a
driver for connecting to FreeBSD jails.&lt;/p>
&lt;p>Ansible 2.0 will offer a &lt;code>docker&lt;/code> connection driver, which can be used
to connect to Docker containers via the &lt;code>docker exec&lt;/code> command.
Assuming you have a running container named &lt;code>target&lt;/code>, you can run an
ad-hoc command like this:&lt;/p>
&lt;pre>&lt;code>$ ansible all -i target, -c docker -m command -a 'uptime'
target | SUCCESS | rc=0 &amp;gt;&amp;gt;
03:54:21 up 7 days, 15:00, 0 users, load average: 0.81, 0.60, 0.46
&lt;/code>&lt;/pre>
&lt;p>You can specify the connection driver as part of a play in your
playbook:&lt;/p>
&lt;pre>&lt;code>- hosts: target
connection: docker
tasks:
- package:
name: git
state: latest
&lt;/code>&lt;/pre>
&lt;p>Or as a variable in your inventory. Here&amp;rsquo;s an example that has both a
docker container and an ssh-accessible host:&lt;/p>
&lt;pre>&lt;code>target ansible_connection=docker
server ansible_host=192.168.1.20 ansible_user=root
&lt;/code>&lt;/pre>
&lt;p>Given the following playbook:&lt;/p>
&lt;pre>&lt;code>- hosts: all
tasks:
- ping:
&lt;/code>&lt;/pre>
&lt;p>If we run it like this, assuming the above inventory is in the file
&lt;code>inventory&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ ansible-playbook -i inventory playbook.yml
&lt;/code>&lt;/pre>
&lt;p>The output will look something like:&lt;/p>
&lt;pre>&lt;code>TASK [ping] ********************************************************************
&amp;lt;192.168.1.20&amp;gt; ESTABLISH SSH CONNECTION FOR USER: root
&amp;lt;192.168.1.20&amp;gt; SSH: EXEC ssh -C -q -o ControlMaster=auto -o ControlPersist=60s ... 192.168.1.20 ...
&amp;lt;192.168.1.20&amp;gt; PUT /tmp/tmpbtrmo5 TO /root/.ansible/tmp/ansible-tmp-1444795190.49-64658551273604/ping
&amp;lt;192.168.1.20&amp;gt; SSH: EXEC sftp -b - -C -o ControlMaster=auto -o ControlPersist=60s ... 192.168.1.20 ...
ESTABLISH DOCKER CONNECTION FOR USER: lars
&amp;lt;target&amp;gt; EXEC ['/usr/bin/docker', 'exec', '-i', u'target', '/bin/sh', '-c', ...
&amp;lt;target&amp;gt; PUT /tmp/tmpNmcPTf TO /root/.ansible/tmp/ansible-tmp-1444795190.53-251446545325875/ping
&amp;lt;192.168.1.20&amp;gt; ESTABLISH SSH CONNECTION FOR USER: root
&amp;lt;192.168.1.20&amp;gt; SSH: EXEC ssh -C -q -o ControlMaster=auto -o ControlPersist=60s ... 192.168.1.20 ...
ok: [server -&amp;gt; localhost] =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;}
&amp;lt;target&amp;gt; EXEC ['/usr/bin/docker', 'exec', '-i', u'target', '/bin/sh', '-c', ...
ok: [target -&amp;gt; localhost] =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;}
PLAY RECAP *********************************************************************
server : ok=2 changed=0 unreachable=0 failed=0
target : ok=2 changed=0 unreachable=0 failed=0
&lt;/code>&lt;/pre>
&lt;p>Now you have a unified mechanism for managing configuration changes in
traditional hosts as well as in Docker containers.&lt;/p></content></item><item><title>Running NTP in a Container</title><link>https://blog.oddbit.com/post/2015-10-09-running-ntp-in-a-container/</link><pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-09-running-ntp-in-a-container/</guid><description>Someone asked on IRC about running ntpd in a container on Atomic, so I&amp;rsquo;ve put together a small example. We&amp;rsquo;ll start with a very simple Dockerfile:
FROM alpine RUN apk update RUN apk add openntpd ENTRYPOINT [&amp;quot;ntpd&amp;quot;] I&amp;rsquo;m using the alpine image as my starting point because it&amp;rsquo;s very small, which makes this whole process go a little faster. I&amp;rsquo;m installing the openntpd package, which provides the ntpd binary.
By setting an ENTRYPOINT here, the ntpd binary will be started by default, and any arguments passed to docker run after the image name will be passed to ntpd.</description><content>&lt;p>Someone asked on IRC about running ntpd in a container on &lt;a href="http://www.projectatomic.io/">Atomic&lt;/a>,
so I&amp;rsquo;ve put together a small example. We&amp;rsquo;ll start with a very simple
&lt;code>Dockerfile&lt;/code>:&lt;/p>
&lt;pre>&lt;code>FROM alpine
RUN apk update
RUN apk add openntpd
ENTRYPOINT [&amp;quot;ntpd&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m using the &lt;code>alpine&lt;/code> image as my starting point because it&amp;rsquo;s very
small, which makes this whole process go a little faster. I&amp;rsquo;m
installing the &lt;a href="http://www.openntpd.org/">openntpd&lt;/a> package, which provides the &lt;code>ntpd&lt;/code> binary.&lt;/p>
&lt;p>By setting an &lt;code>ENTRYPOINT&lt;/code> here, the &lt;code>ntpd&lt;/code> binary will be started by
default, and any arguments passed to &lt;code>docker run&lt;/code> after the image name
will be passed to &lt;code>ntpd&lt;/code>.&lt;/p>
&lt;p>We need to first build the image:&lt;/p>
&lt;pre>&lt;code># docker build -t larsks/ntpd .
&lt;/code>&lt;/pre>
&lt;p>And then we can try to run it:&lt;/p>
&lt;pre>&lt;code># docker run larsks/ntpd -h
ntpd: unrecognized option: h
usage: ntpd [-dnSsv] [-f file] [-p file]
&lt;/code>&lt;/pre>
&lt;p>That confirms that we can run the command. Now we need to provide it
with a configuration file. I looked briefly at &lt;a href="http://www.openbsd.org/cgi-bin/man.cgi/OpenBSD-current/man5/ntpd.conf.5?query=ntpd.conf">the ntpd.conf man
page&lt;/a>, and I think we can get away with just providing the
name of an ntp server. I created &lt;code>/etc/ntpd.conf&lt;/code> on my atomic host
with the following content:&lt;/p>
&lt;pre>&lt;code>servers pool.ntp.org
&lt;/code>&lt;/pre>
&lt;p>And then I tried running the container like this:&lt;/p>
&lt;pre>&lt;code>docker run -v /etc/ntpd.conf:/etc/ntpd.conf larsks/ntpd -d -f /etc/ntpd.conf
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-v&lt;/code> in the above command line makes &lt;code>/etc/ntpd.conf&lt;/code> on the host
available as &lt;code>/etc/ntpd.conf&lt;/code> inside the container. This gets me:&lt;/p>
&lt;pre>&lt;code>ntpd: can't set priority: Permission denied
reset adjtime failed: Operation not permitted
adjtimex (2) failed: Operation not permitted
adjtimex adjusted frequency by 0.000000ppm
fatal: privsep dir /var/empty could not be opened: No such file or directory
Lost child: child exited
dispatch_imsg in main: pipe closed
Terminating
&lt;/code>&lt;/pre>
&lt;p>The first few errors (&amp;ldquo;Permission denied&amp;rdquo;) mean that we need to pass
&lt;code>--privileged&lt;/code> on the &lt;code>docker run&lt;/code> command line. Normally, Docker
runs containers with limited capabilities, but because an ntp service
needs to be able to set the time in the kernel it won&amp;rsquo;t run in that
limited environment.&lt;/p>
&lt;p>The &amp;ldquo;fatal: privsep dir /var/empty could not be opened&amp;rdquo; suggests we
need an empty directory at &lt;code>/var/empty&lt;/code>. With the above two facts in
mind, I tried:&lt;/p>
&lt;pre>&lt;code>docker run --privileged -v /var/empty \
-v /etc/ntpd.conf:/etc/ntpd.conf larsks/ntpd -d -f /etc/ntpd.conf -s
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-s&lt;/code> on the end means &amp;ldquo;Try to set the time immediately at
startup.&amp;rdquo; This results in:&lt;/p>
&lt;pre>&lt;code>adjtimex adjusted frequency by 0.000000ppm
ntp engine ready
reply from 104.131.53.252: offset -3.543963 delay 0.018517, next query 5s
set local clock to Fri Oct 9 18:03:41 UTC 2015 (offset -3.543963s)
reply from 198.23.200.19: negative delay -7.039390s, next query 3190s
reply from 107.170.224.8: negative delay -6.983865s, next query 3139s
reply from 209.118.204.201: negative delay -6.982698s, next query 3216s
reply from 104.131.53.252: offset 3.523820 delay 0.018231, next query 8s
&lt;/code>&lt;/pre>
&lt;p>So that seems to work correctly. To make this service persistent, I
can add &lt;code>-d&lt;/code> to start the container in the background, and
&lt;code>--restart=always&lt;/code> to make Docker responsible for restarting it if it
fails:&lt;/p>
&lt;pre>&lt;code>docker run --privileged -v /var/empty \
--restart=always -d \
-v /etc/ntpd.conf:/etc/ntpd.conf larsks/ntpd -d -f /etc/ntpd.conf -s
&lt;/code>&lt;/pre>
&lt;p>And my Atomic host has an ntp service keeping the time in sync.&lt;/p></content></item><item><title>Heat-kubernetes Demo with Autoscaling</title><link>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</link><pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</guid><description>Next week is the Red Hat Summit in Boston, and I&amp;rsquo;ll be taking part in a Project Atomic presentation in which I will discuss various (well, two) options for deploying Atomic into an OpenStack environment, focusing on my heat-kubernetes templates.
As part of that presentation, I&amp;rsquo;ve put together a short demonstration video:
This shows off the autoscaling behavior available with recent versions of these templates (and also serves as a very brief introduction to working with Kubernetes).</description><content>&lt;p>Next week is the &lt;a href="http://www.redhat.com/summit/">Red Hat Summit&lt;/a> in Boston, and I&amp;rsquo;ll be taking part
in a &lt;a href="http://www.projectatomic.io/">Project Atomic&lt;/a> presentation in which I will discuss various
(well, two) options for deploying Atomic into an OpenStack
environment, focusing on my &lt;a href="https://github.com/projectatomic/heat-kubernetes/">heat-kubernetes&lt;/a> templates.&lt;/p>
&lt;p>As part of that presentation, I&amp;rsquo;ve put together a short demonstration video:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>This shows off the autoscaling behavior available with recent versions
of these templates (and also serves as a very brief introduction to
working with Kubernetes).&lt;/p></content></item><item><title>Suggestions for the Docker MAINTAINER directive</title><link>https://blog.oddbit.com/post/2015-04-27-suggestions-for-the-docker-mai/</link><pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-04-27-suggestions-for-the-docker-mai/</guid><description>Because nobody asked for it, this is my opinion on the use of the MAINTAINER directive in your Dockerfiles.
The documentation says simply:
The MAINTAINER instruction allows you to set the Author field of the generated images. Many people end up putting the name and email address of an actual person here. I think this is ultimately a bad idea, and does a disservice both to members of a project that produce Docker images and to people consuming those images.</description><content>&lt;p>Because nobody asked for it, this is my opinion on the use of the
&lt;code>MAINTAINER&lt;/code> directive in your Dockerfiles.&lt;/p>
&lt;p>The &lt;a href="https://docs.docker.com/reference/builder/#maintainer">documentation&lt;/a> says simply:&lt;/p>
&lt;pre>&lt;code>The MAINTAINER instruction allows you to set the Author field of the generated images.
&lt;/code>&lt;/pre>
&lt;p>Many people end up putting the name and email address of an actual
person here. I think this is ultimately a bad idea, and does a
disservice both to members of a project that produce Docker images and
to people consuming those images.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Your image probably has (or will have) more than one &amp;ldquo;maintainer&amp;rdquo;&lt;/p>
&lt;p>Any non-trivial project is going to have more than one person
contributing to things. If you are the original creator of a
Dockerfile, but later on someone else starts making some changes,
which of you is the &amp;ldquo;maintainer&amp;rdquo;?&lt;/p>
&lt;p>Furthermore, asserting individual ownership over something that is
better off being maintained collectively tends to discourage
people from making changes (oh, this belongs to Bob, I&amp;rsquo;d better not
touch it&amp;hellip;).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Individual contributors may feel overwhelmed&lt;/p>
&lt;p>The individual responsible for creating a Docker image may or may be
great at communicating with consumers. If all questions about an
image are going into a well-intentioned by ultimately unresponsive
black hole, nobody is going to be happy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Individual contributors come and go&lt;/p>
&lt;p>Most open source projects have fluid membership. Someone who is
around now and actively maintaining things may not be around several
months down the road. Having an absentee member listed as the
&amp;ldquo;maintainer&amp;rdquo; of your images means that email about those images will
probably not reach anybody in a position to respond.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Your project probably has a bug tracker&lt;/p>
&lt;p>Most projects have some sort of bug tracking mechanism available.
These are generally in place both to keep track of the bug reports
and support requests coming in as well as acting as a mechanism to
distribute the work involved in responding to them to all members of
a project.&lt;/p>
&lt;p>Ideally, you want questions about any images you maintain to go
through the same tracking mechanism.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>For all of these reasons, the &lt;code>MAINTAINER&lt;/code> field of your Dockerfiles
should point to either a web site URL or to a common project email
address that goes into a bug tracker or is at least distributed to
more than one person.&lt;/p></content></item><item><title>Converting hexadecimal ip addresses to dotted quads with Bash</title><link>https://blog.oddbit.com/post/2015-03-08-converting-hexadecimal-ip-addr/</link><pubDate>Sun, 08 Mar 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-03-08-converting-hexadecimal-ip-addr/</guid><description>This is another post that is primarily for my own benefit for the next time I forget how to do this.
I wanted to read routing information directly from /proc/net/route using bash, because you never know what may or may not be available in the minimal environment of a Docker container (for example, the iproute package is not installed by default in the Fedora Docker images). The contents of /proc/net/route looks something like:</description><content>&lt;p>This is another post that is primarily for my own benefit for the next
time I forget how to do this.&lt;/p>
&lt;p>I wanted to read routing information directly from &lt;code>/proc/net/route&lt;/code>
using &lt;code>bash&lt;/code>, because you never know what may or may not be available
in the minimal environment of a Docker container (for example, the
&lt;code>iproute&lt;/code> package is not installed by default in the Fedora Docker
images). The contents of &lt;code>/proc/net/route&lt;/code> looks something like:&lt;/p>
&lt;pre>&lt;code>Iface Destination Gateway Flags RefCnt Use Metric Mask MTU Window IRTT
eth0 00000000 0101A8C0 0003 0 0 1024 00000000 0 0 0
eth0 37E9BB42 0101A8C0 0007 0 0 20 FFFFFFFF 0 0 0
&lt;/code>&lt;/pre>
&lt;p>If I want the address of the default gateway, I can trivially get the
hexadecimal form like this:&lt;/p>
&lt;pre>&lt;code>awk '$2 == &amp;quot;00000000&amp;quot; {print $3}' /proc/net/route
&lt;/code>&lt;/pre>
&lt;p>Which gives me:&lt;/p>
&lt;pre>&lt;code>0101A8C0
&lt;/code>&lt;/pre>
&lt;p>This is in little-endian order; that is, the above bytes represent &lt;code>1 1 168 192&lt;/code>, which you may recognize better as &lt;code>192.168.1.1&lt;/code>. So, we
need to convert this into a sequence of individual octets, reverse the
order, and produce the decimal equivalent of each octet.&lt;/p>
&lt;p>The following gives us the octets in the correct order, prefixed by
&lt;code>0x&lt;/code> (which we&amp;rsquo;re going to want in the next step):&lt;/p>
&lt;pre>&lt;code>awk '$2 == &amp;quot;00000000&amp;quot; {print $3}' /proc/net/route |
sed 's/../0x&amp;amp; /g' | tr ' ' '\n' | tac
&lt;/code>&lt;/pre>
&lt;p>We can put this into a bash array like this:&lt;/p>
&lt;pre>&lt;code>octets=($(
awk '$2 == &amp;quot;00000000&amp;quot; {print $3}' /proc/net/route |
sed 's/../0x&amp;amp; /g' | tr ' ' '\n' | tac
))
&lt;/code>&lt;/pre>
&lt;p>And we convert those hexadecimal octets into decimal like this:&lt;/p>
&lt;pre>&lt;code>printf &amp;quot;%d.&amp;quot; ${octets[@]} | sed 's/\.$/\n/'
&lt;/code>&lt;/pre>
&lt;p>An interesting feature of the Bash &lt;code>printf&lt;/code> command &amp;ndash; and one that
may be surprising to people who are coming from a C background &amp;ndash; is
that:&lt;/p>
&lt;blockquote>
&lt;p>The format is re-used as necessary to consume all of the arguments.&lt;/p>
&lt;/blockquote>
&lt;p>That means, that a command like this:&lt;/p>
&lt;pre>&lt;code>printf &amp;quot;%d.&amp;quot; 1 2 3 4
&lt;/code>&lt;/pre>
&lt;p>Will yield:&lt;/p>
&lt;pre>&lt;code>1.2.3.4.
&lt;/code>&lt;/pre>
&lt;p>If we put this all together, we might end up with something like:&lt;/p>
&lt;pre>&lt;code>hexaddr=$(awk '$2 == &amp;quot;00000000&amp;quot; {print $3}' /proc/net/route)
ipaddr=$(printf &amp;quot;%d.&amp;quot; $(
echo $hexaddr | sed 's/../0x&amp;amp; /g' | tr ' ' '\n' | tac
) | sed 's/\.$/\n/')&lt;/code>&lt;/pre></content></item><item><title>Unpacking Docker images with Undocker</title><link>https://blog.oddbit.com/post/2015-02-13-unpacking-docker-images/</link><pubDate>Fri, 13 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-13-unpacking-docker-images/</guid><description>In some ways, the most exciting thing about Docker isn&amp;rsquo;t the ability to start containers. That&amp;rsquo;s been around for a long time in various forms, such as LXC or OpenVZ. What Docker brought to the party was a convenient method of building and distributing the filesystems necessary for running containers. Suddenly, it was easy to build a containerized service and to share it with other people.
I was taking a closer at the systemd-nspawn command, which it seems has been developing it&amp;rsquo;s own set of container-related superpowers recently, including a number of options for setting up the network environment of a container.</description><content>&lt;p>In some ways, the most exciting thing about &lt;a href="http://docker.com/">Docker&lt;/a> isn&amp;rsquo;t the ability
to start containers. That&amp;rsquo;s been around for a long time in various
forms, such as &lt;a href="https://linuxcontainers.org/">LXC&lt;/a> or &lt;a href="http://openvz.org/Main_Page">OpenVZ&lt;/a>. What Docker brought to the
party was a convenient method of building and distributing the
filesystems necessary for running containers. Suddenly, it was easy
to build a containerized service &lt;em>and&lt;/em> to share it with other people.&lt;/p>
&lt;p>I was taking a closer at the &lt;a href="http://www.freedesktop.org/software/systemd/man/systemd-nspawn.html">systemd-nspawn&lt;/a> command, which it
seems has been developing it&amp;rsquo;s own set of container-related
superpowers recently, including a number of options for setting up the
network environment of a container. Like Docker, &lt;code>systemd-nspawn&lt;/code>
needs a filesystem on which to operate, but &lt;em>unlike&lt;/em> Docker, there is
no convenient distribution mechanism and no ecosystem of existing
images. In fact, the official documentation seems to assume that
you&amp;rsquo;ll be building your own from scratch. Ain&amp;rsquo;t nobody got time for
that&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;but with that attracting Docker image ecosystem sitting right next
door, surely there was something we can do?&lt;/p>
&lt;h2 id="the-format-of-a-docker-image">The format of a Docker image&lt;/h2>
&lt;p>A Docker image is a tar archive that contains a top level
&lt;code>repositories&lt;/code> files, and then a number of layers stored as
directories containing a &lt;code>json&lt;/code> file with some metadata about the
layer and a tar file named &lt;code>layer.tar&lt;/code> with the layer content. For
example, if you &lt;code>docker save busybox&lt;/code>, you get:&lt;/p>
&lt;pre>&lt;code>4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/
4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/VERSION
4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/json
4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/layer.tar
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/VERSION
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer.tar
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/VERSION
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/json
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/layer.tar
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/VERSION
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/json
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/layer.tar
repositories
&lt;/code>&lt;/pre>
&lt;p>In order to re-create the filesystem that would result from starting a
Docker container with this image, you need to unpack the &lt;code>layer.tar&lt;/code>
files from the bottom up. You can find the topmost layer in the
&lt;code>repositories&lt;/code> file, which looks like this:&lt;/p>
&lt;p>{
&amp;ldquo;busybox&amp;rdquo;: {
&amp;ldquo;latest&amp;rdquo;: &amp;ldquo;4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125&amp;rdquo;
}
}&lt;/p>
&lt;p>From there, you can investigate the &lt;code>json&lt;/code> file for each layer looking
for the &lt;code>parent&lt;/code> tag.&lt;/p>
&lt;h2 id="introducing-undocker">Introducing undocker&lt;/h2>
&lt;p>I wrote the &lt;a href="http://github.com/larsks/undocker/">undocker&lt;/a> command to extract all or part of the layers
of a Docker image onto the local filesystem. In other words, if you
want to use the &lt;code>busybox&lt;/code> Docker image, you can fetch and unpack the
image:&lt;/p>
&lt;pre>&lt;code># docker pull busybox
# docker save busybox | undocker -o busybox
&lt;/code>&lt;/pre>
&lt;p>This will first look in the &lt;code>repositories&lt;/code> file for the &lt;code>busybox&lt;/code>
entry with the &lt;code>latest&lt;/code> tag, then build the necessary chain of layers
and unpack them in the correct order.&lt;/p>
&lt;p>Once you have the filesystem extracted, you can boot it with
&lt;code>systemd-nspawn&lt;/code>:&lt;/p>
&lt;pre>&lt;code># systemd-nspawn -D busybox /bin/sh
Spawning container busybox on /root/busybox.
Press ^] three times within 1s to kill container.
Timezone America/New_York does not exist in container, not updating container timezone.
Failed to copy /etc/resolv.conf to /root/busybox/etc/resolv.conf: Too many levels of symbolic links
/bin/sh: can't access tty; job control turned off
/ #
&lt;/code>&lt;/pre>
&lt;p>Undocker is able to extract specific layers from the image as well.
We can get a list of layers with the &lt;code>--layers&lt;/code> option:&lt;/p>
&lt;pre>&lt;code>$ docker save busybox | undocker --layers
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2
4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125
&lt;/code>&lt;/pre>
&lt;p>And we can extract one or more specific layers with the &lt;code>--layer&lt;/code>
(&lt;code>-l&lt;/code>) option:&lt;/p>
&lt;pre>&lt;code>$ docker save busybox |
undocker -vi -o busybox -l ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2
INFO:undocker:extracting image busybox (4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125)
INFO:undocker:extracting layer ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m using the &lt;code>-i&lt;/code> (&lt;code>--ignore-errors&lt;/code>) option here because this layer
contains a device node (&lt;code>/dev/console&lt;/code>), and I am running this as an
unprivileged user. Without the &lt;code>-i&lt;/code> option, we would see:&lt;/p>
&lt;pre>&lt;code>OSError: [Errno 1] Operation not permitted
&lt;/code>&lt;/pre>
&lt;p>A Docker image archive can actually contain multiple images, each with
multiple tags. For a single image, &lt;code>undocker&lt;/code> will default to
extracting the &lt;code>latest&lt;/code> tag. If the &lt;code>latest&lt;/code> tag doesn&amp;rsquo;t exist,
you&amp;rsquo;ll see:&lt;/p>
&lt;pre>&lt;code># docker pull fedora:20
# docker save fedora:20 | undocker -o fedora
ERROR:undocker:failed to find image fedora with tag latest
&lt;/code>&lt;/pre>
&lt;p>You can specify an explicit tag in the same way you provide one to
Docker:&lt;/p>
&lt;pre>&lt;code># docker save fedora:20 | undocker -o fedora fedora:20
&lt;/code>&lt;/pre>
&lt;p>If an archive contains multiple images, you&amp;rsquo;ll get a different error:&lt;/p>
&lt;pre>&lt;code># docker save busybox larsks/thttpd | undocker -o busybox
ERROR:undocker:No image name specified and multiple images contained in archive
&lt;/code>&lt;/pre>
&lt;p>You can get a list of available images and tags with the &lt;code>--list&lt;/code>
option:&lt;/p>
&lt;pre>&lt;code># docker save busybox larsks/thttpd | undocker --list
larsks/thttpd: latest
busybox: latest
# docker save fedora | undocker --list
fedora: heisenbug 20 21 rawhide latest
&lt;/code>&lt;/pre>
&lt;p>You can specify the image (and tag) to extract on the command line:&lt;/p>
&lt;pre>&lt;code># docker save busybox larsks/thttpd | undocker -o busybox busybox
&lt;/code>&lt;/pre></content></item><item><title>Installing nova-docker with devstack</title><link>https://blog.oddbit.com/post/2015-02-11-installing-novadocker-with-dev/</link><pubDate>Wed, 11 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-11-installing-novadocker-with-dev/</guid><description>This is a long-form response to this question, and describes how to get the nova-docker driver up running with devstack under Ubuntu 14.04 (Trusty). I wrote a similar post for Fedora 21, although that one was using the RDO Juno packages, while this one is using devstack and the upstream sources.
Getting started We&amp;rsquo;ll be using the Ubuntu 14.04 cloud image (because my test environment runs on OpenStack).
First, let&amp;rsquo;s install a few prerequisites:</description><content>&lt;p>This is a long-form response to &lt;a href="https://ask.openstack.org/en/question/60679/installing-docker-on-openstack-with-ubuntu/">this question&lt;/a>, and describes
how to get the &lt;a href="http://github.com/stackforge/nova-docker/">nova-docker&lt;/a> driver up running with &lt;a href="http://devstack.org/">devstack&lt;/a>
under Ubuntu 14.04 (Trusty). I wrote a &lt;a href="https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/">similar post&lt;/a> for Fedora
21, although that one was using the &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> Juno packages, while this
one is using &lt;a href="http://devstack.org/">devstack&lt;/a> and the upstream sources.&lt;/p>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>We&amp;rsquo;ll be using the &lt;a href="https://cloud-images.ubuntu.com/trusty/current/">Ubuntu 14.04 cloud image&lt;/a> (because my test
environment runs on &lt;a href="http://www.openstack.org/">OpenStack&lt;/a>).&lt;/p>
&lt;p>First, let&amp;rsquo;s install a few prerequisites:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get update
$ sudo apt-get -y install git git-review python-pip python-dev
&lt;/code>&lt;/pre>
&lt;p>And generally make sure things are up-to-date:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get -y upgrade
&lt;/code>&lt;/pre>
&lt;h2 id="installing-docker">Installing Docker&lt;/h2>
&lt;p>We need to install Docker if we&amp;rsquo;re going to use &lt;code>nova-docker&lt;/code>.&lt;/p>
&lt;p>Ubuntu 14.04 includes a fairly dated version of Docker, so I followed
&lt;a href="https://docs.docker.com/installation/ubuntulinux/#docker-maintained-package-installation">the instructions&lt;/a> on the Docker website for installing the current
version of Docker on Ubuntu; this ultimately got me:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get -y install lxc-docker
$ sudo docker version
Client version: 1.5.0
Client API version: 1.17
Go version (client): go1.4.1
Git commit (client): a8a31ef
OS/Arch (client): linux/amd64
Server version: 1.5.0
Server API version: 1.17
Go version (server): go1.4.1
Git commit (server): a8a31ef
&lt;/code>&lt;/pre>
&lt;p>Docker by default creates its socket (&lt;code>/var/run/docker.socket&lt;/code>) with
&lt;code>root:root&lt;/code> ownership. We&amp;rsquo;re going to be running devstack as the
&lt;code>ubuntu&lt;/code> user, so let&amp;rsquo;s change that by editing &lt;code>/etc/default/docker&lt;/code>
and setting:&lt;/p>
&lt;pre>&lt;code>DOCKER_OPTS='-G ubuntu'
&lt;/code>&lt;/pre>
&lt;p>And restart &lt;code>docker&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ sudo restart docker
&lt;/code>&lt;/pre>
&lt;p>And verify that we can access Docker as the &lt;code>ubuntu&lt;/code> user:&lt;/p>
&lt;pre>&lt;code>$ docker version
Client version: 1.5.0
Client API version: 1.17
[...]
&lt;/code>&lt;/pre>
&lt;h2 id="installing-nova-docker">Installing nova-docker&lt;/h2>
&lt;p>As the &lt;code>ubuntu&lt;/code> user, let&amp;rsquo;s get the &lt;code>nova-docker&lt;/code> source code:&lt;/p>
&lt;pre>&lt;code>$ git clone http://github.com/stackforge/nova-docker.git
$ cd nova-docker
&lt;/code>&lt;/pre>
&lt;p>As of this writing (&lt;code>HEAD&lt;/code> is &amp;ldquo;984900a Give some time for docker.stop
to work&amp;rdquo;), you need to apply &lt;a href="https://review.openstack.org/#/c/154750/">a patch&lt;/a> to &lt;code>nova-docker&lt;/code> to get it to
work with the current Nova &lt;code>master&lt;/code> branch:&lt;/p>
&lt;pre>&lt;code>$ git fetch https://review.openstack.org/stackforge/nova-docker \
refs/changes/50/154750/3 \
&amp;amp;&amp;amp; git checkout FETCH_HEAD
&lt;/code>&lt;/pre>
&lt;p>Once &lt;a href="https://review.openstack.org/#/c/154750/">that change&lt;/a> has merged (&lt;strong>update&lt;/strong>, 2015-02-12: the
patch has merged), this step should no longer be
necessary. With the patch we applied, we can install the
&lt;code>nova-docker&lt;/code> driver:&lt;/p>
&lt;pre>&lt;code>$ sudo pip install .
&lt;/code>&lt;/pre>
&lt;h2 id="configuring-devstack">Configuring devstack&lt;/h2>
&lt;p>Now we&amp;rsquo;re ready to get devstack up and running. Start by cloning the
repository:&lt;/p>
&lt;pre>&lt;code>$ git clone https://git.openstack.org/openstack-dev/devstack
$ cd devstack
&lt;/code>&lt;/pre>
&lt;p>Then create a &lt;code>local.conf&lt;/code> file with the following content:&lt;/p>
&lt;pre>&lt;code>[[local|localrc]]
ADMIN_PASSWORD=secret
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=super-secret-admin-token
VIRT_DRIVER=novadocker.virt.docker.DockerDriver
DEST=$HOME/stack
SERVICE_DIR=$DEST/status
DATA_DIR=$DEST/data
LOGFILE=$DEST/logs/stack.sh.log
LOGDIR=$DEST/logs
# The default fixed range (10.0.0.0/24) conflicted with an address
# range I was using locally.
FIXED_RANGE=10.254.1.0/24
NETWORK_GATEWAY=10.254.1.1
# This enables Neutron, because that's how I roll.
disable_service n-net
enable_service q-svc
enable_service q-agt
enable_service q-dhcp
enable_service q-l3
enable_service q-meta
# I am disabling horizon (because I rarely use the web ui)
# and tempest in order to make the installer complete a
# little faster.
disable_service horizon
disable_service tempest
# Introduce glance to docker images
[[post-config|$GLANCE_API_CONF]]
[DEFAULT]
container_formats=ami,ari,aki,bare,ovf,ova,docker
# Configure nova to use the nova-docker driver
[[post-config|$NOVA_CONF]]
[DEFAULT]
compute_driver=novadocker.virt.docker.DockerDriver
&lt;/code>&lt;/pre>
&lt;p>This will result in things getting installed in subdirectories of
&lt;code>$HOME/stack&lt;/code>. We enable Neutron and leave pretty much everything
else set to default values.&lt;/p>
&lt;h2 id="start-the-installation">Start the installation&lt;/h2>
&lt;p>So, now we&amp;rsquo;re all ready to roll!&lt;/p>
&lt;pre>&lt;code>$ ./stack.sh
[Call Trace]
./stack.sh:151:source
/home/ubuntu/devstack/stackrc:665:die
[ERROR] /home/ubuntu/devstack/stackrc:665 Could not determine host ip address. See local.conf for suggestions on setting HOST_IP.
/home/ubuntu/devstack/functions-common: line 322: /home/ubuntu/stack/logs/error.log: No such file or directory
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;or not. This error happens if devstack is unable to turn your
hostname into an IP address. We can set &lt;code>HOST_IP&lt;/code> in our
environment:&lt;/p>
&lt;pre>&lt;code>$ HOST_IP=10.0.0.232 ./stack.sh
&lt;/code>&lt;/pre>
&lt;p>And then go grab a cup of coffee or something.&lt;/p>
&lt;h2 id="install-nova-docker-rootwrap-filters">Install nova-docker rootwrap filters&lt;/h2>
&lt;p>Once &lt;code>stack.sh&lt;/code> is finished running, we need to install a &lt;code>rootwrap&lt;/code>
configuration file for &lt;code>nova-docker&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ sudo cp nova-docker/etc/nova/rootwrap.d/docker.filters \
/etc/nova/rootwrap.d/
&lt;/code>&lt;/pre>
&lt;h2 id="starting-a-docker-container">Starting a Docker container&lt;/h2>
&lt;p>Now that our environment is up and running, we should be able to start
a container. We&amp;rsquo;ll start by grabbing some admin credentials for our
OpenStack environment:&lt;/p>
&lt;pre>&lt;code>$ . openrc admin
&lt;/code>&lt;/pre>
&lt;p>Next, we need an appropriate image; my &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">larsks/thttpd&lt;/a> image
is small (so it&amp;rsquo;s quick to download) and does not require any
interactive terminal (so it&amp;rsquo;s appropriate for nova-docker), so let&amp;rsquo;s
start with that:&lt;/p>
&lt;pre>&lt;code>$ docker pull larsks/thttpd
$ docker save larsks/thttpd |
glance image-create --name larsks/thttpd \
--is-public true --container-format docker \
--disk-format raw
&lt;/code>&lt;/pre>
&lt;p>And now we&amp;rsquo;ll boot it up. I like to do this as a non-admin user:&lt;/p>
&lt;pre>&lt;code>$ . openrc demo
$ nova boot --image larsks/thttpd --flavor m1.small test0
&lt;/code>&lt;/pre>
&lt;p>After a bit, we should see:&lt;/p>
&lt;pre>&lt;code>$ nova list
+----...+-------+--------+...+-------------+--------------------+
| ID ...| Name | Status |...| Power State | Networks |
+----...+-------+--------+...+-------------+--------------------+
| 0c3...| test0 | ACTIVE |...| Running | private=10.254.1.4 |
+----...+-------+--------+...+-------------+--------------------+
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s create a floating ip address:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-create
+------------+-----------+----------+--------+
| Ip | Server Id | Fixed Ip | Pool |
+------------+-----------+----------+--------+
| 172.24.4.3 | - | - | public |
+------------+-----------+----------+--------+
&lt;/code>&lt;/pre>
&lt;p>And assign it to our container:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-associate test0 172.24.4.3
&lt;/code>&lt;/pre>
&lt;p>And now access our service:&lt;/p>
&lt;pre>&lt;code>$ curl http://172.24.4.3
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Your web server is working&amp;lt;/title&amp;gt;
[...]
&lt;/code>&lt;/pre></content></item><item><title>External networking for Kubernetes services</title><link>https://blog.oddbit.com/post/2015-02-10-external-networking-for-kubern/</link><pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-10-external-networking-for-kubern/</guid><description>I have recently started running some &amp;ldquo;real&amp;rdquo; services (that is, &amp;ldquo;services being consumed by someone other than myself&amp;rdquo;) on top of Kubernetes (running on bare metal), which means I suddenly had to confront the question of how to provide external access to Kubernetes hosted services. Kubernetes provides two solutions to this problem, neither of which is particularly attractive out of the box:
There is a field createExternalLoadBalancer that can be set in a service description.</description><content>&lt;p>I have recently started running some &amp;ldquo;real&amp;rdquo; services (that is,
&amp;ldquo;services being consumed by someone other than myself&amp;rdquo;) on top of
Kubernetes (running on bare metal), which means I suddenly had to
confront the question of how to provide external access to Kubernetes
hosted services. Kubernetes provides two solutions to this problem,
neither of which is particularly attractive out of the box:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>There is a field &lt;code>createExternalLoadBalancer&lt;/code> that can be set in a
service description. This is meant to integrate with load
balancers provided by your local cloud environment, but at the
moment there is only support for this when running under &lt;a href="https://cloud.google.com/compute/">GCE&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A service description can have a list of public IP addresses
associated with it in the &lt;code>publicIPS&lt;/code> field. This will cause
&lt;code>kube-proxy&lt;/code> to create rules in the &lt;code>KUBE-PROXY&lt;/code> chain of your
&lt;code>nat&lt;/code> table to direct traffic inbound to those addresses to the
appropriate local &lt;code>kube-proxy&lt;/code> port.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The second option is a good starting point, since if you were to
simply list the public IP addresses of your Kubernetes minions in the
&lt;code>publicIPs&lt;/code> field, everything would Just Work. That is, inbound
traffic to the appropriate port on your minions would get directed to
&lt;code>kube-proxy&lt;/code> by the &lt;code>nat&lt;/code> rules. That&amp;rsquo;s great for simple cases, but
in practice it means that you cannot have more that &lt;em>N&lt;/em> services
exposed on a given port where &lt;em>N&lt;/em> is the number of minions in your
cluster. That limit is difficult if you &amp;ndash; like I do &amp;ndash; have an
all-in-one (e.g., on a single host) Kubernetes deployment on which you
wish to host multiple web services exposed on port 80 (and even in a
larger environment, you really don&amp;rsquo;t want &amp;ldquo;number of things on port
XX&amp;rdquo; tightly coupled to &amp;ldquo;number of minions&amp;rdquo;).&lt;/p>
&lt;h2 id="introducing-kiwi">Introducing Kiwi&lt;/h2>
&lt;p>To overcome this problem, I wrote &lt;a href="http://github.com/larsks/kiwi/">Kiwi&lt;/a>, a service that listens to
Kubernetes for events concerning new/modified/deleted services, and in
response to those events manages (a) the assignment of IP addresses to
network interfaces on your minions and (b) creating additional
firewall rules to permit traffic inbound to your services to pass a
default-deny firewall configuration.&lt;/p>
&lt;p>Kiwi uses &lt;a href="https://github.com/coreos/etcd">etcd&lt;/a> to coordinate ownership of IP addresses between
minions in your Kubernetes cluster.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>Kiwi listens to event streams from both Kubernetes and Etcd.&lt;/p>
&lt;p>On the Kubernetes side, Kiwi listens to &lt;code>/api/v1beta/watch/services&lt;/code>,
which produces events in response to new, modified, or deleted
services. The Kubernetes API uses a server-push model, in which a
client makes a single HTTP request and then receives a series of
events over the same connection. A event looks something like:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;type&amp;quot;: &amp;quot;ADDED&amp;quot;,
&amp;quot;object&amp;quot;: {
&amp;quot;portalIP&amp;quot;: &amp;quot;10.254.93.176&amp;quot;,
&amp;quot;containerPort&amp;quot;: 80,
&amp;quot;publicIPs&amp;quot;: [
&amp;quot;192.168.1.100&amp;quot;
],
&amp;quot;selector&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;test-web&amp;quot;
},
&amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;,
&amp;quot;port&amp;quot;: 8080,
&amp;quot;kind&amp;quot;: &amp;quot;Service&amp;quot;,
&amp;quot;id&amp;quot;: &amp;quot;test-web&amp;quot;,
&amp;quot;uid&amp;quot;: &amp;quot;72bc1286-a440-11e4-b83e-20cf30467e62&amp;quot;,
&amp;quot;creationTimestamp&amp;quot;: &amp;quot;2015-01-24T22:15:43-05:00&amp;quot;,
&amp;quot;selfLink&amp;quot;: &amp;quot;/api/v1beta1/services/test-web&amp;quot;,
&amp;quot;resourceVersion&amp;quot;: 245,
&amp;quot;apiVersion&amp;quot;: &amp;quot;v1beta1&amp;quot;,
&amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>I am using the Python &lt;a href="http://docs.python-requests.org/en/latest/">requests&lt;/a> library, which it turns out &lt;a href="https://github.com/kennethreitz/requests/issues/2433">has a
bug&lt;/a> in its handling of streaming server responses, but I was
able to work around that issue once I realized what was going on.&lt;/p>
&lt;p>On the Etcd side, Kiwi uses keys under the &lt;code>/kiwi/publicips&lt;/code> prefix to
coordinate address ownership among Kiwi instances. It listens to
events from Etcd regarding key create/delete/set/etc operations in
this prefix by calling
&lt;code>/v2/keys/kiwi/publicips?watch=true&amp;amp;recursive=true&lt;/code>. This is a
long-poll request, rather than a streaming request: that means that a
request will only ever receive a single event, but it may need to wait
for a while before it receives that response. This model worked well
with the &lt;code>requests&lt;/code> library out of the box.&lt;/p>
&lt;p>After receiving an event from Kubernetes, Kiwi iterates over the
public IP addresses in the &lt;code>publicIPs&lt;/code> key, and for any address that
is not already being manged by the local instance it makes a claim on
that address by attempting to atomically create a key in etcd under
&lt;code>/kiwi/publicips/&lt;/code> (such as &lt;code>/kiwi/publicips/192.168.1.100&lt;/code>). If this
attempt succeeds, Kiwi on the local minion has claimed that address
and proceeds to assign it to the local interface. If the attempt to
set that key does not succeed, it means the address is already being
managed by Kiwi on another minion.&lt;/p>
&lt;p>The address keys are set with a TTL of 20 seconds, after which they
will be expired. If an address expires, other Kiwi instances will
receive notification from Etcd and ownership of that address will
transfer to another Kiwi instance.&lt;/p>
&lt;h2 id="getting-started-with-kiwi">Getting started with Kiwi&lt;/h2>
&lt;p>The easiest way to get started with Kiwi is to use the &lt;a href="https://registry.hub.docker.com/u/larsks/kiwi/">larsks/kiwi&lt;/a>
Docker image that is automatically built from the &lt;a href="http://github.com/larsks/kiwi/">Git
repository&lt;/a>. For example, if you want to host public ip
addresses on &lt;code>eth0&lt;/code> in the range &lt;code>192.168.1.32/28&lt;/code>, you would start it
like this:&lt;/p>
&lt;pre>&lt;code>docker run --privileged --net=host larsks/kiwi \
--interface eth0 \
--range 192.168.1.32/28
&lt;/code>&lt;/pre>
&lt;p>You need both &lt;code>--privileged&lt;/code> and &lt;code>--net=host&lt;/code> in order for Kiwi to
assign addresses to your host interfaces and to manage the iptables
configuration.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>Start Kiwi as described above. Next, plae the following content in a
file called &lt;code>service.yaml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kind: Service
apiVersion: v1beta1
id: test-web
port: 8888
selector:
name: test-web
containerPort: 80
publicIPs:
- 192.168.1.100
&lt;/code>&lt;/pre>
&lt;p>Create the service using &lt;code>kubectl&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kubectl create -f service.yaml
&lt;/code>&lt;/pre>
&lt;p>After a short pause, you should see the address show up on interface
&lt;code>eth0&lt;/code>; the entry will look something like:&lt;/p>
&lt;pre>&lt;code>inet 192.168.1.100/32 scope global dynamic eth0:kube
valid_lft 17sec preferred_lft 17sec
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>eth0:kube&lt;/code> is a label applied to the address; this allows Kiwi to
clean up these addresses at startup (by getting a list of
Kiwi-configured addresses with &lt;code>ip addr show label eth0:kube&lt;/code>).&lt;/p>
&lt;p>The &lt;code>valid_lft&lt;/code> and &lt;code>preferred_lft&lt;/code> fields control the lifetime of the
interface. When these counters reach 0, the addresses are removed by
the kernel. This ensure that if Kiwi dies, the addresses can
successfully be re-assigned on another node.&lt;/p></content></item><item><title>Installing nova-docker on Fedora 21/RDO Juno</title><link>https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/</link><pubDate>Fri, 06 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/</guid><description>This post comes about indirectly by a request on IRC in #rdo for help getting nova-docker installed on Fedora 21. I ran through the process from start to finish and decided to write everything down for posterity.
Getting started I started with the Fedora 21 Cloud Image, because I&amp;rsquo;m installing onto OpenStack and the cloud images include some features that are useful in this environment.
We&amp;rsquo;ll be using OpenStack packages from the RDO Juno repository.</description><content>&lt;p>This post comes about indirectly by a request on IRC in &lt;code>#rdo&lt;/code> for help getting &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> installed on Fedora 21. I ran through the process from start to finish and decided to write everything down for posterity.&lt;/p>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>I started with the &lt;a href="https://getfedora.org/en/cloud/download/">Fedora 21 Cloud Image&lt;/a>, because I&amp;rsquo;m
installing onto OpenStack and the cloud images include
some features that are useful in this environment.&lt;/p>
&lt;p>We&amp;rsquo;ll be using OpenStack packages from the &lt;a href="https://repos.fedorapeople.org/repos/openstack/openstack-juno/">RDO Juno&lt;/a> repository.
Because there is often some skew between the RDO packages and the
current Fedora selinux policy, we&amp;rsquo;re going to start by putting SELinux
into permissive mode (sorry, Dan):&lt;/p>
&lt;pre>&lt;code># setenforce 0
# sed -i '/^SELINUX=/ s/=.*/=permissive/' /etc/selinux/config
&lt;/code>&lt;/pre>
&lt;p>Next, install the RDO Juno repository:&lt;/p>
&lt;pre>&lt;code># yum -y install \
https://repos.fedorapeople.org/repos/openstack/openstack-juno/rdo-release-juno-1.noarch.rpm
&lt;/code>&lt;/pre>
&lt;p>And upgrade all our existing packages:&lt;/p>
&lt;pre>&lt;code># yum -y upgrade
&lt;/code>&lt;/pre>
&lt;h2 id="install-openstack">Install OpenStack&lt;/h2>
&lt;p>We&amp;rsquo;ll be using &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a> to install OpenStack onto this host.
Start by installing the package:&lt;/p>
&lt;pre>&lt;code># yum -y install openstack-packstack
&lt;/code>&lt;/pre>
&lt;p>And then run a &lt;code>--allinone&lt;/code> install, which sets up all OpenStack
services on a single host:&lt;/p>
&lt;pre>&lt;code># packstack --allinone
&lt;/code>&lt;/pre>
&lt;h2 id="install-nova-docker-prequisites">Install nova-docker prequisites&lt;/h2>
&lt;p>Once &lt;code>packstack&lt;/code> has completed successfully, we need to install some
prerequisites for &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a>.&lt;/p>
&lt;pre>&lt;code># yum -y install git python-pip python-pbr \
docker-io fedora-repos-rawhide
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>fedora-repos-rawhide&lt;/code> package provides a yum configuration for the
&lt;code>rawhide&lt;/code> repository (disabled by default). We&amp;rsquo;re going to need that
to pick up more recent versions of &lt;code>systemd&lt;/code> (because of &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1187882">this
bug&lt;/a>) and
&lt;code>python-six&lt;/code> (because &lt;code>nova-docker&lt;/code> needs the &lt;code>six.add_metaclass&lt;/code>
method):&lt;/p>
&lt;pre>&lt;code># yum --enablerepo=rawhide install python-six systemd
&lt;/code>&lt;/pre>
&lt;p>At this point, having upgraded &lt;code>systemd&lt;/code>, you should probably reboot:&lt;/p>
&lt;pre>&lt;code># reboot
&lt;/code>&lt;/pre>
&lt;h2 id="configure-docker">Configure Docker&lt;/h2>
&lt;p>Once things are up and running, we will expect the &lt;code>nova-compute&lt;/code>
service to launch Docker containers. In order for this to work, the
&lt;code>nova&lt;/code> user will need access to the Docker socket,
&lt;code>/var/run/docker.sock&lt;/code>. By default, this is owned by &lt;code>root:root&lt;/code> and
has mode &lt;code>660&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ls -l /var/run/docker.sock
srw-rw----. 1 root root 0 Feb 1 12:43 /var/run/docker.sock
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>nova-compute&lt;/code> service runs as the &lt;code>nova&lt;/code> user and will not have
access to that socket. There are a few ways of resolving this; an
expedient method is simply to make this socket owned by the &lt;code>nova&lt;/code>
group, which we can do with &lt;code>docker&lt;/code>&amp;rsquo;s &lt;code>-G&lt;/code> option.&lt;/p>
&lt;p>Edit &lt;code>/etc/sysconfig/docker&lt;/code>, and modify the &lt;code>OPTIONS=&lt;/code> line to look
like:&lt;/p>
&lt;pre>&lt;code>OPTIONS='--selinux-enabled -G nova'
&lt;/code>&lt;/pre>
&lt;p>Then enable and start the &lt;code>docker&lt;/code> service:&lt;/p>
&lt;pre>&lt;code># systemctl enable docker
# systemctl start docker
&lt;/code>&lt;/pre>
&lt;h2 id="install-nova-docker">Install nova-docker&lt;/h2>
&lt;p>Clone the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> repository:&lt;/p>
&lt;pre>&lt;code># git clone http://github.com/stackforge/nova-docker.git
# cd nova-docker
&lt;/code>&lt;/pre>
&lt;p>And check out the &lt;code>stable/juno&lt;/code> branch, since we&amp;rsquo;re operating with an
OpenStack Juno environment:&lt;/p>
&lt;pre>&lt;code># git checkout stable/juno
&lt;/code>&lt;/pre>
&lt;p>Now install the driver:&lt;/p>
&lt;pre>&lt;code># python setup.py install
&lt;/code>&lt;/pre>
&lt;h2 id="configure-nova">Configure Nova&lt;/h2>
&lt;p>Following the &lt;a href="https://github.com/stackforge/nova-docker/blob/master/README.rst">README&lt;/a> from &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a>, we need to modify
the Nova configuration to use the &lt;code>nova-docker&lt;/code> driver. Edit
&lt;code>/etc/nova/nova.conf&lt;/code> and add the following line to the &lt;code>DEFAULT&lt;/code>
section:&lt;/p>
&lt;pre>&lt;code>compute_driver=novadocker.virt.docker.DockerDriver
&lt;/code>&lt;/pre>
&lt;p>If there is already a line setting &lt;code>compute_driver&lt;/code>, then comment it
out or delete before adding the new one.&lt;/p>
&lt;p>Modify the Glance configuration to permit storage of Docker images.
Edit &lt;code>/etc/glance/glance-api.conf&lt;/code>, and add the following line to the
&lt;code>DEFAULT&lt;/code> section:&lt;/p>
&lt;pre>&lt;code>container_formats=ami,ari,aki,bare,ovf,ova,docker
&lt;/code>&lt;/pre>
&lt;p>Next, we need to augment the &lt;code>rootwrap&lt;/code> configuration such that
&lt;code>nova-docker&lt;/code> is able run the &lt;code>ln&lt;/code> command with &lt;code>root&lt;/code> privileges.
We&amp;rsquo;ll install the &lt;a href="https://github.com/stackforge/nova-docker/blob/master/etc/nova/rootwrap.d/docker.filters">docker.filters&lt;/a> file from the &lt;code>nova-docker&lt;/code>
source:&lt;/p>
&lt;pre>&lt;code># mkdir -p /etc/nova/rootwrap.d
# cp etc/nova/rootwrap.d/docker.filters \
/etc/nova/rootwrap.d/docker.filters
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ve changed a number of configuration files, so we should restart
the affected services:&lt;/p>
&lt;pre>&lt;code># openstack-service restart nova glance
&lt;/code>&lt;/pre>
&lt;h2 id="testing-things-out">Testing things out&lt;/h2>
&lt;p>Let&amp;rsquo;s try starting a container! We need to select one that will run
in the &lt;code>nova-docker&lt;/code> environment. Generally, that means one that does
not expect to have an interactive terminal and that will automatically
start some sort of web-accessible service. I have a &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">minimal thttpd
container&lt;/a> that fits the bill nicely:&lt;/p>
&lt;pre>&lt;code># docker pull larsks/thttpd
&lt;/code>&lt;/pre>
&lt;p>We need to store this image into Glance using the same name:&lt;/p>
&lt;pre>&lt;code># docker save larsks/thttpd |
glance image-create --name larsks/thttpd \
--container-format docker --disk-format raw --is-public true
&lt;/code>&lt;/pre>
&lt;p>And now we should be able to start a container:&lt;/p>
&lt;pre>&lt;code># nova boot --image larsks/thttpd --flavor m1.tiny test0
&lt;/code>&lt;/pre>
&lt;p>After a bit, &lt;code>nova list&lt;/code> should show:&lt;/p>
&lt;pre>&lt;code>+------...+-------+--------+...+------------------+
| ID ...| Name | Status |...| Networks |
+------...+-------+--------+...+------------------+
| 430a1...| test0 | ACTIVE |...| private=10.0.0.6 |
+------...+-------+--------+...+------------------+
&lt;/code>&lt;/pre>
&lt;p>And we should also see the container if we run &lt;code>docker ps&lt;/code>:&lt;/p>
&lt;pre>&lt;code># docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
ee864da30cf1 larsks/thttpd:latest &amp;quot;/thttpd -D -l /dev/ 7 hours ago Up 7 hours nova-430a197e-a0ca-4e72-a7db-1969d0773cf7
&lt;/code>&lt;/pre>
&lt;h2 id="getting-connected">Getting connected&lt;/h2>
&lt;p>At this point, the container will &lt;em>not&lt;/em> be network accessible; it&amp;rsquo;s
attached to a private tenant network. Let&amp;rsquo;s assign it a floating ip
address:&lt;/p>
&lt;pre>&lt;code># nova floating-ip-create public
+--------------+-----------+----------+--------+
| Ip | Server Id | Fixed Ip | Pool |
+--------------+-----------+----------+--------+
| 172.24.4.229 | - | - | public |
+--------------+-----------+----------+--------+
# nova floating-ip-associate test0 172.24.4.229
&lt;/code>&lt;/pre>
&lt;p>This isn&amp;rsquo;t going to be immediately accessible because Packstack left
us without a route to the floating ip network. We can fix that
temporarily like this:&lt;/p>
&lt;pre>&lt;code># ip addr add 172.24.4.225/28 dev br-ex
&lt;/code>&lt;/pre>
&lt;p>And now we can ping our Docker container:&lt;/p>
&lt;pre>&lt;code># ping -c2 172.24.4.229
PING 172.24.4.229 (172.24.4.229) 56(84) bytes of data.
64 bytes from 172.24.4.229: icmp_seq=1 ttl=63 time=0.291 ms
64 bytes from 172.24.4.229: icmp_seq=2 ttl=63 time=0.074 ms
--- 172.24.4.229 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.074/0.182/0.291/0.109 ms
&lt;/code>&lt;/pre>
&lt;p>And access the webserver:&lt;/p>
&lt;pre>&lt;code># curl http://172.24.4.229
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Your web server is working&amp;lt;/title&amp;gt;
.
.
.&lt;/code>&lt;/pre></content></item><item><title>Creating minimal Docker images from dynamically linked ELF binaries</title><link>https://blog.oddbit.com/post/2015-02-05-creating-minimal-docker-images/</link><pubDate>Thu, 05 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-05-creating-minimal-docker-images/</guid><description>In this post, we&amp;rsquo;ll look at a method for building minimal Docker images for dynamically linked ELF binaries, and then at a tool for automating this process.
It is tempting, when creating a simple Docker image, to start with one of the images provided by the major distributions. For example, if you need an image that provides tcpdump for use on your Atomic host, you might do something like:
FROM fedora RUN yum -y install tcpdump And while this will work, you end up consuming 250MB for tcpdump.</description><content>&lt;p>In this post, we&amp;rsquo;ll look at a method for building minimal Docker
images for dynamically linked ELF binaries, and then at &lt;a href="https://github.com/larsks/dockerize">a
tool&lt;/a> for automating this process.&lt;/p>
&lt;p>It is tempting, when creating a simple Docker image, to start with one
of the images provided by the major distributions. For example, if
you need an image that provides &lt;code>tcpdump&lt;/code> for use on your &lt;a href="http://www.projectatomic.io/">Atomic&lt;/a>
host, you might do something like:&lt;/p>
&lt;pre>&lt;code>FROM fedora
RUN yum -y install tcpdump
&lt;/code>&lt;/pre>
&lt;p>And while this will work, you end up consuming 250MB for &lt;code>tcpdump&lt;/code>.
In theory, the layering mechanism that Docker uses to build images
will reduce the practical impact of this (because other images based on
the &lt;code>fedora&lt;/code> image will share the common layers), but in practice the
size is noticeable, especially if you often find yourself pulling this
image into a fresh environment with no established cache.&lt;/p>
&lt;p>You can substantially reduce the space requirements for a Docker image
by including only those things that are absolutely necessary. For
statically linked files, that may only be the binary itself, but the
situation is a little more complex for dynamically linked executables.
You might naively start with this (assuming that you had the &lt;code>tcpdump&lt;/code>
binary in your local directory):&lt;/p>
&lt;pre>&lt;code>FROM scratch
COPY tcpdump /usr/sbin/tcpdump
&lt;/code>&lt;/pre>
&lt;p>If you were to build an image with this and tag it &lt;code>tcpdump&lt;/code>&amp;hellip;&lt;/p>
&lt;pre>&lt;code>docker build -t tcpdump .
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then try running it:&lt;/p>
&lt;pre>&lt;code>docker run tcpdump
&lt;/code>&lt;/pre>
&lt;p>You would immediately see:&lt;/p>
&lt;pre>&lt;code>no such file or directory
FATA[0003] Error response from daemon: Cannot start container ...:
no such file or directory
&lt;/code>&lt;/pre>
&lt;p>And this is because the image is missing two things:&lt;/p>
&lt;ul>
&lt;li>The Linux dynamic runtime loader, and&lt;/li>
&lt;li>The shared libraries required by the &lt;code>tcpdump&lt;/code> binary&lt;/li>
&lt;/ul>
&lt;p>The path to the appropriate loader is stored in the ELF binary in the
&lt;code>.interp&lt;/code> section, which we can inspect using the &lt;code>objdump&lt;/code> tool:&lt;/p>
&lt;pre>&lt;code>$ objdump -s -j .interp tcpdump
tcpdump: file format elf64-x86-64
Contents of section .interp:
400238 2f6c6962 36342f6c 642d6c69 6e75782d /lib64/ld-linux-
400248 7838362d 36342e73 6f2e3200 x86-64.so.2.
&lt;/code>&lt;/pre>
&lt;p>Which tells us we need &lt;code>/lib64/ld-linux-x86-64.so.2&lt;/code>.&lt;/p>
&lt;p>We can use the &lt;code>ldd&lt;/code> tool to get a list of shared libraries required
by the binary:&lt;/p>
&lt;pre>&lt;code>$ ldd tcpdump
linux-vdso.so.1 =&amp;gt; (0x00007fffed1fe000)
libcrypto.so.10 =&amp;gt; /lib64/libcrypto.so.10 (0x00007fb2c05a3000)
libpcap.so.1 =&amp;gt; /lib64/libpcap.so.1 (0x00007fb2c0361000)
libc.so.6 =&amp;gt; /lib64/libc.so.6 (0x00007fb2bffa3000)
libdl.so.2 =&amp;gt; /lib64/libdl.so.2 (0x00007fb2bfd9f000)
libz.so.1 =&amp;gt; /lib64/libz.so.1 (0x00007fb2bfb89000)
/lib64/ld-linux-x86-64.so.2 (0x00007fb2c09b7000)
&lt;/code>&lt;/pre>
&lt;p>If we copy all of the dependencies into a local directory, along with
the &lt;code>tcpdump&lt;/code> binary itself, and use the following layout:&lt;/p>
&lt;pre>&lt;code>Dockerfile
usr/sbin/tcpdump
lib64/libcrypto.so.10
lib64/libpcap.so.1
lib64/libc.so.6
lib64/libdl.so.2
lib64/libz.so.1
lib64/ld-linux-x86-64.so.2
&lt;/code>&lt;/pre>
&lt;p>And the following Dockerfile content:&lt;/p>
&lt;pre>&lt;code>FROM scratch
COPY . /
ENTRYPOINT [&amp;quot;/usr/sbin/tcpdump&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>And then we turn this into a Docker image and run it, we get:&lt;/p>
&lt;pre>&lt;code>$ docker build -t tcpdump .
[...]
$ docker run tcpdump tcpdump -i eth0 -n
tcpdump: Couldn't find user 'tcpdump'
&lt;/code>&lt;/pre>
&lt;p>Well, so let&amp;rsquo;s create an &lt;code>/etc/passwd&lt;/code> file with the &lt;code>tcpdump&lt;/code> user
and add that to our collection:&lt;/p>
&lt;pre>&lt;code>$ mkdir etc
$ grep tcpdump /etc/passwd &amp;gt; etc/passwd
$ grep tcpdump /etc/group &amp;gt; etc/group
$ docker build -t tcpdump .
$ docker run tcpdump tcpdump -i eth0 -n
tcpdump: Couldn't find user 'tcpdump'
&lt;/code>&lt;/pre>
&lt;p>And &lt;em>this&lt;/em> is because most programs don&amp;rsquo;t reference files like
&lt;code>/etc/passwd&lt;/code> directly, but instead delegate this task to the C
library, which relies on the &lt;a href="http://www.gnu.org/software/libc/manual/html_node/Name-Service-Switch.html">name service switch&lt;/a> (nss)
mechanism to support multiple sources of information. Let&amp;rsquo;s add the
nss libraries necessary for supporting legacy files (&lt;code>/etc/passwd&lt;/code>,
etc) and DNS for hostname lookups:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>lib64/libnss_files.so.2&lt;/code> &amp;ndash; this includes support the traditional
files in &lt;code>/etc&lt;/code>, such as &lt;code>/etc/passwd&lt;/code>, &lt;code>/etc/group&lt;/code>, and
&lt;code>/etc/hosts&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>lib64/libnss_dns.so.2&lt;/code> &amp;ndash; this supports hostname resolution via
dns.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>And we&amp;rsquo;ll also need &lt;code>/etc/nsswitch.conf&lt;/code> to go along with that:&lt;/p>
&lt;pre>&lt;code>passwd: files
shadow: files
group: files
hosts: files dns
&lt;/code>&lt;/pre>
&lt;p>After all this, we have:&lt;/p>
&lt;pre>&lt;code>Dockerfile
usr/sbin/tcpdump
lib64/libcrypto.so.10
lib64/libpcap.so.1
lib64/libc.so.6
lib64/libdl.so.2
lib64/libz.so.1
lib64/ld-linux-x86-64.so.2
lib64/libnss_files.so.2
lib64/libnss_dns.so.2
etc/passwd
etc/group
etc/nsswitch.conf
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s rebuild the image and run it one more time:&lt;/p>
&lt;pre>&lt;code>$ docker build -t tcpdump .
$ docker run tcpdump -i eth0 -n
&lt;/code>&lt;/pre>
&lt;p>And now, finally it runs. Wouldn&amp;rsquo;t it be nice if that process were
easier?&lt;/p>
&lt;h2 id="introducing-dockerize">Introducing Dockerize&lt;/h2>
&lt;p>&lt;a href="https://github.com/larsks/dockerize">Dockerize&lt;/a> is a tool that largely automates the above process. To
build a minimal &lt;code>tcpdump&lt;/code> image, for example, you would run:&lt;/p>
&lt;pre>&lt;code>$ dockerize -u tcpdump -t tcpdump /usr/sbin/tcpdump
&lt;/code>&lt;/pre>
&lt;p>This would include the &lt;code>tcpdump&lt;/code> user (&lt;code>-u tcpdump&lt;/code>) from your local
system, as well as &lt;code>/usr/sbin/tcpdump&lt;/code>, all it&amp;rsquo;s dependencies, and
file-based nss support, and build an image tagged &lt;code>tcpdump&lt;/code> (&lt;code>-t tcpdump&lt;/code>). When you build an image from a single command, like this,
Dockerize will up that command as the Docker &lt;code>ENTRYPOINT&lt;/code>, so you can
run it like this:&lt;/p>
&lt;pre>&lt;code>$ docker run tcpdump -i eth0 -n
&lt;/code>&lt;/pre>
&lt;p>You can also build images containing multiple binaries. For example:&lt;/p>
&lt;pre>&lt;code>$ dockerize -t dockerizeme/xmltools \
/usr/bin/xmllint \
/usr/bin/xml2 \
/usr/bin/2xml \
/usr/bin/tidyp
&lt;/code>&lt;/pre>
&lt;p>In this case, you need to provide the command name when running the
image:&lt;/p>
&lt;pre>&lt;code>$ docker run dockerizeme/xmltools tidyp -h
&lt;/code>&lt;/pre>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>You can find some scripts that generate marginally useful example
images in the &lt;a href="https://github.com/larsks/dockerize/tree/master/examples">examples&lt;/a> folder of the repository.&lt;/p>
&lt;p>These are pushed into the &lt;a href="https://hub.docker.com/u/dockerizeme/">dockerizeme&lt;/a> namespace on the Docker hub,
so you can, for example, get yourself a minimal webserver by running:&lt;/p>
&lt;pre>&lt;code>$ docker run -v $PWD:/content -p 8888:80 dockerizeme/thttpd -d /content
&lt;/code>&lt;/pre>
&lt;p>And then browse to &lt;a href="http://localhost:8888">http://localhost:8888&lt;/a> and see your current
directory.&lt;/p></content></item><item><title>Docker vs. PrivateTmp</title><link>https://blog.oddbit.com/post/2015-01-18-docker-vs-privatetmp/</link><pubDate>Sun, 18 Jan 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-01-18-docker-vs-privatetmp/</guid><description>While working with Docker the other day, I ran into an undesirable interaction between Docker and systemd services that utilize the PrivateTmp directive.
The PrivateTmp directive, if true, &amp;ldquo;sets up a new file system namespace for the executed processes and mounts private /tmp and /var/tmp directories inside it that is not shared by processes outside of the namespace&amp;rdquo;. This is a great idea from a security perspective, but can cause some unanticipated consequences.</description><content>&lt;p>While working with Docker &lt;a href="https://blog.oddbit.com/post/2015-01-17-running-novalibvirt-and-novado/">the other day&lt;/a>, I ran into an
undesirable interaction between Docker and &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a> services that
utilize the &lt;code>PrivateTmp&lt;/code> directive.&lt;/p>
&lt;p>The &lt;a href="http://www.freedesktop.org/software/systemd/man/systemd.exec.html#PrivateTmp=">PrivateTmp&lt;/a> directive, if &lt;code>true&lt;/code>, &amp;ldquo;sets up a new file system
namespace for the executed processes and mounts private &lt;code>/tmp&lt;/code> and
&lt;code>/var/tmp&lt;/code> directories inside it that is not shared by processes outside
of the namespace&amp;rdquo;. This is a great idea from a &lt;a href="https://danwalsh.livejournal.com/51459.html">security
perspective&lt;/a>, but can cause some unanticipated consequences.&lt;/p>
&lt;h2 id="the-problem-in-a-nutshell">The problem in a nutshell&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Start a Docker container:&lt;/p>
&lt;pre>&lt;code> # cid=$(docker run -d larsks/thttpd)
# echo $cid
e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>See the &lt;code>devicemapper&lt;/code> mountpoint created by Docker for the
container:&lt;/p>
&lt;pre>&lt;code> # grep devicemapper/mnt /proc/mounts
/dev/mapper/docker-253:6-98310-e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 /var/lib/docker/devicemapper/mnt/e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 ext4 rw,context=&amp;quot;system_u:object_r:svirt_sandbox_file_t:s0:c261,c1018&amp;quot;,relatime,discard,stripe=16,data=ordered 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Now restart a service &amp;ndash; any service! &amp;ndash; that has
&lt;code>PrivateTmp=true&lt;/code>:&lt;/p>
&lt;pre>&lt;code> # systemctl restart systemd-machined
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Get the PID for that service:&lt;/p>
&lt;pre>&lt;code> # systemctl status systemd-machined | grep PID
Main PID: 18698 (systemd-machine
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>And see that the mount created by the Docker &amp;ldquo;devicemapper&amp;rdquo; storage
driver is visible inside the mount namespace for this process:&lt;/p>
&lt;pre>&lt;code> # grep devicemapper/mnt /proc/18698/mounts
/dev/mapper/docker-253:6-98310-e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 /var/lib/docker/devicemapper/mnt/e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 ext4 rw,context=&amp;quot;system_u:object_r:svirt_sandbox_file_t:s0:c261,c1018&amp;quot;,relatime,discard,stripe=16,data=ordered 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Attempt to destroy the container:&lt;/p>
&lt;pre>&lt;code> # docker rm -f $cid
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Watch Docker fail to destroy the container because it is unable to
remove the mountpoint directory:&lt;/p>
&lt;pre>&lt;code> Jan 17 22:43:03 pk115wp-lkellogg docker-1.4.1-dev[18239]:
time=&amp;quot;2015-01-17T22:43:03-05:00&amp;quot; level=&amp;quot;error&amp;quot; msg=&amp;quot;Handler for DELETE
/containers/{name:.*} returned error: Cannot destroy container e68df3f45d61:
Driver devicemapper failed to remove root filesystem
e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62: Device is
Busy&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Because while that mount is gone from the global namespace:&lt;/p>
&lt;pre>&lt;code> # grep devicemapper/mnt /proc/mounts
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>It still exists inside the mount namespace for the service we restarted:&lt;/p>
&lt;pre>&lt;code># grep devicemapper/mnt /proc/18698/mounts
/dev/mapper/docker-253:6-98310-e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 /var/lib/docker/devicemapper/mnt/e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 ext4 rw,context=&amp;quot;system_u:object_r:svirt_sandbox_file_t:s0:c261,c1018&amp;quot;,relatime,discard,stripe=16,data=ordered 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>To resolve this problem, restart the service holding the mount open:&lt;/p>
&lt;pre>&lt;code># systemctl restart systemd-machined
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;p>Now the mountpoint can be deleted.&lt;/p>
&lt;h2 id="its-not-just-docker">It&amp;rsquo;s not just Docker&lt;/h2>
&lt;p>While I ran into this problem while working with Docker, there is
nothing particularly Docker-specific about the problem. You can
replicate this behavior by hand without involving either &lt;code>systemd&lt;/code> or
Docker:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create a parent mountpoint, and make it private:&lt;/p>
&lt;pre>&lt;code> # mkdir /tmp/parent /tmp/parent-backing
# mount --bind --make-private /tmp/parent-backing /tmp/parent
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Create a private mount on a directory &lt;em>inside&lt;/em> &lt;code>/tmp/parent&lt;/code>:&lt;/p>
&lt;pre>&lt;code> # mkdir /tmp/testmount /tmp/parent/mnt
# mount --bind --make-private /tmp/testmount /tmp/parent/mnt
# grep /tmp/parent/mnt /proc/self/mounts
tmpfs /tmp/parent/mnt tmpfs rw,seclabel 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>In another window, create a new mount namespace using &lt;code>unshare&lt;/code>:&lt;/p>
&lt;pre>&lt;code> # unshare -m env PS1='unshare# ' bash
unshare#
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Unmount &lt;code>/tmp/parent/mnt&lt;/code> in the global namespace:&lt;/p>
&lt;pre>&lt;code> # umount /tmp/parent/mnt
# grep /tmp/parent/mnt /proc/self/mounts
#
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Try to delete the mountpoint directory:&lt;/p>
&lt;pre>&lt;code> # rmdir /tmp/parent/mnt
rmdir: failed to remove /tmp/parent/mnt: Device or resource busy
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>See that the mount still exists in your &lt;code>unshare&lt;/code> namespace:&lt;/p>
&lt;pre>&lt;code> unshare# grep /tmp/parent/mnt /proc/self/mounts
tmpfs /tmp/parent/mnt tmpfs rw,seclabel 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="so-whats-going-on-here">So what&amp;rsquo;s going on here?&lt;/h2>
&lt;p>To understand what&amp;rsquo;s going on in these examples, you probably want to
start by at least glancing through the &lt;a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">sharedsubtree.txt&lt;/a> kernel
documentation.&lt;/p>
&lt;p>The Docker &lt;code>devicemapper&lt;/code> driver creates a &lt;em>private&lt;/em> mount on
&lt;code>/var/lib/docker/devicemapper&lt;/code>. A &lt;em>private&lt;/em> mount is one that does
not propagate mount operations between parent and child mount
namespaces.&lt;/p>
&lt;p>Container filesystems are mounted underneath
&lt;code>/var/lib/docker/devicemapper/mnt&lt;/code>, e.g:&lt;/p>
&lt;pre>&lt;code> /dev/mapper/docker-253:6-98310-e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 /var/lib/docker/devicemapper/mnt/e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 ext4 rw,context=&amp;quot;system_u:object_r:svirt_sandbox_file_t:s0:c261,c1018&amp;quot;,relatime,discard,stripe=16,data=ordered 0 0
&lt;/code>&lt;/pre>
&lt;p>When you create a new mount namespace as a child of the global mount
namespace, either via the &lt;code>unshare&lt;/code> command or by starting a systemd
service with &lt;code>PrivateTmp=true&lt;/code>, it inherits these private mounts.
When Docker unmounts the the container filesystem in the global
namespace, the fact that the &lt;code>/var/lib/docker/devicemapper&lt;/code> mountpoint
is marked &lt;em>private&lt;/em> means that the unmount operation does not
propagate to other namespaces.&lt;/p>
&lt;h2 id="the-solution">The solution&lt;/h2>
&lt;p>The simplest solution to this problem is to set the &lt;code>MountFlags=slave&lt;/code>
option in the &lt;code>docker.service&lt;/code> file:&lt;/p>
&lt;pre>&lt;code>MountFlags=slave
&lt;/code>&lt;/pre>
&lt;p>This will cause SystemD to run Docker in a cloned mount namespace and
sets the &lt;code>MS_SLAVE&lt;/code> flag on all mountpoints; it is effectively
equivalent to:&lt;/p>
&lt;pre>&lt;code># unshare -m
# mount --make-rslave /
&lt;/code>&lt;/pre>
&lt;p>With this change, mounts performed by Docker will not be visible in
the global mount namespace, and they will thus not propagate into the
mount namespaces of other services.&lt;/p>
&lt;h2 id="not-necessarily-the-solution">Not necessarily the solution&lt;/h2>
&lt;p>There was an &lt;a href="http://pkgs.fedoraproject.org/cgit/docker-io.git/commit/?id=6c9e373ee06cb1aee07d3cae426c46002663010d">attempt to fix this problem&lt;/a> committed to the Fedora
&lt;code>docker-io&lt;/code> package that set &lt;code>MountFlags=private&lt;/code>. This will prevent
the symptoms I originally encountered, in which Docker is unable to
remove a mountpoint because it is still held open by another mount
namespace&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;but it will result in behavior that might be confusing to a system
administrator. Specifically, mounts made in the global mount
namespace after Docker starts will not be visible to Docker
containers. This means that if you were to make a remote filesystem
available on your Docker host:&lt;/p>
&lt;pre>&lt;code># mount my-fileserver:/vol/webcontent /srv/content
&lt;/code>&lt;/pre>
&lt;p>And then attempt to bind that into a Docker container as a volume:&lt;/p>
&lt;pre>&lt;code># docker run -v /srv/content:/content larsks/thttpd -d /content
&lt;/code>&lt;/pre>
&lt;p>Your content would not be visible. The mount of
&lt;code>my-fileserver:/vol/webcontent&lt;/code> would not propagate from the global
namespace into the Docker mount namespace because of the &lt;em>private&lt;/em>
flag.&lt;/p>
&lt;h2 id="thanks">Thanks&lt;/h2>
&lt;p>I had some help figuring this out. Thanks to &lt;a href="https://en.wikipedia.org/wiki/Lennart_Poettering">Lennart Poettering&lt;/a>,
Andrey Borzenkov, and &lt;a href="http://blog.verbum.org/">Colin Walters&lt;/a>.&lt;/p></content></item><item><title>Running nova-libvirt and nova-docker on the same host</title><link>https://blog.oddbit.com/post/2015-01-17-running-novalibvirt-and-novado/</link><pubDate>Sat, 17 Jan 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-01-17-running-novalibvirt-and-novado/</guid><description>I regularly use OpenStack on my laptop with libvirt as my hypervisor. I was interested in experimenting with recent versions of the nova-docker driver, but I didn&amp;rsquo;t have a spare system available on which to run the driver, and I use my regular nova-compute service often enough that I didn&amp;rsquo;t want to simply disable it temporarily in favor of nova-docker.
NB As pointed out by gustavo in the comments, running two neutron-openvswitch-agents on the same host &amp;ndash; as suggested in this article &amp;ndash; is going to lead to nothing but sadness and doom.</description><content>&lt;p>I regularly use &lt;a href="http://www.openstack.org/">OpenStack&lt;/a> on my laptop with &lt;a href="http://www.libvirt.org/">libvirt&lt;/a> as my
hypervisor. I was interested in experimenting with recent versions of
the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> driver, but I didn&amp;rsquo;t have a spare system available
on which to run the driver, and I use my regular &lt;code>nova-compute&lt;/code> service
often enough that I didn&amp;rsquo;t want to simply disable it temporarily in
favor of &lt;code>nova-docker&lt;/code>.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong> As pointed out by &lt;em>gustavo&lt;/em> in the comments, running two
&lt;code>neutron-openvswitch-agents&lt;/code> on the same host &amp;ndash; as suggested in this
article &amp;ndash; is going to lead to nothing but sadness and doom. So
kids, don&amp;rsquo;t try this at home. I&amp;rsquo;m leaving the article here because I
think it still has some interesting bits.&lt;/p>
&lt;hr>
&lt;p>I guess the simplest solution would be to spin up a vm on which to run
&lt;code>nova-docker&lt;/code>, but why use a simple solution when there are things to
be learned? I wanted to know if it were possible (and if so, how) to
run both hypervisors on the same physical host.&lt;/p>
&lt;p>The naive solution would be to start up another instance of
&lt;code>nova-compute&lt;/code> configured to use the Docker driver. Unfortunately,
Nova only permits a single service instance per &amp;ldquo;host&amp;rdquo;, so starting up
the second instance of &lt;code>nova-compute&lt;/code> would effectively &amp;ldquo;mask&amp;rdquo; the
original one.&lt;/p>
&lt;p>Fortunately, Nova&amp;rsquo;s definition of what constitutes a &amp;ldquo;host&amp;rdquo; is
somewhat flexible. Nova supports a &lt;code>host&lt;/code> configuration key in
&lt;code>nova.conf&lt;/code> that will cause Nova to identify the host on which it is
running using your explicitly configured value, rather than your
system hostname. We can take advantage of this to get a second
&lt;code>nova-compute&lt;/code> instance running on the same system.&lt;/p>
&lt;h2 id="install-nova-docker">Install nova-docker&lt;/h2>
&lt;p>We&amp;rsquo;ll start by installing the &lt;code>nova-docker&lt;/code> driver from
&lt;a href="https://github.com/stackforge/nova-docker">https://github.com/stackforge/nova-docker&lt;/a>. If you&amp;rsquo;re running the
Juno release of OpenStack (which I am), you&amp;rsquo;re going to want to use
the &lt;code>stable/juno&lt;/code> branch of the &lt;code>nova-docker&lt;/code> repository. So:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/stackforge/nova-docker
$ cd nova-docker
$ git checkout stable/juno
$ sudo python setup.py install
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll want to read the project&amp;rsquo;s &lt;a href="https://github.com/stackforge/nova-docker/blob/master/README.rst">README&lt;/a> for complete installation
instructions.&lt;/p>
&lt;h2 id="configure-nova-docker">Configure nova-docker&lt;/h2>
&lt;p>Now, rather than configuring &lt;code>/etc/nova/nova.conf&lt;/code>, we&amp;rsquo;re going to
create a new configuration file, &lt;code>/etc/nova/nova-docker.conf&lt;/code>, with
only the configuration keys that differ from our primary Nova
configuration:&lt;/p>
&lt;pre>&lt;code>[DEFAULT]
host=nova-docker
compute_driver=novadocker.virt.docker.DockerDriver
log_file=/var/log/nova/nova-docker.log
state_path=/var/lib/nova-docker
&lt;/code>&lt;/pre>
&lt;p>You can see that we&amp;rsquo;ve set the value of &lt;code>host&lt;/code> to &lt;code>nova-docker&lt;/code>, to
differentiate this &lt;code>nova-compute&lt;/code> service from the &lt;code>libvirt&lt;/code>-backed
one that is already running. We&amp;rsquo;ve provided the service with a
dedicated log file and state directory to prevent conflicts with the
already-running &lt;code>nova-compute&lt;/code> service.&lt;/p>
&lt;p>To use this configuration file, we&amp;rsquo;ll launch a new instance of the
&lt;code>nova-compute&lt;/code> service pointing at both the original configuration
file, &lt;code>/etc/nova/nova.conf&lt;/code>, as well as this &lt;code>nova-docker&lt;/code>
configuration file. The command line would look something like:&lt;/p>
&lt;pre>&lt;code>nova-compute --config-file /etc/nova/nova.conf \
--config-file /etc/nova/nova-docker.conf
&lt;/code>&lt;/pre>
&lt;p>The ordering of configuration files on the command line is
significant: later configuration files will override values from
earlier files.&lt;/p>
&lt;p>I&amp;rsquo;m running &lt;a href="http://www.fedora.org/">Fedora&lt;/a> 21 on my laptop, which uses &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a>, so I
created a modified version of the &lt;code>openstack-nova-compute.service&lt;/code>
unit on my system, and saved it as
&lt;code>/etc/systemd/system/openstack-nova-docker.service&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=OpenStack Nova Compute Server (Docker)
After=syslog.target network.target
[Service]
Environment=LIBGUESTFS_ATTACH_METHOD=appliance
Type=notify
Restart=always
User=nova
ExecStart=/usr/bin/nova-compute --config-file /etc/nova/nova.conf --config-file /etc/nova/nova-docker.conf
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;p>And then activated the service;&lt;/p>
&lt;pre>&lt;code># systemctl enable openstack-nova-docker
# systemctl start openstack-nova-docker
&lt;/code>&lt;/pre>
&lt;p>Now, if I run &lt;code>nova service-list&lt;/code> with administrator credentials, I
can see both &lt;code>nova-compute&lt;/code> instances:&lt;/p>
&lt;pre>&lt;code>+----+------------------+------------------+----------+---------+-------...
| Id | Binary | Host | Zone | Status | State ...
+----+------------------+------------------+----------+---------+-------...
| 1 | nova-consoleauth | host.example.com | internal | enabled | up ...
| 2 | nova-scheduler | host.example.com | internal | enabled | up ...
| 3 | nova-conductor | host.example.com | internal | enabled | up ...
| 5 | nova-cert | host.example.com | internal | enabled | up ...
| 6 | nova-compute | host.example.com | nova | enabled | up ...
| 7 | nova-compute | nova-docker | nova | enabled | up ...
+----+------------------+------------------+----------+---------+-------...
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-docker-container-take-1">Booting a Docker container (take 1)&lt;/h2>
&lt;p>Let&amp;rsquo;s try starting a Docker container using the new &lt;code>nova-compute&lt;/code>
service. We&amp;rsquo;ll first need to load a Docker image into Glance (you
followed the &lt;code>nova-docker&lt;/code> &lt;a href="https://github.com/stackforge/nova-docker#1-enable-the-driver-in-glances-configuration">instructions for configuring
Glance&lt;/a>, right?). We&amp;rsquo;ll use my &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">larsks/thttpd&lt;/a> image,
because it&amp;rsquo;s very small and doesn&amp;rsquo;t require any configuration:&lt;/p>
&lt;pre>&lt;code>$ docker pull larsks/thttpd
$ docker save larsks/thttpd |
glance image-create --is-public True --container-format docker \
--disk-format raw --name larsks/thttpd
&lt;/code>&lt;/pre>
&lt;p>(Note that you will probably require administrative credentials to load
this image into Glance.)&lt;/p>
&lt;p>Now that we have an appropriate image available we can try booting a container:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image larsks/thttpd --flavor m1.small test1
&lt;/code>&lt;/pre>
&lt;p>If we wait a moment and then run &lt;code>nova list&lt;/code>, we see:&lt;/p>
&lt;pre>&lt;code>| 9a783952-a888-4fcd-8f5d-cd9291ed1969 | test1 | ERROR | spawning ...
&lt;/code>&lt;/pre>
&lt;p>&lt;a href="http://www.sadtrombone.com/">What happened?&lt;/a> Looking at the appropriate log file
(&lt;code>/var/log/nova/nova-docker.log&lt;/code>), we find:&lt;/p>
&lt;pre>&lt;code>Cannot setup network: Unexpected vif_type=binding_failed
Traceback (most recent call last):
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/driver.py&amp;quot;, line 367, in _start_container
self.plug_vifs(instance, network_info)
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/driver.py&amp;quot;, line 187, in plug_vifs
self.vif_driver.plug(instance, vif)
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/vifs.py&amp;quot;, line 63, in plug
_(&amp;quot;Unexpected vif_type=%s&amp;quot;) % vif_type)
NovaException: Unexpected vif_type=binding_failed
&lt;/code>&lt;/pre>
&lt;p>The message &lt;code>vif_type=binding_failed&lt;/code> is Nova&amp;rsquo;s way of saying &amp;ldquo;I have
no idea what happened, go ask Neutron&amp;rdquo;. Looking in Neutron&amp;rsquo;s
&lt;code>/var/log/neutron/server.log&lt;/code>, we find:&lt;/p>
&lt;pre>&lt;code>Failed to bind port 82c07caa-b2c2-45e9-955d-e8b35112437c on host
nova-docker
&lt;/code>&lt;/pre>
&lt;p>And this tells us our problem: we have told our &lt;code>nova-docker&lt;/code> service
that it is running on a host called &amp;ldquo;nova-docker&amp;rdquo;, and Neutron doesn&amp;rsquo;t
know anything about that host.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong>&lt;/p>
&lt;p>If you were to try to delete this failed instance, you would find that
it is un-deletable. In the end, I was only able to delete it by
directly editing the &lt;code>nova&lt;/code> database using &lt;a href="https://blog.oddbit.com/assets/2015/01/17/delete-deleting-instances.sql">this sql script&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="adding-a-neutron-agent">Adding a Neutron agent&lt;/h2>
&lt;p>We&amp;rsquo;re going to need to set up an instance of
&lt;code>neutron-openvswitch-agent&lt;/code> to service network requests on our
&amp;ldquo;nova-docker&amp;rdquo; host. Like Nova, Neutron also supports a &lt;code>host&lt;/code>
configuration key, so we&amp;rsquo;re going to pursue a solution similar to what
we used with Nova by creating a new configuration file,
&lt;code>/etc/neutron/ovs-docker.conf&lt;/code>, with the following content:&lt;/p>
&lt;pre>&lt;code>[DEFAULT]
host = nova-docker
&lt;/code>&lt;/pre>
&lt;p>And then we&amp;rsquo;ll set up the corresponding service by dropping the
following into &lt;code>/etc/systemd/system/docker-openvswitch-agent.service&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=OpenStack Neutron Open vSwitch Agent (Docker)
After=syslog.target network.target
[Service]
Type=simple
User=neutron
ExecStart=/usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini --config-file /etc/neutron/ovs-docker.conf --log-file /var/log/neutron/docker-openvswitch-agent.log
PrivateTmp=true
KillMode=process
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong>&lt;/p>
&lt;p>While working on this configuration I ran into an undesirable
interaction between Docker and &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a>&amp;rsquo;s &lt;code>PrivateTmp&lt;/code> directive.&lt;/p>
&lt;p>This directive causes the service to run with
a private &lt;a href="http://lwn.net/Articles/531114/">mount namespace&lt;/a> such that &lt;code>/tmp&lt;/code> for the service is not
the same as &lt;code>/tmp&lt;/code> for other services. This is a great idea from a
security perspective, but can cause problems in the following
scenario:&lt;/p>
&lt;ol>
&lt;li>Start a Docker container with &lt;code>nova boot ...&lt;/code>&lt;/li>
&lt;li>Restart any service that uses the &lt;code>PrivateTmp&lt;/code> directive&lt;/li>
&lt;li>Attempt to delete the Docker container with &lt;code>nova delete ...&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>Docker will fail to destroy the container because the private
namespace created by the &lt;code>PrivateTmp&lt;/code> directive preserves a reference
to the Docker &lt;code>devicemapper&lt;/code> mount in
&lt;code>/var/lib/docker/devicemapper/mnt/...&lt;/code> that was active at the time the
service was restarted. To recover from this situation, you will need
to restart whichever service is still holding a reference to the
Docker mounts.&lt;/p>
&lt;p>I have &lt;a href="http://lists.freedesktop.org/archives/systemd-devel/2015-January/027162.html">posted to the systemd-devel&lt;/a> mailing
list to see if there are any solutions to this behavior. As I note in
that email, this behavior appears to be identical to that described in
Fedora bug &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=851970">851970&lt;/a>, which was closed two years ago.&lt;/p>
&lt;p>&lt;strong>Update&lt;/strong> I wrote a &lt;a href="https://blog.oddbit.com/post/2015-01-18-docker-vs-privatetmp/">separate post&lt;/a> about this issue, which
includes some discussion about what&amp;rsquo;s going on and a solution.&lt;/p>
&lt;hr>
&lt;p>If we activate this service&amp;hellip;&lt;/p>
&lt;pre>&lt;code># systemctl enable docker-openvswitch-agent
# systemctl start docker-openvswitch-agent
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then run &lt;code>neutron agent-list&lt;/code> with administrative credentials,
we&amp;rsquo;ll see the new agent:&lt;/p>
&lt;pre>&lt;code>$ neutron agent-list
+--------------------------------------+--------------------+------------------+-------+...
| id | agent_type | host | alive |...
+--------------------------------------+--------------------+------------------+-------+...
| 2e40062a-1c30-46a3-8719-3ce93a56b4ce | Open vSwitch agent | nova-docker | :-) |...
| 63edb2a4-f980-4f88-b9c0-9610a1b20f13 | L3 agent | host.example.com | :-) |...
| 8482c5c3-208c-4145-9f7d-606be3da11ed | Loadbalancer agent | host.example.com | :-) |...
| 9922ed54-00fa-41d4-96e8-ac8af8c291fd | Open vSwitch agent | host.example.com | :-) |...
| b8becb9c-7290-42be-9faf-fd3baeea3dcf | Metadata agent | host.example.com | :-) |...
| c46be41b-e93a-40ab-a37e-4d67b770a3df | DHCP agent | host.example.com | :-) |...
+--------------------------------------+--------------------+------------------+-------+...
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-docker-container-take-2">Booting a Docker container (take 2)&lt;/h2>
&lt;p>Now that we have both the &lt;code>nova-docker&lt;/code> service running and a
corresponding &lt;code>neutron-openvswitch-agent&lt;/code> available, let&amp;rsquo;s try
starting our container one more time:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image larsks/thttpd --flavor m1.small test1
$ nova list
+--------------------------------------+---------+--------+...
| ID | Name | Status |...
+--------------------------------------+---------+--------+...
| 642b7d61-9189-40ea-86f5-2424c3c86028 | test1 | ACTIVE |...
+--------------------------------------+---------+--------+...
&lt;/code>&lt;/pre>
&lt;p>If we assign a floating IP address:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-create ext-nat
+-----------------+-----------+----------+---------+
| Ip | Server Id | Fixed Ip | Pool |
+-----------------+-----------+----------+---------+
| 192.168.200.211 | - | - | ext-nat |
+-----------------+-----------+----------+---------+
$ nova floating-ip-associate test1 192.168.200.211
&lt;/code>&lt;/pre>
&lt;p>We can then browse to &lt;code>http://192.168.200.211&lt;/code> and see the sample
page:&lt;/p>
&lt;pre>&lt;code>$ curl http://192.168.200.211/
.
.
.
____ _ _ _ _
/ ___|___ _ __ __ _ _ __ __ _| |_ _ _| | __ _| |_(_) ___ _ __ ___
| | / _ \| '_ \ / _` | '__/ _` | __| | | | |/ _` | __| |/ _ \| '_ \/ __|
| |__| (_) | | | | (_| | | | (_| | |_| |_| | | (_| | |_| | (_) | | | \__ \
\____\___/|_| |_|\__, |_| \__,_|\__|\__,_|_|\__,_|\__|_|\___/|_| |_|___/
|___/
.
.
.
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-libvirt-instance">Booting a libvirt instance&lt;/h2>
&lt;p>To show that we really are running two hypervisors on the same host,
we can launch a traditional &lt;code>libvirt&lt;/code> instance alongside our Docker
container:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image cirros --flavor m1.small --key-name lars test2
&lt;/code>&lt;/pre>
&lt;p>Wait a bit, then:&lt;/p>
&lt;pre>&lt;code>$ nova list
+--------------------------------------+-------+--------+...
| ID | Name | Status |...
+--------------------------------------+-------+--------+...
| 642b7d61-9189-40ea-86f5-2424c3c86028 | test1 | ACTIVE |...
| 7fec33c9-d50f-477e-957c-a05ee9bd0b0b | test2 | ACTIVE |...
+--------------------------------------+-------+--------+...&lt;/code>&lt;/pre></content></item><item><title>Building a minimal web server for testing Kubernetes</title><link>https://blog.oddbit.com/post/2015-01-04-building-a-minimal-web-server-/</link><pubDate>Sun, 04 Jan 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-01-04-building-a-minimal-web-server-/</guid><description>I have recently been doing some work with Kubernetes, and wanted to put together a minimal image with which I could test service and pod deployment. Size in this case was critical: I wanted something that would download quickly when initially deployed, because I am often setting up and tearing down Kubernetes as part of my testing (and some of my test environments have poor external bandwidth).
Building thttpd My go-to minimal webserver is thttpd.</description><content>&lt;p>I have recently been doing some work with &lt;a href="https://github.com/googlecloudplatform/kubernetes">Kubernetes&lt;/a>, and wanted
to put together a minimal image with which I could test service and
pod deployment. Size in this case was critical: I wanted something
that would download quickly when initially deployed, because I am
often setting up and tearing down Kubernetes as part of my testing
(and some of my test environments have poor external bandwidth).&lt;/p>
&lt;h2 id="building-thttpd">Building thttpd&lt;/h2>
&lt;p>My go-to minimal webserver is &lt;a href="http://acme.com/software/thttpd/">thttpd&lt;/a>. For the normal case,
building the software is a simple matter of &lt;code>./configure&lt;/code> followed by
&lt;code>make&lt;/code>. This gets you a dynamically linked binary; using &lt;code>ldd&lt;/code> you
could build a Docker image containing only the necessary shared
libraries:&lt;/p>
&lt;pre>&lt;code>$ mkdir thttpd-root thttpd-root/lib64
$ cp thttpd thttpd-root/
$ cd thttpd-root
$ cp $(ldd thttpd | awk '$3 ~ &amp;quot;/&amp;quot; {print $3}') lib64/
$ cp /lib64/ld-linux-x86-64.so.2 lib64/
&lt;/code>&lt;/pre>
&lt;p>Which gets us:&lt;/p>
&lt;pre>&lt;code>$ find * -type f
lib64/ld-linux-x86-64.so.2
lib64/libdl.so.2
lib64/libc.so.6
lib64/libcrypt.so.1
lib64/libfreebl3.so
thttpd
&lt;/code>&lt;/pre>
&lt;p>However, if we try to run &lt;code>thttpd&lt;/code> via a &lt;code>chroot&lt;/code> into this directory,
it will fail:&lt;/p>
&lt;pre>&lt;code>$ sudo chroot $PWD /thttpd -D
/thttpd: unknown user - 'nobody'
&lt;/code>&lt;/pre>
&lt;p>A little &lt;code>strace&lt;/code> will show us what&amp;rsquo;s going on:&lt;/p>
&lt;pre>&lt;code>$ sudo strace chroot $PWD /thttpd -D
[...]
open(&amp;quot;/etc/nsswitch.conf&amp;quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
open(&amp;quot;/lib64/libnss_compat.so.2&amp;quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
[...]
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s looking for an &lt;a href="https://en.wikipedia.org/wiki/Name_Service_Switch">NSS&lt;/a> configuration and related libraries. So
let&amp;rsquo;s give it what it wants:&lt;/p>
&lt;pre>&lt;code>$ mkdir etc
$ cat &amp;gt; etc/nsswitch.conf &amp;lt;&amp;lt;EOF
passwd: files
group: files
EOF
$ grep nobody /etc/passwd &amp;gt; etc/passwd
$ grep nobody /etc/group &amp;gt; etc/group
$ cp /lib64/libnss_files.so.2 lib64/
&lt;/code>&lt;/pre>
&lt;p>And now:&lt;/p>
&lt;pre>&lt;code>$ sudo chroot $PWD /thttpd -D
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and it keeps running. This gives a filesystem that is almost
exactly 3MB in size. Can we do better?&lt;/p>
&lt;h2 id="building-a-static-binary">Building a static binary&lt;/h2>
&lt;p>In theory, building a static binary should be as simple as:&lt;/p>
&lt;pre>&lt;code>$ make CCOPT='-O2 -static'
&lt;/code>&lt;/pre>
&lt;p>But on my Fedora 21 system, this gets me several warnings:&lt;/p>
&lt;pre>&lt;code>thttpd.c:(.text.startup+0xf81): warning: Using 'initgroups' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
thttpd.c:(.text.startup+0x146d): warning: Using 'getpwnam' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
thttpd.c:(.text.startup+0x65d): warning: Using 'getaddrinfo' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
&lt;/code>&lt;/pre>
&lt;p>And then a bunch of errors:&lt;/p>
&lt;pre>&lt;code>/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/libcrypt.a(md5-crypt.o): In function `__md5_crypt_r':
(.text+0x11c): undefined reference to `NSSLOW_Init'
/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/libcrypt.a(md5-crypt.o): In function `__md5_crypt_r':
(.text+0x136): undefined reference to `NSSLOWHASH_NewContext'
/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/libcrypt.a(md5-crypt.o): In function `__md5_crypt_r':
(.text+0x14a): undefined reference to `NSSLOWHASH_Begin'
/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/libcrypt.a(md5-crypt.o): In function `__md5_crypt_r':
[...]
&lt;/code>&lt;/pre>
&lt;p>Fortunately (?), this is a distribution-specific problem. Building
&lt;code>thttpd&lt;/code> inside an Ubuntu Docker container seems to work fine:&lt;/p>
&lt;pre>&lt;code>$ docker run -it --rm -v $PWD:/src ubuntu
root@1e126269241c:/# apt-get update; apt-get -y install make gcc
root@1e126269241c:/# make -C /src CCOPT='-O2 -static'
root@1e126269241c:/# exit
&lt;/code>&lt;/pre>
&lt;p>Now we have a statically built binary:&lt;/p>
&lt;pre>&lt;code>$ file thttpd
thttpd: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.24, BuildID[sha1]=bb211a88e9e1d51fa2e937b2b7ea892d87a287d5, not stripped
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s rebuild our &lt;code>chroot&lt;/code> environment:&lt;/p>
&lt;pre>&lt;code>$ rm -rf thttpd-root
$ mkdir thttpd-root thttpd-root/lib64
$ cp thttpd thttpd-root/
$ cd thttpd-root
&lt;/code>&lt;/pre>
&lt;p>And try running &lt;code>thttpd&lt;/code> again:&lt;/p>
&lt;pre>&lt;code>$ sudo chroot $PWD /thttpd -D
/thttpd: unknown user - 'nobody'
&lt;/code>&lt;/pre>
&lt;p>Bummer. It looks like the NSS libraries are still biting us, and it
looks as if statically compiling code that uses NSS &lt;a href="https://stackoverflow.com/questions/3430400/linux-static-linking-is-dead">may be tricky&lt;/a>.
Fortunately, it&amp;rsquo;s relatively simple to patch out the parts of the
&lt;code>thttpd&lt;/code> code that are trying to switch to another uid/gid. The
following &lt;a href="https://github.com/larsks/docker-image-thttpd/blob/master/builder/thttpd-runasroot.patch">patch&lt;/a> will do the trick:&lt;/p>
&lt;pre>&lt;code>diff --git a/thttpd.c b/thttpd.c
index fe21b44..397feb1 100644
--- a/thttpd.c
+++ b/thttpd.c
@@ -400,22 +400,6 @@ main( int argc, char** argv )
if ( throttlefile != (char*) 0 )
read_throttlefile( throttlefile );
- /* If we're root and we're going to become another user, get the uid/gid
- ** now.
- */
- if ( getuid() == 0 )
- {
- pwd = getpwnam( user );
- if ( pwd == (struct passwd*) 0 )
- {
- syslog( LOG_CRIT, &amp;quot;unknown user - '%.80s'&amp;quot;, user );
- (void) fprintf( stderr, &amp;quot;%s: unknown user - '%s'\n&amp;quot;, argv0, user );
- exit( 1 );
- }
- uid = pwd-&amp;gt;pw_uid;
- gid = pwd-&amp;gt;pw_gid;
- }
-
/* Log file. */
if ( logfile != (char*) 0 )
{
@@ -441,17 +425,6 @@ main( int argc, char** argv )
(void) fprintf( stderr, &amp;quot;%s: logfile is not an absolute path, you may not be able to re-open it\n&amp;quot;, argv0 );
}
(void) fcntl( fileno( logfp ), F_SETFD, 1 );
- if ( getuid() == 0 )
- {
- /* If we are root then we chown the log file to the user we'll
- ** be switching to.
- */
- if ( fchown( fileno( logfp ), uid, gid ) &amp;lt; 0 )
- {
- syslog( LOG_WARNING, &amp;quot;fchown logfile - %m&amp;quot; );
- perror( &amp;quot;fchown logfile&amp;quot; );
- }
- }
}
}
else
@@ -680,41 +653,6 @@ main( int argc, char** argv )
stats_bytes = 0;
stats_simultaneous = 0;
- /* If we're root, try to become someone else. */
- if ( getuid() == 0 )
- {
- /* Set aux groups to null. */
- if ( setgroups( 0, (const gid_t*) 0 ) &amp;lt; 0 )
- {
- syslog( LOG_CRIT, &amp;quot;setgroups - %m&amp;quot; );
- exit( 1 );
- }
- /* Set primary group. */
- if ( setgid( gid ) &amp;lt; 0 )
- {
- syslog( LOG_CRIT, &amp;quot;setgid - %m&amp;quot; );
- exit( 1 );
- }
- /* Try setting aux groups correctly - not critical if this fails. */
- if ( initgroups( user, gid ) &amp;lt; 0 )
- syslog( LOG_WARNING, &amp;quot;initgroups - %m&amp;quot; );
-#ifdef HAVE_SETLOGIN
- /* Set login name. */
- (void) setlogin( user );
-#endif /* HAVE_SETLOGIN */
- /* Set uid. */
- if ( setuid( uid ) &amp;lt; 0 )
- {
- syslog( LOG_CRIT, &amp;quot;setuid - %m&amp;quot; );
- exit( 1 );
- }
- /* Check for unnecessary security exposure. */
- if ( ! do_chroot )
- syslog(
- LOG_WARNING,
- &amp;quot;started as root without requesting chroot(), warning only&amp;quot; );
- }
-
/* Initialize our connections table. */
connects = NEW( connecttab, max_connects );
if ( connects == (connecttab*) 0 )
&lt;/code>&lt;/pre>
&lt;p>After patching this and re-building thttpd in the Ubuntu container, we
have a functioning statically linked binary:&lt;/p>
&lt;pre>&lt;code>$ ./thttpd -D -l /dev/stderr -p 8080
127.0.0.1 - - [04/Jan/2015:16:44:26 -0500] &amp;quot;GET / HTTP/1.1&amp;quot; 200 1351 &amp;quot;&amp;quot; &amp;quot;curl/7.37.0&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>That line of output represents me running &lt;code>curl&lt;/code> in another window.&lt;/p>
&lt;h2 id="automating-the-process">Automating the process&lt;/h2>
&lt;p>I have put together an environment to perform the above steps and
build a minimal Docker image with the resulting binary. You can find
the code at &lt;a href="https://github.com/larsks/docker-image-thttpd">https://github.com/larsks/docker-image-thttpd&lt;/a>.&lt;/p>
&lt;p>If you check out the code:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/larsks/docker-image-thttpd
$ cd docker-image-thttpd
&lt;/code>&lt;/pre>
&lt;p>And run &lt;code>make&lt;/code>, this will:&lt;/p>
&lt;ol>
&lt;li>build an Ubuntu-based image with scripts in place to produce a
statically-linked thttpd,&lt;/li>
&lt;li>Boot a container from that image and drop the static &lt;code>thttpd&lt;/code>
binary into a local directory, and&lt;/li>
&lt;li>Produce a minimal Docker image containing just &lt;code>thttpd&lt;/code> and a
simple &lt;code>index.html&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>The final image is just over 1MB in size, and downloads to a new
Kubernetes environment in seconds. You can grab the finished image
via:&lt;/p>
&lt;pre>&lt;code>docker pull larsks/thttpd
&lt;/code>&lt;/pre>
&lt;p>(Or you can grab the above repository from GitHub and build it
yourself locally).&lt;/p></content></item><item><title>Building Docker images with Puppet</title><link>https://blog.oddbit.com/post/2014-10-22-building-docker-images-with-pu/</link><pubDate>Wed, 22 Oct 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-10-22-building-docker-images-with-pu/</guid><description>I like Docker, but I&amp;rsquo;m not a huge fan of using shell scripts for complex system configuration&amp;hellip;and Dockerfiles are basically giant shell scripts.
I was curious whether or not it would be possible to use Puppet during the docker build process. As a test case, I used the ssh module included in the openstack-puppet-modules package.
I started with a manifest like this (in puppet/node.pp):
class { 'ssh': } And a Dockerfile like this:</description><content>&lt;p>I like &lt;a href="http://docker.com/">Docker&lt;/a>, but I&amp;rsquo;m not a huge fan of using shell scripts for
complex system configuration&amp;hellip;and Dockerfiles are basically giant
shell scripts.&lt;/p>
&lt;p>I was curious whether or not it would be possible to use Puppet during
the &lt;code>docker build&lt;/code> process. As a test case, I used the
&lt;a href="https://github.com/saz/puppet-ssh">ssh&lt;/a> module included in the openstack-puppet-modules package.&lt;/p>
&lt;p>I started with a manifest like this (in &lt;code>puppet/node.pp&lt;/code>):&lt;/p>
&lt;pre>&lt;code>class { 'ssh': }
&lt;/code>&lt;/pre>
&lt;p>And a Dockerfile like this:&lt;/p>
&lt;pre>&lt;code>FROM larsks/rdo-puppet-base
COPY puppet /puppet
RUN cd /puppet; puppet apply \
--modulepath /usr/share/openstack-puppet/modules \
node.pp
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>larsks/rdo-puppet-base&lt;/code> module includes &amp;ldquo;puppet&amp;rdquo; and all the Puppet
modules required by RDO (installed in
&lt;code>/usr/share/openstack-puppet/modules&lt;/code>).&lt;/p>
&lt;p>Running &lt;code>docker build&lt;/code> with this &lt;code>Dockerfile&lt;/code> results in:&lt;/p>
&lt;pre>&lt;code>Error: Could not run: Could not retrieve facts for
a9cde05eb735.example.com: no address for
a9cde05eb735.example.com
&lt;/code>&lt;/pre>
&lt;p>Puppet is trying to determine the FQDN of the container, and is then
trying to determine the canonical ip address of the container. This is
never going to work, absent some mechanism that automatically
registers DNS entries when you boot containers (e.g., &lt;a href="https://github.com/crosbymichael/skydock">skydock&lt;/a>).&lt;/p>
&lt;p>The obvious way to fix this would be to modify &lt;code>/etc/hosts&lt;/code> and add
the calculated fqdn to the entry for &lt;code>localhost&lt;/code>, but &lt;code>/etc/hosts&lt;/code>
inside Docker containers is read-only.&lt;/p>
&lt;p>Since Puppet is using Facter to get information about the host, I
looked into whether or not it would be possible (and convenient) to
override Facter generated facts. It turns out that it &lt;a href="http://www.puppetcookbook.com/posts/override-a-facter-fact.html">is relatively
easy&lt;/a>; just set &lt;code>FACTER_&amp;lt;fact_name&amp;gt;&lt;/code> in the environment.
For example:&lt;/p>
&lt;pre>&lt;code>FACTER_fqdn=localhost
&lt;/code>&lt;/pre>
&lt;p>I modified the Dockerfile to look like this:&lt;/p>
&lt;pre>&lt;code>FROM larsks/rdo-puppet-base
COPY puppet /puppet
RUN cd /puppet; FACTER_fqdn=localhost puppet apply \
--modulepath=/usr/share/openstack-puppet/modules \
node.pp
&lt;/code>&lt;/pre>
&lt;p>Running this yields:&lt;/p>
&lt;pre>&lt;code>Error: Could not start Service[sshd]: Execution of '/sbin/service
sshd start' returned 1: Redirecting to /bin/systemctl start sshd.service
Failed to get D-Bus connection: No connection to service manager.
Wrapped exception:
Execution of '/sbin/service sshd start' returned 1: Redirecting to
/bin/systemctl start sshd.service
Failed to get D-Bus connection: No connection to service manager.
&lt;/code>&lt;/pre>
&lt;p>This is happening because the Puppet module is trying to manipulate
the corresponding service resource, but there is no service manager
(e.g., &amp;ldquo;systemd&amp;rdquo; or &amp;ldquo;upstart&amp;rdquo;, etc) inside the container.&lt;/p>
&lt;p>Some modules provide a module parameter to disable service management,
but that solution isn&amp;rsquo;t available in this module. Instead, I created
a &amp;ldquo;dummy&amp;rdquo; service provider. The &amp;ldquo;code&amp;rdquo; (or lack thereof) looks like
this:&lt;/p>
&lt;pre>&lt;code>Puppet::Type.type(:service).provide :dummy, :parent =&amp;gt; :base do
desc &amp;quot;Dummy service provider&amp;quot;
def startcmd
true;
end
def stopcmd
true;
end
def restartcmd
true
end
def statuscmd
true
end
end
&lt;/code>&lt;/pre>
&lt;p>I dropped this into a &lt;code>dummy_service&lt;/code> puppet module with the
following structure:&lt;/p>
&lt;pre>&lt;code>dummy_service/
lib/
puppet/
provider/
service/
dummy.rb
&lt;/code>&lt;/pre>
&lt;p>I installed the whole thing into &lt;code>/usr/share/puppet/modules&lt;/code> in the
base image (&lt;code>larsks/rdo-puppet-base&lt;/code>) by adding the following to the
relevant &lt;code>Dockerfile&lt;/code>:&lt;/p>
&lt;pre>&lt;code>COPY dummy_service /usr/share/puppet/modules/dummy_service
&lt;/code>&lt;/pre>
&lt;p>I modified the &lt;code>Dockerfile&lt;/code> for my ssh image to look like this:&lt;/p>
&lt;pre>&lt;code>FROM larsks/rdo-puppet-base
COPY puppet /puppet
RUN cd /puppet; \
FACTER_fqdn=localhost \
puppet apply \
--modulepath=/usr/share/openstack-puppet/modules:/usr/share/puppet/modules \
node.pp
&lt;/code>&lt;/pre>
&lt;p>And finally I modified &lt;code>node.pp&lt;/code> to look like this:&lt;/p>
&lt;pre>&lt;code>Service {
provider =&amp;gt; dummy,
}
class { 'ssh': }
&lt;/code>&lt;/pre>
&lt;p>This sets the default &lt;code>provider&lt;/code> for &lt;code>service&lt;/code> resources to &lt;code>dummy&lt;/code>.&lt;/p>
&lt;p>With these changes, the &lt;code>docker build&lt;/code> operation completes
successfully:&lt;/p>
&lt;pre>&lt;code>Sending build context to Docker daemon 49.15 kB
Sending build context to Docker daemon
Step 0 : FROM larsks/rdo-puppet-base
---&amp;gt; 2554b6fb47bb
Step 1 : COPY puppet /puppet
---&amp;gt; Using cache
---&amp;gt; bf867271fd0f
Step 2 : RUN cd /puppet; FACTER_fqdn=localhost puppet apply --modulepath=/usr/share/openstack-puppet/modules:/usr/share/puppet/modules node.pp
---&amp;gt; Running in 91b08a7a0ff5
Notice: Compiled catalog for c6f07ae86c40.redhat.com in environment production in 0.58 seconds
Notice: /Stage[main]/Ssh::Server::Install/Package[openssh-server]/ensure: created
Notice: /Stage[main]/Ssh::Client::Config/File[/etc/ssh/ssh_config]/content: content changed '{md5}e233b9bb27ac15b968d8016d7be7d7ce' to '{md5}34815c31785be0c717f766e8d2c8d4d7'
Notice: Finished catalog run in 47.61 seconds
---&amp;gt; e830e6adce26
Removing intermediate container 91b08a7a0ff5
Successfully built e830e6adce26
&lt;/code>&lt;/pre>
&lt;p>Obviously, in order to turn this into a functional module you would
need to add an appropriate &lt;code>CMD&lt;/code> or &lt;code>ENTRYPOINT&lt;/code> script to make it
generate host keys and start &lt;code>sshd&lt;/code>, but I think this successfully
demonstrates what is necessary to make a stock Puppet module run
as part of the &lt;code>docker build&lt;/code> process.&lt;/p></content></item><item><title>Docker networking with dedicated network containers</title><link>https://blog.oddbit.com/post/2014-10-06-docker-networking-with-dedicat/</link><pubDate>Mon, 06 Oct 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-10-06-docker-networking-with-dedicat/</guid><description>The current version of Docker has a very limited set of networking options:
bridge &amp;ndash; connect a container to the Docker bridge host &amp;ndash; run the container in the global network namespace container:xxx &amp;ndash; connect a container to the network namespace of another container none &amp;ndash; do not configure any networking If you need something more than that, you can use a tool like pipework to provision additional network interfaces inside the container, but this leads to a synchronization problem: pipework can only be used after your container is running.</description><content>&lt;p>The current version of Docker has a very limited set of networking
options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>bridge&lt;/code> &amp;ndash; connect a container to the Docker bridge&lt;/li>
&lt;li>&lt;code>host&lt;/code> &amp;ndash; run the container in the global network namespace&lt;/li>
&lt;li>&lt;code>container:xxx&lt;/code> &amp;ndash; connect a container to the network namespace of
another container&lt;/li>
&lt;li>&lt;code>none&lt;/code> &amp;ndash; do not configure any networking&lt;/li>
&lt;/ul>
&lt;p>If you need something more than that, you can use a tool like
&lt;a href="https://github.com/jpetazzo/pipework">pipework&lt;/a> to provision additional network interfaces inside the
container, but this leads to a synchronization problem: &lt;code>pipework&lt;/code> can
only be used after your container is running. This means that when
starting your container, you must have logic that will wait until the
necessary networking is available before starting your service.&lt;/p>
&lt;p>The &lt;a href="https://github.com/GoogleCloudPlatform/kubernetes">kubernetes&lt;/a> project uses a clever solution to this problem:&lt;/p>
&lt;p>Begin by starting a no-op container &amp;ndash; that is, a container that does
not run any services &amp;ndash; with &lt;code>--net=none&lt;/code>. It needs to run
&lt;em>something&lt;/em>; otherwise it will exit. The &lt;code>kubernetes/pause&lt;/code> image
implements an extremely minimal &amp;ldquo;do nothing but wait&amp;rdquo; solution.&lt;/p>
&lt;p>Once you have this no-op container running, you can set up the
corresponding network namespace to meet your requirements. For
example, you can create a &lt;code>veth&lt;/code> device pair and place one end in the
interface and attach another to a bridge on your system. &lt;a href="https://github.com/jpetazzo/pipework">Pipework&lt;/a>
can help with this, but you can also perform all the &lt;a href="https://blog.oddbit.com/post/2014-08-11-four-ways-to-connect-a-docker/">changes by
hand&lt;/a>&lt;/p>
&lt;p>Once your networking is configured, start your actual service
container with &lt;code>--net=container:&amp;lt;id-of-noop-container&amp;gt;&lt;/code>. Your service
container will start with your configured network environment.&lt;/p>
&lt;p>You could, I suppose, decide to link &lt;em>every&lt;/em> service container with
it&amp;rsquo;s own network container, but that would get messy. Kubernetes
groups containers together into &amp;ldquo;pods&amp;rdquo;, in which all containers in a
pod share the same network namespace, which reduces the number of
&amp;ldquo;networking containers&amp;rdquo; necessary for services that have the same
networking requirements.&lt;/p>
&lt;p>This solution &amp;ndash; linking your service container with a no-op container
used to implement networking &amp;ndash; solves the problems identified at the
beginning of this post: because you can perform all your network
configuration prior to starting your service, your service container
does not need any special logic to deal with interfaces that will be
created after the container starts. The networking will already be
in place when the service starts.&lt;/p>
&lt;p>Docker issue &lt;a href="https://github.com/docker/docker/issues/7455">7455&lt;/a> proposes a docker-native solution that would
accomplish largely the same thing without requiring the separate
networking container (by permitting you to pre-configure a network
namespace and then pass that to docker using something like
&lt;code>--net=netns:&amp;lt;name-of-network-namespace&amp;gt;&lt;/code>).&lt;/p></content></item><item><title>Docker plugin bugs</title><link>https://blog.oddbit.com/post/2014-09-01-docker-plugin-bugs/</link><pubDate>Mon, 01 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-01-docker-plugin-bugs/</guid><description>This is a companion to my article on the Docker plugin for Heat.
While writing that article, I encountered a number of bugs in the Docker plugin and elsewhere. I&amp;rsquo;ve submitted patches for most of the issues I encountered:
Bugs in the Heat plugin https://bugs.launchpad.net/heat/+bug/1364017
docker plugin fails to delete a container resource in CREATE_FAILED state.
https://bugs.launchpad.net/heat/+bug/1364041
docker plugin volumes_from parameter should be a list.
https://bugs.launchpad.net/heat/+bug/1364039
docker plugin volumes_from parameter results in an error</description><content>&lt;p>This is a companion to my &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/">article on the Docker plugin for Heat&lt;/a>.&lt;/p>
&lt;p>While writing that article, I encountered a number of bugs in the
Docker plugin and elsewhere. I&amp;rsquo;ve submitted patches for most of the
issues I encountered:&lt;/p>
&lt;h2 id="bugs-in-the-heat-plugin">Bugs in the Heat plugin&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364017">https://bugs.launchpad.net/heat/+bug/1364017&lt;/a>&lt;/p>
&lt;p>docker plugin fails to delete a container resource in
&lt;code>CREATE_FAILED&lt;/code> state.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364041">https://bugs.launchpad.net/heat/+bug/1364041&lt;/a>&lt;/p>
&lt;p>docker plugin &lt;code>volumes_from&lt;/code> parameter should be a list.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364039">https://bugs.launchpad.net/heat/+bug/1364039&lt;/a>&lt;/p>
&lt;p>docker plugin &lt;code>volumes_from&lt;/code> parameter results in an error&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364019">https://bugs.launchpad.net/heat/+bug/1364019&lt;/a>&lt;/p>
&lt;p>docker plugin does not actually remove containers on delete&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugs-in-docker-python-module">Bugs in docker Python module&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/docker/docker-py/pull/310">https://github.com/docker/docker-py/pull/310&lt;/a>&lt;/p>
&lt;p>allow ports to be specified as &lt;code>port/proto&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul></content></item><item><title>Annotated documentation for DockerInc::Docker::Container</title><link>https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/</guid><description>This is a companion to my article on the Docker plugin for Heat.
DockerInc::Docker::Container Properties cmd : List
Command to run after spawning the container.
Optional property.
Example:
cmd: [ 'thttpd', '-C', '/etc/thttpd.conf', '-D', '-c', '*.cgi'] dns : List
Set custom DNS servers.
Example:
dns: - 8.8.8.8 - 8.8.4.4 docker_endopint : String
Docker daemon endpoint. By default the local Docker daemon will be used.
Example:
docker_endpoint: tcp://192.168.1.100:2375 env : String</description><content>&lt;p>This is a companion to my &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/">article on the Docker plugin for Heat&lt;/a>.&lt;/p>
&lt;h2 id="dockerincdockercontainer">DockerInc::Docker::Container&lt;/h2>
&lt;h3 id="properties">Properties&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;code>cmd&lt;/code> : List&lt;/p>
&lt;p>Command to run after spawning the container.&lt;/p>
&lt;p>Optional property.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> cmd: [ 'thttpd', '-C', '/etc/thttpd.conf', '-D', '-c', '*.cgi']
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>dns&lt;/code> : List&lt;/p>
&lt;p>Set custom DNS servers.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> dns:
- 8.8.8.8
- 8.8.4.4
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>docker_endopint&lt;/code> : String&lt;/p>
&lt;p>Docker daemon endpoint. By default the local Docker daemon will
be used.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> docker_endpoint: tcp://192.168.1.100:2375
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>env&lt;/code> : String&lt;/p>
&lt;p>Set environment variables.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> env:
- MYSQL_ROOT_PASSWORD=secret
- &amp;quot;ANOTHER_VARIABLE=something long with spaces&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hostname&lt;/code> : String&lt;/p>
&lt;p>Hostname of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> hostname: mywebserver
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>image&lt;/code> : String&lt;/p>
&lt;p>Image name to boot.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> image: mysql
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>links&lt;/code> : Mapping&lt;/p>
&lt;p>Links to other containers.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> links:
name_in_this_container: name_of_that_container
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>memory&lt;/code> : Number&lt;/p>
&lt;p>Memory limit in bytes.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> # 512 MB
memory: 536870912
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>name&lt;/code> : String&lt;/p>
&lt;p>Name of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> name: dbserver
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>open_stdin&lt;/code> : Boolean&lt;/p>
&lt;p>True to open &lt;code>stdin&lt;/code>.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> open_stdin: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>port_bindings&lt;/code> : Map&lt;/p>
&lt;p>TCP/UDP port bindings.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> # bind port 8080 in the container to port 80 on the host
port_bindings:
8080: 80
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>port_specs&lt;/code> : List&lt;/p>
&lt;p>List of TCP/UDP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> port_specs:
- 80
- 53/udp
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>privileged&lt;/code> : Boolean&lt;/p>
&lt;p>Enable extended privileges.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> privileged: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>stdin_once&lt;/code> : Boolean&lt;/p>
&lt;p>If &lt;code>true&lt;/code>, close &lt;code>stdin&lt;/code> after the one attached client disconnects.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> stdin_once: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>tty&lt;/code> : Boolean&lt;/p>
&lt;p>Allocate a pseudo-tty if &lt;code>true&lt;/code>.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> tty: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>user&lt;/code> : String&lt;/p>
&lt;p>Username or UID for running the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> username: apache
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>volumes&lt;/code> : Map&lt;/p>
&lt;p>Create a bind mount.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> volumes:
/var/tmp/data_on_host: /srv/data_in_container
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>volumes_from&lt;/code> : String&lt;/p>
&lt;p>&lt;em>This option is broken in the current version of the Docker
plugin.&lt;/em>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="attributes">Attributes&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;code>info&lt;/code> : Map&lt;/p>
&lt;p>Information about the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> info:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;info&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> {
&amp;quot;HostsPath&amp;quot;: &amp;quot;/var/lib/docker/containers/d6d84d1bbf2984fa3e04cea36c8d10d27d318b6d96b57c41fca2cbc1da23bf71/hosts&amp;quot;,
&amp;quot;Created&amp;quot;: &amp;quot;2014-09-01T14:21:02.7577874Z&amp;quot;,
&amp;quot;Image&amp;quot;: &amp;quot;a950533b3019d8f6dfdcb8fdc42ef810b930356619b3e4786d4f2acec514238d&amp;quot;,
&amp;quot;Args&amp;quot;: [
&amp;quot;mysqld&amp;quot;,
&amp;quot;--datadir=/var/lib/mysql&amp;quot;,
&amp;quot;--user=mysql&amp;quot;
],
&amp;quot;Driver&amp;quot;: &amp;quot;devicemapper&amp;quot;,
&amp;quot;HostConfig&amp;quot;: {
&amp;quot;CapDrop&amp;quot;: null,
&amp;quot;PortBindings&amp;quot;: {
&amp;quot;3306/tcp&amp;quot;: [
{
&amp;quot;HostPort&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;HostIp&amp;quot;: &amp;quot;&amp;quot;
}
]
},
&amp;quot;NetworkMode&amp;quot;: &amp;quot;&amp;quot;,
.
.
.
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>logs&lt;/code> : String&lt;/p>
&lt;p>Logs from the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> logs:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;logs&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>logs_head&lt;/code> : String&lt;/p>
&lt;p>Most recent log line from the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> logs:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;logs_head&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;2014-09-01 14:21:04 0 [Warning] TIMESTAMP with implicit DEFAULT
value is deprecated. Please use --explicit_defaults_for_timestamp
server option (see documentation for more details).&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_gateway&lt;/code> : String&lt;/p>
&lt;p>IP address of the network gateway for the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_gateway:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_gateway&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;172.17.42.1&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_info&lt;/code> : Map&lt;/p>
&lt;p>Information about the network configuration of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_info:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_info&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> {
&amp;quot;Bridge&amp;quot;: &amp;quot;docker0&amp;quot;,
&amp;quot;TcpPorts&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;PortMapping&amp;quot;: null,
&amp;quot;IPPrefixLen&amp;quot;: 16,
&amp;quot;UdpPorts&amp;quot;: &amp;quot;&amp;quot;,
&amp;quot;IPAddress&amp;quot;: &amp;quot;172.17.0.10&amp;quot;,
&amp;quot;Gateway&amp;quot;: &amp;quot;172.17.42.1&amp;quot;,
&amp;quot;Ports&amp;quot;: {
&amp;quot;3306/tcp&amp;quot;: [
{
&amp;quot;HostPort&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;HostIp&amp;quot;: &amp;quot;0.0.0.0&amp;quot;
}
]
}
}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_ip&lt;/code> : String&lt;/p>
&lt;p>IP address assigned to the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_ip:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_ip&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;172.17.0.10&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_tcp_ports&lt;/code> : String&lt;/p>
&lt;p>A comma delimited list of TCP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_tcp_ports:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_tcp_ports&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;8443,8080&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_udp_ports&lt;/code> : String&lt;/p>
&lt;p>A comma delimited list of TCP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_udp_ports:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_udp_ports&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;8443,8080&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul></content></item><item><title>Docker plugin for OpenStack Heat</title><link>https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/</guid><description>I have been looking at both Docker and OpenStack recently. In my last post I talked a little about the Docker driver for Nova; in this post I&amp;rsquo;ll be taking an in-depth look at the Docker plugin for Heat, which has been available since the Icehouse release but is surprisingly under-documented.
The release announcement on the Docker blog includes an example Heat template, but it is unfortunately grossly inaccurate and has led many people astray.</description><content>&lt;p>I have been looking at both Docker and OpenStack recently. In my &lt;a href="https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/">last
post&lt;/a> I talked a little about the &lt;a href="https://github.com/stackforge/nova-docker">Docker driver for Nova&lt;/a>; in
this post I&amp;rsquo;ll be taking an in-depth look at the Docker plugin for
Heat, which has been available &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">since the Icehouse release&lt;/a> but is
surprisingly under-documented.&lt;/p>
&lt;p>The &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">release announcement&lt;/a> on the Docker blog includes an
example Heat template, but it is unfortunately grossly inaccurate and
has led many people astray. In particular:&lt;/p>
&lt;ul>
&lt;li>It purports to but does not actually install Docker, due to a basic
&lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a> syntax error, and&lt;/li>
&lt;li>Even if you were to fix that problem, the lack of synchronization
between the two resources in the template would mean that you would
never be able to successfully launch a container.&lt;/li>
&lt;/ul>
&lt;p>In this post, I will present a fully functional example that will work
with the Icehouse release of Heat. We will install the Docker plugin
for Heat, then write a template that will (a) launch a Fedora 20
server and automatically install Docker, and then (b) use the Docker
plugin to launch some containers on that server.&lt;/p>
&lt;p>The &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a> referenced in this article can be found on GitHub:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/heat-docker-example">https://github.com/larsks/heat-docker-example&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="installing-the-docker-plugin">Installing the Docker plugin&lt;/h2>
&lt;p>The first thing we need to do is install the Docker plugin. I am
running &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> packages for Icehouse locally, which do not include
the Docker plugin. We&amp;rsquo;r going to install the plugin from the Heat
sources.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Download the Heat repository:&lt;/p>
&lt;pre>&lt;code> $ git clone https://github.com/openstack/heat.git
Cloning into 'heat'...
remote: Counting objects: 50382, done.
remote: Compressing objects: 100% (22/22), done.
remote: Total 50382 (delta 7), reused 1 (delta 0)
Receiving objects: 100% (50382/50382), 19.84 MiB | 1.81 MiB/s, done.
Resolving deltas: 100% (34117/34117), done.
Checking connectivity... done.
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>This will result in a directory called &lt;code>heat&lt;/code> in your current
working directory. Change into this directory:&lt;/p>
&lt;pre>&lt;code> $ cd heat
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Patch the Docker plugin.&lt;/p>
&lt;p>You have now checked out the &lt;code>master&lt;/code> branch of the Heat
repository; this is the most recent code committed to the project.
At this point we could check out the &lt;code>stable/icehouse&lt;/code> branch of
the repository to get the version of the plugin released at the
same time as the version of Heat that we&amp;rsquo;re running, but we would
find that the Docker plugin was, at that point in time, somewhat
crippled; in particular:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It does not support mapping container ports to host ports, so
there is no easy way to expose container services for external
access, and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It does not know how to automatically &lt;code>pull&lt;/code> missing images, so
you must arrange to run &lt;code>docker pull&lt;/code> a priori for each image you
plan to use in your Heat template.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>That would make us sad, so instead we&amp;rsquo;re going to use the plugin
from the &lt;code>master&lt;/code> branch, which only requires a trivial change in
order to work with the Icehouse release of Heat.&lt;/p>
&lt;p>Look at the file
&lt;code>contrib/heat_docker/heat_docker/resources/docker_container.py&lt;/code>.
Locate the following line:&lt;/p>
&lt;pre>&lt;code> attributes_schema = {
&lt;/code>&lt;/pre>
&lt;p>Add a line immediately before that so that the file look like
this:&lt;/p>
&lt;pre>&lt;code> attributes.Schema = lambda x: x
attributes_schema = {
&lt;/code>&lt;/pre>
&lt;p>If you&amp;rsquo;re curious, here is what we accomplished with that
additional line:&lt;/p>
&lt;p>The code following that point contains multiple stanzas of the
form:&lt;/p>
&lt;pre>&lt;code> INFO: attributes.Schema(
_('Container info.')
),
&lt;/code>&lt;/pre>
&lt;p>In Icehouse, the &lt;code>heat.engine.attributes&lt;/code> module does not have a
&lt;code>Schema&lt;/code> class so this fails. Our patch above adds a module
member named &lt;code>Schema&lt;/code> that simply returns it&amp;rsquo;s arguments (that
is, it is an identity function).&lt;/p>
&lt;p>(&lt;strong>NB&lt;/strong>: At the time this was written, Heat&amp;rsquo;s &lt;code>master&lt;/code> branch was
at &lt;code>a767880&lt;/code>.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Install the Docker plugin into your Heat plugin directory, which
on my system is &lt;code>/usr/lib/heat&lt;/code> (you can set this explicitly using
the &lt;code>plugin_dirs&lt;/code> directive in &lt;code>/etc/heat/heat.conf&lt;/code>):&lt;/p>
&lt;pre>&lt;code> $ rsync -a --exclude=tests/ contrib/heat_docker/heat_docker \
/usr/lib/heat
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re excluding the &lt;code>tests&lt;/code> directory here because it has
additional prerequisites that aren&amp;rsquo;t operationally necessary but
that will prevent Heat from starting up if they are missing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Restart your &lt;code>heat-engine&lt;/code> service. On Fedora, that would be:&lt;/p>
&lt;pre>&lt;code> # systemctl restart openstack-heat-engine
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Verify that the new &lt;code>DockerInc::Docker::Container&lt;/code> resource is
available:&lt;/p>
&lt;pre>&lt;code> $ heat resource-type-list | grep Docker
| DockerInc::Docker::Container |
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="templates-installing-docker">Templates: Installing docker&lt;/h2>
&lt;p>We would like our template to automatically install Docker on a Nova
server. The example in the &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">Docker blog&lt;/a> mentioned earlier
attempts to do this by setting the &lt;code>user_data&lt;/code> parameter of a
&lt;code>OS::Nova::Server&lt;/code> resource like this:&lt;/p>
&lt;pre>&lt;code>user_data: #include https://get.docker.io
&lt;/code>&lt;/pre>
&lt;p>Unfortunately, an unquoted &lt;code>#&lt;/code> introduces a comment in &lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a>, so
this is completely ignored. It would be written more correctly like
this (the &lt;code>|&lt;/code> introduces a block of literal text):&lt;/p>
&lt;pre>&lt;code>user_data: |
#include https://get.docker.io
&lt;/code>&lt;/pre>
&lt;p>Or possibly like this, although this would restrict you to a single
line and thus wouldn&amp;rsquo;t be used much in practice:&lt;/p>
&lt;pre>&lt;code>user_data: &amp;quot;#include https://get.docker.io&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And, all other things being correct, this would install Docker on a
system&amp;hellip;but would not necessarily start it, nor would it configure
Docker to listen on a TCP socket. On my Fedora system, I ended up
creating the following &lt;code>user_data&lt;/code> script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
yum -y upgrade
# I have occasionally seen 'yum install' fail with errors
# trying to contact mirrors. Because it can be a pain to
# delete and re-create the stack, just loop here until it
# succeeds.
while :; do
yum -y install docker-io
[ -x /usr/bin/docker ] &amp;amp;&amp;amp; break
sleep 5
done
# Add a tcp socket for docker
cat &amp;gt; /etc/systemd/system/docker-tcp.socket &amp;lt;&amp;lt;EOF
[Unit]
Description=Docker remote access socket
[Socket]
ListenStream=2375
BindIPv6Only=both
Service=docker.service
[Install]
WantedBy=sockets.target
EOF
# Start and enable the docker service.
for sock in docker.socket docker-tcp.socket; do
systemctl start $sock
systemctl enable $sock
done
&lt;/code>&lt;/pre>
&lt;p>This takes care of making sure our packages are current, installing
Docker, and arranging for it to listen on a tcp socket. For that last
bit, we&amp;rsquo;re creating a new &lt;code>systemd&lt;/code> socket file
(&lt;code>/etc/systemd/system/docker-tcp.socket&lt;/code>), which means that &lt;code>systemd&lt;/code>
will actually open the socket for listening and start &lt;code>docker&lt;/code> if
necessary when a client connects.&lt;/p>
&lt;h2 id="templates-synchronizing-resources">Templates: Synchronizing resources&lt;/h2>
&lt;p>In our Heat template, we are starting a Nova server that will run
Docker, and then we are instantiating one or more Docker containers
that will run on this server. This means that timing is suddenly very
important. If we use the &lt;code>user_data&lt;/code> script as presented in the
previous section, we would probably end up with an error like this in
our &lt;code>heat-engine.log&lt;/code>:&lt;/p>
&lt;pre>&lt;code>2014-08-29 17:10:37.598 15525 TRACE heat.engine.resource ConnectionError:
HTTPConnectionPool(host='192.168.200.11', port=2375): Max retries exceeded
with url: /v1.12/containers/create (Caused by &amp;lt;class 'socket.error'&amp;gt;:
[Errno 113] EHOSTUNREACH)
&lt;/code>&lt;/pre>
&lt;p>This happens because it takes &lt;em>time&lt;/em> to install packages. Absent any
dependencies, Heat creates resources in parallel, so Heat is happily
trying to spawn our Docker containers when our server is still
fetching the Docker package.&lt;/p>
&lt;p>Heat does have a &lt;code>depends_on&lt;/code> property that can be applied to
resources. For example, if we have:&lt;/p>
&lt;pre>&lt;code>docker_server:
type: &amp;quot;OS::Nova::Server&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>We can make a Docker container depend on that resource:&lt;/p>
&lt;pre>&lt;code>docker_container_mysql:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
depends_on:
- docker_server
&lt;/code>&lt;/pre>
&lt;p>Looks good, but this does not, in fact, help us. From Heat&amp;rsquo;s
perspective, the dependency is satisfied as soon as the Nova server
&lt;em>boots&lt;/em>, so really we&amp;rsquo;re back where we started.&lt;/p>
&lt;p>The Heat solution to this is the &lt;code>AWS::CloudFormation::WaitCondition&lt;/code>
resource (and its boon companion, the and
&lt;code>AWS::CloudFormation::WaitConditionHandle&lt;/code> resource). A
&lt;code>WaitCondition&lt;/code> is a resource this is not &amp;ldquo;created&amp;rdquo; until it has
received an external signal. We define a wait condition like this:&lt;/p>
&lt;pre>&lt;code>docker_wait_handle:
type: &amp;quot;AWS::CloudFormation::WaitConditionHandle&amp;quot;
docker_wait_condition:
type: &amp;quot;AWS::CloudFormation::WaitCondition&amp;quot;
depends_on:
- docker_server
properties:
Handle:
get_resource: docker_wait_handle
Timeout: &amp;quot;6000&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And then we make our container depend on the wait condition:&lt;/p>
&lt;pre>&lt;code>docker_container_mysql:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
depends_on:
- docker_wait_condition
&lt;/code>&lt;/pre>
&lt;p>With this in place, Heat will not attempt to create the Docker
container until we signal the wait condition resource. In order to do
that, we need to modify our &lt;code>user_data&lt;/code> script to embed the
notification URL generated by heat. We&amp;rsquo;ll use both the &lt;code>get_resource&lt;/code>
and &lt;code>str_replace&lt;/code> &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#intrinsic-functions">intrinsic function&lt;/a> in order to generate the appropriate
script:&lt;/p>
&lt;pre>&lt;code> user_data:
# We're using Heat's 'str_replace' function in order to
# substitute into this script the Heat-generated URL for
# signaling the docker_wait_condition resource.
str_replace:
template: |
#!/bin/sh
yum -y upgrade
# I have occasionally seen 'yum install' fail with errors
# trying to contact mirrors. Because it can be a pain to
# delete and re-create the stack, just loop here until it
# succeeds.
while :; do
yum -y install docker-io
[ -x /usr/bin/docker ] &amp;amp;&amp;amp; break
sleep 5
done
# Add a tcp socket for docker
cat &amp;gt; /etc/systemd/system/docker-tcp.socket &amp;lt;&amp;lt;EOF
[Unit]
Description=Docker remote access socket
[Socket]
ListenStream=2375
BindIPv6Only=both
Service=docker.service
[Install]
WantedBy=sockets.target
EOF
# Start and enable the docker service.
for sock in docker.socket docker-tcp.socket; do
systemctl start $sock
systemctl enable $sock
done
# Signal heat that we are finished settings things up.
cfn-signal -e0 --data 'OK' -r 'Setup complete' '$WAIT_HANDLE'
params:
&amp;quot;$WAIT_HANDLE&amp;quot;:
get_resource: docker_wait_handle
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>str_replace&lt;/code> function probably deserves a closer look; the
general format is:&lt;/p>
&lt;pre>&lt;code>str_replace:
template:
params:
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>template&lt;/code> is text content containing 0 or more things to be
replaced, and &lt;code>params&lt;/code> is a list of tokens to search for and replace
in the &lt;code>template&lt;/code>.&lt;/p>
&lt;p>We use &lt;code>str_replace&lt;/code> to substitute the token &lt;code>$WAIT_HANDLE&lt;/code> with the
result of calling &lt;code>get_resource&lt;/code> on our &lt;code>docker_wait_handle&lt;/code> resource.
This results in a URL that contains an EC2-style signed URL that will
deliver the necessary notification to Heat. In this example we&amp;rsquo;re
using the &lt;code>cfn-signal&lt;/code> tool, which is included in the Fedora cloud
images, but you could accomplish the same thing with &lt;code>curl&lt;/code>:&lt;/p>
&lt;pre>&lt;code>curl -X PUT -H 'Content-Type: application/json' \
--data-binary '{&amp;quot;Status&amp;quot;: &amp;quot;SUCCESS&amp;quot;,
&amp;quot;Reason&amp;quot;: &amp;quot;Setup complete&amp;quot;,
&amp;quot;Data&amp;quot;: &amp;quot;OK&amp;quot;, &amp;quot;UniqueId&amp;quot;: &amp;quot;00000&amp;quot;}' \
&amp;quot;$WAIT_HANDLE&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>You need to have correctly configured Heat in order for this to work;
I&amp;rsquo;ve written a short &lt;a href="https://blog.oddbit.com/post/2014-08-30-using-wait-conditions-with-hea/">companion article&lt;/a> that contains a checklist
and pointers to additional documentation to help work around some
common issues.&lt;/p>
&lt;h2 id="templates-defining-docker-containers">Templates: Defining Docker containers&lt;/h2>
&lt;p>&lt;strong>UPDATE&lt;/strong>: I have generated some &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/">annotated documentation for the
Docker plugin&lt;/a>.&lt;/p>
&lt;p>Now that we have arranged for Heat to wait for the server to finish
configuration before starting Docker contains, how do we create a
container? As Scott Lowe noticed in his &lt;a href="http://blog.scottlowe.org/2014/08/22/a-heat-template-for-docker-containers/">blog post about Heat and
Docker&lt;/a>, there is very little documentation available out there
for the Docker plugin (something I am trying to remedy with this blog
post!). Things are not quite as bleak as you might think, because
Heat resources are to a certain extent self-documenting. If you run:&lt;/p>
&lt;pre>&lt;code>$ heat resource-template DockerInc::Docker::Container
&lt;/code>&lt;/pre>
&lt;p>You will get a complete description of the attributes and properties
available in the named resource. The &lt;code>parameters&lt;/code> section is probably
the most descriptive:&lt;/p>
&lt;pre>&lt;code>parameters:
cmd:
Default: []
Description: Command to run after spawning the container.
Type: CommaDelimitedList
dns: {Description: Set custom dns servers., Type: CommaDelimitedList}
docker_endpoint: {Description: Docker daemon endpoint (by default the local docker
daemon will be used)., Type: String}
env: {Description: Set environment variables., Type: CommaDelimitedList}
hostname: {Default: '', Description: Hostname of the container., Type: String}
image: {Description: Image name., Type: String}
links: {Description: Links to other containers., Type: Json}
memory: {Default: 0, Description: Memory limit (Bytes)., Type: Number}
name: {Description: Name of the container., Type: String}
open_stdin:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Open stdin.
Type: String
port_bindings: {Description: TCP/UDP ports bindings., Type: Json}
port_specs: {Description: TCP/UDP ports mapping., Type: CommaDelimitedList}
privileged:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Enable extended privileges.
Type: String
stdin_once:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: If true, close stdin after the 1 attached client disconnects.
Type: String
tty:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Allocate a pseudo-tty.
Type: String
user: {Default: '', Description: Username or UID., Type: String}
volumes:
Default: {}
Description: Create a bind mount.
Type: Json
volumes_from: {Default: '', Description: Mount all specified volumes., Type: String}
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>port_specs&lt;/code> and &lt;code>port_bindings&lt;/code> parameters require a little
additional explanation.&lt;/p>
&lt;p>The &lt;code>port_specs&lt;/code> parameter is a list of (TCP) ports that will be
&amp;ldquo;exposed&amp;rdquo; by the container (similar to the &lt;code>EXPOSE&lt;/code> directive in a
Dockerfile). This corresponds to the &lt;code>PortSpecs&lt;/code> argument in the the
&lt;a href="https://docs.docker.com/reference/api/docker_remote_api_v1.14/#create-a-container">/containers/create&lt;/a> call of the &lt;a href="https://docs.docker.com/reference/api/docker_remote_api/">Docker remote API&lt;/a>.
For example:&lt;/p>
&lt;pre>&lt;code>port_specs:
- 3306
- 53/udp
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>port_bindings&lt;/code> parameter is a mapping that allows you to bind
host ports to ports in the container, similar to the &lt;code>-p&lt;/code> argument to
&lt;code>docker run&lt;/code>. This corresponds to the
&lt;a href="https://docs.docker.com/reference/api/docker_remote_api_v1.14/#start-a-container">/containers/(id)/start&lt;/a> call in the &lt;a href="https://docs.docker.com/reference/api/docker_remote_api/">Docker remote API&lt;/a>.
In the mappings, the key (left-hand side) is the &lt;em>container&lt;/em> port, and
the value (right-hand side) is the &lt;em>host&lt;/em> port.&lt;/p>
&lt;p>For example, to bind container port 3306 to host port 3306:&lt;/p>
&lt;pre>&lt;code>port_bindings:
3306: 3306
&lt;/code>&lt;/pre>
&lt;p>To bind port 9090 in a container to port 80 on the host:&lt;/p>
&lt;pre>&lt;code>port_bindings:
9090: 80
&lt;/code>&lt;/pre>
&lt;p>And in theory, this should also work for UDP ports (but in practice
there is an issue between the Docker plugin and the &lt;code>docker-py&lt;/code> Python
module which makes it impossible to expose UDP ports via &lt;code>port_specs&lt;/code>;
this is fixed in
&lt;a href="https://github.com/docker/docker-py/pull/310" class="pull-request">#310&lt;/a>
on GitHub).&lt;/p>
&lt;pre>&lt;code>port_bindings:
53/udp: 5300
&lt;/code>&lt;/pre>
&lt;p>With all of this in mind, we can create a container resource
definition:&lt;/p>
&lt;pre>&lt;code>docker_dbserver:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
# here's where we set the dependency on the WaitCondition
# resource we mentioned earlier.
depends_on:
- docker_wait_condition
properties:
docker_endpoint:
str_replace:
template: &amp;quot;tcp://$HOST:2375&amp;quot;
params:
&amp;quot;$HOST&amp;quot;:
get_attr:
- docker_server_floating
- floating_ip_address
image: mysql
env:
# The official MySQL docker image expect the database root
# password to be provided in the MYSQL_ROOT_PASSWORD
# environment variable.
- str_replace:
template: MYSQL_ROOT_PASSWORD=$PASSWORD
params:
&amp;quot;$PASSWORD&amp;quot;:
get_param:
mysql_root_password
port_specs:
- 3306
port_bindings:
3306: 3306
&lt;/code>&lt;/pre>
&lt;p>Take a close look at how we&amp;rsquo;re setting the &lt;code>docker_endpoint&lt;/code> property:&lt;/p>
&lt;pre>&lt;code>docker_endpoint:
str_replace:
template: &amp;quot;tcp://$HOST:2375&amp;quot;
params:
&amp;quot;$HOST&amp;quot;:
get_attr:
- docker_server_floating
- floating_ip_address
&lt;/code>&lt;/pre>
&lt;p>This uses the &lt;code>get_attr&lt;/code> function to get the &lt;code>floating_ip_address&lt;/code>
attribute from the &lt;code>docker_server_floating&lt;/code> resource, which you can
find in the &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a>. We take the return value from that
function and use &lt;code>str_replace&lt;/code> to substitute that into the
&lt;code>docker_endpoint&lt;/code> URL.&lt;/p>
&lt;h2 id="the-pudding">The pudding&lt;/h2>
&lt;p>Using the &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a> with an appropriate local environment
file, I can launch this stack by runnign:&lt;/p>
&lt;pre>&lt;code>$ heat stack-create -f docker-server.yml -e local.env docker
&lt;/code>&lt;/pre>
&lt;p>And after a while, I can run&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
&lt;/code>&lt;/pre>
&lt;p>And see that the stack has been created successfully:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+------------+-----------------+----------------------+
| id | stack_name | stack_status | creation_time |
+--------------------------------------+------------+-----------------+----------------------+
| c0fd793e-a1f7-4b35-afa9-12ba1005925a | docker | CREATE_COMPLETE | 2014-08-31T03:01:14Z |
+--------------------------------------+------------+-----------------+----------------------+
&lt;/code>&lt;/pre>
&lt;p>And I can ask for status information on the individual resources in
the stack:&lt;/p>
&lt;pre>&lt;code>$ heat resource-list docker
+------------------------+------------------------------------------+-----------------+
| resource_name | resource_type | resource_status |
+------------------------+------------------------------------------+-----------------+
| fixed_network | OS::Neutron::Net | CREATE_COMPLETE |
| secgroup_db | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| secgroup_docker | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| secgroup_webserver | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| docker_wait_handle | AWS::CloudFormation::WaitConditionHandle | CREATE_COMPLETE |
| extrouter | OS::Neutron::Router | CREATE_COMPLETE |
| fixed_subnet | OS::Neutron::Subnet | CREATE_COMPLETE |
| secgroup_common | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| docker_server_eth0 | OS::Neutron::Port | CREATE_COMPLETE |
| extrouter_inside | OS::Neutron::RouterInterface | CREATE_COMPLETE |
| docker_server | OS::Nova::Server | CREATE_COMPLETE |
| docker_server_floating | OS::Neutron::FloatingIP | CREATE_COMPLETE |
| docker_wait_condition | AWS::CloudFormation::WaitCondition | CREATE_COMPLETE |
| docker_webserver | DockerInc::Docker::Container | CREATE_COMPLETE |
| docker_dbserver | DockerInc::Docker::Container | CREATE_COMPLETE |
+------------------------+------------------------------------------+-----------------+
&lt;/code>&lt;/pre>
&lt;p>I can run &lt;code>nova list&lt;/code> and see information about my running Nova
server:&lt;/p>
&lt;pre>&lt;code>+--------...+-----------------...+------------------------------------------------------------+
| ID ...| Name ...| Networks |
+--------...+-----------------...+------------------------------------------------------------+
| 301c5ec...| docker-docker_se...| docker-fixed_network-whp3fxhohkxk=10.0.0.2, 192.168.200.46 |
+--------...+-----------------...+------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>I can point a Docker client at the remote address and see the running
containers:&lt;/p>
&lt;pre>&lt;code>$ docker-1.2 -H tcp://192.168.200.46:2375 ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
f2388c871b20 mysql:5 /entrypoint.sh mysql 5 minutes ago Up 5 minutes 0.0.0.0:3306-&amp;gt;3306/tcp grave_almeida
9596cbe51291 larsks/simpleweb:latest /bin/sh -c '/usr/sbi 11 minutes ago Up 11 minutes 0.0.0.0:80-&amp;gt;80/tcp hungry_tesla
&lt;/code>&lt;/pre>
&lt;p>And I can point a &lt;code>mysql&lt;/code> client at the remote address and access the
database server:&lt;/p>
&lt;pre>&lt;code>$ mysql -h 192.168.200.46 -u root -psecret mysql
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
[...]
MySQL [mysql]&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="when-things-go-wrong">When things go wrong&lt;/h2>
&lt;p>Your &lt;code>heat-engine&lt;/code> log, generally &lt;code>/var/log/heat/engine.log&lt;/code>, is going
to be your best source of information if things go wrong. The &lt;code>heat stack-show&lt;/code> command will generally provide useful fault information if
your stack ends up in the &lt;code>CREATE_FAILED&lt;/code> (or &lt;code>DELETE_FAILED&lt;/code>) state.&lt;/p></content></item><item><title>nova-docker and environment variables</title><link>https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/</link><pubDate>Thu, 28 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/</guid><description>I&amp;rsquo;ve been playing with Docker a bit recently, and decided to take a look at the nova-docker driver for OpenStack.
The nova-docker driver lets Nova, the OpenStack Compute service, spawn Docker containers instead of hypervisor-based servers. For certain workloads, this leads to better resource utilization than you would get with a hypervisor-based solution, while at the same time givin you better support for multi-tenancy and flexible networking than you get with Docker by itself.</description><content>&lt;p>I&amp;rsquo;ve been playing with &lt;a href="https://docker.com/">Docker&lt;/a> a bit recently, and decided to take
a look at the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> driver for &lt;a href="http://openstack.org/">OpenStack&lt;/a>.&lt;/p>
&lt;p>The &lt;code>nova-docker&lt;/code> driver lets Nova, the OpenStack Compute service,
spawn Docker containers instead of hypervisor-based servers. For
certain workloads, this leads to better resource utilization than you
would get with a hypervisor-based solution, while at the same time
givin you better support for multi-tenancy and flexible networking
than you get with Docker by itself.&lt;/p>
&lt;p>The &lt;a href="https://wiki.openstack.org/wiki/Docker">Docker driver wiki&lt;/a> was mostly sufficient for getting the
&lt;code>nova-docker&lt;/code> driver installed in my existing OpenStack deployment,
although I did make a few &lt;a href="https://wiki.openstack.org/w/index.php?title=Docker&amp;amp;diff=61664&amp;amp;oldid=58546">small changes&lt;/a> to the wiki to reflect
some missing steps. Other than that, the installation was relatively
simple and I was soon able to spin up Docker containers using &lt;code>nova boot ...&lt;/code>&lt;/p>
&lt;p>The one problem I encountered is that it is not possible to pass
environment variable to Docker containers via the &lt;code>nova-docker&lt;/code>
driver. Many existing images (such as the &lt;a href="https://registry.hub.docker.com/_/mysql/">official MySQL image&lt;/a>)
expect configuration information to be passed in using environment
variables; for example, the &lt;code>mysql&lt;/code> image expects to be started like
this:&lt;/p>
&lt;pre>&lt;code>docker run --name some-mysql \
-e MYSQL_ROOT_PASSWORD=mysecretpassword -d mysql
&lt;/code>&lt;/pre>
&lt;p>I have proposed a &lt;a href="https://review.openstack.org/#/c/117583/">patch&lt;/a> to the &lt;code>nova-docker&lt;/code> driver that permits
one to provide environment variables via the Nova metadata service.
With this patch in place, I would start the &lt;code>mysql&lt;/code> container like
this:&lt;/p>
&lt;pre>&lt;code>nova boot --image mysql --flavor m1.small \
--meta ENV_MYSQL_ROOT_PASSWORD=mysecretpassword \
some-mysql
&lt;/code>&lt;/pre>
&lt;p>That is, the driver looks for metadata items that begin with &lt;code>ENV_&lt;/code>
and transforms these into Docker environment variables after stripping
&lt;code>ENV_&lt;/code> from the name.&lt;/p>
&lt;p>While this patch works great in my testing environment, it&amp;rsquo;s unlikely
to get accepted. Generally, the metadata provided by Nova belongs to
the tenant and is not meant to be operationally significant to the
compute driver itself.&lt;/p>
&lt;p>It sounds as if there is a lot of work going on right now regarding
container support in OpenStack, so it is very likely that a better
solution will show up in the near future.&lt;/p>
&lt;p>In the absence of that support, I hope others find this patch helpful.&lt;/p></content></item><item><title>Four ways to connect a docker container to a local network</title><link>https://blog.oddbit.com/post/2014-08-11-four-ways-to-connect-a-docker/</link><pubDate>Mon, 11 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-11-four-ways-to-connect-a-docker/</guid><description>Update (2018-03-22) Since I wrote this document back in 2014, Docker has developed the macvlan network driver. That gives you a supported mechanism for direct connectivity to a local layer 2 network. I&amp;rsquo;ve written an article about working with the macvlan driver.
This article discusses four ways to make a Docker container appear on a local network. These are not suggested as practical solutions, but are meant to illustrate some of the underlying network technology available in Linux.</description><content>&lt;p>&lt;strong>Update (2018-03-22)&lt;/strong> Since I wrote this document back in 2014,
Docker has developed the &lt;a href="https://docs.docker.com/network/macvlan/">macvlan network
driver&lt;/a>. That gives you a
&lt;em>supported&lt;/em> mechanism for direct connectivity to a local layer 2
network. I&amp;rsquo;ve &lt;a href="https://blog.oddbit.com/2018/03/12/using-docker-macvlan-networks/">written an article about working with the macvlan
driver&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>This article discusses four ways to make a Docker container appear on
a local network. These are not suggested as practical solutions, but
are meant to illustrate some of the underlying network technology
available in Linux.&lt;/p>
&lt;p>If you were actually going to use one of these solutions as anything
other than a technology demonstration, you might look to the &lt;a href="https://github.com/jpetazzo/pipework">pipework&lt;/a> script, which can automate many of these configurations.&lt;/p>
&lt;h2 id="goals-and-assumptions">Goals and Assumptions&lt;/h2>
&lt;p>In the following examples, we have a host with address 10.12.0.76 on
the 10.12.0.0/21 network. We are creating a Docker container that we
want to expose as 10.12.0.117.&lt;/p>
&lt;p>I am running Fedora 20 with Docker 1.1.2. This means, in particular,
that my &lt;code>utils-linux&lt;/code> package is recent enough to include the
&lt;a href="http://man7.org/linux/man-pages/man1/nsenter.1.html">nsenter&lt;/a> command. If you don&amp;rsquo;t have that handy, there is a
convenient Docker recipe to build it for you at &lt;a href="https://github.com/jpetazzo/nsenter">jpetazzo/nsenter&lt;/a>
on GitHub.&lt;/p>
&lt;h2 id="a-little-help-along-the-way">A little help along the way&lt;/h2>
&lt;p>In this article we will often refer to the PID of a docker container.
In order to make this convenient, drop the following into a script
called &lt;code>docker-pid&lt;/code>, place it somewhere on your &lt;code>PATH&lt;/code>, and make it
executable:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
exec docker inspect --format '{{ .State.Pid }}' &amp;quot;$@&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>This allows us to conveniently get the PID of a docker container by
name or ID:&lt;/p>
&lt;pre>&lt;code>$ docker-pid web
22041
&lt;/code>&lt;/pre>
&lt;p>In a script called &lt;code>docker-ip&lt;/code>, place the following:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
exec docker inspect --format '{{ .NetworkSettings.IPAddress }}' &amp;quot;$@&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And now we can get the ip address of a container like this:&lt;/p>
&lt;pre>&lt;code>$ docker-ip web
172.17.0.4
&lt;/code>&lt;/pre>
&lt;h2 id="using-nat">Using NAT&lt;/h2>
&lt;p>This uses the standard Docker network model combined with NAT rules on
your host to redirect inbound traffic to/outbound traffic from the
appropriate IP address.&lt;/p>
&lt;p>Assign our target address to your host interface:&lt;/p>
&lt;pre>&lt;code># ip addr add 10.12.0.117/21 dev em1
&lt;/code>&lt;/pre>
&lt;p>Start your docker container, using the &lt;code>-p&lt;/code> option to bind exposed
ports to an ip address and port on the host:&lt;/p>
&lt;pre>&lt;code># docker run -d --name web -p 10.12.0.117:80:80 larsks/simpleweb
&lt;/code>&lt;/pre>
&lt;p>With this command, Docker will set up the &lt;a href="https://docs.docker.com/articles/networking/">standard network&lt;/a> model:&lt;/p>
&lt;ul>
&lt;li>It will create a &lt;a href="http://lwn.net/Articles/232688/">veth&lt;/a> interface pair.&lt;/li>
&lt;li>Connect one end to the &lt;code>docker0&lt;/code> bridge.&lt;/li>
&lt;li>Place the other inside the container namespace as &lt;code>eth0&lt;/code>.&lt;/li>
&lt;li>Assign an ip address from the network used by the &lt;code>docker0&lt;/code> bridge.&lt;/li>
&lt;/ul>
&lt;p>Because we added &lt;code>-p 10.12.0.117:80:80&lt;/code> to our command line, Docker
will also create the following rule in the &lt;code>nat&lt;/code> table &lt;code>DOCKER&lt;/code>
chain (which is run from the &lt;code>PREROUTING&lt;/code> chain):&lt;/p>
&lt;pre>&lt;code>-A DOCKER -d 10.12.0.117/32 ! -i docker0 -p tcp -m tcp
--dport 80 -j DNAT --to-destination 172.17.0.4:80
&lt;/code>&lt;/pre>
&lt;p>This matches traffic TO our target address (&lt;code>-d 10.12.0.117/32&lt;/code>) not
originating on the &lt;code>docker0&lt;/code> bridge (&lt;code>! -i docker0&lt;/code>) destined for
&lt;code>tcp&lt;/code> port &lt;code>80&lt;/code> (&lt;code>-p tcp -m tcp --dport 80&lt;/code>). Matching traffic has
it&amp;rsquo;s destination set to the address of our docker container (&lt;code>-j DNAT --to-destination 172.17.0.4:80&lt;/code>).&lt;/p>
&lt;p>From a host elsewhere on the network, we can now access the web server
at our selected ip address:&lt;/p>
&lt;pre>&lt;code>$ curl http://10.12.0.117/hello.html
Hello world
&lt;/code>&lt;/pre>
&lt;p>If our container were to initiate a network connection with another
system, that connection would appear to originate with ip address of
our &lt;em>host&lt;/em>. We can fix that my adding a &lt;code>SNAT&lt;/code> rule to the
&lt;code>POSTROUTING&lt;/code> chain to modify the source address:&lt;/p>
&lt;pre>&lt;code># iptables -t nat -I POSTROUTING -s $(docker-ip web) \
-j SNAT --to-source 10.12.0.117
&lt;/code>&lt;/pre>
&lt;p>Note here the use of &lt;code>-I POSTROUTING&lt;/code>, which places the rule at the
&lt;em>top&lt;/em> of the &lt;code>POSTROUTING&lt;/code> chain. This is necessary because, by
default, Docker has already added the following rule to the top of the
&lt;code>POSTROUTING&lt;/code> chain:&lt;/p>
&lt;pre>&lt;code>-A POSTROUTING -s 172.17.0.0/16 ! -d 172.17.0.0/16 -j MASQUERADE
&lt;/code>&lt;/pre>
&lt;p>Because this &lt;code>MASQUERADE&lt;/code> rule matches traffic from any container, we
need to place our rule earlier in the &lt;code>POSTROUTING&lt;/code> chain for it to
have any affect.&lt;/p>
&lt;p>With these rules in place, traffic to 10.12.0.117 (port 80) is
directed to our &lt;code>web&lt;/code> container, and traffic &lt;em>originating&lt;/em> in the web
container will appear to come from 10.12.0.117.&lt;/p>
&lt;h2 id="with-linux-bridge-devices">With Linux Bridge devices&lt;/h2>
&lt;p>The previous example was relatively easy to configure, but has a few
shortcomings. If you need to configure an interface using DHCP, or if
you have an application that needs to be on the same layer 2 broadcast
domain as other devices on your network, NAT rules aren&amp;rsquo;t going to
work out.&lt;/p>
&lt;p>This solution uses a Linux bridge device, created using &lt;code>brctl&lt;/code>, to
connect your containers directly to a physical network.&lt;/p>
&lt;p>Start by creating a new bridge device. In this example, we&amp;rsquo;ll create
one called &lt;code>br-em1&lt;/code>:&lt;/p>
&lt;pre>&lt;code># brctl addbr br-em1
# ip link set br-em1 up
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re going to add &lt;code>em1&lt;/code> to this bridge, and move the ip address from
&lt;code>em1&lt;/code> onto the bridge.&lt;/p>
&lt;p>&lt;strong>WARNING&lt;/strong>: This is not something you should do remotely, especially
for the first time, and making this persistent varies from
distribution to distribution, so this will not be a persistent
configuration.&lt;/p>
&lt;p>Look at the configuration of interface &lt;code>em1&lt;/code> and note the existing ip
address:&lt;/p>
&lt;pre>&lt;code># ip addr show em1
2: em1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc mq master br-em1 state UP group default qlen 1000
link/ether 00:1d:09:63:71:30 brd ff:ff:ff:ff:ff:ff
inet 10.12.0.76/21 scope global br-em1
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>Look at your current routes and note the default route:&lt;/p>
&lt;pre>&lt;code># ip route
default via 10.12.7.254 dev em1
10.12.0.0/21 dev em1 proto kernel scope link src 10.12.0.76
&lt;/code>&lt;/pre>
&lt;p>Now, add this device to your bridge:&lt;/p>
&lt;pre>&lt;code># brctl addif br-em1 em1
&lt;/code>&lt;/pre>
&lt;p>Configure the bridge with the address that used to belong to
&lt;code>em1&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ip addr del 10.12.0.76/21 dev em1
# ip addr add 10.12.0.76/21 dev br-em1
&lt;/code>&lt;/pre>
&lt;p>And move the default route to the bridge:&lt;/p>
&lt;pre>&lt;code># ip route del default
# ip route add default via 10.12.7.254 dev br-em1
&lt;/code>&lt;/pre>
&lt;p>If you were doing this remotely; you would do this all in one line
like this:&lt;/p>
&lt;pre>&lt;code># ip addr add 10.12.0.76/21 dev br-em1; \
ip addr del 10.12.0.76/21 dev em1; \
brctl addif br-em1 em1; \
ip route del default; \
ip route add default via 10.12.7.254 dev br-em1
&lt;/code>&lt;/pre>
&lt;p>At this point, verify that you still have network connectivity:&lt;/p>
&lt;pre>&lt;code># curl http://google.com/
&amp;lt;HTML&amp;gt;&amp;lt;HEAD&amp;gt;&amp;lt;meta http-equiv=&amp;quot;content-type&amp;quot; content=&amp;quot;text/html;charset=utf-8&amp;quot;&amp;gt;
[...]
&lt;/code>&lt;/pre>
&lt;p>Start up the web container:&lt;/p>
&lt;pre>&lt;code># docker run -d --name web larsks/simpleweb
&lt;/code>&lt;/pre>
&lt;p>This will give us the normal &lt;code>eth0&lt;/code> interface inside the container,
but we&amp;rsquo;re going to ignore that and add a new one.&lt;/p>
&lt;p>Create a &lt;a href="http://lwn.net/Articles/232688/">veth&lt;/a> interface pair:&lt;/p>
&lt;pre>&lt;code># ip link add web-int type veth peer name web-ext
&lt;/code>&lt;/pre>
&lt;p>Add the &lt;code>web-ext&lt;/code> link to the &lt;code>br-eth0&lt;/code> bridge:&lt;/p>
&lt;pre>&lt;code># brctl addif br-em1 web-ext
&lt;/code>&lt;/pre>
&lt;p>And add the &lt;code>web-int&lt;/code> interface to the namespace of the container:&lt;/p>
&lt;pre>&lt;code># ip link set netns $(docker-pid web) dev web-int
&lt;/code>&lt;/pre>
&lt;p>Next, we&amp;rsquo;ll use the &lt;a href="http://man7.org/linux/man-pages/man1/nsenter.1.html">nsenter&lt;/a> command (part of the &lt;code>util-linux&lt;/code> package) to run some commands inside the &lt;code>web&lt;/code> container. Start by bringing up the link inside the container:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip link set web-int up
&lt;/code>&lt;/pre>
&lt;p>Assign our target ip address to the interface:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip addr add 10.12.0.117/21 dev web-int
&lt;/code>&lt;/pre>
&lt;p>And set a new default route inside the container:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip route del default
# nsenter -t $(docker-pid web) -n ip route add default via 10.12.7.254 dev web-int
&lt;/code>&lt;/pre>
&lt;p>Again, we can verify from another host that the web server is
available at 10.12.0.117:&lt;/p>
&lt;pre>&lt;code>$ curl http://10.12.0.117/hello.html
Hello world
&lt;/code>&lt;/pre>
&lt;p>Note that in this example we have assigned a static ip address, but we
could just have easily acquired an address using DHCP. After running:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip link set web-int up
&lt;/code>&lt;/pre>
&lt;p>We can run:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n -- dhclient -d web-int
Internet Systems Consortium DHCP Client 4.2.6
Copyright 2004-2014 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/
Listening on LPF/web-int/6e:f0:a8:c6:f0:43
Sending on LPF/web-int/6e:f0:a8:c6:f0:43
Sending on Socket/fallback
DHCPDISCOVER on web-int to 255.255.255.255 port 67 interval 4 (xid=0x3aaab45b)
DHCPREQUEST on web-int to 255.255.255.255 port 67 (xid=0x3aaab45b)
DHCPOFFER from 10.12.7.253
DHCPACK from 10.12.7.253 (xid=0x3aaab45b)
bound to 10.12.6.151 -- renewal in 714 seconds.
&lt;/code>&lt;/pre>
&lt;h2 id="with-open-vswitch-bridge-devices">With Open vSwitch Bridge devices&lt;/h2>
&lt;p>This process is largely the same as in the previous example, but we
use &lt;a href="http://openvswitch.org/">Open vSwitch&lt;/a> instead of the legacy Linux bridge devices.
These instructions assume that you have already installed and started
Open vSwitch on your system.&lt;/p>
&lt;p>Create an OVS bridge using the &lt;code>ovs-vsctl&lt;/code> command:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-br br-em1
# ip link set br-em1 up
&lt;/code>&lt;/pre>
&lt;p>And add your external interface:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-port br-em1 em1
&lt;/code>&lt;/pre>
&lt;p>And then proceed as in the previous set of instructions.&lt;/p>
&lt;p>The equivalent all-in-one command is:&lt;/p>
&lt;pre>&lt;code># ip addr add 10.12.0.76/21 dev br-em1; \
ip addr del 10.12.0.76/21 dev em1; \
ovs-vsctl add-port br-em1 em1; \
ip route del default; \
ip route add default via 10.12.7.254 dev br-em1
&lt;/code>&lt;/pre>
&lt;p>Once that completes, your openvswitch configuration should look like
this:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl show
0b1d5895-88e6-42e5-a1da-ad464c75198c
Bridge &amp;quot;br-em1&amp;quot;
Port &amp;quot;br-em1&amp;quot;
Interface &amp;quot;br-em1&amp;quot;
type: internal
Port &amp;quot;em1&amp;quot;
Interface &amp;quot;em1&amp;quot;
ovs_version: &amp;quot;2.1.2&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>To add the &lt;code>web-ext&lt;/code> interface to the bridge, run:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-port br-em1 web-ext
&lt;/code>&lt;/pre>
&lt;p>Instead of:&lt;/p>
&lt;pre>&lt;code># brctl addif br-em1 web-ext
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>WARNING&lt;/strong>: The Open vSwitch configuration persists between reboots.
This means that when your system comes back up, &lt;code>em1&lt;/code> will still be a
member of &lt;code>br-em&lt;/code>, which will probably result in no network
connectivity for your host.&lt;/p>
&lt;p>Before rebooting your system, make sure to &lt;code>ovs-vsctl del-port br-em1 em1&lt;/code>.&lt;/p>
&lt;h2 id="with-macvlan-devices">With macvlan devices&lt;/h2>
&lt;p>This process is similar to the previous two, but instead of using a
bridge device we will create a &lt;a href="http://backreference.org/2014/03/20/some-notes-on-macvlanmacvtap/">macvlan&lt;/a>, which is a virtual network
interface associated with a physical interface. Unlike the previous
two solutions, this does not require any interruption to your primary
network interface.&lt;/p>
&lt;p>Start by creating a docker container as in the previous examples:&lt;/p>
&lt;pre>&lt;code># docker run -d --name web larsks/simpleweb
&lt;/code>&lt;/pre>
&lt;p>Create a &lt;code>macvlan&lt;/code> interface associated with your physical interface:&lt;/p>
&lt;pre>&lt;code># ip link add em1p0 link em1 type macvlan mode bridge
&lt;/code>&lt;/pre>
&lt;p>This creates a new &lt;code>macvlan&lt;/code> interface named &lt;code>em1p0&lt;/code> (but you can
name it anything you want) associated with interface &lt;code>em1&lt;/code>. We are
setting it up in &lt;code>bridge&lt;/code> mode, which permits all &lt;code>macvlan&lt;/code> interfaces
to communicate with eachother.&lt;/p>
&lt;p>Add this interface to the container&amp;rsquo;s network namespace:&lt;/p>
&lt;pre>&lt;code># ip link set netns $(docker-pid web) em1p0
&lt;/code>&lt;/pre>
&lt;p>Bring up the link:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip link set em1p0 up
&lt;/code>&lt;/pre>
&lt;p>And configure the ip address and routing:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip route del default
# nsenter -t $(docker-pid web) -n ip addr add 10.12.0.117/21 dev em1p0
# nsenter -t $(docker-pid web) -n ip route add default via 10.12.7.254 dev em1p0
&lt;/code>&lt;/pre>
&lt;p>And demonstrate that &lt;em>from another host&lt;/em> the web server is available
at 10.12.0.117:&lt;/p>
&lt;pre>&lt;code>$ curl http://10.12.0.117/hello.html
Hello world
&lt;/code>&lt;/pre>
&lt;p>But note that if you were to try the same thing on the host, you would
get:&lt;/p>
&lt;pre>&lt;code>curl: (7) Failed connect to 10.12.0.117:80; No route to host
&lt;/code>&lt;/pre>
&lt;p>The &lt;em>host&lt;/em> is unable to communicate with &lt;code>macvlan&lt;/code> devices via the
primary interface. You can create &lt;em>another&lt;/em> &lt;code>macvlan&lt;/code> interface on
the host, give it an address on the appropriate network, and then set
up routes to your containers via that interface:&lt;/p>
&lt;pre>&lt;code># ip link add em1p1 link em1 type macvlan mode bridge
# ip addr add 10.12.6.144/21 dev em1p1
# ip route add 10.12.0.117 dev em1p1
&lt;/code>&lt;/pre></content></item><item><title>Tracking down a kernel bug with git bisect</title><link>https://blog.oddbit.com/post/2014-07-21-tracking-down-a-kernel-bug-wit/</link><pubDate>Mon, 21 Jul 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-07-21-tracking-down-a-kernel-bug-wit/</guid><description>After a recent upgrade of my Fedora 20 system to kernel 3.15.mumble, I started running into a problem (BZ 1121345) with my Docker containers. Operations such as su or runuser would fail with the singularly unhelpful System error message:
$ docker run -ti fedora /bin/bash bash-4.2# su -c 'uptime' su: System error Hooking up something (like, say, socat unix-listen:/dev/log -) to /dev/log revealed that the system was logging:
Jul 19 14:31:18 su: PAM audit_log_acct_message() failed: Operation not permitted Downgrading the kernel to 3.</description><content>&lt;p>After a recent upgrade of my Fedora 20 system to kernel 3.15.mumble, I
started running into a problem (&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1121345">BZ 1121345&lt;/a>) with my &lt;a href="https://www.docker.com/">Docker&lt;/a>
containers. Operations such as &lt;code>su&lt;/code> or &lt;code>runuser&lt;/code> would fail with the
singularly unhelpful &lt;code>System error&lt;/code> message:&lt;/p>
&lt;pre>&lt;code>$ docker run -ti fedora /bin/bash
bash-4.2# su -c 'uptime'
su: System error
&lt;/code>&lt;/pre>
&lt;p>Hooking up something (like, say, &lt;code>socat unix-listen:/dev/log -&lt;/code>) to
&lt;code>/dev/log&lt;/code> revealed that the system was logging:&lt;/p>
&lt;pre>&lt;code>Jul 19 14:31:18 su: PAM audit_log_acct_message() failed: Operation not permitted
&lt;/code>&lt;/pre>
&lt;p>Downgrading the kernel to 3.14 immediately resolved the problem,
suggesting that this was at least partly a kernel issue. This seemed
like a great opportunity to play with the &lt;a href="http://git-scm.com/docs/git-bisect">git bisect&lt;/a> command,
which uses a binary search to find which commit introduced a
particular problem.&lt;/p>
&lt;p>Unfortunately, between the version I knew to work correctly (3.14) and
the version I knew to have a problem (3.15) there were close to 15,000
commits, which seemed like a large space to search by hand.&lt;/p>
&lt;p>Fortunately, &lt;code>git bisect&lt;/code> can be easily automated via &lt;code>git bisect run&lt;/code>
subcommand, which after checking out a commit will run a script to
determine if the current commit is &amp;ldquo;good&amp;rdquo; or &amp;ldquo;bad&amp;rdquo;. So all I have to
do is write a script&amp;hellip;that&amp;rsquo;s not so bad!&lt;/p>
&lt;figure class="left" >
&lt;img src="ha-ha.jpg" />
&lt;/figure>
&lt;p>It actually ended up being somewhat tricky.&lt;/p>
&lt;h2 id="testing-kernels-is-hard">Testing kernels is hard&lt;/h2>
&lt;p>In order to test for this problem, I would need to use arbitrary
kernels generated during the &lt;code>git bisect&lt;/code> operation to boot a system
functional enough to run docker, and then run docker and somehow
communicate the result of that test back to the build environment.&lt;/p>
&lt;p>I started with the &lt;a href="http://fedoraproject.org/get-fedora#clouds">Fedora 20 cloud image&lt;/a>, which is nice and
small but still the same platform as my laptop on which I was
experiencing the problem. I would need to correct a few things before
moving forward:&lt;/p>
&lt;p>The Fedora cloud images (a) do not support password authentication and
(b) expect a datasource to be available to &lt;a href="http://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> (without
which you get errors on the console and potentially a delay waiting
for the &lt;code>login:&lt;/code> prompt), so prior to using the image in this test I
made some changes by mounting it locally:&lt;/p>
&lt;pre>&lt;code># modprobe nbd max_part=8
# qemu-nbd -c /dev/nbd0 Fedora-x86_64-20-20140407-sda.qcow2
# mount /dev/nbd0p1 /mnt
# systemd-nspawn -D /mnt
&lt;/code>&lt;/pre>
&lt;p>And then:&lt;/p>
&lt;ul>
&lt;li>I set a password for the &lt;code>root&lt;/code> account and&lt;/li>
&lt;li>I removed the &lt;code>cloud-init&lt;/code> package.&lt;/li>
&lt;/ul>
&lt;p>For this test I would be using the &lt;code>qemu-system-x86_64&lt;/code> command
directly, rather than working through &lt;code>libvirt&lt;/code> (&lt;code>qemu&lt;/code> has options
for convenient debugging with &lt;code>gdb&lt;/code>, and is also able to access the
filesystem as the calling &lt;code>uid&lt;/code> whereas &lt;code>libvirt&lt;/code> is typically running
as another user).&lt;/p>
&lt;p>I would need to perform an initial &lt;code>docker pull&lt;/code> in the image, which
meant I was going to need a functioning network, so first I had to set
up a network environment for qemu.&lt;/p>
&lt;h3 id="network-configuration">Network configuration&lt;/h3>
&lt;p>I created a bridge interface named &lt;code>qemu0&lt;/code> to be used by &lt;code>qemu&lt;/code>. I added
to &lt;code>/etc/sysconfig/network-scripts/ifcfg-qemu0&lt;/code> the following:&lt;/p>
&lt;pre>&lt;code>DEVICE=qemu0
TYPE=Bridge
ONBOOT=yes
BOOTPROTO=none
STP=no
NAME=&amp;quot;Bridge qemu0&amp;quot;
IPADDR=192.168.210.1
NETMASK=255.255.255.0
&lt;/code>&lt;/pre>
&lt;p>This is largely equivalent to the following, but persists after reboot:&lt;/p>
&lt;pre>&lt;code>brctl addbr qemu0
ip addr add 192.168.210.1/24 dev qemu0
ip link set qemu0 up
&lt;/code>&lt;/pre>
&lt;p>I created a &lt;a href="https://www.kernel.org/doc/Documentation/networking/tuntap.txt">tap&lt;/a> interface named &lt;code>linux0&lt;/code>:&lt;/p>
&lt;pre>&lt;code>ip tuntap add dev linux0 mode tap user lars
&lt;/code>&lt;/pre>
&lt;p>And added it to the bridge:&lt;/p>
&lt;pre>&lt;code>brctl addif qemu0 linux0
&lt;/code>&lt;/pre>
&lt;p>I also started up &lt;code>dnsmasq&lt;/code> process listening on &lt;code>qemu0&lt;/code> to provide
DNS lookup and DHCP service to qemu instances attached to this bridge.
The &lt;code>dnsmasq&lt;/code> configuration looked like this:&lt;/p>
&lt;pre>&lt;code>listen-address=192.168.210.1
bind-interfaces
dhcp-range=192.168.210.10,192.168.210.254
&lt;/code>&lt;/pre>
&lt;h3 id="running-qemu">Running qemu&lt;/h3>
&lt;p>With the network environment set up, I needed to figure out an
appropriate qemu command line. This is what I finally ended up with,
in a script called &lt;code>boot-kernel&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
qemu-system-x86_64 -m 1024M \
-drive file=fedora.img,if=virtio \
-append &amp;quot;console=hvc0 root=/dev/vda1 selinux=0 $BOOT_ARGS&amp;quot; \
-initrd initrd.img \
-kernel arch/x86_64/boot/bzImage \
-machine accel=kvm \
-netdev tap,id=net0,ifname=linux0,script=no,downscript=no \
-device virtio-net,netdev=net0,mac=52:54:00:c0:ff:ee \
-chardev stdio,id=stdio,mux=on \
-device virtio-serial-pci \
-device virtconsole,chardev=stdio \
-mon chardev=stdio \
-fsdev local,id=fs0,path=$PWD,security_model=none \
-device virtio-9p-pci,fsdev=fs0,mount_tag=kernel_src \
-display none \
$QEMU_ARGS
&lt;/code>&lt;/pre>
&lt;p>These lines set up the networking:&lt;/p>
&lt;pre>&lt;code> -netdev tap,id=net0,ifname=linux0,script=no,downscript=no \
-device virtio-net,netdev=net0,mac=52:54:00:c0:ff:ee \
&lt;/code>&lt;/pre>
&lt;p>These lines set up console on &lt;code>stdin&lt;/code>/&lt;code>stdout&lt;/code> and multiplex the
console with the qemu monitor:&lt;/p>
&lt;pre>&lt;code> -chardev stdio,id=stdio,mux=on \
-device virtio-serial-pci \
-device virtconsole,chardev=stdio \
-mon chardev=stdio \
&lt;/code>&lt;/pre>
&lt;p>These lines set up access to the current working directory as a &lt;code>9p&lt;/code>
filesystem:&lt;/p>
&lt;pre>&lt;code> -fsdev local,id=fs0,path=$PWD,security_model=none \
-device virtio-9p-pci,fsdev=fs0,mount_tag=kernel_src \
&lt;/code>&lt;/pre>
&lt;p>Within the qemu instance, this lets me access my working directory with:&lt;/p>
&lt;pre>&lt;code>mount -t 9p kernel_src /mnt
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>$BOOT_ARGS&lt;/code> and &lt;code>$QEMU_ARGS&lt;/code> in the script allow me to modify the
behavior of the script by setting environment variables when calling
it, like this:&lt;/p>
&lt;pre>&lt;code>QEMU_ARGS=&amp;quot;-s&amp;quot; sh boot-kernel
&lt;/code>&lt;/pre>
&lt;h3 id="first-boot">First boot&lt;/h3>
&lt;p>I tried to boot the image using my existing kernel and initrd from
&lt;code>/boot&lt;/code>, and ran into a problem:&lt;/p>
&lt;pre>&lt;code>[ 184.060756] dracut-initqueue[218]: Warning: Could not boot.
[ 184.062855] dracut-initqueue[218]: Warning: /dev/ssd/root does not exist
Starting Dracut Emergency Shell...
Warning: /dev/ssd/root does not exist
Generating &amp;quot;/run/initramfs/rdsosreport.txt&amp;quot;
Entering emergency mode. Exit the shell to continue.
&lt;/code>&lt;/pre>
&lt;p>The what now? &lt;code>/dev/ssd/root&lt;/code> is the root device for my host system,
but wasn&amp;rsquo;t anywhere in the kernel command line I used when booting
qemu. It turns out that this was embedded in the initrd image in
&lt;code>/etc/cmdline.d/90lvm.conf&lt;/code>. After removing that file from the
image&amp;hellip;&lt;/p>
&lt;pre>&lt;code># mkdir initrd
# cd initrd
# zcat /boot/initramfs-3.15.6-200.fc20.x86_64.img | cpio -id
# rm -rf etc/cmdline.d
# find . -print | cpio -o -Hcrc | gzip &amp;gt; ../initrd.img
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;I was able to boot successfully and log in.&lt;/p>
&lt;h3 id="i-bet-you-thought-we-were-done">I bet you thought we were done!&lt;/h3>
&lt;p>Modern systems are heavily modular. Without access to a module tree
matching the kernel, I would be unable to successfully boot the
system, let alone use Docker. Looking at which modules were loaded
when I ran &lt;code>docker&lt;/code> with the above image, I set up a custom kernel
configuration that would permit me to boot and run docker without
requiring any loadable modules. This would allow me to use the same
image for each kernel without needing to re-populate it with modules
each time I built a kernel.&lt;/p>
&lt;p>The kernel configuration I ended up with is available &lt;a href="https://blog.oddbit.com/assets/2014/07/21/kernel-config.txt">here&lt;/a>.&lt;/p>
&lt;h3 id="testing-docker">Testing docker&lt;/h3>
&lt;p>The last step in this process is putting together something that tests
&lt;code>docker&lt;/code> and exposes the result of that test to the build environment.
I added the following script to the image as &lt;code>/root/docker-test&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
grep NO_DOCKER_TEST /proc/cmdline &amp;amp;&amp;amp; exit 0
if [ -d /mnt/test_result ]; then
docker run --rm -i fedora sh -c 'su -c true &amp;amp;&amp;amp; echo OKAY || echo FAILED' \
&amp;gt; /mnt/test_result/stdout \
2&amp;gt; /mnt/test_result/stderr
poweroff
fi
&lt;/code>&lt;/pre>
&lt;p>This relies on the following entry in &lt;code>/etc/fstab&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kernel_src /mnt 9p defaults 0 0
&lt;/code>&lt;/pre>
&lt;p>That mounts the build directory as a &lt;code>9p&lt;/code> filesystem on &lt;code>/mnt&lt;/code>. This
allows us to write out test results to, e.g.,
&lt;code>/mnt/test_result/stdout&lt;/code> and have that appear in the &lt;code>test_result&lt;/code>
directory inside the kernel source.&lt;/p>
&lt;p>This script is run at the end of the boot process via an entry in
&lt;code>/etc/rc.d/rc.local&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
sh /root/docker-test
&lt;/code>&lt;/pre>
&lt;p>Running the &lt;code>boot-kernel&lt;/code> script without additional configuration will
cause the image to boot up, run the docker test, and then exit.&lt;/p>
&lt;h2 id="running-git-bisect">Running git-bisect&lt;/h2>
&lt;p>At this point we have just about everything we need to start running
&lt;code>git bisect&lt;/code>. For the initial run, I&amp;rsquo;m going to use git tag &lt;code>v3.14&lt;/code>
as the &amp;ldquo;known good&amp;rdquo; commit and &lt;code>v3.15&lt;/code> as the &amp;ldquo;known bad&amp;rdquo; commit, so
we start &lt;code>git bisect&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code>$ git bisect start v3.15 v3.14
&lt;/code>&lt;/pre>
&lt;p>Then we run &lt;code>git bisect run sh bisect-test&lt;/code>, where &lt;code>bisect-test&lt;/code> is
the following shell script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
# Rebuild the kernel
make olddefconfig
make -j8
# Clear out old test results and run the test
rm -f test_result/{stdout,stderr}
sh boot-kernel
# Report results to git-bisect
if grep OKAY test_result/stdout; then
exit 0
else
exit 1
fi
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then we go out for a cup of coffee or something, because that&amp;rsquo;s
going to take a while.&lt;/p>
&lt;h2 id="keep-digging-watson">Keep digging, Watson&lt;/h2>
&lt;p>The initial run of &lt;code>git bisect&lt;/code> narrowed the change down to the
&lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b7d3622">following commit&lt;/a>:&lt;/p>
&lt;pre>&lt;code>commit b7d3622a39fde7658170b7f3cf6c6889bb8db30d
Merge: f3411cb d8ec26d
Author: Eric Paris &amp;lt;eparis@redhat.com&amp;gt;
Date: Fri Mar 7 11:41:32 2014 -0500
Merge tag 'v3.13' into for-3.15
Linux 3.13
Conflicts:
include/net/xfrm.h
Simple merge where v3.13 removed 'extern' from definitions and the audit
tree did s/u32/unsigned int/ to the same definitions.
&lt;/code>&lt;/pre>
&lt;p>As you can see (from the &lt;code>Merge:&lt;/code> header), this is a merge commit, in
which an entire set of changes was joined into the &lt;code>master&lt;/code> branch.
So while this commit is technically the first commit in which this
problem appears in the &lt;code>master&lt;/code> branch&amp;hellip;it is not actually the commit
that introduced the problem.&lt;/p>
&lt;p>I was in luck, though, because looking at the history for the left
side of this branch (starting with &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f3411cb">f3411cb&lt;/a>) showed a series of
patches to the audit subsystem:&lt;/p>
&lt;pre>&lt;code>$ git log --oneline f3411cb
f3411cb audit: whitespace fix in kernel-parameters.txt
8626877 audit: fix location of __net_initdata for audit_net_ops
4f06632 audit: remove pr_info for every network namespace
262fd3a audit: Modify a set of system calls in audit class definitions
3e1d0bb audit: Convert int limit uses to u32
d957f7b audit: Use more current logging style
b8dbc32 audit: Use hex_byte_pack_upper
06bdadd audit: correct a type mismatch in audit_syscall_exit()
1ce319f audit: reorder AUDIT_TTY_SET arguments
0e23bac audit: rework AUDIT_TTY_SET to only grab spin_lock once
3f0c5fa audit: remove needless switch in AUDIT_SET
70249a9 audit: use define's for audit version
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;etc.&lt;/p>
&lt;p>I picked as a starting point the merge commit previous to &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f3411cb">f3411cb&lt;/a>:&lt;/p>
&lt;pre>&lt;code>$ git log --merges -1
commit fc582aef7dcc27a7120cf232c1e76c569c7b6eab
Merge: 9175c9d 5e01dc7
Author: Eric Paris &amp;lt;eparis@redhat.com&amp;gt;
Date: Fri Nov 22 18:57:08 2013 -0500
Merge tag 'v3.12'
Linux 3.12
Conflicts:
fs/exec.c
&lt;/code>&lt;/pre>
&lt;p>And ran &lt;code>git bisect&lt;/code> again from &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=fc582ae">that commit&lt;/a> through to &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f3411cb">f3411cb&lt;/a>:&lt;/p>
&lt;pre>&lt;code>$ git bisect start f3411cb fc582ae
$ git bisect run sh bisect-test
&lt;/code>&lt;/pre>
&lt;p>Which ultimately ended up with &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=33faba7">this commit&lt;/a>:&lt;/p>
&lt;pre>&lt;code>33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb is the first bad commit
commit 33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb
Author: Richard Guy Briggs &amp;lt;rgb@redhat.com&amp;gt;
Date: Tue Jul 16 13:18:45 2013 -0400
audit: listen in all network namespaces
Convert audit from only listening in init_net to use register_pernet_subsys()
to dynamically manage the netlink socket list.
Signed-off-by: Richard Guy Briggs &amp;lt;rgb@redhat.com&amp;gt;
Signed-off-by: Eric Paris &amp;lt;eparis@redhat.com&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Running &lt;code>git bisect log&lt;/code> shows us what revisions were checked as part
of this process:&lt;/p>
&lt;pre>&lt;code># bad: [f3411cb2b2e396a41ed3a439863f028db7140a34] audit: whitespace fix in kernel-parameters.txt
# good: [fc582aef7dcc27a7120cf232c1e76c569c7b6eab] Merge tag 'v3.12'
git bisect start 'f3411cb' 'fc582ae'
# bad: [ff235f51a138fc61e1a22dcb8b072d9c78c2a8cc] audit: Added exe field to audit core dump signal log
git bisect bad ff235f51a138fc61e1a22dcb8b072d9c78c2a8cc
# bad: [51cc83f024ee51de9da70c17e01ec6de524f5906] audit: add audit_backlog_wait_time configuration option
git bisect bad 51cc83f024ee51de9da70c17e01ec6de524f5906
# bad: [ae887e0bdcddb9d7acd8f1eb7b7795b438aa4950] audit: make use of remaining sleep time from wait_for_auditd
git bisect bad ae887e0bdcddb9d7acd8f1eb7b7795b438aa4950
# good: [2f2ad1013322c8f6c40fc6dafdbd32442fa730ad] audit: restore order of tty and ses fields in log output
git bisect good 2f2ad1013322c8f6c40fc6dafdbd32442fa730ad
# bad: [e789e561a50de0aaa8c695662d97aaa5eac9d55f] audit: reset audit backlog wait time after error recovery
git bisect bad e789e561a50de0aaa8c695662d97aaa5eac9d55f
# bad: [33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb] audit: listen in all network namespaces
git bisect bad 33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb
# first bad commit: [33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb] audit: listen in all network namespaces
&lt;/code>&lt;/pre>
&lt;p>The commit found by &lt;code>git bisect&lt;/code> seems like a reasonable candidate;
it&amp;rsquo;s a patch against the audit subsystem and has something to do with
namespaces, which are central to Docker&amp;rsquo;s proper operation.&lt;/p>
&lt;h2 id="debugging-the-problem">Debugging the problem&lt;/h2>
&lt;p>We can boot the kernel built from 33faba7 with the &lt;code>boot-kernel&lt;/code>
script, adding the &lt;code>-s&lt;/code> argument to qemu to start a &lt;code>gdbserver&lt;/code> on
port &lt;code>1234&lt;/code>:&lt;/p>
&lt;pre>&lt;code>sh BOOT_ARGS=NO_DOCKER_TEST QEMU_ARGS=&amp;quot;-s&amp;quot; boot-kernel
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>A caveat about attaching to qemu with gdb: qemu has a &lt;code>-S&lt;/code> option
that will cause the virtual machine to halt at startup, such that
you can attach before it starts booting and &amp;ndash; in theory &amp;ndash; set
breakpoints in the early boot process. In practice this doesn&amp;rsquo;t
work well at all (possibly because the vm switches from 32- to
64-bit operation during the boot process, which makes gdb unhappy).
You&amp;rsquo;re better off attaching after the kernel has booted.&lt;/p>
&lt;/blockquote>
&lt;p>In another window, we attach &lt;code>gdb&lt;/code> to the running &lt;code>qemu&lt;/code> process:&lt;/p>
&lt;pre>&lt;code>$ gdb vmlinux
Reading symbols from vmlinux...done.
(gdb) target remote :1234
Remote debugging using :1234
native_safe_halt () at /home/lars/src/linux/arch/x86/include/asm/irqflags.h:50
50 }
(gdb)
&lt;/code>&lt;/pre>
&lt;p>I know we&amp;rsquo;re getting the &lt;code>EPERM&lt;/code> in response to sending audit
messages. Looking through the code in &lt;code>kernel/audit.c&lt;/code>, the
&lt;code>audit_receive_msg&lt;/code> seems like a reasonable place to start poking
about. At the beginning of &lt;code>audit_receive_msg&lt;/code>, I see the following
code:&lt;/p>
&lt;pre>&lt;code>err = audit_netlink_ok(skb, msg_type);
if (err)
return err;
&lt;/code>&lt;/pre>
&lt;p>So let&amp;rsquo;s set a breakpoint there if &lt;code>audit_netlink_ok()&lt;/code> returns an
error:&lt;/p>
&lt;pre>&lt;code>(gdb) br kernel/audit.c:752 if (err != 0)
&lt;/code>&lt;/pre>
&lt;p>And let our qemu process continue running:&lt;/p>
&lt;pre>&lt;code>(gdb) continue
Continuing.
&lt;/code>&lt;/pre>
&lt;p>Inside the qemu instance I start docker:&lt;/p>
&lt;pre>&lt;code>-bash-4.2# docker run -it fedora /bin/su -c uptime
&lt;/code>&lt;/pre>
&lt;p>And eventually &lt;code>gdb&lt;/code> hits the breakpoint:&lt;/p>
&lt;pre>&lt;code>Breakpoint 1, audit_receive_msg (nlh=0xffff88003819a400,
skb=0xffff880038044300) at kernel/audit.c:752
752 if (err)
&lt;/code>&lt;/pre>
&lt;p>If I look at the value of &lt;code>err&lt;/code> at this point:&lt;/p>
&lt;pre>&lt;code>(gdb) print err
$1 = -1
&lt;/code>&lt;/pre>
&lt;p>That it is, in fact, &lt;code>-EPERM&lt;/code>, which suggests we&amp;rsquo;re on the right
track. Taking a closer look at &lt;code>audit_netlink_ok()&lt;/code>, it&amp;rsquo;s obvious
that there are only three places where it can return &lt;code>-EPERM&lt;/code>. I
tried setting some breakpoint in this function but they weren&amp;rsquo;t
working correctly, probably due to to optimizations performed when
compiling the kernel. So instead of &lt;code>gdb&lt;/code>, in this step we just add a
bunch of &lt;code>pr_err()&lt;/code> statements to print out debugging information on
the console:&lt;/p>
&lt;pre>&lt;code>if ((current_user_ns() != &amp;amp;init_user_ns) ||
(task_active_pid_ns(current) != &amp;amp;init_pid_ns)) {
pr_err(&amp;quot;currnet_user_ns() check failed\n&amp;quot;);
return -EPERM;
}
.
.
.
case AUDIT_MAKE_EQUIV:
if (!capable(CAP_AUDIT_CONTROL)) {
pr_err(&amp;quot;CAP_AUDIT_CONTROL check failed\n&amp;quot;);
err = -EPERM;
}
break;
case AUDIT_USER:
.
.
.
case AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:
case AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:
if (!capable(CAP_AUDIT_WRITE)) {
pr_err(&amp;quot;CAP_AUDIT_WRITE check failed\n&amp;quot;);
err = -EPERM;
}
break;
&lt;/code>&lt;/pre>
&lt;p>With these in place, if I run the &lt;code>docker&lt;/code> command again I see:&lt;/p>
&lt;pre>&lt;code>[ 12.239860] currnet_user_ns() check failed
su: System error
&lt;/code>&lt;/pre>
&lt;p>It looks like we&amp;rsquo;ve found out where it&amp;rsquo;s failing! Of course, we&amp;rsquo;re
checking code right now that is several commits behind v3.15, so let&amp;rsquo;s
take a look the same function in the 3.15 release:&lt;/p>
&lt;pre>&lt;code>$ git checkout v3.15
&lt;/code>&lt;/pre>
&lt;p>Looking at &lt;code>audit_netlink_ok&lt;/code> in &lt;code>kernel/audit.c&lt;/code>, it looks as if that
initial check has changed:&lt;/p>
&lt;pre>&lt;code> /* Only support initial user namespace for now. */
/*
* We return ECONNREFUSED because it tricks userspace into thinking
* that audit was not configured into the kernel. Lots of users
* configure their PAM stack (because that's what the distro does)
* to reject login if unable to send messages to audit. If we return
* ECONNREFUSED the PAM stack thinks the kernel does not have audit
* configured in and will let login proceed. If we return EPERM
* userspace will reject all logins. This should be removed when we
* support non init namespaces!!
*/
if (current_user_ns() != &amp;amp;init_user_ns)
return -ECONNREFUSED;
&lt;/code>&lt;/pre>
&lt;p>So let&amp;rsquo;s insert our print statements into this version of the code and
see if we get the same behavior:&lt;/p>
&lt;pre>&lt;code>if (current_user_ns() != &amp;amp;init_user_ns) {
pr_err(&amp;quot;current_user-ns() check failed\n&amp;quot;);
return -ECONNREFUSED;
}
.
.
.
case AUDIT_MAKE_EQUIV:
/* Only support auditd and auditctl in initial pid namespace
* for now. */
if ((task_active_pid_ns(current) != &amp;amp;init_pid_ns)) {
pr_err(&amp;quot;init_pid_ns check failed\n&amp;quot;);
return -EPERM;
}
if (!netlink_capable(skb, CAP_AUDIT_CONTROL)) {
pr_err(&amp;quot;CAP_AUDIT_CONTROL check failed\n&amp;quot;);
err = -EPERM;
}
break;
.
.
.
case AUDIT_USER:
case AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:
case AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:
if (!netlink_capable(skb, CAP_AUDIT_WRITE)) {
pr_err(&amp;quot;CAP_AUDIT_WRITE check failed\n&amp;quot;);
err = -EPERM;
}
break;
&lt;/code>&lt;/pre>
&lt;p>Running the v3.15 kernel, I see:&lt;/p>
&lt;pre>&lt;code>[ 26.273992] audit: CAP_AUDIT_WRITE check failed
su: System error
&lt;/code>&lt;/pre>
&lt;p>So it looks like the intial failure in &lt;code>audit_netlink_ok()&lt;/code> was fixed,
but we&amp;rsquo;re stilling failing the &lt;code>CAP_AUDIT_WRITE&lt;/code> check.&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>What&amp;rsquo;s going on here?&lt;/p>
&lt;p>Prior to &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=33faba7">33faba7&lt;/a>, audit messages were only accepted in the main
network namespace. Inside other network namespaces, processes sending
audit messages would simply receive &lt;code>ECONNREFUSED&lt;/code>. For example, this
is the result of using &lt;code>strace&lt;/code> on that &lt;code>docker run&lt;/code> command in a
pre-&lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=33faba7">33faba7&lt;/a> kernel:&lt;/p>
&lt;pre>&lt;code>539 sendto(3, &amp;quot;...authentication acct=\&amp;quot;root\&amp;quot; exe=\&amp;quot;/usr/bin/su\&amp;quot; hostname=? a&amp;quot;...,
112, 0, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = -1 ECONNREFUSED (Connection refused)
&lt;/code>&lt;/pre>
&lt;p>With &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=33faba7">33faba7&lt;/a>, audit messages are now accepted inside network
namespaces. This means that instead of simply getting &lt;code>ECONNREFUSED&lt;/code>,
messages must pass the kernel capability check. I spoke with some of
the audit subsystem maintainers (including Richard Guy Briggs, the
author of this patch series), and the general consensus is that &amp;ldquo;if
you want to write audit messages you need &lt;code>CAP_AUDIT_WRITE&lt;/code>&amp;rdquo;.&lt;/p>
&lt;p>So while this patch did change the behavior of the kernel from the
perspective of container tools such as Docker, the fix needs to be in
the tool creating the namespaces.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>This issue was reported against Fedora in &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1121345">BZ 1121345&lt;/a> and &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1119849">BZ
1119849&lt;/a>. This issue was also reported against Docker in &lt;a href="https://github.com/dotcloud/docker/issues/6345">GHI 6345&lt;/a>
and &lt;a href="https://github.com/dotcloud/docker/issues/7123">GHI 7123&lt;/a>.&lt;/p>
&lt;p>This problem has been corrected upstream in
&lt;a href="https://github.com/dotcloud/docker/pull/7179" class="pull-request">#7179&lt;/a>
.&lt;/p>
&lt;p>Package &lt;a href="https://admin.fedoraproject.org/updates/FEDORA-2014-8877/docker-io-1.0.0-9.fc20">docker-io-1.0.0-9.fc20&lt;/a>, which includes
the above fix, is now available for Fedora 20 (and Fedora 19).&lt;/p></content></item></channel></rss>