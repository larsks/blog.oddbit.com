<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>tech on blog.oddbit.com</title><link>https://blog.oddbit.com/categories/tech/</link><description>Recent content in tech on blog.oddbit.com</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Lars Kellogg-Stedman</copyright><lastBuildDate>Fri, 17 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.oddbit.com/categories/tech/rss.xml" rel="self" type="application/rss+xml"/><item><title>Applying custom configuration to Nginx Gateway Fabric</title><link>https://blog.oddbit.com/post/2023-11-17-nginx-gateway-configuration/</link><pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2023-11-17-nginx-gateway-configuration/</guid><description>In this post, we take a look at how to apply custom Nginx configuration directives when you&amp;rsquo;re using the NGINX Gateway Fabric.
What&amp;rsquo;s the NGINX Gateway Fabric? The NGINX Gateway Fabric is an implementation of the Kubernetes Gateway API.
What&amp;rsquo;s the Gateway API? The Gateway API is an evolution of the Ingress API; it aims to provide a flexible mechanism for managing north/south network traffic (that is, traffic entering or exiting your Kubernetes cluster), with additional work to support east/west traffic (traffic between pods in your cluster).</description><content>&lt;p>In this post, we take a look at how to apply custom Nginx configuration directives when you&amp;rsquo;re using the &lt;a href="https://github.com/nginxinc/nginx-gateway-fabric">NGINX Gateway Fabric&lt;/a>.&lt;/p>
&lt;h2 id="whats-the-nginx-gateway-fabric">What&amp;rsquo;s the NGINX Gateway Fabric?&lt;/h2>
&lt;p>The NGINX Gateway Fabric is an implementation of the Kubernetes &lt;a href="https://gateway-api.sigs.k8s.io/">Gateway API&lt;/a>.&lt;/p>
&lt;h2 id="whats-the-gateway-api">What&amp;rsquo;s the Gateway API?&lt;/h2>
&lt;p>The Gateway API is an evolution of the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a> API; it aims to provide a flexible mechanism for managing north/south network traffic (that is, traffic entering or exiting your Kubernetes cluster), with additional work to support east/west traffic (traffic between pods in your cluster).&lt;/p>
&lt;h2 id="whats-this-about-custom-configuration">What&amp;rsquo;s this about custom configuration?&lt;/h2>
&lt;p>I&amp;rsquo;ve deployed a local development cluster, and I wanted to be able to push images into an image registry hosted on the cluster. This requires (a) running a registry, which is easy, and (b) somehow exposing that registry outside the cluster, which is also easy unless you decide to make it more complex.&lt;/p>
&lt;p>In this case, I decided that rather than running an Ingress provider I was going to start familiarizing myself with the Gateway API, so I deployed NGINX Gateway Fabric. My first attempt at pushing an image into the registry looked like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ podman push --tls-verify=false example registry.apps.cluster1.house/example:latest
Getting image source signatures
Copying blob b9fe5313d237 done |
Copying blob cc2447e1835a done |
Copying blob cb8b0886acfb done |
Copying blob c4219a5645ea [===&amp;gt;----------------------------------] 9.3MiB / 80.2MiB | 372.7 MiB/s
Copying blob c6e5c62d1726 done |
Copying blob 9ee7eb11f876 done |
Copying blob f064c46326cb done |
Copying blob 9c45ffa2a02a done |
Copying blob 9a6c9897f309 done |
Copying blob 27a0dbb2828e done |
Error: writing blob: uploading layer chunked: StatusCode: 413, &amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;413 Request Entity Too Large&amp;lt;...
&lt;/code>&lt;/pre>&lt;p>Nginx, by default, restricts the maximum size of a request body to &lt;code>1m&lt;/code>, which is to say, 1 megabyte. You can increase (or remove) this limit by setting the &lt;a href="https://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size">&lt;code>client_max_body_size&lt;/code>&lt;/a> parameter&amp;hellip;but how do you do this in the context of a managed deployment like the NGINX Gateway Fabric?&lt;/p>
&lt;h2 id="via-the-api">Via the API?&lt;/h2>
&lt;p>As of this writing, there is no mechanism to apply custom configuration options via the API (although there is ongoing work to provide this, see issue &lt;a href="https://github.com/nginxinc/nginx-gateway-fabric/issues/1258">#1258&lt;/a>).&lt;/p>
&lt;h2 id="what-about-dropping-a-config-file-into-confd">What about dropping a config file into conf.d?&lt;/h2>
&lt;p>My first thought was that I could mount a custom configuration file into &lt;code>/etc/nginx/conf.d&lt;/code>, along the lines of:&lt;/p>
&lt;pre tabindex="0">&lt;code>...
containers:
- name: nginx
volumeMounts:
- name: nginx-extra-conf
mountPath: /etc/nginx/conf.d/client_max_body_size.conf
subPath: client_max_body_size
...
volumes:
- name: nginx-extra-conf
configMap:
name: nginx-extra-conf
&lt;/code>&lt;/pre>&lt;p>&amp;hellip;but this fails because the Nginx controller &lt;a href="https://github.com/nginxinc/nginx-gateway-fabric/blob/7de105c7dd09ccfca5823d6941ac12c520257221/internal/mode/static/manager.go#L123-L129">explicitly cleans out that directory on startup&lt;/a> and is unhappy if it is unable to delete a file.&lt;/p>
&lt;h2 id="replacing-nginxconf">Replacing nginx.conf&lt;/h2>
&lt;p>Right now, the solution is to replace &lt;code>/etc/nginx/nginx.conf&lt;/code>. This is a relatively simple operation using &lt;a href="https://kustomize.io">kustomize&lt;/a> to apply a patch to the deployment manifests.&lt;/p>
&lt;h3 id="grab-the-original-configuration">Grab the original configuration&lt;/h3>
&lt;p>First, we need to retrieve the &lt;em>original&lt;/em> &lt;code>nginx.conf&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>mkdir configs
podman run --rm --entrypoint cat \
ghcr.io/nginxinc/nginx-gateway-fabric/nginx:1.0.0 /etc/nginx/nginx.conf &amp;gt; configs/nginx.conf
&lt;/code>&lt;/pre>&lt;p>Modify &lt;code>configs/nginx.conf&lt;/code> as necessary; in my case, I added the following line to the &lt;code>http&lt;/code> section:&lt;/p>
&lt;pre tabindex="0">&lt;code>client_max_body_size 0;
&lt;/code>&lt;/pre>&lt;h3 id="patch-the-deployment">Patch the deployment&lt;/h3>
&lt;p>We can deploy the stock NGINX Gateway Fabric with a &lt;code>kustomization.yaml&lt;/code> file like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
commonLabels:
nginxGatewayVersion: v1.0.0
resources:
- https://github.com/nginxinc/nginx-gateway-fabric/releases/download/v1.0.0/crds.yaml
- https://github.com/nginxinc/nginx-gateway-fabric/releases/download/v1.0.0/nginx-gateway.yaml
- https://raw.githubusercontent.com/nginxinc/nginx-gateway-fabric/v1.0.0/deploy/manifests/service/nodeport.yaml
&lt;/code>&lt;/pre>&lt;p>To patch the Deployment resource, we extend the &lt;code>kustomization.yaml&lt;/code> with the following patch:&lt;/p>
&lt;pre tabindex="0">&lt;code>patches:
- patch: |
apiVersion: apps/v1
kind: Deployment
metadata:
name: nginx-gateway
namespace: nginx-gateway
spec:
template:
spec:
containers:
- name: nginx
volumeMounts:
- mountPath: /etc/nginx/nginx.conf
name: nginx-conf-override
subPath: nginx.conf
volumes:
- name: nginx-conf-override
configMap:
name: nginx-conf-override
&lt;/code>&lt;/pre>&lt;p>And then we add a &lt;code>confdigMapGenerator&lt;/code> to generate the &lt;code>nginx-conf-override&lt;/code> ConfigMap:&lt;/p>
&lt;pre tabindex="0">&lt;code>configMapGenerator:
- name: nginx-conf-override
namespace: nginx-gateway
options:
disableNameSuffixHash: true
files:
- configs/nginx.conf
&lt;/code>&lt;/pre>&lt;p>Now when we deploy from this directory&amp;hellip;&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl apply -k . --server-side
&lt;/code>&lt;/pre>&lt;p>&amp;hellip;the deployment includes our patched &lt;code>nginx.conf&lt;/code> and we are able to successfully push images into the cluster registry.&lt;/p>
&lt;hr>
&lt;p>I&amp;rsquo;ve included the complete &lt;a href="kustomization.yaml">&lt;code>kustomization.yaml&lt;/code>&lt;/a> alongside this post.&lt;/p></content></item><item><title>Processing deeply nested JSON with jq streams</title><link>https://blog.oddbit.com/post/2023-07-27-jq-streams/</link><pubDate>Thu, 27 Jul 2023 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2023-07-27-jq-streams/</guid><description>I recently found myself wanting to perform a few transformations on a large OpenAPI schema. In particular, I wanted to take the schema available from the /openapi/v2 endpoint of a Kubernetes server and minimize it by (a) extracting a subset of the definitions and (b) removing all the description attributes.
The first task is relatively easy, since everything of interest exists at the same level in the schema. If I want one or more specific definitions, I can simply ask for those by key.</description><content>&lt;p>I &lt;a href="https://stackoverflow.com/a/76762619/147356">recently&lt;/a> found myself wanting to perform a few transformations on a large &lt;a href="https://www.openapis.org/">OpenAPI&lt;/a> schema. In particular, I wanted to take the schema available from the &lt;code>/openapi/v2&lt;/code> endpoint of a Kubernetes server and minimize it by (a) extracting a subset of the definitions and (b) removing all the &lt;code>description&lt;/code> attributes.&lt;/p>
&lt;p>The first task is relatively easy, since everything of interest exists at the same level in the schema. If I want one or more specific definitions, I can simply ask for those by key. For example, if I want the definition of a &lt;a href="https://docs.openshift.com/container-platform/4.13/rest_api/workloads_apis/deploymentconfig-apps-openshift-io-v1.html">&lt;code>DeploymentConfig&lt;/code>&lt;/a> object, I can run:&lt;/p>
&lt;pre tabindex="0">&lt;code>jq &amp;#39;.definitions.&amp;#34;com.github.openshift.api.apps.v1.DeploymentConfig&amp;#34;&amp;#39; &amp;lt; openapi.json
&lt;/code>&lt;/pre>&lt;p>So simple! And so wrong! Because while that does extract the required definition, that definition is not self-contained: it refers to &lt;em>other&lt;/em> definitions via &lt;a href="https://json-schema.org/understanding-json-schema/structuring.html#ref">&lt;code>$ref&lt;/code>&lt;/a> pointers. The &lt;em>real&lt;/em> solution would require code that parses the schema, resolves all the &lt;code>$ref&lt;/code> pointers, and spits out a fully resolved schema. Fortunately, in this case we can get what we need by asking for schemas matching a few specific prefixes. Using &lt;code>jq&lt;/code>, we can match keys against a prefix by:&lt;/p>
&lt;ul>
&lt;li>Using the &lt;code>to_entries&lt;/code> filter to transform a dictionary into a list of &lt;code>{&amp;quot;key&amp;quot;: ..., &amp;quot;value&amp;quot;: ...}&lt;/code> dictionaries, and then&lt;/li>
&lt;li>Using &lt;code>select&lt;/code> with the &lt;code>startswith&lt;/code> function to match specific keys, and finally&lt;/li>
&lt;li>Reconstructing the data with &lt;code>from_entries&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Which looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>jq &amp;#39;[.definitions|to_entries[]|select(
(.key|startswith(&amp;#34;com.github.openshift.api.apps.v1.Deployment&amp;#34;)) or
(.key|startswith(&amp;#34;io.k8s.apimachinery&amp;#34;)) or
(.key|startswith(&amp;#34;io.k8s.api.core&amp;#34;))
)]|from_entries&amp;#39; &amp;lt; openapi.json
&lt;/code>&lt;/pre>&lt;p>That works, but results in almost 500KB of output, which seems excessive. We could further reduce the size of the document by removing all the &lt;code>description&lt;/code> elements, but here is where things get tricky: &lt;code>description&lt;/code> attributes can occur throughout the schema hierarchy, so we can&amp;rsquo;t use a simple path (&lt;code>...|del(.value.description)&lt;/code> to remove them.&lt;/p>
&lt;p>A simple solution is to use sed:&lt;/p>
&lt;pre tabindex="0">&lt;code>jq ... | sed &amp;#39;/&amp;#34;description&amp;#34;/d&amp;#39;
&lt;/code>&lt;/pre>&lt;p>While normally I would never use &lt;code>sed&lt;/code> for processing JSON, that actually works in this case: because we&amp;rsquo;re first running the JSON document through &lt;code>jq&lt;/code>, we can be confident about the formatting of the document being passed through &lt;code>sed&lt;/code>, and anywhere the string &lt;code>&amp;quot;description&amp;quot;&lt;/code> is contained in the value of an attribute the quotes will be escaped so we would see &lt;code>\&amp;quot;description\&amp;quot;&lt;/code>.&lt;/p>
&lt;p>We could stop here and things would be just fine&amp;hellip;but I was looking for a way to perform the same operation in a structured fashion. What I really wanted was an equivalent to xpath&amp;rsquo;s &lt;code>//&lt;/code> operator (e.g., the path &lt;code>//description&lt;/code> would find all &lt;code>&amp;lt;description&amp;gt;&lt;/code> elements in a document, regardless of how deeply they were nested), but no such equivalent exists in &lt;code>jq&lt;/code>. Then I came across the &lt;code>tostream&lt;/code> filter, which is really neat: it transforms a JSON document into a sequence of &lt;code>[path, leaf-value]&lt;/code> nodes (or &lt;code>[path]&lt;/code> to indicate the end of an array or object).&lt;/p>
&lt;p>That probably requires an example. The document:&lt;/p>
&lt;pre tabindex="0">&lt;code>{
&amp;#34;name&amp;#34;: &amp;#34;gizmo&amp;#34;,
&amp;#34;color&amp;#34;: &amp;#34;red&amp;#34;,
&amp;#34;count&amp;#34;: {
&amp;#34;local&amp;#34;: 1,
&amp;#34;warehouse&amp;#34;: 3
}
}
&lt;/code>&lt;/pre>&lt;p>When converted into a stream becomes:&lt;/p>
&lt;pre tabindex="0">&lt;code>[[&amp;#34;name&amp;#34;],&amp;#34;gizmo&amp;#34;]
[[&amp;#34;color&amp;#34;],&amp;#34;red&amp;#34;]
[[&amp;#34;count&amp;#34;,&amp;#34;local&amp;#34;],1]
[[&amp;#34;count&amp;#34;,&amp;#34;warehouse&amp;#34;],3]
[[&amp;#34;count&amp;#34;,&amp;#34;warehouse&amp;#34;]]
[[&amp;#34;count&amp;#34;]]
&lt;/code>&lt;/pre>&lt;p>You can see how each attribute is represented by a tuple. For example, for &lt;code>.count.local&lt;/code>, the first element of the tuple is &lt;code>[&amp;quot;count&amp;quot;, &amp;quot;local&amp;quot;]&lt;/code>, representing that path to the value in the document, and the second element is the value itself (&lt;code>1&lt;/code>). The &amp;ldquo;end&amp;rdquo; of an object is indicated by a 1-tuple (&lt;code>[path]&lt;/code>), such as &lt;code>[[&amp;quot;count&amp;quot;]]&lt;/code> at the end of this example.&lt;/p>
&lt;p>If we convert the OpenAPI schema to a stream, we&amp;rsquo;ll end up with nodes for the &lt;code>description&lt;/code> attributes that look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>[
[
&amp;#34;com.github.openshift.api.apps.v1.DeploymentCause&amp;#34;,
&amp;#34;properties&amp;#34;,
&amp;#34;imageTrigger&amp;#34;,
&amp;#34;description&amp;#34;
],
&amp;#34;ImageTrigger contains the image trigger details, if this trigger was fired based on an image change&amp;#34;
]
&lt;/code>&lt;/pre>&lt;p>To match those, we need to look for nodes for which the last element of the first item is &lt;code>description&lt;/code>. That is:&lt;/p>
&lt;pre tabindex="0">&lt;code>...|tostream|select(.[0][-1]==&amp;#34;description&amp;#34;))
&lt;/code>&lt;/pre>&lt;p>Of course, we don&amp;rsquo;t want to &lt;em>select&lt;/em> those nodes; we want to delete them:&lt;/p>
&lt;pre tabindex="0">&lt;code>...|tostream|del(select(.[0][-1]==&amp;#34;description&amp;#34;)))
&lt;/code>&lt;/pre>&lt;p>And lastly, we need to feed the result back to the &lt;code>fromstream&lt;/code> function to reconstruct the document. Putting all of that together &amp;ndash; and populating some required top-level keys so that we end up with a valid OpenAPI schema &amp;ndash; looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>jq &amp;#39;
fromstream(
{
&amp;#34;swagger&amp;#34;: .swagger,
&amp;#34;definitions&amp;#34;: [
.definitions|to_entries[]|select(
(.key|startswith(&amp;#34;com.github.openshift.api.apps.v1.Deployment&amp;#34;)) or
(.key|startswith(&amp;#34;io.k8s.apimachinery&amp;#34;)) or
(.key|startswith(&amp;#34;io.k8s.api.core&amp;#34;))
)]|from_entries
}|tostream|del(select(.[0][-1]==&amp;#34;description&amp;#34;))|select(. != null)
)
&amp;#39;
&lt;/code>&lt;/pre>&lt;p>In my environment, this reduces the size of the resulting file from about 500KB to around 175KB.&lt;/p></content></item><item><title>Managing containers with Pytest fixtures</title><link>https://blog.oddbit.com/post/2023-07-15-pytest-and-containers/</link><pubDate>Sat, 15 Jul 2023 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2023-07-15-pytest-and-containers/</guid><description>A software fixture &amp;ldquo;sets up a system for the software testing process by initializing it, thereby satisfying any preconditions the system may have&amp;rdquo;. They allow us to perform setup and teardown tasks, provide state or set up services required for our tests, and perform other initialization tasks. In this article, we&amp;rsquo;re going to explore how to use fixtures in Pytest to create and tear down containers as part of a test run.</description><content>&lt;p>A &lt;a href="https://en.wikipedia.org/wiki/Test_fixture#Software">software fixture&lt;/a> &amp;ldquo;sets up a system for the software testing process by initializing it, thereby satisfying any preconditions the system may have&amp;rdquo;. They allow us to perform setup and teardown tasks, provide state or set up services required for our tests, and perform other initialization tasks. In this article, we&amp;rsquo;re going to explore how to use fixtures in &lt;a href="https://docs.pytest.org/en/7.4.x/">Pytest&lt;/a> to create and tear down containers as part of a test run.&lt;/p>
&lt;h2 id="pytest-fixtures">Pytest Fixtures&lt;/h2>
&lt;p>Pytest &lt;a href="https://docs.pytest.org/en/6.2.x/fixture.html">fixtures&lt;/a> are created through the use of the &lt;code>fixture&lt;/code> decorator. A fixture is accessed by including a function parameter with the fixture name in our test functions. For example, if we define an &lt;code>example&lt;/code> fixture:&lt;/p>
&lt;pre tabindex="0">&lt;code>@pytest.fixture
def example():
return &amp;#34;hello world&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Then we can write a test function like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>def test_something(example):
...
&lt;/code>&lt;/pre>&lt;p>And it will receive the string &amp;ldquo;hello world&amp;rdquo; as the value of the &lt;code>example&lt;/code> parameter.&lt;/p>
&lt;p>There are a number of built-in fixtures available; for example, the &lt;code>tmp_path&lt;/code> fixture provides access to a temporary directory that is unique to each test function. The following function would create a file named &lt;code>myfile&lt;/code> in the temporary directory; the file (in fact, the entire directory) will be removed automatically when the function completes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">test_something&lt;/span>(tmp_path):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> (tmp_path &lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#e6db74">&amp;#34;myfile&amp;#34;&lt;/span>)&lt;span style="color:#f92672">.&lt;/span>open() &lt;span style="color:#66d9ef">as&lt;/span> fd:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fd&lt;span style="color:#f92672">.&lt;/span>write(&lt;span style="color:#e6db74">&amp;#39;this is a test&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A fixture can declare a &lt;a href="https://docs.pytest.org/en/6.2.x/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session">scope&lt;/a>; the default is the &lt;code>function&lt;/code> scope &amp;ndash; a new value will be generated for each function. A fixture can also be declared with a scope of &lt;code>class&lt;/code>, &lt;code>module&lt;/code>, &lt;code>package&lt;/code>, or &lt;code>session&lt;/code> (where &amp;ldquo;session&amp;rdquo; means, effectively, a distinct run of &lt;code>pytest&lt;/code>).&lt;/p>
&lt;p>Fixtures can be located in the same files as your tests, or they can be placed in a &lt;a href="https://docs.pytest.org/en/6.2.x/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session">&lt;code>conftest.py&lt;/code>&lt;/a> file where they can be shared between multiple sets of tests.&lt;/p>
&lt;h2 id="communicating-with-docker">Communicating with Docker&lt;/h2>
&lt;p>In order to manage containers as part of the test process we&amp;rsquo;re going to need to interact with Docker. While we could call out to the &lt;code>docker&lt;/code> CLI from our tests, a more graceful solution is to use the &lt;a href="https://docker-py.readthedocs.io/en/stable/">Docker client for Python&lt;/a>. That means we&amp;rsquo;ll need a Docker client instance, so we start with a very simple fixture:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>(scope&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;session&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">docker_client&lt;/span>():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;Return a Docker client&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> docker&lt;span style="color:#f92672">.&lt;/span>from_env()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This returns a Docker client initialized using values from the environment (in other words, it behaves very much like the &lt;code>docker&lt;/code> cli).&lt;/p>
&lt;p>I&amp;rsquo;ve made this a &lt;code>session&lt;/code> scoped fixture (which means we create one Docker client object at per pytest run, and every test using this fixture will receive the same object). This makes sense in general because a Docker client is stateless; there isn&amp;rsquo;t any data we need to reset between tests.&lt;/p>
&lt;h2 id="starting-a-container-version-1">Starting a container, version 1&lt;/h2>
&lt;p>For the purposes of this article, let&amp;rsquo;s assume we want to spin up a MariaDB server in a container. From the command line we might run something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run -d \
-e MARIADB_ROOT_PASSWORD=secret \
-e MARIADB_USER=testuser \
-e MARIADB_DATABASE=testdb \
mariadb:10
&lt;/code>&lt;/pre>&lt;p>Looking through the Docker &lt;a href="https://docker-py.readthedocs.io/en/stable/">python API documentation&lt;/a>, a naïve Python equivalent might look like this:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="561734289" type="checkbox" />
&lt;label for="561734289">
&lt;span class="collapsable-code__language">python&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-python" >&lt;code>
import docker
import pytest
@pytest.fixture
def mariadb_container(
docker_client,
):
&amp;#34;&amp;#34;&amp;#34;Create a MariaDB container&amp;#34;&amp;#34;&amp;#34;
container = docker_client.containers.run(
&amp;#34;docker.io/mariadb:11&amp;#34;,
detach=True,
environment={
&amp;#34;MARIADB_ROOT_PASSWORD&amp;#34;: &amp;#34;secret&amp;#34;,
&amp;#34;MYSQL_PWD&amp;#34;: &amp;#34;secret&amp;#34;,
&amp;#34;MARIADB_DATABASE&amp;#34;: &amp;#34;testdb&amp;#34;,
},
)
return container
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>This works, but it&amp;rsquo;s not great. In particular, the container we create will hang around until we remove it manually, since we didn&amp;rsquo;t arrange to remove the container on completion. Since this is a &lt;code>function&lt;/code> scoped fixture, we would end up with one container per test (potentially leading to hundreds of containers running for a large test suite).&lt;/p>
&lt;h2 id="starting-a-container-version-2">Starting a container, version 2&lt;/h2>
&lt;p>Let&amp;rsquo;s take care of the biggest problem with the previous implementation and ensure that our containers get cleaned up. We can add cleanup code to a fixture by using a &lt;a href="https://docs.pytest.org/en/6.2.x/fixture.html#yield-fixtures-recommended">yield fixture&lt;/a>; instead of &lt;code>return&lt;/code>-ing a value, we &lt;a href="https://docs.python.org/3/reference/expressions.html#yield-expressions">&lt;code>yield&lt;/code>&lt;/a> a value, and any cleanup code after the &lt;code>yield&lt;/code> statement runs when the fixture is no longer in scope.&lt;/p>
&lt;p>That might look like:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="638197245" type="checkbox" />
&lt;label for="638197245">
&lt;span class="collapsable-code__language">python&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-python" >&lt;code>
import docker
import pytest
@pytest.fixture
def mariadb_container(
docker_client,
):
&amp;#34;&amp;#34;&amp;#34;Create a MariaDB container&amp;#34;&amp;#34;&amp;#34;
container = docker_client.containers.run(
&amp;#34;docker.io/mariadb:11&amp;#34;,
detach=True,
environment={
&amp;#34;MARIADB_ROOT_PASSWORD&amp;#34;: &amp;#34;secret&amp;#34;,
&amp;#34;MYSQL_PWD&amp;#34;: &amp;#34;secret&amp;#34;,
&amp;#34;MARIADB_DATABASE&amp;#34;: &amp;#34;testdb&amp;#34;,
},
)
yield container
container.remove(force=True)
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>That&amp;rsquo;s better, but we&amp;rsquo;re not out of the woods yet. How would we use this fixture in a test? Maybe we would try something like this:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="381472569" type="checkbox" />
&lt;label for="381472569">
&lt;span class="collapsable-code__language">python&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-python" >&lt;code>
import mysql.connector
def test_simple_select(mariadb_container):
# get the address of the mariadb container
mariadb_container.reload()
addr = mariadb_container.attrs[&amp;#34;NetworkSettings&amp;#34;][&amp;#34;Networks&amp;#34;][&amp;#34;bridge&amp;#34;][&amp;#34;IPAddress&amp;#34;]
# create a connection objects
conn = mysql.connector.connect(
host=addr, user=&amp;#34;root&amp;#34;, password=&amp;#34;secret&amp;#34;, database=&amp;#34;testdb&amp;#34;
)
# try a simple select statement
curs = conn.cursor()
curs.execute(&amp;#34;select 1&amp;#34;)
res = curs.fetchone()
assert res[0] == 1
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>First of all, that&amp;rsquo;s not a great test; there&amp;rsquo;s too much setup happening in the test that we would have to repeat before every additional test. And more importantly, if you were to try to run that test it would probably fail with:&lt;/p>
&lt;pre tabindex="0">&lt;code>E mysql.connector.errors.InterfaceError: 2003: Can&amp;#39;t connect to MySQL
server on &amp;#39;172.17.0.2:3306&amp;#39; (111 Connection refused)
&lt;/code>&lt;/pre>&lt;p>The problem is that when we start the MariaDB container, MariaDB isn&amp;rsquo;t ready to handle connections immediately. It takes a couple of seconds after starting the container before the server is ready. Because we haven&amp;rsquo;t accounted for that in our test, there&amp;rsquo;s nothing listening when we try to connect.&lt;/p>
&lt;h2 id="a-step-back-and-a-moving-forward">A step back and a moving forward&lt;/h2>
&lt;p>To resolve the issues in the previous example, let&amp;rsquo;s first take a step back. For our test, we don&amp;rsquo;t actually &lt;em>want&lt;/em> a container; what we want is the ability to perform SQL queries in our test with a minimal amount of boilerplate. Ideally, our test would look more like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">test_simple_select&lt;/span>(mariadb_cursor):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> curs&lt;span style="color:#f92672">.&lt;/span>execute(&lt;span style="color:#e6db74">&amp;#39;select 1&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> res &lt;span style="color:#f92672">=&lt;/span> curs&lt;span style="color:#f92672">.&lt;/span>fetchone()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">assert&lt;/span> res[&lt;span style="color:#ae81ff">0&lt;/span>] &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>How do we get there?&lt;/p>
&lt;p>Working backwards, we would need a &lt;code>mariadb_cursor&lt;/code> fixture:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_cursor&lt;/span>(&lt;span style="color:#f92672">...&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But to get a database cursor, we need a database connection:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_connection&lt;/span>(&lt;span style="color:#f92672">...&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And to create a database connection, we need to know the address of the database server:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_host&lt;/span>(&lt;span style="color:#f92672">...&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s start filling in all those ellipses.&lt;/p>
&lt;p>What would the &lt;code>mariadb_host&lt;/code> fixture look like? We saw in our earlier test code how to get the address of a Docker container. Much like the situation with the database server, we want to account for the fact that it might take a nonzero amount of time for the container network setup to complete, so we can use a simple loop in which we check for the address and return it if it&amp;rsquo;s available, otherwise sleep a bit and try again:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_host&lt;/span>(mariadb_container):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mariadb_container&lt;span style="color:#f92672">.&lt;/span>reload()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">try&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networks &lt;span style="color:#f92672">=&lt;/span> list(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mariadb_container&lt;span style="color:#f92672">.&lt;/span>attrs[&lt;span style="color:#e6db74">&amp;#34;NetworkSettings&amp;#34;&lt;/span>][&lt;span style="color:#e6db74">&amp;#34;Networks&amp;#34;&lt;/span>]&lt;span style="color:#f92672">.&lt;/span>values()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> addr &lt;span style="color:#f92672">=&lt;/span> networks[&lt;span style="color:#ae81ff">0&lt;/span>][&lt;span style="color:#e6db74">&amp;#34;IPAddress&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> addr
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">except&lt;/span> &lt;span style="color:#a6e22e">KeyError&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> time&lt;span style="color:#f92672">.&lt;/span>sleep(&lt;span style="color:#ae81ff">0.5&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This works by repeatedly refreshing information about the container until we can find an ip address.&lt;/p>
&lt;p>Now that we have the address of the database server, we can create a connection:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_connection&lt;/span>(mariadb_host):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">while&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">try&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conn &lt;span style="color:#f92672">=&lt;/span> mysql&lt;span style="color:#f92672">.&lt;/span>connector&lt;span style="color:#f92672">.&lt;/span>connect(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> host&lt;span style="color:#f92672">=&lt;/span>mariadb_host, user&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;root&amp;#34;&lt;/span>, password&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;secret&amp;#34;&lt;/span>, database&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;testdb&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> conn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">except&lt;/span> mysql&lt;span style="color:#f92672">.&lt;/span>connector&lt;span style="color:#f92672">.&lt;/span>errors&lt;span style="color:#f92672">.&lt;/span>InterfaceError:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> time&lt;span style="color:#f92672">.&lt;/span>sleep(&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The logic here is very similar; we keep attempting to establish a connection until we&amp;rsquo;re successful, at which point we return the connection object.&lt;/p>
&lt;p>Now that we have a fixture that gives us a functioning database connection, we can use that to acquire a cursor:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> contextlib &lt;span style="color:#f92672">import&lt;/span> closing
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_cursor&lt;/span>(mariadb_connection):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">with&lt;/span> closing(mariadb_connection&lt;span style="color:#f92672">.&lt;/span>cursor()) &lt;span style="color:#66d9ef">as&lt;/span> cursor:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> cursor
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;a href="https://docs.python.org/3/library/contextlib.html#contextlib.closing">&lt;code>closing&lt;/code>&lt;/a> method from the &lt;code>contextlib&lt;/code> module returns a &lt;a href="https://docs.python.org/3/library/stdtypes.html#context-manager-types">context manager&lt;/a> that calls the &lt;code>close&lt;/code> method on the given object when leaving the &lt;code>with&lt;/code> context; this ensures that the cursor is closed when we&amp;rsquo;re done with it. We could have accomplished the same thing by writing this instead:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_cursor&lt;/span>(mariadb_connection):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cursor &lt;span style="color:#f92672">=&lt;/span> mariadb_connection&lt;span style="color:#f92672">.&lt;/span>cursor()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> cursor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cursor&lt;span style="color:#f92672">.&lt;/span>close()
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Putting all of this together gets us a &lt;code>conftest.py&lt;/code> that looks something like:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="158937642" type="checkbox" />
&lt;label for="158937642">
&lt;span class="collapsable-code__language">python&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-python" >&lt;code>
import pytest
import docker
import time
import mysql.connector
from contextlib import closing
@pytest.fixture(scope=&amp;#34;session&amp;#34;)
def docker_client():
&amp;#34;&amp;#34;&amp;#34;Return a Docker client&amp;#34;&amp;#34;&amp;#34;
return docker.from_env()
@pytest.fixture
def mariadb_container(
docker_client,
):
&amp;#34;&amp;#34;&amp;#34;Create a MariaDB container&amp;#34;&amp;#34;&amp;#34;
container = docker_client.containers.run(
&amp;#34;docker.io/mariadb:11&amp;#34;,
detach=True,
environment={
&amp;#34;MARIADB_ROOT_PASSWORD&amp;#34;: &amp;#34;secret&amp;#34;,
&amp;#34;MYSQL_PWD&amp;#34;: &amp;#34;secret&amp;#34;,
&amp;#34;MARIADB_DATABASE&amp;#34;: &amp;#34;testdb&amp;#34;,
},
)
yield container
container.remove(force=True)
@pytest.fixture
def mariadb_host(mariadb_container):
while True:
mariadb_container.reload()
try:
networks = list(
mariadb_container.attrs[&amp;#34;NetworkSettings&amp;#34;][&amp;#34;Networks&amp;#34;].values()
)
addr = networks[0][&amp;#34;IPAddress&amp;#34;]
return addr
except KeyError:
time.sleep(0.5)
@pytest.fixture
def mariadb_connection(mariadb_host):
while True:
try:
conn = mysql.connector.connect(
host=mariadb_host, user=&amp;#34;root&amp;#34;, password=&amp;#34;secret&amp;#34;, database=&amp;#34;testdb&amp;#34;
)
return conn
except mysql.connector.errors.InterfaceError:
time.sleep(1)
@pytest.fixture
def mariadb_cursor(mariadb_connection):
with closing(mariadb_connection.cursor()) as cursor:
yield cursor
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>And &lt;em>that&lt;/em> allows us to dramatically simplify our test:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="946871352" type="checkbox" />
&lt;label for="946871352">
&lt;span class="collapsable-code__language">python&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-python" >&lt;code>
def test_simple_select(mariadb_cursor):
mariadb_cursor.execute(&amp;#34;select 1&amp;#34;)
res = mariadb_cursor.fetchone()
assert res[0] == 1
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>So we&amp;rsquo;ve accomplished our goal.&lt;/p>
&lt;h2 id="additional-improvements">Additional improvements&lt;/h2>
&lt;h3 id="things-were-ignoring">Things we&amp;rsquo;re ignoring&lt;/h3>
&lt;p>In order to keep this post to a reasonable size, we haven&amp;rsquo;t bothered to create an actual application, which means we haven&amp;rsquo;t had to worry about things like initializing the database schema. In reality, we would probably handle that in a new or existing fixture.&lt;/p>
&lt;h3 id="replaced-hardcoded-values">Replaced hardcoded values&lt;/h3>
&lt;p>While our fixture does the job, we&amp;rsquo;re using a number of hardcoded values (for the username, the database name, the password, etc). This isn&amp;rsquo;t inherently bad for a test environment, but it can sometimes mask errors in our code (for example, if we pick values that match default values in our code, we might miss errors that crop up when using non-default values).&lt;/p>
&lt;p>We can replace fixed strings with fixtures that produce random values (or values with a random component, if we want something a little more human readable). In the following example, we have a &lt;code>random_string&lt;/code> fixture that produces an 8 character random string, and then we use that to produce a password and a database name:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> string
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> random
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">random_string&lt;/span>():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>join(random&lt;span style="color:#f92672">.&lt;/span>choices(string&lt;span style="color:#f92672">.&lt;/span>ascii_letters &lt;span style="color:#f92672">+&lt;/span> string&lt;span style="color:#f92672">.&lt;/span>digits, k&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">8&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_dbpass&lt;/span>(random_string):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;secret-&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>random_string&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_dbname&lt;/span>(random_string):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> &lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;testdb-&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>random_string&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We would incorporate these into our existing fixtures wherever we need the database password or name:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">@pytest.fixture&lt;/span>(scope&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;session&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">mariadb_container&lt;/span>(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> docker_client,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> random_string,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mariadb_dbpass,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mariadb_dbname,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;&amp;#34;&amp;#34;Create a MariaDB container&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> container &lt;span style="color:#f92672">=&lt;/span> docker_client&lt;span style="color:#f92672">.&lt;/span>containers&lt;span style="color:#f92672">.&lt;/span>run(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;docker.io/mariadb:11&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#34;mariadb-test-&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>random_string&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> detach&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> environment&lt;span style="color:#f92672">=&lt;/span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;MARIADB_ROOT_PASSWORD&amp;#34;&lt;/span>: mariadb_dbpass,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;MYSQL_PWD&amp;#34;&lt;/span>: mariadb_dbpass,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#34;MARIADB_DATABASE&amp;#34;&lt;/span>: mariadb_dbname,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">yield&lt;/span> container
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> container&lt;span style="color:#f92672">.&lt;/span>remove(force&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(and so forth)&lt;/p>
&lt;h3 id="consider-a-session-scoped-container">Consider a session scoped container&lt;/h3>
&lt;p>The fixtures we&amp;rsquo;ve developed in this post have all been &lt;code>function&lt;/code> scoped, which means that we&amp;rsquo;re creating and tearing down a container for every single function. This will substantially increase the runtime of our tests. We may want to consider using &lt;code>session&lt;/code> scoped fixtures instead; this would bring up a container and it use it for all our tests, only cleaning it up at the end of the test run.&lt;/p>
&lt;p>The advantage here is that the impact on the test run time is minimal. The disadvantage is that we have to be very careful about the interaction between tests, since we would no longer be starting each test with a clean version of the database.&lt;/p>
&lt;p>Keep in mind that in Pytest, a fixture can only reference other fixtures that come from the same or &amp;ldquo;broader&amp;rdquo; scope (so, a &lt;code>function&lt;/code> scoped fixture can use a &lt;code>session&lt;/code> scoped fixture, but the opposite is not true). In particular, that means if we were to make our &lt;code>mariadb_container&lt;/code> fixture &lt;code>session&lt;/code>-scoped, we would need to make the same change to its dependencies (&lt;code>mariadb_dbname&lt;/code>, &lt;code>mariadb_dbpass&lt;/code>, etc).&lt;/p>
&lt;hr>
&lt;p>You can find a version of &lt;code>conftest.py&lt;/code> with these changes &lt;a href="ex3/conftest.py">here&lt;/a>.&lt;/p></content></item><item><title>NAT between identical networks using VRF</title><link>https://blog.oddbit.com/post/2023-02-19-vrf-and-nat/</link><pubDate>Sun, 19 Feb 2023 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2023-02-19-vrf-and-nat/</guid><description>Last week, Oskar Stenberg asked on Unix &amp;amp; Linux if it were possible to configure connectivity between two networks, both using the same address range, without involving network namespaces. That is, given this high level view of the network&amp;hellip;
&amp;hellip;can we set things up so that hosts on the &amp;ldquo;inner&amp;rdquo; network can communicate with hosts on the &amp;ldquo;outer&amp;rdquo; network using the range 192.168.3.0/24, and similarly for communication in the other direction?</description><content>&lt;p>Last week, Oskar Stenberg asked on &lt;a href="https://unix.stackexchange.com/q/735931/4989">Unix &amp;amp; Linux&lt;/a> if it were possible to configure connectivity between two networks, both using the same address range, without involving network namespaces. That is, given this high level view of the network&amp;hellip;&lt;/p>
&lt;p>&lt;a href="https://excalidraw.com/#json=uuXRRZ2ybaAXiUvbQVkNO,krx3lsbf12c-tDhuWtRjbg">&lt;img src="the-problem.svg" alt="two networks with the same address range connected by a host named &amp;ldquo;middleman&amp;rdquo;">&lt;/a>&lt;/p>
&lt;p>&amp;hellip;can we set things up so that hosts on the &amp;ldquo;inner&amp;rdquo; network can communicate with hosts on the &amp;ldquo;outer&amp;rdquo; network using the range &lt;code>192.168.3.0/24&lt;/code>, and similarly for communication in the other direction?&lt;/p>
&lt;h2 id="setting-up-a-lab">Setting up a lab&lt;/h2>
&lt;p>When investigating this sort of networking question, I find it easiest to reproduce the topology in a virtual environment so that it&amp;rsquo;s easy to test things out. I generally use &lt;a href="https://mininet.org">Mininet&lt;/a> for this, which provides a simple Python API for creating virtual nodes and switches and creating links between them.&lt;/p>
&lt;p>I created the following network topology for this test:&lt;/p>
&lt;figure class="center" >
&lt;img src="topology-1.svg" alt="virtual network topology diagram" />
&lt;/figure>
&lt;p>In the rest of this post, I&amp;rsquo;ll be referring to these hostnames.&lt;/p>
&lt;p>See the bottom of this post for a link to the repository that contains the complete test environment.&lt;/p>
&lt;h2 id="vrf-in-theory">VRF in theory&lt;/h2>
&lt;p>VRF stands for &amp;ldquo;Virtual Routing and Forwarding&amp;rdquo;. From the &lt;a href="https://en.wikipedia.org/wiki/Virtual_routing_and_forwarding">Wikipedia article on the topic&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>In IP-based computer networks, virtual routing and forwarding (VRF) is a technology that allows multiple instances of a routing table to co-exist within the same router at the same time. One or more logical or physical interfaces may have a VRF and these VRFs do not share routes therefore the packets are only forwarded between interfaces on the same VRF. VRFs are the TCP/IP layer 3 equivalent of a VLAN. Because the routing instances are independent, the same or overlapping IP addresses can be used without conflicting with each other. Network functionality is improved because network paths can be segmented without requiring multiple routers.&lt;a href="the-problem.svg">1&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>In Linux, VRF support is implemented as a &lt;a href="https://docs.kernel.org/networking/vrf.html">special type of network device&lt;/a>. A VRF device sets up an isolated routing domain; network traffic on devices associated with a VRF will use the routing table associated with that VRF, rather than the main routing table, which permits us to connect multiple networks with overlapping address ranges.&lt;/p>
&lt;p>We can create new VRF devices with the &lt;code>ip link add&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip link add vrf-inner type vrf table 100
&lt;/code>&lt;/pre>&lt;p>Running the above command results in the following changes:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It creates a new network device named &lt;code>vrf-inner&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It adds a new route policy rule (if it doesn&amp;rsquo;t already exist) that looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>1000: from all lookup [l3mdev-table]
&lt;/code>&lt;/pre>&lt;p>This causes route lookups to use the appropriate route table for interfaces associated with a VRF.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>After creating a VRF device, we can add interfaces to it like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip link set eth0 master vrf-inner
&lt;/code>&lt;/pre>&lt;p>This associates the given interface with the VRF device, and it moves all routes associated with the interface out of the &lt;code>local&lt;/code> and &lt;code>main&lt;/code> routing tables and into the VRF-specific routing table.&lt;/p>
&lt;p>You can see a list of vrf devices by running &lt;code>ip vrf show&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code># ip vrf show
Name Table
-----------------------
vrf-inner 100
&lt;/code>&lt;/pre>&lt;p>You can see a list of devices associated with a particular VRF with the &lt;code>ip link&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code># ip -brief link show master vrf-inner
eth0@if448 UP 72:87:af:d3:b5:f9 &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt;
&lt;/code>&lt;/pre>&lt;h2 id="vrf-in-practice">VRF in practice&lt;/h2>
&lt;p>We&amp;rsquo;re going to create two VRF devices on the &lt;code>middleman&lt;/code> host; one associated with the &amp;ldquo;inner&amp;rdquo; network and one associated with the &amp;ldquo;outer&amp;rdquo; network. In our virtual network topology, the &lt;code>middleman&lt;/code> host has two network interfaces:&lt;/p>
&lt;ul>
&lt;li>&lt;code>middleman-eth0&lt;/code> is connected to the &amp;ldquo;inner&amp;rdquo; network&lt;/li>
&lt;li>&lt;code>middleman-eth1&lt;/code> is connected to the &amp;ldquo;outer&amp;rdquo; network&lt;/li>
&lt;/ul>
&lt;p>Both devices have the same address (&lt;code>192.168.2.1&lt;/code>):&lt;/p>
&lt;pre tabindex="0">&lt;code># ip addr show
2: middleman-eth0@if426: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master vrf-inner state UP group default qlen 1000
link/ether 32:9e:01:2e:78:2f brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 192.168.2.1/24 brd 192.168.2.255 scope global middleman-eth0
valid_lft forever preferred_lft forever
root@mininet-vm:~/unix-735931# ip addr show middleman-eth1
3: middleman-eth1@if427: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue master vrf-outer state UP group default qlen 1000
link/ether 12:be:9a:09:33:93 brd ff:ff:ff:ff:ff:ff link-netnsid 0
inet 192.168.2.1/24 brd 192.168.2.255 scope global middleman-eth1
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;p>And the main routing table looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code># ip route show
192.168.2.0/24 dev middleman-eth1 proto kernel scope link src 192.168.2.1
192.168.2.0/24 dev middleman-eth0 proto kernel scope link src 192.168.2.1
&lt;/code>&lt;/pre>&lt;p>If you&amp;rsquo;re at all familiar with Linux network configuration, that probably looks weird. Right now this isn&amp;rsquo;t a particularly functional network configuration, but we can fix that!&lt;/p>
&lt;p>To create our two VRF devices, we run the following commands:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip link add vrf-inner type vrf table 100
ip link add vrf-outer type vrf table 200
ip link set vrf-inner up
ip link set vrf-outer up
&lt;/code>&lt;/pre>&lt;p>This associates &lt;code>vrf-inner&lt;/code> with route table 100, and &lt;code>vrf-outer&lt;/code> with route table 200. At this point, tables 100 and 200 are empty:&lt;/p>
&lt;pre tabindex="0">&lt;code># ip route show table 100
Error: ipv4: FIB table does not exist.
Dump terminated
# ip route show table 200
Error: ipv4: FIB table does not exist.
Dump terminated
&lt;/code>&lt;/pre>&lt;p>Next, we add our interfaces to the appropriate VRF devices:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip link set middleman-eth0 master vrf-inner
ip link set middleman-eth1 master vrf-outer
&lt;/code>&lt;/pre>&lt;p>After running these commands, there are no routes left in the main routing table:&lt;/p>
&lt;pre tabindex="0">&lt;code># ip route show
&amp;lt;no output&amp;gt;
&lt;/code>&lt;/pre>&lt;p>And the routes associated with our two physical interfaces are now contained by the appropriate VRF routing tables. Here&amp;rsquo;s table 100:&lt;/p>
&lt;pre tabindex="0">&lt;code>root@mininet-vm:~/unix-735931# ip route show table 100
broadcast 192.168.2.0 dev middleman-eth0 proto kernel scope link src 192.168.2.1
192.168.2.0/24 dev middleman-eth0 proto kernel scope link src 192.168.2.1
local 192.168.2.1 dev middleman-eth0 proto kernel scope host src 192.168.2.1
broadcast 192.168.2.255 dev middleman-eth0 proto kernel scope link src 192.168.2.1
&lt;/code>&lt;/pre>&lt;p>And table 200:&lt;/p>
&lt;pre tabindex="0">&lt;code>root@mininet-vm:~/unix-735931# ip route show table 200
broadcast 192.168.2.0 dev middleman-eth1 proto kernel scope link src 192.168.2.1
192.168.2.0/24 dev middleman-eth1 proto kernel scope link src 192.168.2.1
local 192.168.2.1 dev middleman-eth1 proto kernel scope host src 192.168.2.1
broadcast 192.168.2.255 dev middleman-eth1 proto kernel scope link src 192.168.2.1
&lt;/code>&lt;/pre>&lt;p>This configuration effectively gives us two isolated networks:&lt;/p>
&lt;figure class="center" >
&lt;img src="topology-2.svg" alt="virtual network topology diagram" />
&lt;/figure>
&lt;p>We can verify that nodes in the &amp;ldquo;inner&amp;rdquo; and &amp;ldquo;outer&amp;rdquo; networks are now able to communicate with &lt;code>middleman&lt;/code>. We can reach &lt;code>middleman&lt;/code> from &lt;code>innernode0&lt;/code>; in this case, we&amp;rsquo;re communicating with interface &lt;code>middleman-eth0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>innernode0# ping -c1 192.168.2.1
PING 192.168.2.1 (192.168.2.1) 56(84) bytes of data.
64 bytes from 192.168.2.1: icmp_seq=1 ttl=64 time=0.126 ms
--- 192.168.2.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.126/0.126/0.126/0.000 ms
&lt;/code>&lt;/pre>&lt;p>Similarly, we can reach &lt;code>middleman&lt;/code> from &lt;code>outernode&lt;/code>, but in this case we&amp;rsquo;re communicating with interface &lt;code>middleman-eth1&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>outernode0# ping -c1 192.168.2.1
PING 192.168.2.1 (192.168.2.1) 56(84) bytes of data.
64 bytes from 192.168.2.1: icmp_seq=1 ttl=64 time=1.02 ms
--- 192.168.2.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.020/1.020/1.020/0.000 ms
&lt;/code>&lt;/pre>&lt;h2 id="configure-routing-on-the-nodes">Configure routing on the nodes&lt;/h2>
&lt;p>Our goal is to let nodes on one side of the network to use the address range &lt;code>192.168.3.0/24&lt;/code> to refer to nodes on the other side of the network. Right now, if we were to try to access &lt;code>192.168.3.10&lt;/code> from &lt;code>innernode0&lt;/code>, the attempt would fail with:&lt;/p>
&lt;pre tabindex="0">&lt;code>innernode0# ping 192.168.3.10
ping: connect: Network is unreachable
&lt;/code>&lt;/pre>&lt;p>The &amp;ldquo;network is unreachable&amp;rdquo; message means that &lt;code>innernode0&lt;/code> has no idea where to send that request. That&amp;rsquo;s because at the moment, the routing table on all the nodes look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>innernode0# ip route
192.168.2.0/24 dev outernode0-eth0 proto kernel scope link src 192.168.2.10
&lt;/code>&lt;/pre>&lt;p>There is neither a default gateway nor a network-specific route appropriate for &lt;code>192.168.3.0/24&lt;/code> addresses. Let&amp;rsquo;s add a network route that will route that address range through &lt;code>middleman&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>innernode0# ip route add 192.168.3.0/24 via 192.168.2.1
innernode0# ip route
192.168.2.0/24 dev innernode0-eth0 proto kernel scope link src 192.168.2.10
192.168.3.0/24 via 192.168.2.1 dev innernode0-eth0
&lt;/code>&lt;/pre>&lt;p>This same change needs to be made on all the &lt;code>innernode*&lt;/code> and &lt;code>outernode*&lt;/code> nodes.&lt;/p>
&lt;p>With the route in place, attempts to reach &lt;code>192.168.3.10&lt;/code> from &lt;code>innernode0&lt;/code> will still fail, but now they&amp;rsquo;re getting rejected by &lt;code>middleman&lt;/code> because &lt;em>it&lt;/em> doesn&amp;rsquo;t have any appropriate routes:&lt;/p>
&lt;pre tabindex="0">&lt;code>innernode0# ping -c1 192.168.3.10
PING 192.168.3.10 (192.168.3.10) 56(84) bytes of data.
From 192.168.2.1 icmp_seq=1 Destination Net Unreachable
--- 192.168.3.10 ping statistics ---
1 packets transmitted, 0 received, +1 errors, 100% packet loss, time 0ms
&lt;/code>&lt;/pre>&lt;p>We need to tell &lt;code>middleman&lt;/code> what to do with these packets.&lt;/p>
&lt;h2 id="configure-routing-and-nat-on-middleman">Configure routing and NAT on middleman&lt;/h2>
&lt;p>In order to achieve our desired connectivity, we need to:&lt;/p>
&lt;ol>
&lt;li>Map the &lt;code>192.168.3.0/24&lt;/code> destination address to the equivalent &lt;code>192.168.2.0/24&lt;/code> address &lt;em>before&lt;/em> the kernel makes a routing decision.&lt;/li>
&lt;li>Map the &lt;code>192.168.2.0/24&lt;/code> source address to the equivalent &lt;code>192.168.3.0/24&lt;/code> address &lt;em>after&lt;/em> the kernel makes a routing decision (so that replies will go back to &amp;ldquo;other&amp;rdquo; side).&lt;/li>
&lt;li>Ensure that the kernel uses the routing table for the &lt;em>target&lt;/em> network when making routing decisions for these connections.&lt;/li>
&lt;/ol>
&lt;p>We can achieve (1) and (2) using the netfilter &lt;a href="https://www.netfilter.org/documentation/HOWTO/netfilter-extensions-HOWTO-4.html#ss4.4">&lt;code>NETMAP&lt;/code>&lt;/a> extension by adding the following two rules:&lt;/p>
&lt;pre tabindex="0">&lt;code>iptables -t nat -A PREROUTING -d 192.168.3.0/24 -j NETMAP --to 192.168.2.0/24
iptables -t nat -A POSTROUTING -s 192.168.2.0/24 -j NETMAP --to 192.168.3.0/24
&lt;/code>&lt;/pre>&lt;p>For incoming traffic destined for the 192.168.3.0/24 network, this maps the destination address to the matching &lt;code>192.168.2.0/24&lt;/code> address. For outgoing traffic with a source address on the &lt;code>192.168.2.0/24&lt;/code> network, this maps the source to the equivalent &lt;code>192.168.3.0/24&lt;/code> network (so that the recipient see the traffic as coming from &amp;ldquo;the other side&amp;rdquo;).&lt;/p>
&lt;p>(For those of you wondering, &amp;ldquo;can we do this using &lt;code>nftables&lt;/code> instead?&amp;rdquo;, as of this writing &lt;a href="https://wiki.nftables.org/wiki-nftables/index.php/Supported_features_compared_to_xtables#NETMAP">&lt;code>nftables&lt;/code> does not appear to have &lt;code>NETMAP&lt;/code> support&lt;/a>, so we have to use &lt;code>iptables&lt;/code> for this step.)&lt;/p>
&lt;p>With this change in place, re-trying that &lt;code>ping&lt;/code> command on &lt;code>innernode0&lt;/code> will apparently succeed:&lt;/p>
&lt;pre tabindex="0">&lt;code>innernode0 ping -c1 192.168.3.10
PING 192.168.3.10 (192.168.3.10) 56(84) bytes of data.
64 bytes from 192.168.3.10: icmp_seq=1 ttl=63 time=0.063 ms
--- 192.168.3.10 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.063/0.063/0.063/0.000 ms
&lt;/code>&lt;/pre>&lt;p>However, running &lt;code>tcpdump&lt;/code> on &lt;code>middleman&lt;/code> will show us that we haven&amp;rsquo;t yet achieved our goal:&lt;/p>
&lt;pre tabindex="0">&lt;code>12:59:52.899054 middleman-eth0 In IP 192.168.2.10 &amp;gt; 192.168.3.10: ICMP echo request, id 16520, seq 1, length 64
12:59:52.899077 middleman-eth0 Out IP 192.168.3.10 &amp;gt; 192.168.2.10: ICMP echo request, id 16520, seq 1, length 64
12:59:52.899127 middleman-eth0 In IP 192.168.2.10 &amp;gt; 192.168.3.10: ICMP echo reply, id 16520, seq 1, length 64
12:59:52.899130 middleman-eth0 Out IP 192.168.3.10 &amp;gt; 192.168.2.10: ICMP echo reply, id 16520, seq 1, length 64
&lt;/code>&lt;/pre>&lt;p>You can see that our packet is coming on on &lt;code>middleman-eth0&lt;/code>&amp;hellip;and going right back out the same interface. We have thus far achieved a very complicated loopback interface.&lt;/p>
&lt;p>The missing piece is some logic to have the kernel use the routing table for the &amp;ldquo;other side&amp;rdquo; when making routing decisions for these packets. We&amp;rsquo;re going to do that by:&lt;/p>
&lt;ol>
&lt;li>Tagging packets with a mark that indicates the interface on which they were recieved&lt;/li>
&lt;li>Using this mark to select an appropriate routing table&lt;/li>
&lt;/ol>
&lt;p>We add the packet mark by adding these rules to the &lt;code>MANGLE&lt;/code> table &lt;code>PREROUTING&lt;/code> chain:&lt;/p>
&lt;pre tabindex="0">&lt;code>iptables -t mangle -A PREROUTING -i middleman-eth0 -d 192.168.3.0/24 -j MARK --set-mark 100
iptables -t mangle -A PREROUTING -i middleman-eth1 -d 192.168.3.0/24 -j MARK --set-mark 200
&lt;/code>&lt;/pre>&lt;p>And we utilize that mark in route lookups by adding the following two route policy rules:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip rule add prio 100 fwmark 100 lookup 200
ip rule add prio 200 fwmark 200 lookup 100
&lt;/code>&lt;/pre>&lt;p>It is critical that these rules come before (aka &amp;ldquo;have a higher priority than&amp;rdquo;, aka &amp;ldquo;have a lower number than&amp;rdquo;) the &lt;code>l3mdev&lt;/code> rule added when we created the VRF devices.&lt;/p>
&lt;h2 id="validation-does-it-actually-work">Validation: Does it actually work?&lt;/h2>
&lt;p>With that last set of changes in place, if we repeat the &lt;code>ping&lt;/code> test from &lt;code>innernode0&lt;/code> to &lt;code>outernode0&lt;/code> and run &lt;code>tcpdump&lt;/code> on &lt;code>middleman&lt;/code>, we see:&lt;/p>
&lt;pre tabindex="0">&lt;code>13:05:27.667793 middleman-eth0 In IP 192.168.2.10 &amp;gt; 192.168.3.10: ICMP echo request, id 16556, seq 1, length 64
13:05:27.667816 middleman-eth1 Out IP 192.168.3.10 &amp;gt; 192.168.2.10: ICMP echo request, id 16556, seq 1, length 64
13:05:27.667863 middleman-eth1 In IP 192.168.2.10 &amp;gt; 192.168.3.10: ICMP echo reply, id 16556, seq 1, length 64
13:05:27.667868 middleman-eth0 Out IP 192.168.3.10 &amp;gt; 192.168.2.10: ICMP echo reply, id 16556, seq 1, length 64
&lt;/code>&lt;/pre>&lt;p>Now we finally see the desired behavior: the request from &lt;code>innernode0&lt;/code> comes in on &lt;code>eth0&lt;/code>, goes out on &lt;code>eth1&lt;/code> with the addresses appropriately mapped and gets delivered to &lt;code>outernode0&lt;/code>. The reply from &lt;code>outernode0&lt;/code> goes through the process in reverse, and arrives back at &lt;code>innernode0&lt;/code>.&lt;/p>
&lt;h2 id="connection-tracking-or-one-more-thing">Connection tracking (or, &amp;ldquo;One more thing&amp;hellip;&amp;rdquo;)&lt;/h2>
&lt;p>There is a subtle problem with the configuration we&amp;rsquo;ve implemented so far: the Linux connection tracking mechanism (&amp;quot;&lt;a href="https://arthurchiao.art/blog/conntrack-design-and-implementation/">conntrack&lt;/a>&amp;quot;) by default identifies a connection by the 4-tuple &lt;code>(source_address, source_port, destination_address, destination_port)&lt;/code>. To understand why this is a problem, assume that we&amp;rsquo;re running a web server on port 80 on all the &amp;ldquo;inner&amp;rdquo; and &amp;ldquo;outer&amp;rdquo; nodes.&lt;/p>
&lt;p>To connect from &lt;code>innernode0&lt;/code> to &lt;code>outernode0&lt;/code>, we could use the following command. We&amp;rsquo;re using the &lt;code>--local-port&lt;/code> option here because we want to control the source port of our connections:&lt;/p>
&lt;pre tabindex="0">&lt;code>innernode0# curl --local-port 4000 192.168.3.10
&lt;/code>&lt;/pre>&lt;p>To connect from &lt;code>outernode0&lt;/code> to &lt;code>innernode0&lt;/code>, we would use the same command:&lt;/p>
&lt;pre tabindex="0">&lt;code>outernode0# curl --local-port 4000 192.168.3.10
&lt;/code>&lt;/pre>&lt;p>If we look at the connection tracking table on &lt;code>middleman&lt;/code>, we will see a single connection:&lt;/p>
&lt;pre tabindex="0">&lt;code>middleman# conntrack -L
tcp 6 115 TIME_WAIT src=192.168.2.10 dst=192.168.3.10 sport=4000 dport=80 src=192.168.2.10 dst=192.168.3.10 sport=80 dport=4000 [ASSURED] mark=0 use=1
&lt;/code>&lt;/pre>&lt;p>This happens because the 4-tuple for our two connections is identical. Conflating connections like this can cause traffic to stop flowing if both connections are active at the same time.&lt;/p>
&lt;p>We need to provide the connection track subsystem with some additional information to uniquely identify these connections. We can do this by using the netfilter &lt;code>CT&lt;/code> module to assign each connection to a unique conntrack origination &amp;ldquo;zone&amp;rdquo;:&lt;/p>
&lt;pre tabindex="0">&lt;code>iptables -t raw -A PREROUTING -s 192.168.2.0/24 -i middleman-eth0 -j CT --zone-orig 100
iptables -t raw -A PREROUTING -s 192.168.2.0/24 -i middleman-eth1 -j CT --zone-orig 200
&lt;/code>&lt;/pre>&lt;p>What is a &amp;ldquo;zone&amp;rdquo;? From &lt;a href="https://lore.kernel.org/all/4B9158F5.5040205@parallels.com/T/">the patch adding this feature&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>A zone is simply a numerical identifier associated with a network
device that is incorporated into the various hashes and used to
distinguish entries in addition to the connection tuples.&lt;/p>
&lt;/blockquote>
&lt;p>With these rules in place, if we repeat the test with &lt;code>curl&lt;/code> we will see two distinct connections:&lt;/p>
&lt;pre tabindex="0">&lt;code>middleman# conntrack -L
tcp 6 117 TIME_WAIT src=192.168.2.10 dst=192.168.3.10 sport=4000 dport=80 zone-orig=100 src=192.168.2.10 dst=192.168.3.10 sport=80 dport=26148 [ASSURED] mark=0 use=1
tcp 6 115 TIME_WAIT src=192.168.2.10 dst=192.168.3.10 sport=4000 dport=80 zone-orig=200 src=192.168.2.10 dst=192.168.3.10 sport=80 dport=4000 [ASSURED] mark=0 use=1
&lt;/code>&lt;/pre>&lt;h2 id="repository-and-demo">Repository and demo&lt;/h2>
&lt;p>You can find a complete test environment in &lt;a href="https://github.com/larsks/unix-example-735931-1-1-nat">this repository&lt;/a>; that includes the mininet topology I mentioned at the beginning of this post as well as shell scripts to implement all the address, route, and netfilter configurations.&lt;/p>
&lt;p>And here&amp;rsquo;s a video that runs through the steps described in this post:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Kws98JNKcxE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></content></item><item><title>Simple error handling in C</title><link>https://blog.oddbit.com/post/2023-02-17-c-error-handling/</link><pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2023-02-17-c-error-handling/</guid><description>Overview I was recently working with someone else&amp;rsquo;s C source and I wanted to add some basic error checking without mucking up the code with a bunch of if statements and calls to perror. I ended up implementing a simple must function that checks the return value of an expression, and exits with an error if the return value is less than 0. You use it like this:
must(fd = open(&amp;#34;textfile.</description><content>&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>I was recently working with someone else&amp;rsquo;s C source and I wanted to add some basic error checking without mucking up the code with a bunch of &lt;code>if&lt;/code> statements and calls to &lt;code>perror&lt;/code>. I ended up implementing a simple &lt;code>must&lt;/code> function that checks the return value of an expression, and exits with an error if the return value is less than 0. You use it like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">must&lt;/span>(fd &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">open&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;textfile.txt&amp;#34;&lt;/span>, O_RDONLY));
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">must&lt;/span>(&lt;span style="color:#a6e22e">close&lt;/span>(fd));
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the event that an expression returns an error, the code will exit with a message that shows the file, line, and function in which the error occurred, along with the actual text of the called function and the output of &lt;code>perror&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>example.c:24 in main: fd = open(&amp;#34;does-not-exist.xt&amp;#34;, O_RDONLY): [2]: No such file or directory
&lt;/code>&lt;/pre>&lt;p>To be clear, this is only useful when you&amp;rsquo;re using functions that conform to standard Unix error reporting conventions, and if you&amp;rsquo;re happy with &amp;ldquo;exit with an error message&amp;rdquo; as the failure handling mechanism.&lt;/p>
&lt;h2 id="implementation">Implementation&lt;/h2>
&lt;p>The implementation starts with a macro defined in &lt;code>must.h&lt;/code>:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="351246798" type="checkbox" />
&lt;label for="351246798">
&lt;span class="collapsable-code__language">c&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-c" >&lt;code>
#ifndef _MUST
#define _MUST
#define must(x) _must(__FILE__, __LINE__, __func__, #x, (x))
void _must(const char *fileName, int lineNumber, const char *funcName,
const char *calledFunction, int err);
#endif
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>The &lt;code>__FILE__&lt;/code>, &lt;code>__LINE__&lt;/code>, and &lt;code>__func__&lt;/code> symbols are standard predefined symbols provided by &lt;code>gcc&lt;/code>; they are documented &lt;a href="https://gcc.gnu.org/onlinedocs/cpp/Standard-Predefined-Macros.html">here&lt;/a>. The expression &lt;code>#x&lt;/code> is using the &lt;a href="https://gcc.gnu.org/onlinedocs/cpp/Stringizing.html#Stringizing">stringify&lt;/a> operator to convert the macro argument into a string.&lt;/p>
&lt;p>The above macro transforms a call to &lt;code>must()&lt;/code> into a call to the &lt;code>_must()&lt;/code> function, which is defined in &lt;code>must.c&lt;/code>:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="751986432" type="checkbox" />
&lt;label for="751986432">
&lt;span class="collapsable-code__language">c&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-c" >&lt;code>
#include &amp;#34;must.h&amp;#34;
#include &amp;lt;errno.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
void _must(const char *fileName, int lineNumber, const char *funcName,
const char *calledFunction, int err) {
if (err &amp;lt; 0) {
char buf[256];
snprintf(buf, 256, &amp;#34;%s:%d in %s: %s: [%d]&amp;#34;, fileName, lineNumber, funcName,
calledFunction, errno);
perror(buf);
exit(1);
}
}
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>In this function we check the value of &lt;code>err&lt;/code> (which will be the return value of the expression passed as the argument to the &lt;code>must()&lt;/code> macro), and if it evaluates to a number less than 0, we use &lt;code>snprintf()&lt;/code> to generate a string that we can pass to &lt;code>perror()&lt;/code>, and finally call &lt;code>perror()&lt;/code> which will print our information string, a colon, and then the error message corresponding to the value of &lt;code>errno&lt;/code>.&lt;/p>
&lt;h2 id="example">Example&lt;/h2>
&lt;p>You can see &lt;code>must()&lt;/code> used in practice in the following example program:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="678349215" type="checkbox" />
&lt;label for="678349215">
&lt;span class="collapsable-code__language">c&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-c" >&lt;code>
#include &amp;#34;must.h&amp;#34;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
int main() {
int fd;
char buf[1024];
printf(&amp;#34;opening a file that does exist\n&amp;#34;);
must(fd = open(&amp;#34;file-that-exists.txt&amp;#34;, O_RDONLY));
while (1) {
int nb;
must(nb = read(fd, buf, sizeof(buf)));
if (!nb)
break;
must(write(STDOUT_FILENO, buf, nb));
}
must(close(fd));
printf(&amp;#34;opening a file that doesn&amp;#39;t exist\n&amp;#34;);
must(fd = open(&amp;#34;file-that-does-not-exist.xt&amp;#34;, O_RDONLY));
return 0;
}
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>Provided the &lt;code>file-that-exists.txt&lt;/code> (a) exists and (b) contains the text &lt;code>Hello, world.&lt;/code>, and that &lt;code>file-that-does-not-exist.txt&lt;/code> does not, in fact, exist, running the above code will produce the following output:&lt;/p>
&lt;pre tabindex="0">&lt;code>opening a file that does exist
Hello, world.
opening a file that doesn&amp;#39;t exist
example.c:24 in main: fd = open(&amp;#34;file-that-does-not-exist.xt&amp;#34;, O_RDONLY): [2]: No such file or directory
&lt;/code>&lt;/pre></content></item><item><title>Packet, packet, who's got the packet?</title><link>https://blog.oddbit.com/post/2023-02-14-whos-got-the-packet/</link><pubDate>Tue, 14 Feb 2023 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2023-02-14-whos-got-the-packet/</guid><description>In this question, August Vrubel has some C code that sets up a tun interface and then injects a packet, but the packet seemed to disappear into the ether. In this post, I&amp;rsquo;d like to take a slightly extended look at my answer because I think it&amp;rsquo;s a great opportunity for learning a bit more about performing network diagnostics.
The original code looked like this:
c original sendpacket.c #include &amp;lt;arpa/inet.h&amp;gt; #include &amp;lt;fcntl.</description><content>&lt;p>In &lt;a href="https://unix.stackexchange.com/q/735522/4989">this question&lt;/a>, August Vrubel has some C code that sets up a &lt;a href="https://docs.kernel.org/networking/tuntap.html">tun&lt;/a> interface and then injects a packet, but the packet seemed to disappear into the ether. In this post, I&amp;rsquo;d like to take a slightly extended look at &lt;a href="https://unix.stackexchange.com/a/735534/4989">my answer&lt;/a> because I think it&amp;rsquo;s a great opportunity for learning a bit more about performing network diagnostics.&lt;/p>
&lt;p>The original code looked like this:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="528319746" type="checkbox" />
&lt;label for="528319746">
&lt;span class="collapsable-code__language">c&lt;/span>
&lt;span class="collapsable-code__title">original sendpacket.c&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-c" >&lt;code>
#include &amp;lt;arpa/inet.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;linux/if.h&amp;gt;
#include &amp;lt;linux/if_tun.h&amp;gt;
#include &amp;lt;netinet/in.h&amp;gt;
#include &amp;lt;poll.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;sys/ioctl.h&amp;gt;
#include &amp;lt;sys/socket.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
static int tunAlloc(void) {
int fd;
struct ifreq ifr = {.ifr_name = &amp;#34;tun0&amp;#34;, .ifr_flags = IFF_TUN | IFF_NO_PI};
fd = open(&amp;#34;/dev/net/tun&amp;#34;, O_RDWR);
ioctl(fd, TUNSETIFF, (void *)&amp;amp;ifr);
ioctl(fd, TUNSETOWNER, geteuid());
return fd;
}
// this is a test
static void bringInterfaceUp(void) {
int sock;
struct sockaddr_in addr = {.sin_family = AF_INET};
struct ifreq ifr = {.ifr_name = &amp;#34;tun0&amp;#34;};
inet_aton(&amp;#34;172.30.0.1&amp;#34;, &amp;amp;addr.sin_addr);
memcpy(&amp;amp;ifr.ifr_addr, &amp;amp;addr, sizeof(struct sockaddr));
sock = socket(AF_INET, SOCK_DGRAM, 0);
ioctl(sock, SIOCSIFADDR, &amp;amp;ifr);
ioctl(sock, SIOCGIFFLAGS, &amp;amp;ifr);
ifr.ifr_flags |= IFF_UP | IFF_RUNNING;
ioctl(sock, SIOCSIFFLAGS, &amp;amp;ifr);
close(sock);
}
static void emitPacket(int tap_fd) {
unsigned char packet[] = {
0x45, 0x00, 0x00, 0x3c, 0xd8, 0x6f, 0x40, 0x00, 0x3f, 0x06, 0x08, 0x91,
172, 30, 0, 1, 192, 168, 255, 8, 0xa2, 0x9a, 0x27, 0x11,
0x80, 0x0b, 0x63, 0x79, 0x00, 0x00, 0x00, 0x00, 0xa0, 0x02, 0xfa, 0xf0,
0x89, 0xd8, 0x00, 0x00, 0x02, 0x04, 0x05, 0xb4, 0x04, 0x02, 0x08, 0x0a,
0x5b, 0x76, 0x5f, 0xd4, 0x00, 0x00, 0x00, 0x00, 0x01, 0x03, 0x03, 0x07};
write(tap_fd, packet, sizeof(packet));
}
int main() {
int tap_fd;
tap_fd = tunAlloc();
bringInterfaceUp();
emitPacket(tap_fd);
close(tap_fd);
return 0;
}
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>A problem with the original code is that it creates the interface, sends the packet, and tears down the interface with no delays, making it very difficult to inspect the interface configuration, perform packet captures, or otherwise figure out what&amp;rsquo;s going on.&lt;/p>
&lt;p>In order to resolve those issues, I added some prompts before sending the packet and before tearing down the &lt;code>tun&lt;/code> interface (and also some minimal error checking), giving us:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="268197435" type="checkbox" />
&lt;label for="268197435">
&lt;span class="collapsable-code__language">c&lt;/span>
&lt;span class="collapsable-code__title">sendpacket.c with prompts and error checking&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-c" >&lt;code>
#include &amp;lt;arpa/inet.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;linux/if.h&amp;gt;
#include &amp;lt;linux/if_tun.h&amp;gt;
#include &amp;lt;netinet/in.h&amp;gt;
#include &amp;lt;poll.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;sys/ioctl.h&amp;gt;
#include &amp;lt;sys/socket.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#define must(x) _must(#x, __FILE__, __LINE__, __func__, (x))
void _must(const char *call, const char *filename, int line,
const char *funcname, int err) {
char buf[1024];
snprintf(buf, 1023, &amp;#34;%s (@ %s:%d)&amp;#34;, call, filename, line);
if (err &amp;lt; 0) {
perror(buf);
exit(1);
}
}
static int tunAlloc(void) {
int fd;
struct ifreq ifr = {.ifr_name = &amp;#34;tun0&amp;#34;, .ifr_flags = IFF_TUN | IFF_NO_PI};
fd = open(&amp;#34;/dev/net/tun&amp;#34;, O_RDWR);
must(ioctl(fd, TUNSETIFF, (void *)&amp;amp;ifr));
must(ioctl(fd, TUNSETOWNER, geteuid()));
return fd;
}
static void bringInterfaceUp(void) {
int sock;
struct sockaddr_in addr = {.sin_family = AF_INET};
struct ifreq ifr = {.ifr_name = &amp;#34;tun0&amp;#34;};
inet_aton(&amp;#34;172.30.0.1&amp;#34;, &amp;amp;addr.sin_addr);
memcpy(&amp;amp;ifr.ifr_addr, &amp;amp;addr, sizeof(struct sockaddr));
sock = socket(AF_INET, SOCK_DGRAM, 0);
must(ioctl(sock, SIOCSIFADDR, &amp;amp;ifr));
must(ioctl(sock, SIOCGIFFLAGS, &amp;amp;ifr));
ifr.ifr_flags |= IFF_UP | IFF_RUNNING;
must(ioctl(sock, SIOCSIFFLAGS, &amp;amp;ifr));
close(sock);
}
static void emitPacket(int tap_fd) {
unsigned char packet[] = {
0x45, 0x00, 0x00, 0x3c, 0xd8, 0x6f, 0x40, 0x00, 0x3f, 0x06, 0x08, 0x91,
172, 30, 0, 1, 192, 168, 255, 8, 0xa2, 0x9a, 0x27, 0x11,
0x80, 0x0b, 0x63, 0x79, 0x00, 0x00, 0x00, 0x00, 0xa0, 0x02, 0xfa, 0xf0,
0x89, 0xd8, 0x00, 0x00, 0x02, 0x04, 0x05, 0xb4, 0x04, 0x02, 0x08, 0x0a,
0x5b, 0x76, 0x5f, 0xd4, 0x00, 0x00, 0x00, 0x00, 0x01, 0x03, 0x03, 0x07};
write(tap_fd, packet, sizeof(packet));
}
void prompt(char *promptString) {
printf(&amp;#34;%s\n&amp;#34;, promptString);
getchar();
}
int main() {
int tap_fd;
tap_fd = tunAlloc();
bringInterfaceUp();
prompt(&amp;#34;interface is up&amp;#34;);
emitPacket(tap_fd);
prompt(&amp;#34;sent packet&amp;#34;);
close(tap_fd);
printf(&amp;#34;all done&amp;#34;);
return 0;
}
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>We start by compiling the code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>gcc -o sendpacket sendpacket.c
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we try running this as a regular user, it will simply fail (which confirms that at least some of our error handling is working correctly):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ ./sendpacket
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ioctl&lt;span style="color:#f92672">(&lt;/span>fd, TUNSETIFF, &lt;span style="color:#f92672">(&lt;/span>void *&lt;span style="color:#f92672">)&lt;/span>&amp;amp;ifr&lt;span style="color:#f92672">)&lt;/span> &lt;span style="color:#f92672">(&lt;/span>@ sendpacket-pause.c:33&lt;span style="color:#f92672">)&lt;/span>: Operation not permitted
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We need to run it as &lt;code>root&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ sudo ./sendpacket
interface is up
&lt;/code>&lt;/pre>&lt;p>The &lt;code>interface is up&lt;/code> prompt means that the code has configured the interface but has not yet sent the packet. Let&amp;rsquo;s take a look at the interface configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ ip addr show tun0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3390: tun0: &amp;lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu &lt;span style="color:#ae81ff">1500&lt;/span> qdisc fq_codel state UNKNOWN group default qlen &lt;span style="color:#ae81ff">500&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> link/none
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inet 172.30.0.1/32 scope global tun0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> valid_lft forever preferred_lft forever
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inet6 fe80::c7ca:fe15:5d5c:2c49/64 scope link stable-privacy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> valid_lft forever preferred_lft forever
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The code will emit a TCP &lt;code>SYN&lt;/code> packet targeting address &lt;code>192.168.255.8&lt;/code>, port &lt;code>10001&lt;/code>. In another terminal, let&amp;rsquo;s watch for that on all interfaces. If we start &lt;code>tcpdump&lt;/code> and press RETURN at the &lt;code>interface is up&lt;/code> prompt, we&amp;rsquo;ll see something like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># tcpdump -nn -i any port 10001&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>22:36:35.336643 tun0 In IP 172.30.0.1.41626 &amp;gt; 192.168.255.8.10001: Flags &lt;span style="color:#f92672">[&lt;/span>S&lt;span style="color:#f92672">]&lt;/span>, seq 2148230009, win 64240, options &lt;span style="color:#f92672">[&lt;/span>mss 1460,sackOK,TS val &lt;span style="color:#ae81ff">1534484436&lt;/span> ecr 0,nop,wscale 7&lt;span style="color:#f92672">]&lt;/span>, length &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And indeed, we see the problem that was described: the packet enters the system on &lt;code>tun0&lt;/code>, but never goes anywhere else. What&amp;rsquo;s going on?&lt;/p>
&lt;h2 id="introducing-pwru-packet-where-are-you">Introducing pwru (Packet, Where are you?)&lt;/h2>
&lt;p>&lt;a href="https://github.com/cilium/pwru">&lt;code>pwru&lt;/code>&lt;/a> is a nifty utility written by the folks at Cilium that takes advantage of &lt;a href="https://ebpf.io/">eBPF&lt;/a> to attach traces to hundreds of kernel functions to trace packet processing through the Linux kernel. It&amp;rsquo;s especially useful when packets seem to be getting dropped with no obvious explanation. Let&amp;rsquo;s see what it can tell us!&lt;/p>
&lt;p>A convenient way to run &lt;code>pwru&lt;/code> is using their official Docker image. We&amp;rsquo;ll run it like this, filtering by protocol and destination port so that we only see results relating to the synthesized packet created by the &lt;code>sendpacket.c&lt;/code> code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>docker run --privileged --rm -t --pid&lt;span style="color:#f92672">=&lt;/span>host &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> -v /sys/kernel/debug/:/sys/kernel/debug/ &lt;span style="color:#ae81ff">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">&lt;/span> cilium/pwru --filter-proto tcp --filter-port &lt;span style="color:#ae81ff">10001&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we run &lt;code>sendpacket&lt;/code> while &lt;code>pwru&lt;/code> is running, the output looks something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>2023/02/15 03:42:33 Per cpu buffer size: 4096 bytes
2023/02/15 03:42:33 Attaching kprobes (via kprobe-multi)...
1469 / 1469 [-----------------------------------------------------------------------------] 100.00% ? p/s
2023/02/15 03:42:33 Attached (ignored 0)
2023/02/15 03:42:33 Listening for events..
SKB CPU PROCESS FUNC
0xffff8ce13e987900 6 [sendpacket-orig] netif_receive_skb
0xffff8ce13e987900 6 [sendpacket-orig] skb_defer_rx_timestamp
0xffff8ce13e987900 6 [sendpacket-orig] __netif_receive_skb
0xffff8ce13e987900 6 [sendpacket-orig] __netif_receive_skb_one_core
0xffff8ce13e987900 6 [sendpacket-orig] ip_rcv
0xffff8ce13e987900 6 [sendpacket-orig] ip_rcv_core
0xffff8ce13e987900 6 [sendpacket-orig] kfree_skb_reason(SKB_DROP_REASON_IP_CSUM)
0xffff8ce13e987900 6 [sendpacket-orig] skb_release_head_state
0xffff8ce13e987900 6 [sendpacket-orig] sock_wfree
0xffff8ce13e987900 6 [sendpacket-orig] skb_release_data
0xffff8ce13e987900 6 [sendpacket-orig] skb_free_head
0xffff8ce13e987900 6 [sendpacket-orig] kfree_skbmem
&lt;/code>&lt;/pre>&lt;p>And now we have a big blinking sign that tells us why the packet is being dropped:&lt;/p>
&lt;pre tabindex="0">&lt;code>0xffff8ce13e987900 6 [sendpacket-orig] kfree_skb_reason(SKB_DROP_REASON_IP_CSUM)
&lt;/code>&lt;/pre>&lt;h2 id="fixing-the-checksum">Fixing the checksum&lt;/h2>
&lt;p>It looks like the synthesized packet data includes a bad checksum. We could update the code to correctly calculate the checksum&amp;hellip;or we could just use &lt;a href="https://www.wireshark.org/">Wireshark&lt;/a> and have it tell us the correct values. Because this isn&amp;rsquo;t meant to be an IP networking primer, we&amp;rsquo;ll just use Wireshark, which gives us the following updated code:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">emitPacket&lt;/span>(&lt;span style="color:#66d9ef">int&lt;/span> tap_fd) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">uint16_t&lt;/span> cs;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">uint8_t&lt;/span> packet[] &lt;span style="color:#f92672">=&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">0x45&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x3c&lt;/span>, &lt;span style="color:#ae81ff">0xd8&lt;/span>, &lt;span style="color:#ae81ff">0x6f&lt;/span>, &lt;span style="color:#ae81ff">0x40&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x3f&lt;/span>, &lt;span style="color:#ae81ff">0x06&lt;/span>, &lt;span style="color:#ae81ff">0xf7&lt;/span>, &lt;span style="color:#ae81ff">0x7b&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">172&lt;/span>, &lt;span style="color:#ae81ff">30&lt;/span>, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#ae81ff">192&lt;/span>, &lt;span style="color:#ae81ff">168&lt;/span>, &lt;span style="color:#ae81ff">255&lt;/span>, &lt;span style="color:#ae81ff">8&lt;/span>, &lt;span style="color:#ae81ff">0xa2&lt;/span>, &lt;span style="color:#ae81ff">0x9a&lt;/span>, &lt;span style="color:#ae81ff">0x27&lt;/span>, &lt;span style="color:#ae81ff">0x11&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">0x80&lt;/span>, &lt;span style="color:#ae81ff">0x0b&lt;/span>, &lt;span style="color:#ae81ff">0x63&lt;/span>, &lt;span style="color:#ae81ff">0x79&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0xa0&lt;/span>, &lt;span style="color:#ae81ff">0x02&lt;/span>, &lt;span style="color:#ae81ff">0xfa&lt;/span>, &lt;span style="color:#ae81ff">0xf0&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">0x78&lt;/span>, &lt;span style="color:#ae81ff">0xc3&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x02&lt;/span>, &lt;span style="color:#ae81ff">0x04&lt;/span>, &lt;span style="color:#ae81ff">0x05&lt;/span>, &lt;span style="color:#ae81ff">0xb4&lt;/span>, &lt;span style="color:#ae81ff">0x04&lt;/span>, &lt;span style="color:#ae81ff">0x02&lt;/span>, &lt;span style="color:#ae81ff">0x08&lt;/span>, &lt;span style="color:#ae81ff">0x0a&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ae81ff">0x5b&lt;/span>, &lt;span style="color:#ae81ff">0x76&lt;/span>, &lt;span style="color:#ae81ff">0x5f&lt;/span>, &lt;span style="color:#ae81ff">0xd4&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x00&lt;/span>, &lt;span style="color:#ae81ff">0x01&lt;/span>, &lt;span style="color:#ae81ff">0x03&lt;/span>, &lt;span style="color:#ae81ff">0x03&lt;/span>, &lt;span style="color:#ae81ff">0x07&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> };
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">write&lt;/span>(tap_fd, packet, &lt;span style="color:#66d9ef">sizeof&lt;/span>(packet));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we repeat our invocation of &lt;code>pwru&lt;/code> and run a test with the updated code, we see:&lt;/p>
&lt;pre tabindex="0">&lt;code>2023/02/15 04:17:29 Per cpu buffer size: 4096 bytes
2023/02/15 04:17:29 Attaching kprobes (via kprobe-multi)...
1469 / 1469 [-----------------------------------------------------------------------------] 100.00% ? p/s
2023/02/15 04:17:29 Attached (ignored 0)
2023/02/15 04:17:29 Listening for events..
SKB CPU PROCESS FUNC
0xffff8cd8a6c5ef00 9 [sendpacket-chec] netif_receive_skb
0xffff8cd8a6c5ef00 9 [sendpacket-chec] skb_defer_rx_timestamp
0xffff8cd8a6c5ef00 9 [sendpacket-chec] __netif_receive_skb
0xffff8cd8a6c5ef00 9 [sendpacket-chec] __netif_receive_skb_one_core
0xffff8cd8a6c5ef00 9 [sendpacket-chec] ip_rcv
0xffff8cd8a6c5ef00 9 [sendpacket-chec] ip_rcv_core
0xffff8cd8a6c5ef00 9 [sendpacket-chec] sock_wfree
0xffff8cd8a6c5ef00 9 [sendpacket-chec] nf_hook_slow
0xffff8cd8a6c5ef00 9 [sendpacket-chec] nf_checksum
0xffff8cd8a6c5ef00 9 [sendpacket-chec] nf_ip_checksum
0xffff8cd8a6c5ef00 9 [sendpacket-chec] __skb_checksum_complete
0xffff8cd8a6c5ef00 9 [sendpacket-chec] tcp_v4_early_demux
0xffff8cd8a6c5ef00 9 [sendpacket-chec] ip_route_input_noref
0xffff8cd8a6c5ef00 9 [sendpacket-chec] ip_route_input_slow
0xffff8cd8a6c5ef00 9 [sendpacket-chec] fib_validate_source
0xffff8cd8a6c5ef00 9 [sendpacket-chec] __fib_validate_source
0xffff8cd8a6c5ef00 9 [sendpacket-chec] ip_handle_martian_source
0xffff8cd8a6c5ef00 9 [sendpacket-chec] kfree_skb_reason(SKB_DROP_REASON_NOT_SPECIFIED)
0xffff8cd8a6c5ef00 9 [sendpacket-chec] skb_release_head_state
0xffff8cd8a6c5ef00 9 [sendpacket-chec] skb_release_data
0xffff8cd8a6c5ef00 9 [sendpacket-chec] skb_free_head
0xffff8cd8a6c5ef00 9 [sendpacket-chec] kfree_skbmem
&lt;/code>&lt;/pre>&lt;h2 id="dealing-with-martians">Dealing with martians&lt;/h2>
&lt;p>Looking at the above output, we&amp;rsquo;re no longer seeing the &lt;code>SKB_DROP_REASON_IP_CSUM&lt;/code> error; instead, we&amp;rsquo;re getting dropped by the routing logic:&lt;/p>
&lt;pre tabindex="0">&lt;code>0xffff8cd8a6c5ef00 9 [sendpacket-chec] fib_validate_source
0xffff8cd8a6c5ef00 9 [sendpacket-chec] __fib_validate_source
0xffff8cd8a6c5ef00 9 [sendpacket-chec] ip_handle_martian_source
0xffff8cd8a6c5ef00 9 [sendpacket-chec] kfree_skb_reason(SKB_DROP_REASON_NOT_SPECIFIED)
&lt;/code>&lt;/pre>&lt;p>Specifically, the packet is being dropped as a &amp;ldquo;martian source&amp;rdquo;, which means a packet that has a source address that is invalid for the interface on which it is being received. Unlike the previous error, we can actually get kernel log messages about this problem. If we had the &lt;code>log_martians&lt;/code> sysctl enabled for all interfaces:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>sysctl -w net.ipv4.conf.all.log_martians&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or if we enabled it specifically for &lt;code>tun0&lt;/code> after the interface is created:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>sysctl -w net.ipv4.conf.tun0.log_martians&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We would see the following message logged by the kernel:&lt;/p>
&lt;pre tabindex="0">&lt;code>Feb 14 12:14:03 madhatter kernel: IPv4: martian source 192.168.255.8 from 172.30.0.1, on dev tun0
&lt;/code>&lt;/pre>&lt;p>We&amp;rsquo;re seeing this particular error because &lt;code>tun0&lt;/code> is configured with address &lt;code>172.30.0.1&lt;/code>, but it claims to be receiving a packet with the same source address from &amp;ldquo;somewhere else&amp;rdquo; on the network. This is a problem because we would never be able to reply to that packet (our replies would get routed to the local host). To deal with this problem, we can either change the source address of the packet, or we can change the IP address assigned to the &lt;code>tun0&lt;/code> interface. Since changing the source address would mean mucking about with checksums again, let&amp;rsquo;s change the address of &lt;code>tun0&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">static&lt;/span> &lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">bringInterfaceUp&lt;/span>(&lt;span style="color:#66d9ef">void&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">int&lt;/span> sock;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">struct&lt;/span> sockaddr_in addr &lt;span style="color:#f92672">=&lt;/span> {.sin_family &lt;span style="color:#f92672">=&lt;/span> AF_INET};
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">struct&lt;/span> ifreq ifr &lt;span style="color:#f92672">=&lt;/span> {.ifr_name &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;tun0&amp;#34;&lt;/span>};
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">inet_aton&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;172.30.0.10&amp;#34;&lt;/span>, &lt;span style="color:#f92672">&amp;amp;&lt;/span>addr.sin_addr);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">memcpy&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>ifr.ifr_addr, &lt;span style="color:#f92672">&amp;amp;&lt;/span>addr, &lt;span style="color:#66d9ef">sizeof&lt;/span>(&lt;span style="color:#66d9ef">struct&lt;/span> sockaddr));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sock &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#a6e22e">socket&lt;/span>(AF_INET, SOCK_DGRAM, &lt;span style="color:#ae81ff">0&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">must&lt;/span>(&lt;span style="color:#a6e22e">ioctl&lt;/span>(sock, SIOCSIFADDR, &lt;span style="color:#f92672">&amp;amp;&lt;/span>ifr));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">must&lt;/span>(&lt;span style="color:#a6e22e">ioctl&lt;/span>(sock, SIOCGIFFLAGS, &lt;span style="color:#f92672">&amp;amp;&lt;/span>ifr));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ifr.ifr_flags &lt;span style="color:#f92672">|=&lt;/span> IFF_UP &lt;span style="color:#f92672">|&lt;/span> IFF_RUNNING;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">must&lt;/span>(&lt;span style="color:#a6e22e">ioctl&lt;/span>(sock, SIOCSIFFLAGS, &lt;span style="color:#f92672">&amp;amp;&lt;/span>ifr));
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">close&lt;/span>(sock);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With this change, &lt;code>tun0&lt;/code> now looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>3452: tun0: &amp;lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 500
link/none
inet 172.30.0.10/32 scope global tun0
valid_lft forever preferred_lft forever
inet6 fe80::bda3:ddc8:e60e:106b/64 scope link stable-privacy
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;p>And if we repeat our earlier test in which we use &lt;code>tcpdump&lt;/code> to watch for our synthesized packet on any interface, we now see the desired behavior:&lt;/p>
&lt;pre tabindex="0">&lt;code># tcpdump -nn -i any port 10001
23:37:55.897786 tun0 In IP 172.30.0.1.41626 &amp;gt; 192.168.255.8.10001: Flags [S], seq 2148230009, win 64240, options [mss 1460,sackOK,TS val 1534484436 ecr 0,nop,wscale 7], length 0
23:37:55.897816 eth0 Out IP 172.30.0.1.41626 &amp;gt; 192.168.255.8.10001: Flags [S], seq 2148230009, win 64240, options [mss 1460,sackOK,TS val 1534484436 ecr 0,nop,wscale 7], length 0
&lt;/code>&lt;/pre>&lt;p>The packet is correctly handled by the kernel and sent out to our default gateway.&lt;/p>
&lt;h2 id="finishing-up">Finishing up&lt;/h2>
&lt;p>The final version of the code looks like this:&lt;/p>
&lt;div class="collapsable-code">
&lt;input id="473918562" type="checkbox" />
&lt;label for="473918562">
&lt;span class="collapsable-code__language">c&lt;/span>
&lt;span class="collapsable-code__title">working sendpacket.c&lt;/span>
&lt;span class="collapsable-code__toggle" data-label-expand="△" data-label-collapse="▽">&lt;/span>
&lt;/label>
&lt;pre class="language-c" >&lt;code>
#include &amp;lt;arpa/inet.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;linux/if.h&amp;gt;
#include &amp;lt;linux/if_tun.h&amp;gt;
#include &amp;lt;netinet/in.h&amp;gt;
#include &amp;lt;poll.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;sys/ioctl.h&amp;gt;
#include &amp;lt;sys/socket.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#define must(x) _must(#x, __FILE__, __LINE__, __func__, (x))
void _must(const char *call, const char *filename, int line,
const char *funcname, int err) {
char buf[1024];
snprintf(buf, 1023, &amp;#34;%s (@ %s:%d)&amp;#34;, call, filename, line);
if (err &amp;lt; 0) {
perror(buf);
exit(1);
}
}
static int tunAlloc(void) {
int fd;
struct ifreq ifr = {.ifr_name = &amp;#34;tun0&amp;#34;, .ifr_flags = IFF_TUN | IFF_NO_PI};
fd = open(&amp;#34;/dev/net/tun&amp;#34;, O_RDWR);
must(ioctl(fd, TUNSETIFF, (void *)&amp;amp;ifr));
must(ioctl(fd, TUNSETOWNER, geteuid()));
return fd;
}
static void bringInterfaceUp(void) {
int sock;
struct sockaddr_in addr = {.sin_family = AF_INET};
struct ifreq ifr = {.ifr_name = &amp;#34;tun0&amp;#34;};
inet_aton(&amp;#34;172.30.0.10&amp;#34;, &amp;amp;addr.sin_addr);
memcpy(&amp;amp;ifr.ifr_addr, &amp;amp;addr, sizeof(struct sockaddr));
sock = socket(AF_INET, SOCK_DGRAM, 0);
must(ioctl(sock, SIOCSIFADDR, &amp;amp;ifr));
must(ioctl(sock, SIOCGIFFLAGS, &amp;amp;ifr));
ifr.ifr_flags |= IFF_UP | IFF_RUNNING;
must(ioctl(sock, SIOCSIFFLAGS, &amp;amp;ifr));
close(sock);
}
static void emitPacket(int tap_fd) {
uint16_t cs;
uint8_t packet[] = {
0x45, 0x00, 0x00, 0x3c, 0xd8, 0x6f, 0x40, 0x00, 0x3f, 0x06, 0xf7, 0x7b,
172, 30, 0, 1, 192, 168, 255, 8, 0xa2, 0x9a, 0x27, 0x11,
0x80, 0x0b, 0x63, 0x79, 0x00, 0x00, 0x00, 0x00, 0xa0, 0x02, 0xfa, 0xf0,
0x78, 0xc3, 0x00, 0x00, 0x02, 0x04, 0x05, 0xb4, 0x04, 0x02, 0x08, 0x0a,
0x5b, 0x76, 0x5f, 0xd4, 0x00, 0x00, 0x00, 0x00, 0x01, 0x03, 0x03, 0x07,
};
write(tap_fd, packet, sizeof(packet));
}
void prompt(char *promptString) {
printf(&amp;#34;%s\n&amp;#34;, promptString);
getchar();
}
int main() {
int tap_fd;
tap_fd = tunAlloc();
bringInterfaceUp();
prompt(&amp;#34;interface is up&amp;#34;);
emitPacket(tap_fd);
prompt(&amp;#34;sent packet&amp;#34;);
close(tap_fd);
printf(&amp;#34;all done&amp;#34;);
return 0;
}
&lt;/code>&lt;/pre>
&lt;/div></content></item><item><title>Setting up an IPv6 VLAN</title><link>https://blog.oddbit.com/post/2022-11-16-home-ipv6-vlan/</link><pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2022-11-16-home-ipv6-vlan/</guid><description>My internet service provider (FIOS) doesn&amp;rsquo;t yet (sad face) offer IPv6 capable service, so I&amp;rsquo;ve set up an IPv6 tunnel using the Hurricane Electric tunnel broker. I want to provide IPv6 connectivity to multiple systems in my house, but not to all systems in my house 1. In order to meet those requirements, I&amp;rsquo;m going to set up the tunnel on the router, and then expose connectivity over an IPv6-only VLAN.</description><content>&lt;p>My internet service provider (&lt;a href="https://www.verizon.com/home/fios/">FIOS&lt;/a>) doesn&amp;rsquo;t yet (sad face) offer IPv6 capable service, so I&amp;rsquo;ve set up an IPv6 tunnel using the &lt;a href="https://www.tunnelbroker.net/">Hurricane Electric&lt;/a> tunnel broker. I want to provide IPv6 connectivity to multiple systems in my house, but not to &lt;strong>all&lt;/strong> systems in my house &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. In order to meet those requirements, I&amp;rsquo;m going to set up the tunnel on the router, and then expose connectivity over an IPv6-only VLAN. In this post, we&amp;rsquo;ll walk through the steps necessary to set that up.&lt;/p>
&lt;p>Parts of this post are going to be device specific: for example, I&amp;rsquo;m using a Ubiquiti &lt;a href="https://store.ui.com/collections/operator-edgemax-routers/products/edgerouter-x">EdgeRouter X&lt;/a> as my Internet router, so the tunnel setup is going to be specific to that device. The section about setting things up on my Linux desktop will be more generally applicable.&lt;/p>
&lt;p>There are three major parts to this post:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="#configure-the-edgerouter">Configure the EdgeRouter&lt;/a>&lt;/p>
&lt;p>This shows how to set up an IPv6 tunnel and configure an IPv6-only
VLAN on the EdgeRouter.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#configure-the-switch">Configure the switch&lt;/a>&lt;/p>
&lt;p>This is only necessary due to the specifics of the connection between
my desktop and the router; you can probably skip this.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#configure-the-desktop">Configure the desktop&lt;/a>&lt;/p>
&lt;p>This shows how to set up the IPv6 VLAN interface under Linux using &lt;code>nmcli&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-we-know">What we know&lt;/h2>
&lt;p>When you set up an IPv6 tunnel with hurricane electric, you receive several bits of information. We care in particular about the following (the IPv6 addresses and client IPv4 addresses here have been munged for privacy reasons):&lt;/p>
&lt;h3 id="ipv6-tunnel-endpoints">IPv6 Tunnel Endpoints&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Server IPv4 Address&lt;/td>
&lt;td>209.51.161.14&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Server IPv6 Address&lt;/td>
&lt;td>2001:470:1236:1212::1/64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Client IPv4 Address&lt;/td>
&lt;td>1.2.3.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Client IPv6 Address&lt;/td>
&lt;td>2001:470:1236:1212::2/64&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="routed-ipv6-prefixes">Routed IPv6 Prefixes&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Routed /64&lt;/td>
&lt;td>2001:470:1237:1212::/64&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We&amp;rsquo;ll refer back to this information as we configured things later on.&lt;/p>
&lt;h2 id="configure-the-edgerouter">Configure the EdgeRouter&lt;/h2>
&lt;h3 id="create-the-tunnel-interface">Create the tunnel interface&lt;/h3>
&lt;p>The first step in the process is to create a tunnel interface &amp;ndash; that is, an interface that looks like an ordinary network interface, but is in fact encapsulating traffic and sending it to the tunnel broker where it will unpacked and sent on its way.&lt;/p>
&lt;p>I&amp;rsquo;ll be creating a &lt;a href="https://wiki.linuxfoundation.org/networking/tunneling#sit_tunnels">SIT&lt;/a> tunnel, which is designed to &amp;ldquo;interconnect isolated IPv6 networks&amp;rdquo; over an IPv4 connection.&lt;/p>
&lt;p>I start by setting the tunnel encapsulation type and assigning an IPv6 address to the tunnel interface. This is the &amp;ldquo;Client IPv6 Address&amp;rdquo; from the earlier table:&lt;/p>
&lt;pre tabindex="0">&lt;code>set interfaces tunnel tun0 encapsulation sit
set interfaces tunnel tun0 address 2001:470:1236:1212::2/64
&lt;/code>&lt;/pre>&lt;p>Next I need to define the local and remote IPv4 endpoints of the tunnel. The remote endpoint is the &amp;ldquo;Server IPv4&amp;rdquo; address. The value &lt;code>0.0.0.0&lt;/code> for the &lt;code>local-ip&lt;/code> option means &amp;ldquo;whichever source address is appropriate for connecting to the given remote address&amp;rdquo;:&lt;/p>
&lt;pre tabindex="0">&lt;code>set interfaces tunnel tun0 remote-ip 209.51.161.14
set interfaces tunnel tun0 local-ip 0.0.0.0
&lt;/code>&lt;/pre>&lt;p>Finally, I associate some firewall rulesets with the interface. This is import because, unlike IPv4, as you assign IPv6 addresses to internal devices they will be &lt;em>directly connected to the internet&lt;/em>. With no firewall rules in place you would find yourself inadvertently exposing services that previously were &amp;ldquo;behind&amp;rdquo; your home router.&lt;/p>
&lt;pre tabindex="0">&lt;code>set interfaces tunnel tun0 firewall in ipv6-name WANv6_IN
set interfaces tunnel tun0 firewall local ipv6-name WANv6_LOCAL
&lt;/code>&lt;/pre>&lt;p>I&amp;rsquo;m using the existing &lt;code>WANv6_IN&lt;/code> and &lt;code>WANv6_LOCAL&lt;/code> rulesets, which by default block all inbound traffic. These correspond to the following &lt;code>ip6tables&lt;/code> chains:&lt;/p>
&lt;pre tabindex="0">&lt;code>root@ubnt:~# ip6tables -S WANv6_IN
-N WANv6_IN
-A WANv6_IN -m comment --comment WANv6_IN-10 -m state --state RELATED,ESTABLISHED -j RETURN
-A WANv6_IN -m comment --comment WANv6_IN-20 -m state --state INVALID -j DROP
-A WANv6_IN -m comment --comment &amp;#34;WANv6_IN-10000 default-action drop&amp;#34; -j LOG --log-prefix &amp;#34;[WANv6_IN-default-D]&amp;#34;
-A WANv6_IN -m comment --comment &amp;#34;WANv6_IN-10000 default-action drop&amp;#34; -j DROP
root@ubnt:~# ip6tables -S WANv6_LOCAL
-N WANv6_LOCAL
-A WANv6_LOCAL -m comment --comment WANv6_LOCAL-10 -m state --state RELATED,ESTABLISHED -j RETURN
-A WANv6_LOCAL -m comment --comment WANv6_LOCAL-20 -m state --state INVALID -j DROP
-A WANv6_LOCAL -p ipv6-icmp -m comment --comment WANv6_LOCAL-30 -j RETURN
-A WANv6_LOCAL -p udp -m comment --comment WANv6_LOCAL-40 -m udp --sport 547 --dport 546 -j RETURN
-A WANv6_LOCAL -m comment --comment &amp;#34;WANv6_LOCAL-10000 default-action drop&amp;#34; -j LOG --log-prefix &amp;#34;[WANv6_LOCAL-default-D]&amp;#34;
-A WANv6_LOCAL -m comment --comment &amp;#34;WANv6_LOCAL-10000 default-action drop&amp;#34; -j DROP
&lt;/code>&lt;/pre>&lt;p>As you can see, both rulesets block all inbound traffic by default unless it is &lt;em>related&lt;/em> to an existing outbound connection.&lt;/p>
&lt;h3 id="create-a-vlan-interface">Create a vlan interface&lt;/h3>
&lt;p>I need to create a network interface on the router that will be the default gateway for my local IPv6-only network. From the tunnel broker, I received the CIDR &lt;code>2001:470:1237:1212::/64&lt;/code> for local use, so:&lt;/p>
&lt;ul>
&lt;li>I&amp;rsquo;ve decided to split this up into smaller networks (because a /64 has over 18 &lt;em>quintillion&lt;/em> available addresses). I&amp;rsquo;m using &lt;code>/110&lt;/code> networks in this example, which means I will only ever be able to have 262,146 devices on each network (note that the decision to use a smaller subnet impacts your choices for address autoconfiguration; see &lt;a href="https://www.rfc-editor.org/rfc/rfc7421">RFC 7421&lt;/a> for the relevant discussion).&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>
&lt;p>I&amp;rsquo;m using the first &lt;code>/110&lt;/code> network for this VLAN, which comprises addresses &lt;code>2001:470:1237:1212::1&lt;/code> through &lt;code>2001:470:1237:1212::3:ffff&lt;/code>. I&amp;rsquo;ll use the first address as the router address.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I&amp;rsquo;ve arbitrarily decided to use VLAN id 10 for this purpose.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>To create an interface for VLAN id 10 with address &lt;code>2001:470:1237:1212::1/110&lt;/code>, we use the &lt;code>set interfaces ... vif&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>set interfaces switch switch0 vif 10 address 2001:470:1237:1212::1/110
&lt;/code>&lt;/pre>&lt;h3 id="configure-the-default-ipv6-route">Configure the default IPv6 route&lt;/h3>
&lt;p>We don&amp;rsquo;t receive &lt;a href="https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol">router advertisements&lt;/a> over the IPv6 tunnel, which means we need to explicitly configure the IPv6 default route. The default gateway will be the &amp;ldquo;Server IPv6 Address&amp;rdquo; we received from the tunnel broker.&lt;/p>
&lt;pre tabindex="0">&lt;code>set protocol static route6 ::/0 next-hop 2001:470:1236:1212::1
&lt;/code>&lt;/pre>&lt;h3 id="enable-router-advertisements">Enable router advertisements&lt;/h3>
&lt;p>IPv6 systems on our local network will use the &lt;a href="https://en.wikipedia.org/wiki/Neighbor_Discovery_Protocol">neighbor discovery protocol&lt;/a> to discover the default gateway for the network. Support for this service is provided by &lt;a href="https://radvd.litech.org/">RADVD&lt;/a>, and we configure it using the &lt;code>set interfaces ... ipv6 router-advert&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>set interfaces switch switch0 vif 10 ipv6 router-advert send-advert true
set interfaces switch switch0 vif 10 ipv6 router-advert managed-flag true
set interfaces switch switch0 vif 10 ipv6 router-advert prefix ::/110
&lt;/code>&lt;/pre>&lt;p>The &lt;code>managed-flag&lt;/code> setting corresponds to the RADVD &lt;code>AdvManagedFlag&lt;/code> configuration setting, which instructs clients to use DHCPv6 for address autoconfiguration.&lt;/p>
&lt;h3 id="configure-the-dhcpv6-service">Configure the DHCPv6 service&lt;/h3>
&lt;p>While in theory it is possible for clients to assign IPv6 addresses without the use of a DHCP server using &lt;a href="https://en.wikipedia.org/wiki/IPv6_address#Stateless_address_autoconfiguration">stateless address autoconfiguration&lt;/a>, this requires that we&amp;rsquo;re using a /64 subnet (see e.g. &lt;a href="https://www.rfc-editor.org/rfc/rfc7421">RFC 7421&lt;/a>). There is no such limitation when using DHCPv6.&lt;/p>
&lt;pre tabindex="0">&lt;code>set service dhcpv6-server shared-network-name VLAN10 subnet 2001:470:1237:1212::/110 address-range start 2001:470:1237:1212::10 stop 2001:470:1237:1212::3:ffff
set service dhcpv6-server shared-network-name VLAN10 subnet 2001:470:1237:1212::/110 name-server 2001:470:1237:1212::1
set service dhcpv6-server shared-network-name VLAN10 subnet 2001:470:1237:1212::/110 domain-search house
set service dhcpv6-server shared-network-name VLAN10 subnet 2001:470:1237:1212::/110 lease-time default 86400
&lt;/code>&lt;/pre>&lt;p>Here I&amp;rsquo;m largely setting things up to mirror the configuration of the IPv4 dhcp server for the &lt;code>name-server&lt;/code>, &lt;code>domain-search&lt;/code>, and &lt;code>lease-time&lt;/code> settings. I&amp;rsquo;m letting the DHCPv6 server allocate pretty much the entire network range, with the exception of the first 10 addresses.&lt;/p>
&lt;h3 id="commit-the-changes">Commit the changes&lt;/h3>
&lt;p>After making the above changes they need to be activated:&lt;/p>
&lt;pre tabindex="0">&lt;code>commit
&lt;/code>&lt;/pre>&lt;h3 id="verify-the-configuration">Verify the configuration&lt;/h3>
&lt;p>This produces the following interface configuration for &lt;code>tun0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>13: tun0@NONE: &amp;lt;POINTOPOINT,NOARP,UP,LOWER_UP&amp;gt; mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
link/sit 0.0.0.0 peer 209.51.161.14
inet6 2001:470:1236:1212::2/64 scope global
valid_lft forever preferred_lft forever
inet6 fe80::c0a8:101/64 scope link
valid_lft forever preferred_lft forever
inet6 fe80::6c07:49c7/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;p>And for &lt;code>switch0.10&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ubnt@ubnt:~$ ip addr show switch0.10
14: switch0.10@switch0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
link/ether 78:8a:20:bb:05:db brd ff:ff:ff:ff:ff:ff
inet6 2001:470:1237:1212::1/110 scope global
valid_lft forever preferred_lft forever
inet6 fe80::7a8a:20ff:febb:5db/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;p>And the following route configuration:&lt;/p>
&lt;pre tabindex="0">&lt;code>ubnt@ubnt:~$ ip -6 route | grep -v fe80
2001:470:1236:1212::/64 dev tun0 proto kernel metric 256 pref medium
2001:470:1237:1212::/110 dev switch0.10 proto kernel metric 256 pref medium
default via 2001:470:1236:1212::1 dev tun0 proto zebra metric 1024 pref medium
&lt;/code>&lt;/pre>&lt;p>We can confirm things are properly configured by accessing a remote service that reports our ip address:&lt;/p>
&lt;pre tabindex="0">&lt;code>ubnt@ubnt:~$ curl https://api64.ipify.org
2001:470:1236:1212::2
&lt;/code>&lt;/pre>&lt;h2 id="configure-the-switch">Configure the switch&lt;/h2>
&lt;p>In my home network, devices in my office connect to a switch, and the switch connects back to the router. I need to configure the switch (an older Netgear M4100-D12G) to pass the VLAN on to the desktop.&lt;/p>
&lt;h3 id="add-vlan-10-to-the-vlan-database-with-name-ipv6net0">Add vlan 10 to the vlan database with name &lt;code>ipv6net0&lt;/code>&lt;/h3>
&lt;p>I start by defining the VLAN in the VLAN database:&lt;/p>
&lt;pre tabindex="0">&lt;code>vlan database
vlan 10
vlan name 10 ipv6net0
exit
&lt;/code>&lt;/pre>&lt;h3 id="configure-vlan-10-as-a-tagged-member-of-ports-1-10">Configure vlan 10 as a tagged member of ports 1-10&lt;/h3>
&lt;p>Next, I configure the switch to pass VLAN 10 as a tagged VLAN on all switch interfaces:&lt;/p>
&lt;pre tabindex="0">&lt;code>configure
interface 0/1-0/10
vlan participation include 10
vlan tagging 10
exit
exit
&lt;/code>&lt;/pre>&lt;h2 id="configure-the-desktop">Configure the desktop&lt;/h2>
&lt;p>With the above configuration in place, traffic on VLAN 10 will arrive on my Linux desktop (which is connected to the switch we configured in the previous step). I can use &lt;a href="https://developer-old.gnome.org/NetworkManager/stable/nmcli.html">&lt;code>nmcli&lt;/code>&lt;/a>, the &lt;a href="https://networkmanager.dev/">NetworkManager&lt;/a> CLI, to add a VLAN interface (I&amp;rsquo;m using &lt;a href="https://getfedora.org/">Fedora&lt;/a> 37, which uses NetworkManager to manage network interface configuration; other distributions may have different tooling).&lt;/p>
&lt;p>The following command will create a &lt;em>connection&lt;/em> named &lt;code>vlan10&lt;/code>. Bringing up the connection will create an interface named &lt;code>vlan10&lt;/code>, configured to receive traffic on VLAN 10 arriving on &lt;code>eth0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>nmcli con add type vlan con-name vlan10 ifname vlan10 dev eth0 id 10 ipv6.method auto
nmcli con up vlan10
&lt;/code>&lt;/pre>&lt;p>This produces the following interface configuration:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ ip addr show vlan10
7972: vlan10@eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
link/ether 2c:f0:5d:c9:12:a9 brd ff:ff:ff:ff:ff:ff
inet6 2001:470:1237:1212::2:c19a/128 scope global dynamic noprefixroute
valid_lft 85860sec preferred_lft 53460sec
inet6 fe80::ced8:1750:d67c:2ead/64 scope link noprefixroute
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;p>And the following route configuration:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ ip -6 route show | grep vlan10
2001:470:1237:1212::2:c19a dev vlan10 proto kernel metric 404 pref medium
2001:470:1237:1212::/110 dev vlan10 proto ra metric 404 pref medium
fe80::/64 dev vlan10 proto kernel metric 1024 pref medium
default via fe80::7a8a:20ff:febb:5db dev vlan10 proto ra metric 404 pref medium
&lt;/code>&lt;/pre>&lt;p>We can confirm things are properly configured by accessing a remote service that reports our ip address:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ curl https://api64.ipify.org
2001:470:1237:1212::2:c19a
&lt;/code>&lt;/pre>&lt;p>Note that unlike access using IPv4, the address visible here is the address assigned to our local interface. There is no NAT happening at the router.&lt;/p>
&lt;hr>
&lt;p>Cover image by &lt;a href="https://www.explainthatstuff.com/chris-woodford.html">Chris Woodford/explainthatstuff.com&lt;/a>, licensed under &lt;a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0&lt;/a>.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Some services (Netflix is a notable example) block access over the IPv6 tunnels because it breaks their geolocation process and prevents them from determining your country of origin. I don&amp;rsquo;t want to break things for other folks in my house just because I want to play with IPv6.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></content></item><item><title>Using KeyOxide</title><link>https://blog.oddbit.com/post/2022-11-13-using-keyoxide/</link><pubDate>Sun, 13 Nov 2022 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2022-11-13-using-keyoxide/</guid><description>In today&amp;rsquo;s post, we look at KeyOxide, a service that allows you to cryptographically assert ownership of online resources using your GPG key. Some aspects of the service are less than obvious; in response to some questions I saw on Mastodon I though I would put together a short guide to making use of the service.
We&amp;rsquo;re going to look at the following high-level tasks:
Create a GPG key
Publish the GPG key</description><content>&lt;p>In today&amp;rsquo;s post, we look at &lt;a href="https://keyoxide.org/">KeyOxide&lt;/a>, a service that allows you to cryptographically assert ownership of online resources using your GPG key. Some aspects of the service are less than obvious; in response to some questions I saw on Mastodon I though I would put together a short guide to making use of the service.&lt;/p>
&lt;p>We&amp;rsquo;re going to look at the following high-level tasks:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="#step-1-create-a-gpg-keypair">Create a GPG key&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#step-2-publish-your-key">Publish the GPG key&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#step-3-add-a-claim">Use the GPG key to assert claims on online resources&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="step-1-create-a-gpg-keypair">Step 1: Create a GPG keypair&lt;/h2>
&lt;p>If you already have a keypair, skip on to &amp;ldquo;&lt;a href="#step-2-publish-your-key">Step 2: Publish your key&lt;/a>&amp;rdquo;.&lt;/p>
&lt;p>The first thing you need to do is set up a GPG&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> keypair and publish it to a keyserver (or a &lt;a href="https://wiki.gnupg.org/WKD">WKD endpoint&lt;/a>). There are many guides out there that step you through the process (for example, GitHub&amp;rsquo;s guide on &lt;a href="https://docs.github.com/en/authentication/managing-commit-signature-verification/generating-a-new-gpg-key">Generating a new GPG key&lt;/a>), but if you&amp;rsquo;re in a hurry and not particularly picky, read on.&lt;/p>
&lt;p>This assumes that you&amp;rsquo;re using a recent version of GPG; at the time of this writing, the current GPG release is 2.3.8, but these instructions seem to work at least with version 2.2.27.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Generate a new keypair using the &lt;code>--quick-gen-key&lt;/code> option:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg --batch --quick-gen-key &amp;lt;your email address&amp;gt;
&lt;/code>&lt;/pre>&lt;p>This will use the GPG defaults for the key algorithm (varies by version) and expiration time (the key never expires&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When prompted, enter a secure passphrase.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>GPG will create a keypair for you; you can view it after the fact by running:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg -qk &amp;lt;your email address&amp;gt;
&lt;/code>&lt;/pre>&lt;p>You should see something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>pub ed25519 2022-11-13 [SC] [expires: 2024-11-12]
EC03DFAC71DB3205EC19BAB1404E03D044EE706B
uid [ultimate] testuser@example.com
sub cv25519 2022-11-13 [E]
&lt;/code>&lt;/pre>&lt;p>In the above output, &lt;code>F79CE5D41D93C2C0E97F9A63C4178440F81E4261&lt;/code> is the &lt;em>key fingerprint&lt;/em>. We&amp;rsquo;re going to need this later.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Now you have created a GPG keypair!&lt;/p>
&lt;h2 id="step-2-publish-your-key">Step 2: Publish your key&lt;/h2>
&lt;p>If you&amp;rsquo;ve already published your key at &lt;a href="https://keys.openpgp.org/">https://keys.openpgp.org/&lt;/a> or at a &lt;a href="https://wiki.gnupg.org/WKD">WKD&lt;/a> endpoint, skip on to &amp;ldquo;&lt;a href="#step-3-add-a-claim">Step 3: Add a claim&lt;/a>&amp;rdquo;.&lt;/p>
&lt;p>In order for KeyOxide to find your GPG key, it needs to be published at a known location. There are two choices:&lt;/p>
&lt;ul>
&lt;li>Publishing your key at the public keyserver at &lt;a href="https://keys.openpgp.org/">https://keys.openpgp.org/&lt;/a>.&lt;/li>
&lt;li>Publishing your key using a &lt;a href="https://wiki.gnupg.org/WKD">WKD&lt;/a> service&lt;/li>
&lt;/ul>
&lt;p>In this post, we&amp;rsquo;re only going to consider the first option.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Export your public key to a file using gpg&amp;rsquo;s &lt;code>--export&lt;/code> option:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg --export -a &amp;lt;your email address&amp;gt; &amp;gt; mykey.asc
&lt;/code>&lt;/pre>&lt;p>This will create a file &lt;code>mykey.asc&lt;/code> in your current directory that looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>-----BEGIN PGP PUBLIC KEY BLOCK-----
[...a bunch of base64 encoded text...]
-----END PGP PUBLIC KEY BLOCK-----
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Go to &lt;a href="https://keys.openpgp.org/upload">https://keys.openpgp.org/upload&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Select the key export you just created, and select &amp;ldquo;upload&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When prompted on the next page, select &amp;ldquo;Send Verification Email&amp;rdquo;. Your key won&amp;rsquo;t discoverable until you have received and responded to the verification email.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When you receive the email, select the verification link.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Now your key has been published! You can verify this by going to &lt;a href="https://keys.openpgp.org/">https://keys.openpgp.org/&lt;/a> and searching for your email address.&lt;/p>
&lt;h2 id="step-3-add-a-claim">Step 3: Add a claim&lt;/h2>
&lt;p>You assert ownership of an online resource through a three step process:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Mark the online resource with your GPG key fingerprint. How you do this depends on the type of resource you&amp;rsquo;re claiming; e.g., for GitHub you create a gist with specific content, while for claiming a DNS domain you create a &lt;code>TXT&lt;/code> record.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Add a notation to your GPG key with a reference to the claim created in the previous step.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Update your published key.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>In this post we&amp;rsquo;re going to look at two specific examples; for other services, see the &amp;ldquo;Service providers&amp;rdquo; section of the &lt;a href="https://docs.keyoxide.org/">KeyOxide documentation&lt;/a>.&lt;/p>
&lt;p>In order to follow any of the following instructions, you&amp;rsquo;re going to need to know your &lt;em>key fingerprint&lt;/em>. When you show your public key by running &lt;code>gpg -k&lt;/code>, you key fingerprint is the long hexadecimal string on the line following the line that starts with &lt;code>pub &lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ gpg -qk testuser@example.com
pub ed25519 2022-11-13 [SC] [expires: 2024-11-12]
EC03DFAC71DB3205EC19BAB1404E03D044EE706B &amp;lt;--- THIS LINE HERE
uid [ultimate] testuser@example.com
sub cv25519 2022-11-13 [E]
&lt;/code>&lt;/pre>&lt;h3 id="add-a-claim-to-your-gpg-key">Add a claim to your GPG key&lt;/h3>
&lt;p>This is a set of common instructions that we&amp;rsquo;ll use every time we need to add a claim to our GPG key.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Edit your GPG key using the &lt;code>--edit-key&lt;/code> option:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg --edit-key &amp;lt;your email address&amp;gt;
&lt;/code>&lt;/pre>&lt;p>This will drop you into the GPG interactive key editor.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Select a user id on which to operate using the &lt;code>uid&lt;/code> command. If you created your key following the instructions earlier in this post, then you only have a single user id:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg&amp;gt; uid 1
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Add an annotation to the key using the &lt;code>notation&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg&amp;gt; notation
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>When prompted, enter the notation (the format of the notation depends on the service you&amp;rsquo;re claiming; see below for details). For example, if we&amp;rsquo;re asserting a Mastodon identity at hachyderm.io, we would enter:&lt;/p>
&lt;pre tabindex="0">&lt;code>Enter the notation: proof@ariadne.id=https://hachyderm.io/@testuser
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Save your changes with the &lt;code>save&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg&amp;gt; save
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h3 id="update-your-published-key">Update your published key&lt;/h3>
&lt;p>After adding an annotation to your key locally, you need to publish those changes. One way of doing this is simply following the &lt;a href="#step-2-publish-your-key">instructions for initially uploading your public key&lt;/a>:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Export the key to a file:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg --export -a &amp;lt;your email address&amp;gt; &amp;gt; mykey.asc
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Upload your key to &lt;a href="https://keys.openpgp.org/upload">https://keys.openpgp.org/upload&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>You won&amp;rsquo;t have to re-verify your key.&lt;/p>
&lt;p>Alternately, you can configure gpg so that you can publish your key from the command line. Create or edit &lt;code>$HOME/.gnupg/gpg.conf&lt;/code> and add the following line:&lt;/p>
&lt;pre tabindex="0">&lt;code>keyserver hkps://keys.openpgp.org
&lt;/code>&lt;/pre>&lt;p>Now every time you need to update the published version of your key:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Upload your public key using the &lt;code>--send-keys&lt;/code> option along with your key fingerprint, e.g:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg --send-keys EC03DFAC71DB3205EC19BAB1404E03D044EE706B
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h3 id="claiming-a-mastodon-identity">Claiming a Mastodon identity&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>On your favorite Mastodon server, go to your profile and select &amp;ldquo;Edit profile&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Look for the &amp;ldquo;Profile metadata section&amp;rdquo;; this allows you to associate four bits of metadata with your Mastodon profile. Assuming that you still have a slot free, give it a name (it could be anything, I went with &amp;ldquo;Keyoxide claim&amp;rdquo;), and for the value enter:&lt;/p>
&lt;pre tabindex="0">&lt;code>openpgp4fpr:&amp;lt;your key fingerprint&amp;gt;
&lt;/code>&lt;/pre>&lt;p>E.g., given the &lt;code>gpg -k&lt;/code> output shown above, I would enter:&lt;/p>
&lt;pre tabindex="0">&lt;code>openpgp4fpr:EC03DFAC71DB3205EC19BAB1404E03D044EE706B
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Click &amp;ldquo;Save Changes&amp;rdquo;&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Now, &lt;a href="#add-a-claim-to-your-gpg-key">add the claim to your GPG key&lt;/a> by adding the notation &lt;code>proof@ariadne.id=https://&amp;lt;your mastodon server&amp;gt;/@&amp;lt;your mastodon username&lt;/code>. I am @larsks@hachyderm.io, so I would enter:&lt;/p>
&lt;pre tabindex="0">&lt;code>proof@ariadne.id=https://hachyderm.io/@larsks
&lt;/code>&lt;/pre>&lt;p>After adding the claim, &lt;a href="#update-your-published-key">update your published key&lt;/a>.&lt;/p>
&lt;h3 id="claiming-a-github-identity">Claiming a Github identity&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Create a &lt;a href="https://gist.github.com">new gist&lt;/a> (it can be either secret or public).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In your gist, name the filename &lt;code>openpgp.md&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Set the content of that file to:&lt;/p>
&lt;pre tabindex="0">&lt;code>openpgp4fpr:&amp;lt;your key fingerprint&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;p>Now, &lt;a href="#add-a-claim-to-your-gpg-key">add the claim to your GPG key&lt;/a> by adding the notation &lt;code>proof@ariadne.id=https://gist.github.com/larsks/&amp;lt;gist id&amp;gt;&lt;/code>. You can see my claim at &lt;a href="https://gist.github.com/larsks/9224f58cf82bdf95ef591a6703eb91c7">https://gist.github.com/larsks/9224f58cf82bdf95ef591a6703eb91c7&lt;/a>; the notation I added to my key is:&lt;/p>
&lt;pre tabindex="0">&lt;code>proof@ariadne.id=https://gist.github.com/larsks/9224f58cf82bdf95ef591a6703eb91c7
&lt;/code>&lt;/pre>&lt;p>After adding the claim, &lt;a href="#update-your-published-key">update your published key&lt;/a>.&lt;/p>
&lt;h2 id="step-4-view-your-claims">Step 4: View your claims&lt;/h2>
&lt;p>You&amp;rsquo;ll note that none of the previous steps required interacting with &lt;a href="https://keyoxide.org/">KeyOxide&lt;/a>. That&amp;rsquo;s because KeyOxide doesn&amp;rsquo;t actually store any of your data: it just provides a mechanism for visualizing and verifying claims.&lt;/p>
&lt;p>You can look up an identity by email address or by GPG key fingerprint.&lt;/p>
&lt;p>To look up an identity using an email address:&lt;/p>
&lt;ol>
&lt;li>Go to &lt;code>https://keyoxide.org/&amp;lt;email address&lt;/code>. For example, to find my identity, visit &lt;a href="https://keyoxide.org/lars@oddbit.com">https://keyoxide.org/lars@oddbit.com&lt;/a>.&lt;/li>
&lt;/ol>
&lt;p>To look up an identity by key fingerprint:&lt;/p>
&lt;ol>
&lt;li>Go to &lt;code>https://keyoxide.org/&amp;lt;fingerprint&amp;gt;&lt;/code>. For example, to find my identity, visit &lt;a href="https://keyoxide.org/3e70a502bb5255b6bb8e86be362d63a80853d4cf">https://keyoxide.org/3e70a502bb5255b6bb8e86be362d63a80853d4cf&lt;/a>.&lt;/li>
&lt;/ol>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>The pedantic among you will already be writing to me about how PGP is the standard and GPG is an implementation of that standard, but I&amp;rsquo;m going to stick with this nomenclature for the sake of simplicity.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>For some thoughts on key expiration, see &lt;a href="https://security.stackexchange.com/questions/14718/does-openpgp-key-expiration-add-to-security">this question&lt;/a> on the Information Security StackExchange.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></content></item><item><title>Delete GitHub workflow runs using the gh cli</title><link>https://blog.oddbit.com/post/2022-09-22-delete-workflow-runs/</link><pubDate>Thu, 22 Sep 2022 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2022-09-22-delete-workflow-runs/</guid><description>Hello, future me. This is for you next time you want to do this.
When setting up the CI for a project I will sometimes end up with a tremendous clutter of workflow runs. Sometimes they have embarrassing mistakes. Who wants to show that to people? I was trying to figure out how to bulk delete workflow runs from the CLI, and I came up with something that works:
gh run list --json databaseId -q &amp;#39;.</description><content>&lt;p>Hello, future me. This is for you next time you want to do this.&lt;/p>
&lt;p>When setting up the CI for a project I will sometimes end up with a tremendous clutter of workflow runs. Sometimes they have embarrassing mistakes. Who wants to show that to people? I was trying to figure out how to bulk delete workflow runs from the CLI, and I came up with something that works:&lt;/p>
&lt;pre tabindex="0">&lt;code>gh run list --json databaseId -q &amp;#39;.[].databaseId&amp;#39; |
xargs -IID gh api \
&amp;#34;repos/$(gh repo view --json nameWithOwner -q .nameWithOwner)/actions/runs/ID&amp;#34; \
-X DELETE
&lt;/code>&lt;/pre>&lt;p>This will delete &lt;em>all&lt;/em> (well, up to 20, or whatever you set in &lt;code>--limit&lt;/code>) your workflow runs. You can add flags to &lt;code>gh run list&lt;/code> to filter runs by workflow or by triggering user.&lt;/p></content></item><item><title>Kubernetes, connection timeouts, and the importance of labels</title><link>https://blog.oddbit.com/post/2022-09-10-kubernetes-labels/</link><pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2022-09-10-kubernetes-labels/</guid><description>We are working with an application that produces resource utilization reports for clients of our OpenShift-based cloud environments. The developers working with the application have been reporting mysterious issues concerning connection timeouts between the application and the database (a MariaDB instance). For a long time we had only high-level verbal descriptions of the problem (&amp;ldquo;I&amp;rsquo;m seeing a lot of connection timeouts!&amp;rdquo;) and a variety of unsubstantiated theories (from multiple sources) about the cause.</description><content>&lt;p>We are working with an application that produces resource utilization reports for clients of our OpenShift-based cloud environments. The developers working with the application have been reporting mysterious issues concerning connection timeouts between the application and the database (a MariaDB instance). For a long time we had only high-level verbal descriptions of the problem (&amp;ldquo;I&amp;rsquo;m seeing a lot of connection timeouts!&amp;rdquo;) and a variety of unsubstantiated theories (from multiple sources) about the cause. Absent a solid reproducer of the behavior in question, we looked at other aspects of our infrastructure:&lt;/p>
&lt;ul>
&lt;li>Networking seemed fine (we weren&amp;rsquo;t able to find any evidence of interface errors, packet loss, or bandwidth issues)&lt;/li>
&lt;li>Storage in most of our cloud environments is provided by remote Ceph clusters. In addition to not seeing any evidence of network problems in general, we weren&amp;rsquo;t able to demonstrate specific problems with our storage, either (we did spot some performance variation between our Ceph clusters that may be worth investigating in the future, but it wasn&amp;rsquo;t the sort that would cause the problems we&amp;rsquo;re seeing)&lt;/li>
&lt;li>My own attempts to reproduce the behavior using &lt;a href="https://dev.mysql.com/doc/refman/8.0/en/mysqlslap.html">mysqlslap&lt;/a> did not demonstrate any problems, even though we were driving a far larger number of connections and queries/second in the benchmarks than we were in the application.&lt;/li>
&lt;/ul>
&lt;p>What was going on?&lt;/p>
&lt;p>I was finally able to get my hands on container images, deployment manifests, and instructions to reproduce the problem this past Friday. After working through some initial errors that weren&amp;rsquo;t the errors we were looking for (insert Jedi hand gesture here), I was able to see the behavior in practice. In a section of code that makes a number of connections to the database, we were seeing:&lt;/p>
&lt;pre tabindex="0">&lt;code>Failed to create databases:
Command returned non-zero value &amp;#39;1&amp;#39;: ERROR 2003 (HY000): Can&amp;#39;t connect to MySQL server on &amp;#39;mariadb&amp;#39; (110)
#0 /usr/share/xdmod/classes/CCR/DB/MySQLHelper.php(521): CCR\DB\MySQLHelper::staticExecuteCommand(Array)
#1 /usr/share/xdmod/classes/CCR/DB/MySQLHelper.php(332): CCR\DB\MySQLHelper::staticExecuteStatement(&amp;#39;mariadb&amp;#39;, &amp;#39;3306&amp;#39;, &amp;#39;root&amp;#39;, &amp;#39;pass&amp;#39;, NULL, &amp;#39;SELECT SCHEMA_N...&amp;#39;)
#2 /usr/share/xdmod/classes/OpenXdmod/Shared/DatabaseHelper.php(65): CCR\DB\MySQLHelper::databaseExists(&amp;#39;mariadb&amp;#39;, &amp;#39;3306&amp;#39;, &amp;#39;root&amp;#39;, &amp;#39;pass&amp;#39;, &amp;#39;mod_logger&amp;#39;)
#3 /usr/share/xdmod/classes/OpenXdmod/Setup/DatabaseSetupItem.php(39): OpenXdmod\Shared\DatabaseHelper::createDatabases(&amp;#39;root&amp;#39;, &amp;#39;pass&amp;#39;, Array, Array, Object(OpenXdmod\Setup\Console))
#4 /usr/share/xdmod/classes/OpenXdmod/Setup/DatabaseSetup.php(109): OpenXdmod\Setup\DatabaseSetupItem-&amp;gt;createDatabases(&amp;#39;root&amp;#39;, &amp;#39;pass&amp;#39;, Array, Array)
#5 /usr/share/xdmod/classes/OpenXdmod/Setup/Menu.php(69): OpenXdmod\Setup\DatabaseSetup-&amp;gt;handle()
#6 /usr/bin/xdmod-setup(37): OpenXdmod\Setup\Menu-&amp;gt;display()
#7 /usr/bin/xdmod-setup(22): main()
#8 {main}
&lt;/code>&lt;/pre>&lt;p>Where &lt;code>110&lt;/code> is &lt;code>ETIMEDOUT&lt;/code>, &amp;ldquo;Connection timed out&amp;rdquo;.&lt;/p>
&lt;p>The application consists of two &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment&lt;/a> resources, one that manages a MariaDB pod and another that manages the application itself. There are also the usual suspects, such as &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaims&lt;/a> for the database backing store, etc, and a &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service&lt;/a> to allow the application to access the database.&lt;/p>
&lt;p>While looking at this problem, I attempted to look at the logs for the application by running:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl logs deploy/moc-xdmod
&lt;/code>&lt;/pre>&lt;p>But to my surprise, I found myself looking at the logs for the MariaDB container instead&amp;hellip;which provided me just about all the information I needed about the problem.&lt;/p>
&lt;h2 id="how-do-deployments-work">How do Deployments work?&lt;/h2>
&lt;p>To understand what&amp;rsquo;s going on, let&amp;rsquo;s first take a closer look at a Deployment manifest. The basic framework is something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: example
spec:
selector:
matchLabels:
app: example
strategy:
type: Recreate
template:
metadata:
labels:
app: example
spec:
containers:
- name: example
image: docker.io/alpine:latest
command:
- sleep
- inf
&lt;/code>&lt;/pre>&lt;p>There are labels in three places in this manifest:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The Deployment itself has labels in the &lt;code>metadata&lt;/code> section.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There are labels in &lt;code>spec.template.metadata&lt;/code> that will be applied to Pods spawned by the Deployment.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There are labels in &lt;code>spec.selector&lt;/code> which, in the words of [the documentation]:&lt;/p>
&lt;blockquote>
&lt;p>defines how the Deployment finds which Pods to manage&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ol>
&lt;p>It&amp;rsquo;s not spelled out explicitly anywhere, but the &lt;code>spec.selector&lt;/code> field is also used to identify to which pods to attach when using the Deployment name in a command like &lt;code>kubectl logs&lt;/code>: that is, given the above manifest, running &lt;code>kubectl logs deploy/example&lt;/code> would look for pods that have label &lt;code>app&lt;/code> set to &lt;code>example&lt;/code>.&lt;/p>
&lt;p>With this in mind, let&amp;rsquo;s take a look at how our application manifests are being deployed. Like most of our applications, this is deployed using &lt;a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/">Kustomize&lt;/a>. The &lt;code>kustomization.yaml&lt;/code> file for the application manifests looked like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>commonLabels:
app: xdmod
resources:
- svc-mariadb.yaml
- deployment-mariadb.yaml
- deployment-xdmod.yaml
&lt;/code>&lt;/pre>&lt;p>That &lt;code>commonLabels&lt;/code> statement will apply the label &lt;code>app: xdmod&lt;/code> to all of the resources managed by the &lt;code>kustomization.yaml&lt;/code> file. The Deployments looked like this:&lt;/p>
&lt;p>For MariaDB:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: mariadb
spec:
selector:
matchLabels:
app: mariadb
template:
metadata:
labels:
app: mariadb
&lt;/code>&lt;/pre>&lt;p>For the application experience connection problems:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: moc-xdmod
spec:
selector:
matchLabels:
app: xdmod
template:
metadata:
labels:
app: xdmod
&lt;/code>&lt;/pre>&lt;p>The problem here is that when these are processed by &lt;code>kustomize&lt;/code>, the &lt;code>app&lt;/code> label hardcoded in the manifests will be replaced by the &lt;code>app&lt;/code> label defined in the &lt;code>commonLabels&lt;/code> section of &lt;code>kustomization.yaml&lt;/code>. When we run &lt;code>kustomize build&lt;/code> on these manifests, we will have as output:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app: xdmod
name: mariadb
spec:
selector:
matchLabels:
app: xdmod
template:
metadata:
labels:
app: xdmod
---
apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app: xdmod
name: moc-xdmod
spec:
selector:
matchLabels:
app: xdmod
template:
metadata:
labels:
app: xdmod
&lt;/code>&lt;/pre>&lt;p>In other words, all of our pods will have the same labels (because the &lt;code>spec.template.metadata.labels&lt;/code> section is identical in both Deployments). When I run &lt;code>kubectl logs deploy/moc-xdmod&lt;/code>, I&amp;rsquo;m just getting whatever the first match is for a query that is effectively the same as &lt;code>kubectl get pod -l app=xdmod&lt;/code>.&lt;/p>
&lt;p>So, that&amp;rsquo;s what was going on with the &lt;code>kubectl logs&lt;/code> command.&lt;/p>
&lt;h2 id="how-do-services-work">How do services work?&lt;/h2>
&lt;p>A Service manifest in Kubernetes looks something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Service
metadata:
name: mariadb
spec:
selector:
app: mariadb
ports:
- protocol: TCP
port: 3306
targetPort: 3306
&lt;/code>&lt;/pre>&lt;p>Here, &lt;code>spec.selector&lt;/code> has a function very similar to what it had in a &lt;code>Deployment&lt;/code>: it selects pods to which the Service will direct traffic. From &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">the documentation&lt;/a>, we know that a Service proxy will select a backend either in a round-robin fashion (using the legacy user-space proxy) or in a random fashion (using the iptables proxy) (there is also an &lt;a href="http://www.linuxvirtualserver.org/software/ipvs.html">IPVS&lt;/a> proxy mode, but that&amp;rsquo;s not available in our environment).&lt;/p>
&lt;p>Given what we know from the previous section about Deployments, you can probably see what&amp;rsquo;s going on here:&lt;/p>
&lt;ol>
&lt;li>There are multiple pods with identical labels that are providing distinct services&lt;/li>
&lt;li>For each incoming connection, the service proxy selects a Pod based on the labels in the service&amp;rsquo;s &lt;code>spec.selector&lt;/code>.&lt;/li>
&lt;li>With only two pods involved, there&amp;rsquo;s a 50% chance that traffic targeting our MariaDB instance will in fact be directed to the application pod, which will simply drop the traffic (because it&amp;rsquo;s not listening on the appropriate port).&lt;/li>
&lt;/ol>
&lt;p>We can see the impact of this behavior by running a simple loop that attempts to connect to MariaDB and run a query:&lt;/p>
&lt;pre tabindex="0">&lt;code>while :; do
_start=$SECONDS
echo -n &amp;#34;$(date +%T) &amp;#34;
timeout 10 mysql -h mariadb -uroot -ppass -e &amp;#39;select 1&amp;#39; &amp;gt; /dev/null &amp;amp;&amp;amp; echo -n OKAY || echo -n FAILED
echo &amp;#34; $(( SECONDS - _start))&amp;#34;
sleep 1
done
&lt;/code>&lt;/pre>&lt;p>Which outputs:&lt;/p>
&lt;pre tabindex="0">&lt;code>01:41:30 OKAY 1
01:41:32 OKAY 0
01:41:33 OKAY 1
01:41:35 OKAY 0
01:41:36 OKAY 3
01:41:40 OKAY 1
01:41:42 OKAY 0
01:41:43 OKAY 3
01:41:47 OKAY 3
01:41:51 OKAY 4
01:41:56 OKAY 1
01:41:58 OKAY 1
01:42:00 FAILED 10
01:42:10 OKAY 0
01:42:11 OKAY 0
&lt;/code>&lt;/pre>&lt;p>Here we can see that connection time is highly variable, and we occasionally hit the 10 second timeout imposed by the &lt;code>timeout&lt;/code> call.&lt;/p>
&lt;h2 id="solving-the-problem">Solving the problem&lt;/h2>
&lt;p>In order to resolve this behavior, we want to ensure (a) that Pods managed by a Deployment are uniquely identified by their labels and that (b) &lt;code>spec.selector&lt;/code> for both Deployments and Services will only select the appropriate Pods. We can do this with a few simple changes.&lt;/p>
&lt;p>It&amp;rsquo;s useful to apply some labels consistently across all of the resource we generate, so we&amp;rsquo;ll keep the existing &lt;code>commonLabels&lt;/code> section of our &lt;code>kustomization.yaml&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>commonLabels:
app: xdmod
&lt;/code>&lt;/pre>&lt;p>But then in each Deployment we&amp;rsquo;ll add a &lt;code>component&lt;/code> label identifying the specific service, like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
name: mariadb
labels:
component: mariadb
spec:
selector:
matchLabels:
component: mariadb
template:
metadata:
labels:
component: mariadb
&lt;/code>&lt;/pre>&lt;p>When we generate the final manifest with &lt;code>kustomize&lt;/code>, we end up with:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app: xdmod
component: mariadb
name: mariadb
spec:
selector:
matchLabels:
app: xdmod
component: mariadb
template:
metadata:
labels:
app: xdmod
component: mariadb
&lt;/code>&lt;/pre>&lt;p>In the above output, you can see that &lt;code>kustomize&lt;/code> has combined the &lt;code>commonLabel&lt;/code> definition with the labels configured individually in the manifests. With this change, &lt;code>spec.selector&lt;/code> will now select only the pod in which MariaDB is running.&lt;/p>
&lt;p>We&amp;rsquo;ll similarly modify the Service manifest to look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Service
metadata:
name: mariadb
spec:
selector:
component: mariadb
ports:
- protocol: TCP
port: 3306
targetPort: 3306
&lt;/code>&lt;/pre>&lt;p>Resulting in a generated manifest that looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Service
metadata:
labels:
app: xdmod
name: mariadb
spec:
ports:
- port: 3306
protocol: TCP
targetPort: 3306
selector:
app: xdmod
component: mariadb
&lt;/code>&lt;/pre>&lt;p>Which, as with the Deployment, will now select only the correct pods.&lt;/p>
&lt;p>With these changes in place, if we re-run the test loop I presented earlier, we see as output:&lt;/p>
&lt;pre tabindex="0">&lt;code>01:57:27 OKAY 0
01:57:28 OKAY 0
01:57:29 OKAY 0
01:57:30 OKAY 0
01:57:31 OKAY 0
01:57:32 OKAY 0
01:57:33 OKAY 0
01:57:34 OKAY 0
01:57:35 OKAY 0
01:57:36 OKAY 0
01:57:37 OKAY 0
01:57:38 OKAY 0
01:57:39 OKAY 0
01:57:40 OKAY 0
&lt;/code>&lt;/pre>&lt;p>There is no variability in connection time, and there are no timeouts.&lt;/p></content></item><item><title>Directing different ports to different containers with Traefik</title><link>https://blog.oddbit.com/post/2022-06-20-traefik-multiple-listeners/</link><pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2022-06-20-traefik-multiple-listeners/</guid><description>This post is mostly for myself: I find the Traefik documentation hard to navigate, so having figured this out in response to a question on Stack Overflow, I&amp;rsquo;m putting it here to help it stick in my head.
The question asks essentially how to perform port-based routing of requests to containers, so that a request for http://example.com goes to one container while a request for http://example.com:9090 goes to a different container.</description><content>&lt;p>This post is mostly for myself: I find the &lt;a href="https://traefik.io">Traefik&lt;/a> documentation hard to navigate, so having figured this out in response to &lt;a href="https://stackoverflow.com/a/72694677/147356">a question on Stack Overflow&lt;/a>, I&amp;rsquo;m putting it here to help it stick in my head.&lt;/p>
&lt;p>The question asks essentially how to perform port-based routing of requests to containers, so that a request for &lt;code>http://example.com&lt;/code> goes to one container while a request for &lt;code>http://example.com:9090&lt;/code> goes to a different container.&lt;/p>
&lt;h2 id="creating-entrypoints">Creating entrypoints&lt;/h2>
&lt;p>A default Traefik configuration will already have a listener on port 80, but if we want to accept connections on port 9090 we need to create a new listener: what Traefik calls an &lt;a href="https://doc.traefik.io/traefik/routing/entrypoints/">entrypoint&lt;/a>. We do this using the &lt;code>--entrypoints.&amp;lt;name&amp;gt;.address&lt;/code> option. For example, &lt;code>--entrypoints.ep1.address=80&lt;/code> creates an entrypoint named &lt;code>ep1&lt;/code> on port 80, while &lt;code>--entrypoints.ep2.address=9090&lt;/code> creates an entrypoint named &lt;code>ep2&lt;/code> on port 9090. Those names are important because we&amp;rsquo;ll use them for mapping containers to the appropriate listener later on.&lt;/p>
&lt;p>This gives us a Traefik configuration that looks something like:&lt;/p>
&lt;pre tabindex="0">&lt;code> proxy:
image: traefik:latest
command:
- --api.insecure=true
- --providers.docker
- --entrypoints.ep1.address=:80
- --entrypoints.ep2.address=:9090
ports:
- &amp;#34;80:80&amp;#34;
- &amp;#34;127.0.0.1:8080:8080&amp;#34;
- &amp;#34;9090:9090&amp;#34;
volumes:
- /var/run/docker.sock:/var/run/docker.sock
&lt;/code>&lt;/pre>&lt;p>We need to publish ports &lt;code>80&lt;/code> and &lt;code>9090&lt;/code> on the host in order to accept connections. Port 8080 is by default the Traefik dashboard; in this configuration I have it bound to &lt;code>localhost&lt;/code> because I don&amp;rsquo;t want to provide external access to the dashboard.&lt;/p>
&lt;h2 id="routing-services">Routing services&lt;/h2>
&lt;p>Now we need to configure our services so that connections on ports 80 and 9090 will get routed to the appropriate containers. We do this using the &lt;code>traefik.http.routers.&amp;lt;name&amp;gt;.entrypoints&lt;/code> label. Here&amp;rsquo;s a simple example:&lt;/p>
&lt;pre tabindex="0">&lt;code>app1:
image: docker.io/alpinelinux/darkhttpd:latest
labels:
- traefik.http.routers.app1.entrypoints=ep1
- traefik.http.routers.app1.rule=Host(`example.com`)
&lt;/code>&lt;/pre>&lt;p>In the above configuration, we&amp;rsquo;re using the following labels:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>traefik.http.routers.app1.entrypoints=ep1&lt;/code>&lt;/p>
&lt;p>This binds our &lt;code>app1&lt;/code> container to the &lt;code>ep1&lt;/code> entrypoint.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>traefik.http.routers.app1.rule=Host(`example.com`)&lt;/code>&lt;/p>
&lt;p>This matches requests with &lt;code>Host: example.com&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>So in combination, these two rules say that any request on port 80 for &lt;code>Host: example.com&lt;/code> will be routed to the &lt;code>app1&lt;/code> container.&lt;/p>
&lt;p>To get port &lt;code>9090&lt;/code> routed to a second container, we add:&lt;/p>
&lt;pre tabindex="0">&lt;code>app2:
image: docker.io/alpinelinux/darkhttpd:latest
labels:
- traefik.http.routers.app2.rule=Host(`example.com`)
- traefik.http.routers.app2.entrypoints=ep2
&lt;/code>&lt;/pre>&lt;p>This is the same thing, except we use entrypoint &lt;code>ep2&lt;/code>.&lt;/p>
&lt;p>With everything running, we can watch the logs from &lt;code>docker-compose up&lt;/code> and see that a request on port 80:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -H &amp;#39;host: example.com&amp;#39; localhost
&lt;/code>&lt;/pre>&lt;p>Is serviced by &lt;code>app1&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>app1_1 | 172.20.0.2 - - [21/Jun/2022:02:44:11 +0000] &amp;#34;GET / HTTP/1.1&amp;#34; 200 354 &amp;#34;&amp;#34; &amp;#34;curl/7.76.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>And that request on port 9090:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -H &amp;#39;host: example.com&amp;#39; localhost:9090
&lt;/code>&lt;/pre>&lt;p>Is serviced by &lt;code>app2&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>app2_1 | 172.20.0.2 - - [21/Jun/2022:02:44:39 +0000] &amp;#34;GET / HTTP/1.1&amp;#34; 200 354 &amp;#34;&amp;#34; &amp;#34;curl/7.76.1&amp;#34;
&lt;/code>&lt;/pre>&lt;hr>
&lt;p>The complete &lt;code>docker-compose.yaml&lt;/code> file from this post looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: &amp;#34;3&amp;#34;
services:
proxy:
image: traefik:latest
command:
- --api.insecure=true
- --providers.docker
- --entrypoints.ep1.address=:80
- --entrypoints.ep2.address=:9090
ports:
- &amp;#34;80:80&amp;#34;
- &amp;#34;8080:8080&amp;#34;
- &amp;#34;9090:9090&amp;#34;
volumes:
- /var/run/docker.sock:/var/run/docker.sock
app1:
image: docker.io/alpinelinux/darkhttpd:latest
labels:
- traefik.http.routers.app1.rule=Host(`example.com`)
- traefik.http.routers.app1.entrypoints=ep1
app2:
image: docker.io/alpinelinux/darkhttpd:latest
labels:
- traefik.http.routers.app2.rule=Host(`example.com`)
- traefik.http.routers.app2.entrypoints=ep2
&lt;/code>&lt;/pre></content></item><item><title>Udev rules for CH340 serial devices</title><link>https://blog.oddbit.com/post/2022-02-13-wemos-udev-rules/</link><pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2022-02-13-wemos-udev-rules/</guid><description>I like to fiddle with Micropython, particularly on the Wemos D1 Mini, because these are such a neat form factor. Unfortunately, they have a cheap CH340 serial adapter on board, which means that from the perspective of Linux these devices are all functionally identical &amp;ndash; there&amp;rsquo;s no way to identify one device from another. This by itself would be a manageable problem, except that the device names assigned to these devices aren&amp;rsquo;t constant: depending on the order in which they get plugged in (and the order in which they are detected at boot), a device might be /dev/ttyUSB0 one day and /dev/ttyUSB2 another day.</description><content>&lt;p>I like to fiddle with &lt;a href="https://micropython.org">Micropython&lt;/a>, particularly on the &lt;a href="https://www.wemos.cc/en/latest/d1/d1_mini.html">Wemos D1 Mini&lt;/a>, because these are such a neat form factor. Unfortunately, they have a cheap CH340 serial adapter on board, which means that from the perspective of Linux these devices are all functionally identical &amp;ndash; there&amp;rsquo;s no way to identify one device from another. This by itself would be a manageable problem, except that the device names assigned to these devices aren&amp;rsquo;t constant: depending on the order in which they get plugged in (and the order in which they are detected at boot), a device might be &lt;code>/dev/ttyUSB0&lt;/code> one day and &lt;code>/dev/ttyUSB2&lt;/code> another day.&lt;/p>
&lt;p>On more than one occasion, I have accidentally re-flashed the wrong device. Ouch.&lt;/p>
&lt;p>A common solution to this problem is to create device names based on the USB topology &amp;ndash; that is, assign names based on a device&amp;rsquo;s position in the USB bus: e.g., when attaching a new USB serial device, expose it at something like &lt;code>/dev/usbserial/&amp;lt;bus&amp;gt;/&amp;lt;device_path&amp;gt;&lt;/code>. While that sounds conceptually simple, it took me a while to figure out the correct &lt;a href="https://en.wikipedia.org/wiki/Udev">udev&lt;/a> rules.&lt;/p>
&lt;p>Looking at the available attributes for a serial device, we see:&lt;/p>
&lt;pre tabindex="0">&lt;code># udevadm info -a -n /dev/ttyUSB0
[...]
looking at device &amp;#39;/devices/pci0000:00/0000:00:1c.0/0000:03:00.0/usb3/3-1/3-1.4/3-1.4.3/3-1.4.3:1.0/ttyUSB0/tty/ttyUSB0&amp;#39;:
KERNEL==&amp;#34;ttyUSB0&amp;#34;
SUBSYSTEM==&amp;#34;tty&amp;#34;
DRIVER==&amp;#34;&amp;#34;
ATTR{power/control}==&amp;#34;auto&amp;#34;
ATTR{power/runtime_active_time}==&amp;#34;0&amp;#34;
ATTR{power/runtime_status}==&amp;#34;unsupported&amp;#34;
ATTR{power/runtime_suspended_time}==&amp;#34;0&amp;#34;
looking at parent device &amp;#39;/devices/pci0000:00/0000:00:1c.0/0000:03:00.0/usb3/3-1/3-1.4/3-1.4.3/3-1.4.3:1.0/ttyUSB0&amp;#39;:
KERNELS==&amp;#34;ttyUSB0&amp;#34;
SUBSYSTEMS==&amp;#34;usb-serial&amp;#34;
DRIVERS==&amp;#34;ch341-uart&amp;#34;
ATTRS{port_number}==&amp;#34;0&amp;#34;
ATTRS{power/control}==&amp;#34;auto&amp;#34;
ATTRS{power/runtime_active_time}==&amp;#34;0&amp;#34;
ATTRS{power/runtime_status}==&amp;#34;unsupported&amp;#34;
ATTRS{power/runtime_suspended_time}==&amp;#34;0&amp;#34;
looking at parent device &amp;#39;/devices/pci0000:00/0000:00:1c.0/0000:03:00.0/usb3/3-1/3-1.4/3-1.4.3/3-1.4.3:1.0&amp;#39;:
KERNELS==&amp;#34;3-1.4.3:1.0&amp;#34;
SUBSYSTEMS==&amp;#34;usb&amp;#34;
DRIVERS==&amp;#34;ch341&amp;#34;
ATTRS{authorized}==&amp;#34;1&amp;#34;
ATTRS{bAlternateSetting}==&amp;#34; 0&amp;#34;
ATTRS{bInterfaceClass}==&amp;#34;ff&amp;#34;
ATTRS{bInterfaceNumber}==&amp;#34;00&amp;#34;
ATTRS{bInterfaceProtocol}==&amp;#34;02&amp;#34;
ATTRS{bInterfaceSubClass}==&amp;#34;01&amp;#34;
ATTRS{bNumEndpoints}==&amp;#34;03&amp;#34;
ATTRS{supports_autosuspend}==&amp;#34;1&amp;#34;
looking at parent device &amp;#39;/devices/pci0000:00/0000:00:1c.0/0000:03:00.0/usb3/3-1/3-1.4/3-1.4.3&amp;#39;:
KERNELS==&amp;#34;3-1.4.3&amp;#34;
SUBSYSTEMS==&amp;#34;usb&amp;#34;
DRIVERS==&amp;#34;usb&amp;#34;
ATTRS{authorized}==&amp;#34;1&amp;#34;
ATTRS{avoid_reset_quirk}==&amp;#34;0&amp;#34;
ATTRS{bConfigurationValue}==&amp;#34;1&amp;#34;
ATTRS{bDeviceClass}==&amp;#34;ff&amp;#34;
ATTRS{bDeviceProtocol}==&amp;#34;00&amp;#34;
ATTRS{bDeviceSubClass}==&amp;#34;00&amp;#34;
ATTRS{bMaxPacketSize0}==&amp;#34;8&amp;#34;
ATTRS{bMaxPower}==&amp;#34;98mA&amp;#34;
ATTRS{bNumConfigurations}==&amp;#34;1&amp;#34;
ATTRS{bNumInterfaces}==&amp;#34; 1&amp;#34;
ATTRS{bcdDevice}==&amp;#34;0262&amp;#34;
ATTRS{bmAttributes}==&amp;#34;80&amp;#34;
ATTRS{busnum}==&amp;#34;3&amp;#34;
ATTRS{configuration}==&amp;#34;&amp;#34;
ATTRS{devnum}==&amp;#34;8&amp;#34;
ATTRS{devpath}==&amp;#34;1.4.3&amp;#34;
ATTRS{idProduct}==&amp;#34;7523&amp;#34;
ATTRS{idVendor}==&amp;#34;1a86&amp;#34;
ATTRS{ltm_capable}==&amp;#34;no&amp;#34;
ATTRS{maxchild}==&amp;#34;0&amp;#34;
ATTRS{power/active_duration}==&amp;#34;48902765&amp;#34;
ATTRS{power/autosuspend}==&amp;#34;2&amp;#34;
ATTRS{power/autosuspend_delay_ms}==&amp;#34;2000&amp;#34;
ATTRS{power/connected_duration}==&amp;#34;48902765&amp;#34;
ATTRS{power/control}==&amp;#34;on&amp;#34;
ATTRS{power/level}==&amp;#34;on&amp;#34;
ATTRS{power/persist}==&amp;#34;1&amp;#34;
ATTRS{power/runtime_active_time}==&amp;#34;48902599&amp;#34;
ATTRS{power/runtime_status}==&amp;#34;active&amp;#34;
ATTRS{power/runtime_suspended_time}==&amp;#34;0&amp;#34;
ATTRS{product}==&amp;#34;USB2.0-Serial&amp;#34;
ATTRS{quirks}==&amp;#34;0x0&amp;#34;
ATTRS{removable}==&amp;#34;unknown&amp;#34;
ATTRS{rx_lanes}==&amp;#34;1&amp;#34;
ATTRS{speed}==&amp;#34;12&amp;#34;
ATTRS{tx_lanes}==&amp;#34;1&amp;#34;
ATTRS{urbnum}==&amp;#34;17&amp;#34;
ATTRS{version}==&amp;#34; 1.10&amp;#34;
[...]
&lt;/code>&lt;/pre>&lt;p>In this output, we find that the device itself (at the top) doesn&amp;rsquo;t have any useful attributes we can use for creating a systematic device name. It&amp;rsquo;s not until we&amp;rsquo;ve moved up the device hierarchy to &lt;code>/devices/pci0000:00/0000:00:1c.0/0000:03:00.0/usb3/3-1/3-1.4/3-1.4.3&lt;/code> that we find topology information (in the &lt;code>busnum&lt;/code> and &lt;code>devpath&lt;/code> attributes). This complicates matters because a udev rule only has access to attributes defined directly on matching device, so we can&amp;rsquo;t right something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>SUBSYSTEM==&amp;#34;usb-serial&amp;#34;, SYMLINK+=&amp;#34;usbserial/$attr{busnum}/$attr{devpath}&amp;#34;
&lt;/code>&lt;/pre>&lt;p>How do we access the attributes of a parent node in our rule?&lt;/p>
&lt;p>The answer is by creating environment variables that preserve the values in which we are interested. I started with this:&lt;/p>
&lt;pre tabindex="0">&lt;code>SUBSYSTEMS==&amp;#34;usb&amp;#34;, ENV{.USB_BUSNUM}=&amp;#34;$attr{busnum}&amp;#34;, ENV{.USB_DEVPATH}=&amp;#34;$attr{devpath}&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Here, my goal was to stash the &lt;code>busnum&lt;/code> and &lt;code>devpath&lt;/code> attributes in &lt;code>.USB_BUSNUM&lt;/code> and &lt;code>.USB_DEVPATH&lt;/code>, but this didn&amp;rsquo;t work: it matches device path &lt;code>/devices/pci0000:00/0000:00:1c.0/0000:03:00.0/usb3/3-1/3-1.4/3-1.4.3/3-1.4.3:1.0&lt;/code>, which is:&lt;/p>
&lt;pre tabindex="0">&lt;code>KERNELS==&amp;#34;3-1.4.3:1.0&amp;#34;
SUBSYSTEMS==&amp;#34;usb&amp;#34;
DRIVERS==&amp;#34;ch341&amp;#34;
ATTRS{authorized}==&amp;#34;1&amp;#34;
ATTRS{bAlternateSetting}==&amp;#34; 0&amp;#34;
ATTRS{bInterfaceClass}==&amp;#34;ff&amp;#34;
ATTRS{bInterfaceNumber}==&amp;#34;00&amp;#34;
ATTRS{bInterfaceProtocol}==&amp;#34;02&amp;#34;
ATTRS{bInterfaceSubClass}==&amp;#34;01&amp;#34;
ATTRS{bNumEndpoints}==&amp;#34;03&amp;#34;
ATTRS{supports_autosuspend}==&amp;#34;1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>We need to match the next device up the chain, so we need to make our match more specific. There are a couple of different options we can pursue; the simplest is probably to take advantage of the fact that the next device up the chain has &lt;code>SUBSYSTEMS==&amp;quot;usb&amp;quot;&lt;/code> and &lt;code>DRIVERS=&amp;quot;usb&amp;quot;&lt;/code>, so we could instead write:&lt;/p>
&lt;pre tabindex="0">&lt;code>SUBSYSTEMS==&amp;#34;usb&amp;#34;, DRIVERS==&amp;#34;usb&amp;#34;, ENV{.USB_BUSNUM}=&amp;#34;$attr{busnum}&amp;#34;, ENV{.USB_DEVPATH}=&amp;#34;$attr{devpath}&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Alternately, we could ask for &amp;ldquo;the first device that has a &lt;code>busnum&lt;/code> attribute&amp;rdquo; like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>SUBSYSTEMS==&amp;#34;usb&amp;#34;, ATTRS{busnum}==&amp;#34;?*&amp;#34;, ENV{.USB_BUSNUM}=&amp;#34;$attr{busnum}&amp;#34;, ENV{.USB_DEVPATH}=&amp;#34;$attr{devpath}&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Where (from the &lt;code>udev(7)&lt;/code> man page), &lt;code>?&lt;/code> matches any single character and &lt;code>*&lt;/code> matches zero or more characters, so this matches any device in which &lt;code>busnum&lt;/code> has a non-empty value. We can test this rule out using the &lt;code>udevadm test&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code># udevadm test $(udevadm info --query=path --name=/dev/ttyUSB0)
[...]
.USB_BUSNUM=3
.USB_DEVPATH=1.4.3
[...]
&lt;/code>&lt;/pre>&lt;p>This shows us that our rule is matching and setting up the appropriate variables. We can now use those in a subsequent rule to create the desired symlink:&lt;/p>
&lt;pre tabindex="0">&lt;code>SUBSYSTEMS==&amp;#34;usb&amp;#34;, ATTRS{busnum}==&amp;#34;?*&amp;#34;, ENV{.USB_BUSNUM}=&amp;#34;$attr{busnum}&amp;#34;, ENV{.USB_DEVPATH}=&amp;#34;$attr{devpath}&amp;#34;
SUBSYSTEMS==&amp;#34;usb-serial&amp;#34;, SYMLINK+=&amp;#34;usbserial/$env{.USB_BUSNUM}/$env{.USB_DEVPATH}&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Re-running the test command, we see:&lt;/p>
&lt;pre tabindex="0">&lt;code># udevadm test $(udevadm info --query=path --name=/dev/ttyUSB0)
[...]
DEVLINKS=/dev/serial/by-path/pci-0000:03:00.0-usb-0:1.4.3:1.0-port0 /dev/usbserial/3/1.4.3 /dev/serial/by-id/usb-1a86_USB2.0-Serial-if00-port0
[...]
&lt;/code>&lt;/pre>&lt;p>You can see the new symlink in the &lt;code>DEVLINKS&lt;/code> value, and looking at &lt;code>/dev/usbserial&lt;/code> we can see the expected symlinks:&lt;/p>
&lt;pre tabindex="0">&lt;code># tree /dev/usbserial
/dev/usbserial/
└── 3
├── 1.1 -&amp;gt; ../../ttyUSB1
└── 1.4.3 -&amp;gt; ../../ttyUSB0
&lt;/code>&lt;/pre>&lt;p>And there have it. Now as long as I attach a specific device to the same USB port on my system, it will have the same device node. I&amp;rsquo;ve updated my tooling to use these paths (&lt;code>/dev/usbserial/3/1.4.3&lt;/code>) instead of the kernel names (&lt;code>/dev/ttyUSB0&lt;/code>), and it has greatly simplified things.&lt;/p></content></item><item><title>A pair of userscripts for cleaning up Stack Exchange sites</title><link>https://blog.oddbit.com/post/2021-09-05-sx-question-filters/</link><pubDate>Sun, 05 Sep 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-09-05-sx-question-filters/</guid><description>I&amp;rsquo;ve been a regular visitor to Stack Overflow and other Stack Exchange sites over the years, and while I&amp;rsquo;ve mostly enjoyed the experience, I&amp;rsquo;ve been frustrated by the lack of control I have over what questions I see. I&amp;rsquo;m not really interested in looking at questions that have already been closed, or that have a negative score, but there&amp;rsquo;s no native facility for filtering questions like this.
I finally spent the time learning just enough JavaScript to hurt myself to put together a pair of scripts that let me present the questions that way I want:</description><content>&lt;p>I&amp;rsquo;ve been a regular visitor to &lt;a href="https://stackoverflow.com">Stack Overflow&lt;/a> and other &lt;a href="https://stackexchange.com">Stack
Exchange&lt;/a> sites over the years, and while I&amp;rsquo;ve mostly enjoyed the
experience, I&amp;rsquo;ve been frustrated by the lack of control I have over
what questions I see. I&amp;rsquo;m not really interested in looking at
questions that have already been closed, or that have a negative
score, but there&amp;rsquo;s no native facility for filtering questions like
this.&lt;/p>
&lt;p>I finally spent the time learning just enough JavaScript &lt;del>&lt;del>to hurt
myself&lt;/del>&lt;/del> to put together a pair of scripts that let me present the
questions that way I want:&lt;/p>
&lt;h2 id="sx-hide-questions">sx-hide-questions&lt;/h2>
&lt;p>The &lt;a href="https://github.com/larsks/sx-question-filter/raw/master/sx-hide-questions.user.js">sx-hide-questions&lt;/a> script will hide:&lt;/p>
&lt;ul>
&lt;li>Questions that are closed&lt;/li>
&lt;li>Questions that are marked as a duplicate&lt;/li>
&lt;li>Questions that have a score below 0&lt;/li>
&lt;/ul>
&lt;p>Because I wanted it to be obvious that the script was actually doing
something, hidden questions don&amp;rsquo;t just disappear; they fade out.&lt;/p>
&lt;p>These behaviors (including the fading) can all be controlled
individually by a set of global variables at the top of the script.&lt;/p>
&lt;figure class="left" >
&lt;img src="fading.gif" />
&lt;/figure>
&lt;h2 id="sx-reorder-questions">sx-reorder questions&lt;/h2>
&lt;p>The &lt;a href="https://github.com/larsks/sx-question-filter/raw/master/sx-reorder-questions.user.js">sx-reorder-questions&lt;/a> script will sort questions such that
anything that has an answer will be at the bottom, and questions that
have not yet been answered appear at the top.&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>If you are using the &lt;a href="https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=en">Tampermonkey&lt;/a> extension, you should be able to
click on the links to the script earlier in this post and be taken
directly to the installation screen. If you&amp;rsquo;re &lt;em>not&lt;/em> running
Tampermonkey, than either (a) install it, or (b) you&amp;rsquo;re on your own.&lt;/p>
&lt;p>You can find both of these scripts in my &lt;a href="https://github.com/larsks/sx-question-filter">sx-question-filter&lt;/a>
repository.&lt;/p>
&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>These scripts rely on the CSS classes and layout of the Stack Exchange
websites. If these change, the scripts will need updating. If you
notice that something no longer works as advertised, please feel free
to submit pull request with the necessary corrections!&lt;/p></content></item><item><title>Kubernetes External Secrets</title><link>https://blog.oddbit.com/post/2021-09-03-kubernetes-external-secrets/</link><pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-09-03-kubernetes-external-secrets/</guid><description>At $JOB we maintain the configuration for our OpenShift clusters in a public git repository. Changes in the git repository are applied automatically using ArgoCD and Kustomize. This works great, but the public nature of the repository means we need to find a secure solution for managing secrets (such as passwords and other credentials necessary for authenticating to external services). In particular, we need a solution that permits our public repository to be the source of truth for our cluster configuration, without compromising our credentials.</description><content>&lt;p>At &lt;em>$JOB&lt;/em> we maintain the configuration for our OpenShift clusters in a public git repository. Changes in the git repository are applied automatically using &lt;a href="https://argo-cd.readthedocs.io/en/stable/">ArgoCD&lt;/a> and &lt;a href="https://kustomize.io/">Kustomize&lt;/a>. This works great, but the public nature of the repository means we need to find a secure solution for managing secrets (such as passwords and other credentials necessary for authenticating to external services). In particular, we need a solution that permits our public repository to be the source of truth for our cluster configuration, without compromising our credentials.&lt;/p>
&lt;h2 id="rejected-options">Rejected options&lt;/h2>
&lt;p>We initially looked at including secrets directly in the repository through the use of the &lt;a href="https://github.com/viaduct-ai/kustomize-sops">KSOPS&lt;/a> plugin for Kustomize, which uses &lt;a href="https://github.com/mozilla/sops">sops&lt;/a> to encrypt secrets with GPG keys. There are some advantages to this arrangement:&lt;/p>
&lt;ul>
&lt;li>It doesn&amp;rsquo;t require any backend service&lt;/li>
&lt;li>It&amp;rsquo;s easy to control read access to secrets in the repository by encrypting them to different recipients.&lt;/li>
&lt;/ul>
&lt;p>There were some minor disadvantages:&lt;/p>
&lt;ul>
&lt;li>We can&amp;rsquo;t install ArgoCD via the operator because we need a customized image that includes KSOPS, so we have to maintain our own ArgoCD image.&lt;/li>
&lt;/ul>
&lt;p>And there was one major problem:&lt;/p>
&lt;ul>
&lt;li>Using GPG-encrypted secrets in a git repository makes it effectively impossible to recover from a key compromise.&lt;/li>
&lt;/ul>
&lt;p>One a private key is compromised, anyone with access to that key and the git repository will be able to decrypt data in historical commits, even if we re-encrypt all the data with a new key.&lt;/p>
&lt;p>Because of these security implications we decided we would need a different solution (it&amp;rsquo;s worth noting here that Bitnami &lt;a href="https://github.com/bitnami-labs/sealed-secrets">Sealed Secrets&lt;/a> suffers from effectively the same problem).&lt;/p>
&lt;h2 id="our-current-solution">Our current solution&lt;/h2>
&lt;p>We&amp;rsquo;ve selected a solution that uses the &lt;a href="https://github.com/external-secrets/kubernetes-external-secrets">External Secrets&lt;/a> project in concert with the AWS &lt;a href="https://aws.amazon.com/secrets-manager/">SecretsManager&lt;/a> service.&lt;/p>
&lt;h3 id="kubernetes-external-secrets">Kubernetes external secrets&lt;/h3>
&lt;p>The &lt;a href="https://github.com/external-secrets/kubernetes-external-secrets">External Secrets&lt;/a> project allows one to store secrets in an external secrets store, such as AWS &lt;a href="https://aws.amazon.com/secrets-manager/">SecretsManager&lt;/a>, Hashicorp &lt;a href="https://www.vaultproject.io/">Vault&lt;/a>, and others &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. The manifests that get pushed into your OpenShift cluster contain only pointers (called &lt;code>ExternalSecrets&lt;/code>) to those secrets; the external secrets controller running on the cluster uses the information contained in the &lt;code>ExternalSecret&lt;/code> in combination with stored credentials to fetch the secret from your chosen backend and realize the actual &lt;code>Secret&lt;/code> resource. An external secret manifest referring to a secret named &lt;code>mysceret&lt;/code> stored in AWS SecretsManager would look something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: &amp;#34;kubernetes-client.io/v1&amp;#34;
kind: ExternalSecret
metadata:
name: example-secret
spec:
backendType: secretsManager
data:
- key: mysecret
name: mysecretvalue
&lt;/code>&lt;/pre>&lt;p>This model means that no encrypted data is ever stored in the git repository, which resolves the main problem we had with the solutions mentioned earlier.&lt;/p>
&lt;p>External Secrets can be installed into your Kubernetes environment using Helm, or you can use &lt;code>helm template&lt;/code> to generate manifests locally and apply them using Kustomize or some other tool (this is the route we took).&lt;/p>
&lt;h3 id="aws-secretsmanager-service">AWS SecretsManager Service&lt;/h3>
&lt;p>AWS &lt;a href="https://aws.amazon.com/secrets-manager/">SecretsManager&lt;/a> is a service for storing and managing secrets and making them accessible via an API. Using SecretsManager we have very granular control over who can view or modify secrets; this allows us, for example, to create cluster-specific secret readers that can only read secrets intended for a specific cluster (e.g. preventing our development environment from accidentally using production secrets).&lt;/p>
&lt;p>SecretsManager provides automatic versioning of secrets to prevent loss of data if you inadvertently change a secret while still requiring the old value.&lt;/p>
&lt;p>We can create secrets through the AWS SecretsManager console, or we can use the &lt;a href="https://aws.amazon.com/cli/">AWS CLI&lt;/a>, which looks something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>aws secretsmanager create-secret \
--name mysecretname \
--secret-string mysecretvalue
&lt;/code>&lt;/pre>&lt;h3 id="two-great-tastes-that-taste-great-together">Two great tastes that taste great together&lt;/h3>
&lt;p>This combination solves a number of our problems:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Because we&amp;rsquo;re not storing actual secrets in the repository, we don&amp;rsquo;t need to worry about encrypting anything.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Because we&amp;rsquo;re not managing encrypted data, replacing secrets is much easier.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There&amp;rsquo;s a robust mechanism for controlling access to secrets.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>This solution offers a separation of concern that simply wasn&amp;rsquo;t possible with the KSOPS model: someone can maintain secrets without having to know anything about Kubernetes manifests, and someone can work on the repository without needing to know any secrets.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="creating-external-secrets">Creating external secrets&lt;/h2>
&lt;p>In its simplest form, an &lt;code>ExternalSecret&lt;/code> resource maps values from specific named secrets in the backend to keys in a &lt;code>Secret&lt;/code> resource. For example, if we wanted to create a &lt;code>Secret&lt;/code> in OpenShift with the username and password for an external service, we could create to separate secrets in SecretsManager. One for the username:&lt;/p>
&lt;pre tabindex="0">&lt;code>aws secretsmanager create-secret \
--name cluster/cluster1/example-secret-username \
--secret-string foo
&lt;/code>&lt;/pre>&lt;p>And one for the password:&lt;/p>
&lt;pre tabindex="0">&lt;code>aws secretsmanager create-secret \
--name cluster/cluster1/example-secret-password \
--secret-string bar \
--tags Key=cluster,Value=cluster1
&lt;/code>&lt;/pre>&lt;p>And then create an &lt;code>ExternalSecret&lt;/code> manifest like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: &amp;#34;kubernetes-client.io/v1&amp;#34;
kind: ExternalSecret
metadata:
name: example-secret
spec:
backendType: secretsManager
data:
- key: cluster/cluster1/example-secret-username
name: username
- key: cluster/cluster1/example-secret-password
name: password
&lt;/code>&lt;/pre>&lt;p>This instructs the External Secrets controller to create an &lt;code>Opaque&lt;/code> secret named &lt;code>example-secret&lt;/code> from data in AWS SecretsManager. The value of the &lt;code>username&lt;/code> key will come from the secret named &lt;code>cluster/cluster1/example-secret-username&lt;/code>, and similarly for &lt;code>password&lt;/code>. The resulting &lt;code>Secret&lt;/code> resource will look something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Secret
metadata:
name: example-secret
type: Opaque
data:
password: YmFy
username: Zm9v
&lt;/code>&lt;/pre>&lt;h3 id="templates-for-structured-data">Templates for structured data&lt;/h3>
&lt;p>In the previous example, we created two separate secrets in SecretsManager for storing a username and password. It might be more convenient if we could store both credentials in a single secret. Thanks to the &lt;a href="https://github.com/external-secrets/kubernetes-external-secrets#templating">templating&lt;/a> support in External Secrets, we can do that!&lt;/p>
&lt;p>Let&amp;rsquo;s redo the previous example, but instead of using two separate secrets, we&amp;rsquo;ll create a single secret named &lt;code>cluster/cluster1/example-secret&lt;/code> in which the secret value is a JSON document containing both the username and password:&lt;/p>
&lt;pre tabindex="0">&lt;code>aws secretsmanager create-secret \
--name cluster/cluster1/example-secret \
--secret-string &amp;#39;{&amp;#34;username&amp;#34;: &amp;#34;foo&amp;#34;, &amp;#34;password&amp;#34;: &amp;#34;bar&amp;#34;}&amp;#39;
&lt;/code>&lt;/pre>&lt;p>NB: The &lt;a href="https://github.com/jpmens/jo">jo&lt;/a> utility is a neat little utility for generating JSON from the command line; using that we could write the above like this&amp;hellip;&lt;/p>
&lt;pre tabindex="0">&lt;code>aws secretsmanager create-secret \
--name cluster/cluster1/example-secret \
--secret-string $(jo username=foo password=bar)
&lt;/code>&lt;/pre>&lt;p>&amp;hellip;which makes it easier to write JSON without missing a quote, closing bracket, etc.&lt;/p>
&lt;p>We can extract these values into the appropriate keys by adding a &lt;code>template&lt;/code> section to our &lt;code>ExternalSecret&lt;/code>, and using the &lt;code>JSON.parse&lt;/code> template function, like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: &amp;#34;kubernetes-client.io/v1&amp;#34;
kind: ExternalSecret
metadata:
name: example-secret
namespace: sandbox
spec:
backendType: secretsManager
data:
- key: cluster/cluster1/example-secret
name: creds
template:
stringData:
username: &amp;#34;&amp;lt;%= JSON.parse(data.creds).username %&amp;gt;&amp;#34;
password: &amp;#34;&amp;lt;%= JSON.parse(data.creds).password %&amp;gt;&amp;#34;
&lt;/code>&lt;/pre>&lt;p>The result secret will look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Secret
metadata:
name: example-secret
type: Opaque
data:
creds: eyJ1c2VybmFtZSI6ICJmb28iLCAicGFzc3dvcmQiOiAiYmFyIn0=
password: YmFy
username: Zm9v
&lt;/code>&lt;/pre>&lt;p>Notice that in addition to the values created in the &lt;code>template&lt;/code> section, the &lt;code>Secret&lt;/code> also contains any keys defined in the &lt;code>data&lt;/code> section of the &lt;code>ExternalSecret&lt;/code>.&lt;/p>
&lt;p>Templating can also be used to override the secret type if you want something other than &lt;code>Opaque&lt;/code>, add metadata, and otherwise influence the generated &lt;code>Secret&lt;/code>.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>E.g. Azure Key Vault, Google Secret Manager, Alibaba Cloud KMS Secret Manager, Akeyless&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></content></item><item><title>Connecting OpenShift to an External Ceph Cluster</title><link>https://blog.oddbit.com/post/2021-08-23-external-ocs/</link><pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-08-23-external-ocs/</guid><description>Red Hat&amp;rsquo;s OpenShift Data Foundation (formerly &amp;ldquo;OpenShift Container Storage&amp;rdquo;, or &amp;ldquo;OCS&amp;rdquo;) allows you to either (a) automatically set up a Ceph cluster as an application running on your OpenShift cluster, or (b) connect your OpenShift cluster to an externally managed Ceph cluster. While setting up Ceph as an OpenShift application is a relatively polished experienced, connecting to an external cluster still has some rough edges.
NB I am not a Ceph expert.</description><content>&lt;p>Red Hat&amp;rsquo;s &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation">OpenShift Data Foundation&lt;/a> (formerly &amp;ldquo;OpenShift
Container Storage&amp;rdquo;, or &amp;ldquo;OCS&amp;rdquo;) allows you to either (a) automatically
set up a Ceph cluster as an application running on your OpenShift
cluster, or (b) connect your OpenShift cluster to an externally
managed Ceph cluster. While setting up Ceph as an OpenShift
application is a relatively polished experienced, connecting to an
external cluster still has some rough edges.&lt;/p>
&lt;p>&lt;strong>NB&lt;/strong> I am not a Ceph expert. If you read this and think I&amp;rsquo;ve made a
mistake with respect to permissions or anything else, please feel free
to leave a comment and I will update the article as necessary. In
particular, I think it may be possible to further restrict the &lt;code>mgr&lt;/code>
permissions shown in this article and I&amp;rsquo;m interested in feedback on
that topic.&lt;/p>
&lt;h2 id="installing-ocs">Installing OCS&lt;/h2>
&lt;p>Regardless of which option you choose, you start by installing the
&amp;ldquo;OpenShift Container Storage&amp;rdquo; operator (the name change apparently
hasn&amp;rsquo;t made it to the Operator Hub yet). When you select &amp;ldquo;external
mode&amp;rdquo;, you will be given the opportunity to download a Python script
that you are expected to run on your Ceph cluster. This script will
create some Ceph authentication principals and will emit a block of
JSON data that gets pasted into the OpenShift UI to configure the
external StorageCluster resource.&lt;/p>
&lt;p>The script has a single required option, &lt;code>--rbd-data-pool-name&lt;/code>, that
you use to provide the name of an existing pool. If you run the script
with only that option, it will create the following ceph principals
and associated capabilities:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>client.csi-rbd-provisioner&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mgr = &amp;#34;allow rw&amp;#34;
caps mon = &amp;#34;profile rbd&amp;#34;
caps osd = &amp;#34;profile rbd&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>&lt;code>client.csi-rbd-node&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mon = &amp;#34;profile rbd&amp;#34;
caps osd = &amp;#34;profile rbd&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>&lt;code>client.healthchecker&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mgr = &amp;#34;allow command config&amp;#34;
caps mon = &amp;#34;allow r, allow command quorum_status, allow command version&amp;#34;
caps osd = &amp;#34;allow rwx pool=default.rgw.meta, allow r pool=.rgw.root, allow rw pool=default.rgw.control, allow rx pool=default.rgw.log, allow x pool=default.rgw.buckets.index&amp;#34;
&lt;/code>&lt;/pre>&lt;p>This account is used to verify the health of the ceph cluster.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If you also provide the &lt;code>--cephfs-filesystem-name&lt;/code> option, the script
will also create:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>client.csi-cephfs-provisioner&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mgr = &amp;#34;allow rw&amp;#34;
caps mon = &amp;#34;allow r&amp;#34;
caps osd = &amp;#34;allow rw tag cephfs metadata=*&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>&lt;code>client.csi-cephfs-node&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mds = &amp;#34;allow rw&amp;#34;
caps mgr = &amp;#34;allow rw&amp;#34;
caps mon = &amp;#34;allow r&amp;#34;
caps osd = &amp;#34;allow rw tag cephfs *=*&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>If you specify &lt;code>--rgw-endpoint&lt;/code>, the script will create a RGW user
named &lt;code>rgw-admin-ops-user&lt;/code>with administrative access to the default
RGW pool.&lt;/p>
&lt;h2 id="so-whats-the-problem">So what&amp;rsquo;s the problem?&lt;/h2>
&lt;p>The above principals and permissions are fine if you&amp;rsquo;ve created an
external Ceph cluster explicitly for the purpose of supporting a
single OpenShift cluster.&lt;/p>
&lt;p>In an environment where a single Ceph cluster is providing storage to
multiple OpenShift clusters, and &lt;em>especially&lt;/em> in an environment where
administration of the Ceph and OpenShift environments are managed by
different groups, the process, principals, and permissions create a
number of problems.&lt;/p>
&lt;p>The first and foremost is that the script provided by OCS both (a)
gathers information about the Ceph environment, and (b) &lt;em>makes changes
to that environment&lt;/em>. If you are installing OCS on OpenShift and want
to connect to a Ceph cluster over which you do not have administrative
control, you may find yourself stymied when the storage administrators
refuse to run your random Python script on the Ceph cluster.&lt;/p>
&lt;p>Ideally, the script would be read-only, and instead of &lt;em>making&lt;/em>
changes to the Ceph cluster it would only &lt;em>validate&lt;/em> the cluster
configuration, and inform the administrator of what changes were
necessary. There should be complete documentation that describes the
necessary configuration scripts so that a Ceph cluster can be
configured correctly without running &lt;em>any&lt;/em> script, and OCS should
provide something more granular than &amp;ldquo;drop a blob of JSON here&amp;rdquo; for
providing the necessary configuration to OpenShift.&lt;/p>
&lt;p>The second major problem is that while the script creates several
principals, it only allows you to set the name of one of them. The
script has a &lt;code>--run-as-user&lt;/code> option, which at first sounds promising,
but ultimately is of questionable use: it only allows you set the Ceph
principal used for cluster health checks.&lt;/p>
&lt;p>There is no provision in the script to create separate principals for
each OpenShift cluster.&lt;/p>
&lt;p>Lastly, the permissions granted to the principals are too broad. For
example, the &lt;code>csi-rbd-node&lt;/code> principal has access to &lt;em>all&lt;/em> RBD pools on
the cluster.&lt;/p>
&lt;h2 id="how-can-we-work-around-it">How can we work around it?&lt;/h2>
&lt;p>If you would like to deploy OCS in an environment where the default
behavior of the configuration script is inappropriate you can work
around this problem by:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Manually generating the necessary principals (with more appropriate
permissions), and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Manually generating the JSON data for input into OCS&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="create-the-storage">Create the storage&lt;/h3>
&lt;p>I&amp;rsquo;ve adopted the following conventions for naming storage pools and
filesystems:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>All resources are prefixed with the name of the cluster (represented
here by &lt;code>${clustername}&lt;/code>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The RBD pool is named &lt;code>${clustername}-rbd&lt;/code>. I create it like this:&lt;/p>
&lt;pre tabindex="0">&lt;code> ceph osd pool create ${clustername}-rbd
ceph osd pool application enable ${clustername}-rbd rbd
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>The CephFS filesystem (if required) is named
&lt;code>${clustername}-fs&lt;/code>, and I create it like this:&lt;/p>
&lt;pre tabindex="0">&lt;code> ceph fs volume create ${clustername}-fs
&lt;/code>&lt;/pre>&lt;p>In addition to the filesystem, this creates two pools:&lt;/p>
&lt;ul>
&lt;li>&lt;code>cephfs.${clustername}-fs.meta&lt;/code>&lt;/li>
&lt;li>&lt;code>cephfs.${clustername}-fs.data&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="creating-the-principals">Creating the principals&lt;/h3>
&lt;p>Assuming that you have followed the same conventions and have an RBD
pool named &lt;code>${clustername}-rbd&lt;/code> and a CephFS filesystem named
&lt;code>${clustername}-fs&lt;/code>, the following set of &lt;code>ceph auth add&lt;/code> commands
should create an appropriate set of principals (with access limited to
just those resources that belong to the named cluster):&lt;/p>
&lt;pre tabindex="0">&lt;code>ceph auth add client.healthchecker-${clustername} \
mgr &amp;#34;allow command config&amp;#34; \
mon &amp;#34;allow r, allow command quorum_status, allow command version&amp;#34;
ceph auth add client.csi-rbd-provisioner-${clustername} \
mgr &amp;#34;allow rw&amp;#34; \
mon &amp;#34;profile rbd&amp;#34; \
osd &amp;#34;profile rbd pool=${clustername}-rbd&amp;#34;
ceph auth add client.csi-rbd-node-${clustername} \
mon &amp;#34;profile rbd&amp;#34; \
osd &amp;#34;profile rbd pool=${clustername}-rbd&amp;#34;
ceph auth add client.csi-cephfs-provisioner-${clustername} \
mgr &amp;#34;allow rw&amp;#34; \
mds &amp;#34;allow rw fsname=${clustername}-fs&amp;#34; \
mon &amp;#34;allow r fsname=${clustername}-fs&amp;#34; \
osd &amp;#34;allow rw tag cephfs metadata=${clustername}-fs&amp;#34;
ceph auth add client.csi-cephfs-node-${clustername} \
mgr &amp;#34;allow rw&amp;#34; \
mds &amp;#34;allow rw fsname=${clustername}-fs&amp;#34; \
mon &amp;#34;allow r fsname=${clustername}-fs&amp;#34; \
osd &amp;#34;allow rw tag cephfs data=${clustername}-fs&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Note that I&amp;rsquo;ve excluded the RGW permissions here; in our OpenShift
environments, we typically rely on the object storage interface
provided by &lt;a href="https://www.noobaa.io/">Noobaa&lt;/a> so I haven&amp;rsquo;t spent time investigating
permissions on the RGW side.&lt;/p>
&lt;h3 id="create-the-json">Create the JSON&lt;/h3>
&lt;p>The final step is to create the JSON blob that you paste into the OCS
installation UI. I use the following script which calls &lt;code>ceph -s&lt;/code>,
&lt;code>ceph mon dump&lt;/code>, and &lt;code>ceph auth get-key&lt;/code> to get the necessary
information from the cluster:&lt;/p>
&lt;pre tabindex="0">&lt;code>#!/usr/bin/python3
import argparse
import json
import subprocess
from urllib.parse import urlparse
usernames = [
&amp;#39;healthchecker&amp;#39;,
&amp;#39;csi-rbd-node&amp;#39;,
&amp;#39;csi-rbd-provisioner&amp;#39;,
&amp;#39;csi-cephfs-node&amp;#39;,
&amp;#39;csi-cephfs-provisioner&amp;#39;,
]
def parse_args():
p = argparse.ArgumentParser()
p.add_argument(&amp;#39;--use-cephfs&amp;#39;, action=&amp;#39;store_true&amp;#39;, dest=&amp;#39;use_cephfs&amp;#39;)
p.add_argument(&amp;#39;--no-use-cephfs&amp;#39;, action=&amp;#39;store_false&amp;#39;, dest=&amp;#39;use_cephfs&amp;#39;)
p.add_argument(&amp;#39;instance_name&amp;#39;)
p.set_defaults(use_rbd=True, use_cephfs=True)
return p.parse_args()
def main():
args = parse_args()
cluster_status = json.loads(subprocess.check_output([&amp;#39;ceph&amp;#39;, &amp;#39;-s&amp;#39;, &amp;#39;-f&amp;#39;, &amp;#39;json&amp;#39;]))
mon_status = json.loads(subprocess.check_output([&amp;#39;ceph&amp;#39;, &amp;#39;mon&amp;#39;, &amp;#39;dump&amp;#39;, &amp;#39;-f&amp;#39;, &amp;#39;json&amp;#39;]))
users = {}
for username in usernames:
key = subprocess.check_output([&amp;#39;ceph&amp;#39;, &amp;#39;auth&amp;#39;, &amp;#39;get-key&amp;#39;, &amp;#39;client.{}-{}&amp;#39;.format(username, args.instance_name)])
users[username] = {
&amp;#39;name&amp;#39;: &amp;#39;client.{}-{}&amp;#39;.format(username, args.instance_name),
&amp;#39;key&amp;#39;: key.decode(),
}
mon_name = mon_status[&amp;#39;mons&amp;#39;][0][&amp;#39;name&amp;#39;]
mon_ip = [
addr for addr in
mon_status[&amp;#39;mons&amp;#39;][0][&amp;#39;public_addrs&amp;#39;][&amp;#39;addrvec&amp;#39;]
if addr[&amp;#39;type&amp;#39;] == &amp;#39;v1&amp;#39;
][0][&amp;#39;addr&amp;#39;]
prom_url = urlparse(cluster_status[&amp;#39;mgrmap&amp;#39;][&amp;#39;services&amp;#39;][&amp;#39;prometheus&amp;#39;])
prom_ip, prom_port = prom_url.netloc.split(&amp;#39;:&amp;#39;)
output = [
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-mon-endpoints&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;ConfigMap&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;data&amp;#34;: &amp;#34;{}={}&amp;#34;.format(mon_name, mon_ip),
&amp;#34;maxMonId&amp;#34;: &amp;#34;0&amp;#34;,
&amp;#34;mapping&amp;#34;: &amp;#34;{}&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-mon&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;admin-secret&amp;#34;: &amp;#34;admin-secret&amp;#34;,
&amp;#34;fsid&amp;#34;: cluster_status[&amp;#39;fsid&amp;#39;],
&amp;#34;mon-secret&amp;#34;: &amp;#34;mon-secret&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-operator-creds&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;userID&amp;#34;: users[&amp;#39;healthchecker&amp;#39;][&amp;#39;name&amp;#39;],
&amp;#34;userKey&amp;#34;: users[&amp;#39;healthchecker&amp;#39;][&amp;#39;key&amp;#39;],
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;ceph-rbd&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;StorageClass&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;pool&amp;#34;: &amp;#34;{}-rbd&amp;#34;.format(args.instance_name),
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;monitoring-endpoint&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;CephCluster&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;MonitoringEndpoint&amp;#34;: prom_ip,
&amp;#34;MonitoringPort&amp;#34;: prom_port,
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-rbd-node&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;userID&amp;#34;: users[&amp;#39;csi-rbd-node&amp;#39;][&amp;#39;name&amp;#39;].replace(&amp;#39;client.&amp;#39;, &amp;#39;&amp;#39;),
&amp;#34;userKey&amp;#34;: users[&amp;#39;csi-rbd-node&amp;#39;][&amp;#39;key&amp;#39;],
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-rbd-provisioner&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;userID&amp;#34;: users[&amp;#39;csi-rbd-provisioner&amp;#39;][&amp;#39;name&amp;#39;].replace(&amp;#39;client.&amp;#39;, &amp;#39;&amp;#39;),
&amp;#34;userKey&amp;#34;: users[&amp;#39;csi-rbd-provisioner&amp;#39;][&amp;#39;key&amp;#39;],
}
}
]
if args.use_cephfs:
output.extend([
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-cephfs-provisioner&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;adminID&amp;#34;: users[&amp;#39;csi-cephfs-provisioner&amp;#39;][&amp;#39;name&amp;#39;].replace(&amp;#39;client.&amp;#39;, &amp;#39;&amp;#39;),
&amp;#34;adminKey&amp;#34;: users[&amp;#39;csi-cephfs-provisioner&amp;#39;][&amp;#39;key&amp;#39;],
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-cephfs-node&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;adminID&amp;#34;: users[&amp;#39;csi-cephfs-node&amp;#39;][&amp;#39;name&amp;#39;].replace(&amp;#39;client.&amp;#39;, &amp;#39;&amp;#39;),
&amp;#34;adminKey&amp;#34;: users[&amp;#39;csi-cephfs-node&amp;#39;][&amp;#39;key&amp;#39;],
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;cephfs&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;StorageClass&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;fsName&amp;#34;: &amp;#34;{}-fs&amp;#34;.format(args.instance_name),
&amp;#34;pool&amp;#34;: &amp;#34;cephfs.{}-fs.data&amp;#34;.format(args.instance_name),
}
}
])
print(json.dumps(output, indent=2))
if __name__ == &amp;#39;__main__&amp;#39;:
main()
&lt;/code>&lt;/pre>&lt;p>If you&amp;rsquo;d prefer a strictly manual process, you can fill in the
necessary values yourself. The JSON produced by the above script
looks like the following, which is invalid JSON because I&amp;rsquo;ve use
inline comments to mark all the values which you would need to
provide:&lt;/p>
&lt;pre tabindex="0">&lt;code>[
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-mon-endpoints&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;ConfigMap&amp;#34;,
&amp;#34;data&amp;#34;: {
# The format is &amp;lt;mon_name&amp;gt;=&amp;lt;mon_endpoint&amp;gt;, and you only need to
# provide a single mon address.
&amp;#34;data&amp;#34;: &amp;#34;ceph0=192.168.122.140:6789&amp;#34;,
&amp;#34;maxMonId&amp;#34;: &amp;#34;0&amp;#34;,
&amp;#34;mapping&amp;#34;: &amp;#34;{}&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-mon&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the fsid of your Ceph cluster.
&amp;#34;fsid&amp;#34;: &amp;#34;c9c32c73-dac4-4cc9-8baa-d73b96c135f4&amp;#34;,
# Do **not** fill in these values, they are unnecessary. OCS
# does not require admin access to your Ceph cluster.
&amp;#34;admin-secret&amp;#34;: &amp;#34;admin-secret&amp;#34;,
&amp;#34;mon-secret&amp;#34;: &amp;#34;mon-secret&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-operator-creds&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key for your healthchecker principal.
# Note that here, unlike elsewhere in this JSON, you must
# provide the &amp;#34;client.&amp;#34; prefix to the principal name.
&amp;#34;userID&amp;#34;: &amp;#34;client.healthchecker-mycluster&amp;#34;,
&amp;#34;userKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;ceph-rbd&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;StorageClass&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name of your RBD pool.
&amp;#34;pool&amp;#34;: &amp;#34;mycluster-rbd&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;monitoring-endpoint&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;CephCluster&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the address and port of the Ceph cluster prometheus
# endpoint.
&amp;#34;MonitoringEndpoint&amp;#34;: &amp;#34;192.168.122.140&amp;#34;,
&amp;#34;MonitoringPort&amp;#34;: &amp;#34;9283&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-rbd-node&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key of the csi-rbd-node principal.
&amp;#34;userID&amp;#34;: &amp;#34;csi-rbd-node-mycluster&amp;#34;,
&amp;#34;userKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-rbd-provisioner&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key of your csi-rbd-provisioner
# principal.
&amp;#34;userID&amp;#34;: &amp;#34;csi-rbd-provisioner-mycluster&amp;#34;,
&amp;#34;userKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-cephfs-provisioner&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key of your csi-cephfs-provisioner
# principal.
&amp;#34;adminID&amp;#34;: &amp;#34;csi-cephfs-provisioner-mycluster&amp;#34;,
&amp;#34;adminKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-cephfs-node&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key of your csi-cephfs-node principal.
&amp;#34;adminID&amp;#34;: &amp;#34;csi-cephfs-node-mycluster&amp;#34;,
&amp;#34;adminKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;cephfs&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;StorageClass&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name of your CephFS filesystem and the name of the
# associated data pool.
&amp;#34;fsName&amp;#34;: &amp;#34;mycluster-fs&amp;#34;,
&amp;#34;pool&amp;#34;: &amp;#34;cephfs.mycluster-fs.data&amp;#34;
}
}
]
&lt;/code>&lt;/pre>&lt;h2 id="associated-bugs">Associated Bugs&lt;/h2>
&lt;p>I&amp;rsquo;ve opened several bug reports to see about adressing some of these
issues:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1996833">#1996833&lt;/a>
&amp;ldquo;ceph-external-cluster-details-exporter.py should have a read-only
mode&amp;rdquo;&lt;/li>
&lt;li>&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1996830">#1996830&lt;/a> &amp;ldquo;OCS
external mode should allow specifying names for all Ceph auth
principals&amp;rdquo;&lt;/li>
&lt;li>&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1996829">#1996829&lt;/a>
&amp;ldquo;Permissions assigned to ceph auth principals when using external
storage are too broad&amp;rdquo;&lt;/li>
&lt;/ul></content></item><item><title>Creating a VXLAN overlay network with Open vSwitch</title><link>https://blog.oddbit.com/post/2021-04-17-vm-ovs-vxlan/</link><pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-04-17-vm-ovs-vxlan/</guid><description>In this post, we&amp;rsquo;ll walk through the process of getting virtual machines on two different hosts to communicate over an overlay network created using the support for VXLAN in Open vSwitch (or OVS).
The test environment For this post, I&amp;rsquo;ll be working with two systems:
node0.ovs.virt at address 192.168.122.107 node1.ovs.virt at address 192.168.122.174 These hosts are running CentOS 8, although once we get past the package installs the instructions will be similar for other distributions.</description><content>&lt;p>In this post, we&amp;rsquo;ll walk through the process of getting virtual
machines on two different hosts to communicate over an overlay network
created using the support for VXLAN in &lt;a href="https://www.openvswitch.org/">Open vSwitch&lt;/a> (or OVS).&lt;/p>
&lt;h2 id="the-test-environment">The test environment&lt;/h2>
&lt;p>For this post, I&amp;rsquo;ll be working with two systems:&lt;/p>
&lt;ul>
&lt;li>&lt;code>node0.ovs.virt&lt;/code> at address 192.168.122.107&lt;/li>
&lt;li>&lt;code>node1.ovs.virt&lt;/code> at address 192.168.122.174&lt;/li>
&lt;/ul>
&lt;p>These hosts are running CentOS 8, although once we get past the
package installs the instructions will be similar for other
distributions.&lt;/p>
&lt;p>While reading through this post, remember that unless otherwise
specified we&amp;rsquo;re going to be running the indicated commands on &lt;em>both&lt;/em>
hosts.&lt;/p>
&lt;h2 id="install-packages">Install packages&lt;/h2>
&lt;p>Before we can get started configuring things we&amp;rsquo;ll need to install OVS
and &lt;a href="https://libvirt.org/">libvirt&lt;/a>. While &lt;code>libvirt&lt;/code> is included with the base CentOS
distribution, for OVS we&amp;rsquo;ll need to add both the &lt;a href="https://fedoraproject.org/wiki/EPEL">EPEL&lt;/a> repository
as well as a recent CentOS &lt;a href="https://www.openstack.org/">OpenStack&lt;/a> repository (OVS is included
in the CentOS OpenStack repositories because it is required by
OpenStack&amp;rsquo;s networking service):&lt;/p>
&lt;pre tabindex="0">&lt;code>yum -y install epel-release centos-release-openstack-victoria
&lt;/code>&lt;/pre>&lt;p>With these additional repositories enabled we can now install the
required packages:&lt;/p>
&lt;pre tabindex="0">&lt;code>yum -y install \
libguestfs-tools-c \
libvirt \
libvirt-daemon-kvm \
openvswitch2.15 \
tcpdump \
virt-install
&lt;/code>&lt;/pre>&lt;h2 id="enable-services">Enable services&lt;/h2>
&lt;p>We need to start both the &lt;code>libvirtd&lt;/code> and &lt;code>openvswitch&lt;/code> services:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl enable --now openvswitch libvirtd
&lt;/code>&lt;/pre>&lt;p>This command will (a) mark the services to start automatically when
the system boots and (b) immediately start the service.&lt;/p>
&lt;h2 id="configure-libvirt">Configure libvirt&lt;/h2>
&lt;p>When &lt;code>libvirt&lt;/code> is first installed it doesn&amp;rsquo;t have any configured
storage pools. Let&amp;rsquo;s create one in the default location,
&lt;code>/var/lib/libvirt/images&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>virsh pool-define-as default --type dir --target /var/lib/libvirt/images
&lt;/code>&lt;/pre>&lt;p>We need to mark the pool active, and we might as well configure it to
activate automatically next time the system boots:&lt;/p>
&lt;pre tabindex="0">&lt;code>virsh pool-start default
virsh pool-autostart default
&lt;/code>&lt;/pre>&lt;h2 id="configure-open-vswitch">Configure Open vSwitch&lt;/h2>
&lt;h3 id="create-the-bridge">Create the bridge&lt;/h3>
&lt;p>With all the prerequisites out of the way we can finally start working
with Open vSwitch. Our first task is to create the OVS bridge that
will host our VXLAN tunnels. To create a bridge named &lt;code>br0&lt;/code>, we run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-br br0
&lt;/code>&lt;/pre>&lt;p>We can inspect the OVS configuration by running &lt;code>ovs-vsctl show&lt;/code>,
which should output something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>cc1e7217-e393-4e21-97c1-92324d47946d
Bridge br0
Port br0
Interface br0
type: internal
ovs_version: &amp;#34;2.15.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s not forget to mark the interface &amp;ldquo;up&amp;rdquo;:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip link set br0 up
&lt;/code>&lt;/pre>&lt;h3 id="create-the-vxlan-tunnels">Create the VXLAN tunnels&lt;/h3>
&lt;p>Up until this point we&amp;rsquo;ve been running identical commands on both
&lt;code>node0&lt;/code> and &lt;code>node1&lt;/code>. In order to create our VXLAN tunnels, we need to
provide a remote endpoint for the VXLAN connection, which is going to
be &amp;ldquo;the other host&amp;rdquo;. On &lt;code>node0&lt;/code>, we run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br0 vx_node1 -- set interface vx_node1 \
type=vxlan options:remote_ip=192.168.122.174
&lt;/code>&lt;/pre>&lt;p>This creates a VXLAN interface named &lt;code>vx_node1&lt;/code> (named that way
because the remote endpoint is &lt;code>node1&lt;/code>). The OVS configuration now
looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>cc1e7217-e393-4e21-97c1-92324d47946d
Bridge br0
Port vx_node1
Interface vx_node1
type: vxlan
options: {remote_ip=&amp;#34;192.168.122.174&amp;#34;}
Port br0
Interface br0
type: internal
ovs_version: &amp;#34;2.15.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>On &lt;code>node1&lt;/code> we will run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br0 vx_node0 -- set interface vx_node0 \
type=vxlan options:remote_ip=192.168.122.107
&lt;/code>&lt;/pre>&lt;p>Which results in:&lt;/p>
&lt;pre tabindex="0">&lt;code>58451994-e0d1-4bf1-8f91-7253ddf4c016
Bridge br0
Port br0
Interface br0
type: internal
Port vx_node0
Interface vx_node0
type: vxlan
options: {remote_ip=&amp;#34;192.168.122.107&amp;#34;}
ovs_version: &amp;#34;2.15.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>At this point, we have a functional overlay network: anything attached
to &lt;code>br0&lt;/code> on either system will appear to share the same layer 2
network. Let&amp;rsquo;s take advantage of this to connect a pair of virtual
machines.&lt;/p>
&lt;h2 id="create-virtual-machines">Create virtual machines&lt;/h2>
&lt;h3 id="download-a-base-image">Download a base image&lt;/h3>
&lt;p>We&amp;rsquo;ll need a base image for our virtual machines. I&amp;rsquo;m going to use the
CentOS 8 Stream image, which we can download to our storage directory
like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -L -o /var/lib/libvirt/images/centos-8-stream.qcow2 \
https://cloud.centos.org/centos/8-stream/x86_64/images/CentOS-Stream-GenericCloud-8-20210210.0.x86_64.qcow2
&lt;/code>&lt;/pre>&lt;p>We need to make sure &lt;code>libvirt&lt;/code> is aware of the new image:&lt;/p>
&lt;pre tabindex="0">&lt;code>virsh pool-refresh default
&lt;/code>&lt;/pre>&lt;p>Lastly, we&amp;rsquo;ll want to set a root password on the image so that we can
log in to our virtual machines:&lt;/p>
&lt;pre tabindex="0">&lt;code>virt-customize -a /var/lib/libvirt/images/centos-8-stream.qcow2 \
--root-password password:secret
&lt;/code>&lt;/pre>&lt;h3 id="create-the-virtual-machine">Create the virtual machine&lt;/h3>
&lt;p>We&amp;rsquo;re going to create a pair of virtual machines (one on each host).
We&amp;rsquo;ll be creating each vm with two network interfaces:&lt;/p>
&lt;ul>
&lt;li>One will be attached to the libvirt &lt;code>default&lt;/code> network; this will
allow us to &lt;code>ssh&lt;/code> into the vm in order to configure things.&lt;/li>
&lt;li>The second will be attached to the OVS bridge&lt;/li>
&lt;/ul>
&lt;p>To create a virtual machine on &lt;code>node0&lt;/code> named &lt;code>vm0.0&lt;/code>, run the
following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>virt-install \
-r 3000 \
--network network=default \
--network bridge=br0,virtualport.type=openvswitch \
--os-variant centos8 \
--disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
--import \
--noautoconsole \
-n vm0.0
&lt;/code>&lt;/pre>&lt;p>The most interesting option in the above command line is probably the
one used to create the virtual disk:&lt;/p>
&lt;pre tabindex="0">&lt;code>--disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
&lt;/code>&lt;/pre>&lt;p>This creates a 10GB &amp;ldquo;&lt;a href="https://en.wikipedia.org/wiki/Copy-on-write">copy-on-write&lt;/a>&amp;rdquo; disk that uses
&lt;code>centos-8-stream.qcow2&lt;/code> as a backing store. That means that reads will
generally come from the &lt;code>centos-8-stream.qcow2&lt;/code> image, but writes will
be stored in the new image. This makes it easy for us to quickly
create multiple virtual machines from the same base image.&lt;/p>
&lt;p>On &lt;code>node1&lt;/code> we would run a similar command, although here we&amp;rsquo;re naming
the virtual machine &lt;code>vm1.0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>virt-install \
-r 3000 \
--network network=default \
--network bridge=br0,virtualport.type=openvswitch \
--os-variant centos8 \
--disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
--import \
--noautoconsole \
-n vm1.0
&lt;/code>&lt;/pre>&lt;h3 id="configure-networking-for-vm00">Configure networking for vm0.0&lt;/h3>
&lt;p>On &lt;code>node0&lt;/code>, get the address of the new virtual machine on the default
network using the &lt;code>virsh domifaddr&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node0 ~]# virsh domifaddr vm0.0
Name MAC address Protocol Address
-------------------------------------------------------------------------------
vnet2 52:54:00:21:6e:4f ipv4 192.168.124.83/24
&lt;/code>&lt;/pre>&lt;p>Connect to the vm using &lt;code>ssh&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node0 ~]# ssh 192.168.124.83
root@192.168.124.83&amp;#39;s password:
Activate the web console with: systemctl enable --now cockpit.socket
Last login: Sat Apr 17 14:08:17 2021 from 192.168.124.1
[root@localhost ~]#
&lt;/code>&lt;/pre>&lt;p>(Recall that the &lt;code>root&lt;/code> password is &lt;code>secret&lt;/code>.)&lt;/p>
&lt;p>Configure interface &lt;code>eth1&lt;/code> with an address. For this post, we&amp;rsquo;ll use
the &lt;code>10.0.0.0/24&lt;/code> range for our overlay network. To assign this vm the
address &lt;code>10.0.0.10&lt;/code>, we can run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip addr add 10.0.0.10/24 dev eth1
ip link set eth1 up
&lt;/code>&lt;/pre>&lt;h3 id="configure-networking-for-vm10">Configure networking for vm1.0&lt;/h3>
&lt;p>We need to repeat the process for &lt;code>vm1.0&lt;/code> on &lt;code>node1&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node1 ~]# virsh domifaddr vm1.0
Name MAC address Protocol Address
-------------------------------------------------------------------------------
vnet0 52:54:00:e9:6e:43 ipv4 192.168.124.69/24
&lt;/code>&lt;/pre>&lt;p>Connect to the vm using &lt;code>ssh&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node0 ~]# ssh 192.168.124.69
root@192.168.124.69&amp;#39;s password:
Activate the web console with: systemctl enable --now cockpit.socket
Last login: Sat Apr 17 14:08:17 2021 from 192.168.124.1
[root@localhost ~]#
&lt;/code>&lt;/pre>&lt;p>We&amp;rsquo;ll use address 10.0.0.11 for this system:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip addr add 10.0.0.11/24 dev eth1
ip link set eth1 up
&lt;/code>&lt;/pre>&lt;h3 id="verify-connectivity">Verify connectivity&lt;/h3>
&lt;p>At this point, our setup is complete. On &lt;code>vm0.0&lt;/code>, we can connect to
&lt;code>vm1.1&lt;/code> over the overlay network. For example, we can ping the remote
host:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@localhost ~]# ping -c2 10.0.0.11
PING 10.0.0.11 (10.0.0.11) 56(84) bytes of data.
64 bytes from 10.0.0.11: icmp_seq=1 ttl=64 time=1.79 ms
64 bytes from 10.0.0.11: icmp_seq=2 ttl=64 time=0.719 ms
--- 10.0.0.11 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 0.719/1.252/1.785/0.533 ms
&lt;/code>&lt;/pre>&lt;p>Or connect to it using &lt;code>ssh&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@localhost ~]# ssh 10.0.0.11 uptime
root@10.0.0.11&amp;#39;s password:
14:21:33 up 1:18, 1 user, load average: 0.00, 0.00, 0.00
&lt;/code>&lt;/pre>&lt;p>Using &lt;code>tcpdump&lt;/code>, we can verify that these connections are going over
the overlay network. Let&amp;rsquo;s watch for VXLAN traffic on &lt;code>node1&lt;/code> by
running the following command (VXLAN is a UDP protocol running on port
4789)&lt;/p>
&lt;pre tabindex="0">&lt;code>tcpdump -i eth0 -n port 4789
&lt;/code>&lt;/pre>&lt;p>When we run &lt;code>ping -c2 10.0.0.11&lt;/code> on &lt;code>vm0.0&lt;/code>, we see the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>14:23:50.312574 IP 192.168.122.107.52595 &amp;gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &amp;gt; 10.0.0.11: ICMP echo request, id 4915, seq 1, length 64
14:23:50.314896 IP 192.168.122.174.59510 &amp;gt; 192.168.122.107.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.11 &amp;gt; 10.0.0.10: ICMP echo reply, id 4915, seq 1, length 64
14:23:51.314080 IP 192.168.122.107.52595 &amp;gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &amp;gt; 10.0.0.11: ICMP echo request, id 4915, seq 2, length 64
14:23:51.314259 IP 192.168.122.174.59510 &amp;gt; 192.168.122.107.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.11 &amp;gt; 10.0.0.10: ICMP echo reply, id 4915, seq 2, length 64
&lt;/code>&lt;/pre>&lt;p>In the output above, we see that each packet in the transaction
results in two lines of output from &lt;code>tcpdump&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>14:23:50.312574 IP 192.168.122.107.52595 &amp;gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &amp;gt; 10.0.0.11: ICMP echo request, id 4915, seq 1, length 64
&lt;/code>&lt;/pre>&lt;p>The first line shows the contents of the VXLAN packet, while the
second lines shows the data that was encapsulated in the VXLAN packet.&lt;/p>
&lt;h2 id="thats-all-folks">That&amp;rsquo;s all folks&lt;/h2>
&lt;p>We&amp;rsquo;ve achieved our goal: we have two virtual machines on two different
hosts communicating over a VXLAN overlay network. If you were to do
this &amp;ldquo;for real&amp;rdquo;, you would probably want to make a number of changes:
for example, the network configuration we&amp;rsquo;ve applied in many cases
will not persist across a reboot; handling persistent network
configuration is still very distribution dependent, so I&amp;rsquo;ve left it
out of this post.&lt;/p></content></item><item><title>Getting started with KSOPS</title><link>https://blog.oddbit.com/post/2021-03-09-getting-started-with-ksops/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-03-09-getting-started-with-ksops/</guid><description>Kustomize is a tool for assembling Kubernetes manifests from a collection of files. We&amp;rsquo;re making extensive use of Kustomize in the operate-first project. In order to keep secrets stored in our configuration repositories, we&amp;rsquo;re using the KSOPS plugin, which enables Kustomize to use sops to encrypt/files using GPG.
In this post, I&amp;rsquo;d like to walk through the steps necessary to get everything up and running.
Set up GPG We encrypt files using GPG, so the first step is making sure that you have a GPG keypair and that your public key is published where other people can find it.</description><content>&lt;p>&lt;a href="https://kustomize.io/">Kustomize&lt;/a> is a tool for assembling Kubernetes manifests from a
collection of files. We&amp;rsquo;re making extensive use of Kustomize in the
&lt;a href="https://www.operate-first.cloud/">operate-first&lt;/a> project. In order to keep secrets stored in our
configuration repositories, we&amp;rsquo;re using the &lt;a href="https://github.com/viaduct-ai/kustomize-sops">KSOPS&lt;/a> plugin, which
enables Kustomize to use &lt;a href="https://github.com/mozilla/sops">sops&lt;/a> to encrypt/files using GPG.&lt;/p>
&lt;p>In this post, I&amp;rsquo;d like to walk through the steps necessary to get
everything up and running.&lt;/p>
&lt;h2 id="set-up-gpg">Set up GPG&lt;/h2>
&lt;p>We encrypt files using GPG, so the first step is making sure that you
have a GPG keypair and that your public key is published where other
people can find it.&lt;/p>
&lt;h3 id="install-gpg">Install GPG&lt;/h3>
&lt;p>GPG will be pre-installed on most Linux distributions. You can check
if it&amp;rsquo;s installed by running e.g. &lt;code>gpg --version&lt;/code>. If it&amp;rsquo;s not
installed, you will need to figure out how to install it for your
operating system.&lt;/p>
&lt;h3 id="create-a-key">Create a key&lt;/h3>
&lt;p>Run the following command to create a new GPG keypair:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg --full-generate-key
&lt;/code>&lt;/pre>&lt;p>This will step you through a series of prompts. First, select a key
type. You can just press &lt;code>&amp;lt;RETURN&amp;gt;&lt;/code> for the default:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg (GnuPG) 2.2.25; Copyright (C) 2020 Free Software Foundation, Inc.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Please select what kind of key you want:
(1) RSA and RSA (default)
(2) DSA and Elgamal
(3) DSA (sign only)
(4) RSA (sign only)
(14) Existing key from card
Your selection?
&lt;/code>&lt;/pre>&lt;p>Next, select a key size. The default is fine:&lt;/p>
&lt;pre tabindex="0">&lt;code>RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (3072)
Requested keysize is 3072 bits
&lt;/code>&lt;/pre>&lt;p>You will next need to select an expiration date for your key. The
default is &amp;ldquo;key does not expire&amp;rdquo;, which is a fine choice for our
purposes. If you&amp;rsquo;re interested in understanding this value in more
detail, the following articles are worth reading:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://security.stackexchange.com/questions/14718/does-openpgp-key-expiration-add-to-security/79386#79386">Does OpenPGP key expiration add to security?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.g-loaded.eu/2010/11/01/change-expiration-date-gpg-key/">How to change the expiration date of a GPG key&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Setting an expiration date will require that you periodically update
the expiration date (or generate a new key).&lt;/p>
&lt;pre tabindex="0">&lt;code>Please specify how long the key should be valid.
0 = key does not expire
&amp;lt;n&amp;gt; = key expires in n days
&amp;lt;n&amp;gt;w = key expires in n weeks
&amp;lt;n&amp;gt;m = key expires in n months
&amp;lt;n&amp;gt;y = key expires in n years
Key is valid for? (0)
Key does not expire at all
Is this correct? (y/N) y
&lt;/code>&lt;/pre>&lt;p>Now you will need to enter your identity, which consists of your name,
your email address, and a comment (which is generally left blank).
Note that you&amp;rsquo;ll need to enter &lt;code>o&lt;/code> for &lt;code>okay&lt;/code> to continue from this
prompt.&lt;/p>
&lt;pre tabindex="0">&lt;code>GnuPG needs to construct a user ID to identify your key.
Real name: Your Name
Email address: you@example.com
Comment:
You selected this USER-ID:
&amp;#34;Your Name &amp;lt;you@example.com&amp;gt;&amp;#34;
Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? o
&lt;/code>&lt;/pre>&lt;p>Lastly, you need to enter a password. In most environments, GPG will
open a new window asking you for a passphrase. After you&amp;rsquo;ve entered and
confirmed the passphrase, you should see your key information on the
console:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg: key 02E34E3304C8ADEB marked as ultimately trusted
gpg: revocation certificate stored as &amp;#39;/home/lars/tmp/gpgtmp/openpgp-revocs.d/9A4EB5B1F34B3041572937C002E34E3304C8ADEB.rev&amp;#39;
public and secret key created and signed.
pub rsa3072 2021-03-11 [SC]
9A4EB5B1F34B3041572937C002E34E3304C8ADEB
uid Your Name &amp;lt;you@example.com&amp;gt;
sub rsa3072 2021-03-11 [E]
&lt;/code>&lt;/pre>&lt;h3 id="publish-your-key">Publish your key&lt;/h3>
&lt;p>You need to publish your GPG key so that others can find it. You&amp;rsquo;ll
need your key id, which you can get by running &lt;code>gpg -k --fingerprint&lt;/code>
like this (using your email address rather than mine):&lt;/p>
&lt;pre tabindex="0">&lt;code>$ gpg -k --fingerprint lars@oddbit.com
&lt;/code>&lt;/pre>&lt;p>The output will look like the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>pub rsa2048/0x362D63A80853D4CF 2013-06-21 [SC]
Key fingerprint = 3E70 A502 BB52 55B6 BB8E 86BE 362D 63A8 0853 D4CF
uid [ultimate] Lars Kellogg-Stedman &amp;lt;lars@oddbit.com&amp;gt;
uid [ultimate] keybase.io/larsks &amp;lt;larsks@keybase.io&amp;gt;
sub rsa2048/0x042DF6CF74E4B84C 2013-06-21 [S] [expires: 2023-07-01]
sub rsa2048/0x426D9382DFD6A7A9 2013-06-21 [E]
sub rsa2048/0xEE1A8B9F9369CC85 2013-06-21 [A]
&lt;/code>&lt;/pre>&lt;p>Look for the &lt;code>Key fingerprint&lt;/code> line, you want the value after the &lt;code>=&lt;/code>.
Use this to publish your key to &lt;code>keys.openpgp.org&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg --keyserver keys.opengpg.org \
--send-keys &amp;#39;3E70 A502 BB52 55B6 BB8E 86BE 362D 63A8 0853 D4CF&amp;#39;
&lt;/code>&lt;/pre>&lt;p>You will shortly receive an email to the address in your key asking
you to approve it. Once you have approved the key, it will be
published on &lt;a href="https://keys.openpgp.org">https://keys.openpgp.org&lt;/a> and people will be able to look
it up by address or key id. For example, you can find my public key
at &lt;a href="https://keys.openpgp.org/vks/v1/by-fingerprint/3E70A502BB5255B6BB8E86BE362D63A80853D4CF">https://keys.openpgp.org/vks/v1/by-fingerprint/3E70A502BB5255B6BB8E86BE362D63A80853D4CF&lt;/a>.&lt;/p>
&lt;h2 id="installing-the-tools">Installing the Tools&lt;/h2>
&lt;p>In this section, we&amp;rsquo;ll get all the necessary tools installed on your
system in order to interact with a repository using Kustomize and
KSOPS.&lt;/p>
&lt;h3 id="install-kustomize">Install Kustomize&lt;/h3>
&lt;p>Pre-compiled binaries of Kustomize are published &lt;a href="https://github.com/kubernetes-sigs/kustomize/releases">on
GitHub&lt;/a>. To install the command, navigate to the current
release (&lt;a href="https://github.com/kubernetes-sigs/kustomize/releases/tag/kustomize%2Fv4.0.5">v4.0.5&lt;/a> as of this writing) and download the appropriate
tarball for your system. E.g, for an x86-64 Linux environment, you
would grab &lt;a href="https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv4.0.5/kustomize_v4.0.5_linux_amd64.tar.gz">kustomize_v4.0.5_linux_amd64.tar.gz&lt;/a>.&lt;/p>
&lt;p>The tarball contains a single file. You need to extract this file and
place it somwhere in your &lt;code>$PATH&lt;/code>. For example, if you use your
&lt;code>$HOME/bin&lt;/code> directory, you could run:&lt;/p>
&lt;pre tabindex="0">&lt;code>tar -C ~/bin -xf kustomize_v4.0.5_linux_amd64.tar.gz
&lt;/code>&lt;/pre>&lt;p>Or to install into &lt;code>/usr/local/bin&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>sudo tar -C /usr/local/bin -xf kustomize_v4.0.5_linux_amd64.tar.gz
&lt;/code>&lt;/pre>&lt;p>Run &lt;code>kustomize&lt;/code> with no arguments to verify the command has been
installed correctly.&lt;/p>
&lt;h3 id="install-sops">Install sops&lt;/h3>
&lt;p>The KSOPS plugin relies on the &lt;a href="https://github.com/mozilla/sops">sops&lt;/a> command, so we need to install
that first. Binary releases are published on GitHub, and the current
release is &lt;a href="https://github.com/mozilla/sops/releases/tag/v3.6.1">v3.6.1&lt;/a>.&lt;/p>
&lt;p>Instead of a tarball, the project publishes the raw binary as well as
packages for a couple of different Linux distributions. For
consistency with the rest of this post we&amp;rsquo;re going to grab the &lt;a href="https://github.com/mozilla/sops/releases/download/v3.6.1/sops-v3.6.1.linux">raw
binary&lt;/a>. We can install that into &lt;code>$HOME/bin&lt;/code> like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -o ~/bin/sops https://github.com/mozilla/sops/releases/download/v3.6.1/sops-v3.6.1.linux
chmod 755 ~/bin/sops
&lt;/code>&lt;/pre>&lt;h3 id="install-ksops">Install KSOPS&lt;/h3>
&lt;p>KSOPS is a Kustomize plugin. The &lt;code>kustomize&lt;/code> command looks for plugins
in subdirectories of &lt;code>$HOME/.config/kustomize/plugin&lt;/code>. Directories are
named after an API and plugin name. In the case of KSOPS, &lt;code>kustomize&lt;/code>
will be looking for a plugin named &lt;code>ksops&lt;/code> in the
&lt;code>$HOME/.config/kustomize/plugin/viaduct.ai/v1/ksops/&lt;/code> directory.&lt;/p>
&lt;p>The current release of KSOPS is &lt;a href="https://github.com/viaduct-ai/kustomize-sops/releases/tag/v2.4.0">v2.4.0&lt;/a>, which is published as a
tarball. We&amp;rsquo;ll start by downloading
&lt;a href="https://github.com/viaduct-ai/kustomize-sops/releases/download/v2.4.0/ksops_2.4.0_Linux_x86_64.tar.gz">ksops_2.4.0_Linux_x86_64.tar.gz&lt;/a>, which contains the following
files:&lt;/p>
&lt;pre tabindex="0">&lt;code>LICENSE
README.md
ksops
&lt;/code>&lt;/pre>&lt;p>To extract the &lt;code>ksops&lt;/code> command to &lt;code>$HOME/bin&lt;/code>, you can run:&lt;/p>
&lt;pre tabindex="0">&lt;code>mkdir -p ~/.config/kustomize/plugin/viaduct.ai/v1/ksops/
tar -C ~/.config/kustomize/plugin/viaduct.ai/v1/ksops -xf ksops_2.4.0_Linux_x86_64.tar.gz ksops
&lt;/code>&lt;/pre>&lt;h2 id="test-it-out">Test it out&lt;/h2>
&lt;p>Let&amp;rsquo;s create a simple Kustomize project to make sure everything is
installed and functioning.&lt;/p>
&lt;p>Start by creating a new directory and changing into it:&lt;/p>
&lt;pre tabindex="0">&lt;code>mkdir kustomize-test
cd kustomize-test
&lt;/code>&lt;/pre>&lt;p>Create a &lt;code>kustomization.yaml&lt;/code> file that looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>generators:
- secret-generator.yaml
&lt;/code>&lt;/pre>&lt;p>Put the following content in &lt;code>secret-generator.yaml&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
apiVersion: viaduct.ai/v1
kind: ksops
metadata:
name: secret-generator
files:
- example-secret.enc.yaml
&lt;/code>&lt;/pre>&lt;p>This instructs Kustomize to use the KSOPS plugin to generate content
from the file &lt;code>example-secret.enc.yaml&lt;/code>.&lt;/p>
&lt;p>Configure &lt;code>sops&lt;/code> to use your GPG key by default by creating a
&lt;code>.sops.yaml&lt;/code> (note the leading dot) similar to the following (you&amp;rsquo;ll
need to put your GPG key fingerprint in the right place):&lt;/p>
&lt;pre tabindex="0">&lt;code>creation_rules:
- encrypted_regex: &amp;#34;^(users|data|stringData)$&amp;#34;
pgp: &amp;lt;YOUR KEY FINGERPRINT HERE&amp;gt;
&lt;/code>&lt;/pre>&lt;p>The &lt;code>encrypted_regex&lt;/code> line tells &lt;code>sops&lt;/code> which attributes in your YAML
files should be encrypted. The &lt;code>pgp&lt;/code> line is a (comma delimited) list
of keys to which data will be encrypted.&lt;/p>
&lt;p>Now, edit the file &lt;code>example-secret.enc.yaml&lt;/code> using the &lt;code>sops&lt;/code> command.
Run:&lt;/p>
&lt;pre tabindex="0">&lt;code>sops example-secret.enc.yaml
&lt;/code>&lt;/pre>&lt;p>This will open up an editor with some default content. Replace the
content with the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Secret
metadata:
name: example-secret
type: Opaque
stringData:
message: this is a test
&lt;/code>&lt;/pre>&lt;p>Save the file and exit your editor. Now examine the file; you will see
that it contains a mix of encrypted and unencrypted content. When
encrypted with my private key, it looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ cat example-secret.enc.yaml
{
&amp;#34;data&amp;#34;: &amp;#34;ENC[AES256_GCM,data:wZvEylsvhfU29nfFW1PbGqyk82x8+Vm/3p2Y89B8a1A26wa5iUTr1hEjDYrQIGQq4rvDyK4Bevxb/PrTzdOoTrYIhaerEWk13g9UrteLoaW0FpfGv9bqk0c12OwTrzS+5qCW2mIlfzQpMH5+7xxeruUXO7w=,iv:H4i1/Znp6WXrMmmP9YVkz+xKOX0XBH7kPFaa36DtTxs=,tag:bZhSzkM74wqayo7McV/VNQ==,type:str]&amp;#34;,
&amp;#34;sops&amp;#34;: {
&amp;#34;kms&amp;#34;: null,
&amp;#34;gcp_kms&amp;#34;: null,
&amp;#34;azure_kv&amp;#34;: null,
&amp;#34;hc_vault&amp;#34;: null,
&amp;#34;lastmodified&amp;#34;: &amp;#34;2021-03-12T03:11:46Z&amp;#34;,
&amp;#34;mac&amp;#34;: &amp;#34;ENC[AES256_GCM,data:2NrsF6iLA3zHeupD314Clg/WyBA8mwCn5SHHI5P9tsOt6472Tevdamv6ARD+xqfrSVWz+Wy4PtWPoeqZrFJwnL/qCR4sdjt/CRzLmcBistUeAnlqoWIwbtMxBqaFg9GxTd7f5q0iHr9QNWGSVV3JMeZZ1jeWyeQohAPpPufsuPQ=,iv:FJvZz8SV+xsy4MC1W9z1Vn0s4Dzw9Gya4v+rSpwZLrw=,tag:pfW8r5856c7qetCNgXMyeA==,type:str]&amp;#34;,
&amp;#34;pgp&amp;#34;: [
{
&amp;#34;created_at&amp;#34;: &amp;#34;2021-03-12T03:11:45Z&amp;#34;,
&amp;#34;enc&amp;#34;: &amp;#34;-----BEGIN PGP MESSAGE-----\n\nwcBMA0Jtk4Lf1qepAQgAGKwk6zDMPUYbUscky07v/7r3fsws3pTVRMgpEdhTra6x\nDxiMaLnjTKJi9fsB7sQuh/PTGWhXGuHtHg0YBtxRkuZY0Kl6xKXTXGBIBhI/Ahgw\n4BSz/rE7gbz1h6X4EFml3e1NeUTvGntA3HjY0o42YN9uwsi9wvMbiR4OLQfwY1gG\np9/v57KJx5ipEKSgt+81KwzOhuW79ttXd2Tvi9rjuAfvmLBU9q/YKMT8miuNhjet\nktNwXNJNpglHJta431YUhPZ6q41LpgvQPMX4bIZm7i7NuR470njYLQPe7xiGqqeT\nBcuF7KkNXGcDu9/RnIyxK4W5Bo9NEa06TqUGTHLEENLgAeSzHdQdUwx/pLLD6OPa\nv/U34YJU4JngqOGqTuDu4orgwLDg++XysBwVsmFp1t/nHvTkwj57wAuxJ4/It/9l\narvRHlCx6uA05IXukmCTvYMPRV3kY/81B+biHcka7uFUOQA=\n=x+7S\n-----END PGP MESSAGE-----&amp;#34;,
&amp;#34;fp&amp;#34;: &amp;#34;3E70A502BB5255B6BB8E86BE362D63A80853D4CF&amp;#34;
}
],
&amp;#34;encrypted_regex&amp;#34;: &amp;#34;^(users|data|stringData)$&amp;#34;,
&amp;#34;version&amp;#34;: &amp;#34;3.6.1&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;p>Finally, attempt to render the project with Kustomize by running:&lt;/p>
&lt;pre tabindex="0">&lt;code>kustomize build --enable-alpha-plugins
&lt;/code>&lt;/pre>&lt;p>This should produce on stdout the unencrypted content of your secret:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Secret
metadata:
name: example-secret
type: Opaque
stringData:
message: this is a test
&lt;/code>&lt;/pre></content></item><item><title>Tools for writing about Git</title><link>https://blog.oddbit.com/post/2021-02-27-index.in/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-02-27-index.in/</guid><description>I sometimes find myself writing articles or documentation about git, so I put together a couple of terrible hacks for generating reproducible histories and pretty graphs of those histories.
git synth The git synth command reads a YAML description of a repository and executes the necessary commands to reproduce that history. It allows you set the name and email address of the author and committer as well as static date, so you every time you generate the repository you can identical commit ids.</description><content>&lt;p>I sometimes find myself writing articles or documentation about
&lt;a href="https://git-scm.org">git&lt;/a>, so I put together a couple of terrible hacks for generating
reproducible histories and pretty graphs of those histories.&lt;/p>
&lt;h2 id="git-synth">git synth&lt;/h2>
&lt;p>The &lt;a href="https://github.com/larsks/git-snippets/blob/master/git-synth">&lt;code>git synth&lt;/code>&lt;/a> command reads a &lt;a href="https://yaml.org/">YAML&lt;/a> description of a
repository and executes the necessary commands to reproduce that
history. It allows you set the name and email address of the author
and committer as well as static date, so you every time you generate
the repository you can identical commit ids.&lt;/p>
&lt;h2 id="git-dot">git dot&lt;/h2>
&lt;p>The &lt;a href="https://github.com/larsks/git-snippets/blob/master/git-dot">&lt;code>git dot&lt;/code>&lt;/a> command generates a representation of a repository
history in the &lt;a href="https://en.wikipedia.org/wiki/DOT_(graph_description_language)">dot&lt;/a> language, and uses &lt;a href="https://graphviz.org/">Graphviz&lt;/a> to render those
into diagrams.&lt;/p>
&lt;h2 id="putting-it-together">Putting it together&lt;/h2>
&lt;p>For example, the following history specification:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;!-- include examplerepo.yml --&amp;gt;
&lt;/code>&lt;/pre>&lt;p>When applied with &lt;code>git synth&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ git synth -r examplerepo examplerepo.yml
&lt;/code>&lt;/pre>&lt;p>Will generate the following repository:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ git -C examplerepo log --graph --all --decorate --oneline
* 28f7b38 (HEAD -&amp;gt; master) H
| * 93e1d18 (topic2) G
| * 3ef811d F
| * 973437c (topic1) E
| * 2c0bd1c D
|/
* cabdedf C
* a5cbd99 B
* d98f949 A
&lt;/code>&lt;/pre>&lt;p>We can run this &lt;code>git dot&lt;/code> command line:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ git -C examplerepo dot -m -g branch --rankdir=RL
&lt;/code>&lt;/pre>&lt;p>To produce the following &lt;code>dot&lt;/code> description of the history:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;!-- include examplerepo.dot --&amp;gt;
&lt;/code>&lt;/pre>&lt;p>Running that through the &lt;code>dot&lt;/code> utility (&lt;code>dot -Tsvg -o repo.svg repo.dot&lt;/code>) results in the following diagram:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-graphviz" data-lang="graphviz">&amp;lt;!-- include examplerepo.dot --&amp;gt;
&lt;/code>&lt;/pre>&lt;h2 id="where-are-these-wonders">Where are these wonders?&lt;/h2>
&lt;p>Both tools live in my &lt;a href="https://github.com/larsks/git-snippets">git-snippets&lt;/a> repository, which is a motley
collection of shells scripts, python programs, and other utilities for
interacting with &lt;code>git&lt;/code>.&lt;/p>
&lt;p>It&amp;rsquo;s all undocumented and uninstallable, but if there&amp;rsquo;s interest in
either of these tools I can probably find the time to polish them up a
bit.&lt;/p></content></item><item><title>File reorganization</title><link>https://blog.oddbit.com/post/2021-02-24-file-reorganization/</link><pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-02-24-file-reorganization/</guid><description>This is just a note that I&amp;rsquo;ve substantially changed how the post sources are organized. I&amp;rsquo;ve tried to ensure that I preserve all the existing links, but if you spot something missing please feel free to leave a comment on this post.</description><content>&lt;p>This is just a note that I&amp;rsquo;ve substantially changed how the post
sources are organized. I&amp;rsquo;ve tried to ensure that I preserve all the
existing links, but if you spot something missing please feel free to
leave a comment on this post.&lt;/p></content></item><item><title>Editing a commit message without git rebase</title><link>https://blog.oddbit.com/post/2021-02-18-editing-a-commit-message-witho/</link><pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-02-18-editing-a-commit-message-witho/</guid><description>While working on a pull request I will make liberal use of git rebase to clean up a series of commits: squashing typos, re-ordering changes for logical clarity, and so forth. But there are some times when all I want to do is change a commit message somewhere down the stack, and I was wondering if I had any options for doing that without reaching for git rebase.
It turns out the answer is &amp;ldquo;yes&amp;rdquo;, as long as you have a linear history.</description><content>&lt;p>While working on a pull request I will make liberal use of &lt;a href="https://git-scm.com/docs/git-rebase">git
rebase&lt;/a> to clean up a series of commits: squashing typos,
re-ordering changes for logical clarity, and so forth. But there are
some times when all I want to do is change a commit message somewhere
down the stack, and I was wondering if I had any options for doing
that without reaching for &lt;code>git rebase&lt;/code>.&lt;/p>
&lt;p>It turns out the answer is &amp;ldquo;yes&amp;rdquo;, as long as you have a linear
history.&lt;/p>
&lt;p>Let&amp;rsquo;s assume we have a git history that looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐
│ 4be811 │ ◀── │ 519636 │ ◀── │ 38f6fe │ ◀── │ 2951ec │ ◀╴╴ │ master │
└────────┘ └────────┘ └────────┘ └────────┘ └────────┘
&lt;/code>&lt;/pre>&lt;p>The corresponding &lt;code>git log&lt;/code> looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>commit 2951ec3f54205580979d63614ef2751b61102c5d
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
Add detailed, high quality documentation
commit 38f6fe61ffd444f601ac01ecafcd524487c83394
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
Fixed bug that would erroneously call rm -rf
commit 51963667037ceb79aff8c772a009a5fbe4b8d7d9
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
A very interesting change
commit 4be8115640821df1565c421d8ed848bad34666e5
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
The beginning of time
&lt;/code>&lt;/pre>&lt;h2 id="mucking-about-with-objects">Mucking about with objects&lt;/h2>
&lt;p>We would like to modify the message on commit &lt;code>519636&lt;/code>.&lt;/p>
&lt;p>We start by extracting the &lt;code>commit&lt;/code> object for that commit using &lt;code>git cat-file&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ git cat-file -p 519636
tree 4b825dc642cb6eb9a060e54bf8d69288fbee4904
parent 4be8115640821df1565c421d8ed848bad34666e5
author Alice User &amp;lt;alice@example.com&amp;gt; 978325200 -0500
committer Alice User &amp;lt;alice@example.com&amp;gt; 978325200 -0500
A very interesting change
&lt;/code>&lt;/pre>&lt;p>We want to produce a commit object that is identical except for an
updated commit message. That sounds like a job for &lt;code>sed&lt;/code>! We can strip
the existing message out like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>git cat-file -p 519636 | sed &amp;#39;/^$/q&amp;#39;
&lt;/code>&lt;/pre>&lt;p>And we can append a new commit message with the power of &lt;code>cat&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>git cat-file -p 519636 | sed &amp;#39;/^$/q&amp;#39;; cat &amp;lt;&amp;lt;EOF
A very interesting change
Completely refactor the widget implementation to prevent
a tear in the time/space continuum when given invalid
input.
EOF
&lt;/code>&lt;/pre>&lt;p>This will give us:&lt;/p>
&lt;pre tabindex="0">&lt;code>tree 4b825dc642cb6eb9a060e54bf8d69288fbee4904
parent 4be8115640821df1565c421d8ed848bad34666e5
author Alice User &amp;lt;alice@example.com&amp;gt; 978325200 -0500
committer Alice User &amp;lt;alice@example.com&amp;gt; 978325200 -0500
A very interesting change
Completely refactor the widget implementation to prevent
a tear in the time/space continuum when given invalid
input.
&lt;/code>&lt;/pre>&lt;p>We need to take this modified commit and store it back into the git
object database. We do that using the &lt;code>git hash-object&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>(git cat-file -p 519636 | sed &amp;#39;/^$/q&amp;#39;; cat &amp;lt;&amp;lt;EOF) | git hash-object -t commit --stdin -w
A very interesting change
Completely refactor the widget implementation to prevent
a tear in the time/space continuum when given invalid
input.
EOF
&lt;/code>&lt;/pre>&lt;p>The &lt;code>-t commit&lt;/code> argument instructs &lt;code>hash-object&lt;/code> to create a new
commit object. The &lt;code>--stdin&lt;/code> argument instructs &lt;code>hash-object&lt;/code> to read
input from &lt;code>stdin&lt;/code>, while the &lt;code>-w&lt;/code> argument instructs &lt;code>hash-object&lt;/code> to
write a new object to the object database, rather than just
calculating the hash and printing it for us.&lt;/p>
&lt;p>This will print the hash of the new object on stdout. We can wrap
everything in a &lt;code>$(...)&lt;/code> expression to capture the output:&lt;/p>
&lt;pre tabindex="0">&lt;code>newref=$(
(git cat-file -p 519636 | sed &amp;#39;/^$/q&amp;#39;; cat &amp;lt;&amp;lt;EOF) | git hash-object -t commit --stdin -w
A very interesting change
Completely refactor the widget implementation to prevent
a tear in the time/space continuum when given invalid
input.
EOF
)
&lt;/code>&lt;/pre>&lt;p>At this point we have successfully created a new commit, but it isn&amp;rsquo;t
reachable from anywhere. If we were to run &lt;code>git log&lt;/code> at this point,
everything would look the same as when we started. We need to walk
back up the tree, starting with the immediate descendant of our target
commit, replacing parent pointers as we go along.&lt;/p>
&lt;p>The first thing we need is a list of revisions from our target commit
up to the current &lt;code>HEAD&lt;/code>. We can get that with &lt;code>git rev-list&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ git rev-list 519636..HEAD
2951ec3f54205580979d63614ef2751b61102c5d
38f6fe61ffd444f601ac01ecafcd524487c83394
&lt;/code>&lt;/pre>&lt;p>We&amp;rsquo;ll process these in reverse order, so first we modify &lt;code>38f6fe&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>oldref=51963667037ceb79aff8c772a009a5fbe4b8d7d9
newref=$(git cat-file -p 38f6fe61ffd444f601ac01ecafcd524487c83394 |
sed &amp;#34;s/parent $oldref/parent $newref/&amp;#34; |
git hash-object -t commit --stdin -w)
&lt;/code>&lt;/pre>&lt;p>And then repeat that for the next commit up the tree:&lt;/p>
&lt;pre tabindex="0">&lt;code>oldref=38f6fe61ffd444f601ac01ecafcd524487c83394
newref=$(git cat-file -p 2951ec3f54205580979d63614ef2751b61102c5d |
sed &amp;#34;s/parent $oldref/parent $newref/&amp;#34; |
git hash-object -t commit --stdin -w)
&lt;/code>&lt;/pre>&lt;p>We&amp;rsquo;ve now replaced all the descendants of the modified commit&amp;hellip;but
&lt;code>git log&lt;/code> would &lt;em>still&lt;/em> show us the old history. The last thing we
need to do is update the branch point to point at the top of the
modified tree. We do that using the &lt;code>git update-ref&lt;/code> command. Assuming
we&amp;rsquo;re on the &lt;code>master&lt;/code> branch, the command would look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>git update-ref refs/heads/master $newref
&lt;/code>&lt;/pre>&lt;p>And at this point, running &lt;code>git log&lt;/code> show us our modified commit in
all its glory:&lt;/p>
&lt;pre tabindex="0">&lt;code>commit 365bc25ee1fe365d5d63d2248b77196d95d9573a
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
Add detailed, high quality documentation
commit 09d6203a2b64c201dde12af7ef5a349e1ae790d7
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
Fixed bug that would erroneously call rm -rf
commit fb01f35c38691eafbf44e9ee86824b594d036ba4
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
A very interesting change
Completely refactor the widget implementation to prevent
a tear in the time/space continuum when given invalid
input.
commit 4be8115640821df1565c421d8ed848bad34666e5
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
The beginning of time
&lt;/code>&lt;/pre>&lt;p>Giving us a modified history that looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐
│ 4be811 │ ◀── │ fb01f3 │ ◀── │ 09d620 │ ◀── │ 365bc2 │ ◀╴╴ │ master │
└────────┘ └────────┘ └────────┘ └────────┘ └────────┘
&lt;/code>&lt;/pre>&lt;h2 id="automating-the-process">Automating the process&lt;/h2>
&lt;p>Now, that was a lot of manual work. Let&amp;rsquo;s try to automate the process.&lt;/p>
&lt;pre tabindex="0">&lt;code>#!/bin/sh
# get the current branch name
branch=$(git rev-parse --symbolic-full-name HEAD)
# git the full commit id of our target commit (this allows us to
# specify the target as a short commit id, or as something like
# `HEAD~3` or `:/interesting`.
oldref=$(git rev-parse &amp;#34;$1&amp;#34;)
# generate a replacement commit object, reading the new commit message
# from stdin.
newref=$(
(git cat-file -p $oldref | sed &amp;#39;/^$/q&amp;#39;; cat) | tee newref.txt | git hash-object -t commit --stdin -w
)
# iterate over commits between our target commit and HEAD in
# reverse order, replacing parent points with updated commit objects
for rev in $(git rev-list --reverse ${oldref}..HEAD); do
newref=$(git cat-file -p $rev |
sed &amp;#34;s/parent $oldref/parent $newref/&amp;#34; |
git hash-object -t commit --stdin -w)
oldref=$rev
done
# update the branch pointer to the head of the modified tree
git update-ref $branch $newref
&lt;/code>&lt;/pre>&lt;p>If we place the above script in &lt;code>editmsg.sh&lt;/code> and restore our original
revision history, we can run:&lt;/p>
&lt;pre tabindex="0">&lt;code>sh editmsg.sh :/interesting &amp;lt;&amp;lt;EOF
A very interesting change
Completely refactor the widget implementation to prevent
a tear in the time/space continuum when given invalid
input.
EOF
&lt;/code>&lt;/pre>&lt;p>And end up with a new history identical to the one we created
manually:&lt;/p>
&lt;pre tabindex="0">&lt;code>commit 365bc25ee1fe365d5d63d2248b77196d95d9573a
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
Add detailed, high quality documentation
commit 09d6203a2b64c201dde12af7ef5a349e1ae790d7
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
Fixed bug that would erroneously call rm -rf
commit fb01f35c38691eafbf44e9ee86824b594d036ba4
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
A very interesting change
Completely refactor the widget implementation to prevent
a tear in the time/space continuum when given invalid
input.
commit 4be8115640821df1565c421d8ed848bad34666e5
Author: Alice User &amp;lt;alice@example.com&amp;gt;
Date: Mon Jan 1 00:00:00 2001 -0500
The beginning of time
&lt;/code>&lt;/pre>&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>The above script is intentionally simple. If you&amp;rsquo;re interesting in
doing something like this in practice, you should be aware of the
following:&lt;/p>
&lt;ul>
&lt;li>The above process works great with a linear history, but will break
things if the rewriting process crosses a merge commit.&lt;/li>
&lt;li>We&amp;rsquo;re assuming that the given target commit is actually reachable
from the current branch.&lt;/li>
&lt;li>We&amp;rsquo;re assuming that the given target actually exists.&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s possible to check for all of these conditions in our script, but
I&amp;rsquo;m leaving that as an exercise for the reader.&lt;/p></content></item><item><title>Object storage with OpenShift Container Storage</title><link>https://blog.oddbit.com/post/2021-02-10-object-storage-with-openshift/</link><pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-02-10-object-storage-with-openshift/</guid><description>OpenShift Container Storage (OCS) from Red Hat deploys Ceph in your OpenShift cluster (or allows you to integrate with an external Ceph cluster). In addition to the file- and block- based volume services provided by Ceph, OCS includes two S3-api compatible object storage implementations.
The first option is the Ceph Object Gateway (radosgw), Ceph&amp;rsquo;s native object storage interface. The second option called the &amp;ldquo;Multicloud Object Gateway&amp;rdquo;, which is in fact a piece of software named Noobaa, a storage abstraction layer that was acquired by Red Hat in 2018.</description><content>&lt;p>&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage">OpenShift Container Storage&lt;/a> (OCS) from Red Hat deploys Ceph in your
OpenShift cluster (or allows you to integrate with an external Ceph
cluster). In addition to the file- and block- based volume services
provided by Ceph, OCS includes two S3-api compatible object storage
implementations.&lt;/p>
&lt;p>The first option is the &lt;a href="https://docs.ceph.com/en/latest/radosgw/">Ceph Object Gateway&lt;/a> (radosgw),
Ceph&amp;rsquo;s native object storage interface. The second option called the
&amp;ldquo;&lt;a href="https://www.openshift.com/blog/introducing-multi-cloud-object-gateway-for-openshift">Multicloud Object Gateway&lt;/a>&amp;rdquo;, which is in fact a piece of software
named &lt;a href="https://www.noobaa.io/">Noobaa&lt;/a>, a storage abstraction layer that was &lt;a href="https://www.redhat.com/en/blog/faq-red-hat-acquires-noobaa">acquired by
Red Hat&lt;/a> in 2018. In this article I&amp;rsquo;d like to demonstrate how to
take advantage of these storage options.&lt;/p>
&lt;h2 id="what-is-object-storage">What is object storage?&lt;/h2>
&lt;p>The storage we interact with regularly on our local computers is
block storage: data is stored as a collection of blocks on some sort
of storage device. Additional layers &amp;ndash; such as a filesystem driver &amp;ndash;
are responsible for assembling those blocks into something useful.&lt;/p>
&lt;p>Object storage, on the other hand, manages data as objects: a single
unit of data and associated metadata (such as access policies). An
object is identified by some sort of unique id. Object storage
generally provides an API that is largely independent of the physical
storage layer; data may live on a variety of devices attached to a
variety of systems, and you don&amp;rsquo;t need to know any of those details in
order to access the data.&lt;/p>
&lt;p>The most well known example of object storage service Amazon&amp;rsquo;s
&lt;a href="https://aws.amazon.com/s3/">S3&lt;/a> service (&amp;ldquo;Simple Storage Service&amp;rdquo;), first introduced in 2006.
The S3 API has become a de-facto standard for object storage
implementations. The two services we&amp;rsquo;ll be discussing in this article
provide S3-compatible APIs.&lt;/p>
&lt;h2 id="creating-buckets">Creating buckets&lt;/h2>
&lt;p>The fundamental unit of object storage is called a &amp;ldquo;bucket&amp;rdquo;.&lt;/p>
&lt;p>Creating a bucket with OCS works a bit like creating a &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">persistent
volume&lt;/a>, although instead of starting with a &lt;code>PersistentVolumeClaim&lt;/code>
you instead start with an &lt;code>ObjectBucketClaim&lt;/code> (&amp;quot;&lt;code>OBC&lt;/code>&amp;quot;). An &lt;code>OBC&lt;/code>
looks something like this when using RGW:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">objectbucket.io/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ObjectBucketClaim&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">example-rgw&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">generateBucketName&lt;/span>: &lt;span style="color:#ae81ff">example-rgw&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">storageClassName&lt;/span>: &lt;span style="color:#ae81ff">ocs-storagecluster-ceph-rgw&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Or like this when using Noobaa (note the different value for
&lt;code>storageClassName&lt;/code>):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">objectbucket.io/v1alpha1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">ObjectBucketClaim&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">example-noobaa&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">generateBucketName&lt;/span>: &lt;span style="color:#ae81ff">example-noobaa&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">storageClassName&lt;/span>: &lt;span style="color:#ae81ff">openshift-storage.noobaa.io&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With OCS 4.5, your out-of-the-box choices for &lt;code>storageClassName&lt;/code> will be
&lt;code>ocs-storagecluster-ceph-rgw&lt;/code>, if you choose to use Ceph Radosgw, or
&lt;code>openshift-storage.noobaa.io&lt;/code>, if you choose to use the Noobaa S3 endpoint.&lt;/p>
&lt;p>Before we continue, I&amp;rsquo;m going to go ahead and create these resources
in my OpenShift environment. To do so, I&amp;rsquo;m going to use &lt;a href="https://kustomize.io/">Kustomize&lt;/a>
to deploy the resources described in the following &lt;code>kustomization.yml&lt;/code>
file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">namespace&lt;/span>: &lt;span style="color:#ae81ff">oddbit-ocs-example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">resources&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">obc-noobaa.yml&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">obc-rgw.yml&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Running &lt;code>kustomize build | oc apply -f-&lt;/code> from the directory containing
this file populates the specified namespace with the two
&lt;code>ObjectBucketClaims&lt;/code> mentioned above:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ kustomize build | oc apply -f-
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>objectbucketclaim.objectbucket.io/example-noobaa created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>objectbucketclaim.objectbucket.io/example-rgw created
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Verifying that things seem healthy:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ oc get objectbucketclaim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STORAGE-CLASS PHASE AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>example-noobaa openshift-storage.noobaa.io Bound 2m59s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>example-rgw ocs-storagecluster-ceph-rgw Bound 2m59s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Each &lt;code>ObjectBucketClaim&lt;/code> will result in a OpenShift creating a new
&lt;code>ObjectBucket&lt;/code> resource (which, like &lt;code>PersistentVolume&lt;/code> resources, are
not namespaced). The &lt;code>ObjectBucket&lt;/code> resource will be named
&lt;code>obc-&amp;lt;namespace-name&amp;gt;-&amp;lt;objectbucketclaim-name&amp;gt;&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ oc get objectbucket obc-oddbit-ocs-example-example-rgw obc-oddbit-ocs-example-example-noobaa
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STORAGE-CLASS CLAIM-NAMESPACE CLAIM-NAME RECLAIM-POLICY PHASE AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>obc-oddbit-ocs-example-example-rgw ocs-storagecluster-ceph-rgw oddbit-ocs-example example-rgw Delete Bound 67m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>obc-oddbit-ocs-example-example-noobaa openshift-storage.noobaa.io oddbit-ocs-example example-noobaa Delete Bound 67m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Each &lt;code>ObjectBucket&lt;/code> resource corresponds to a bucket in the selected
object storage backend.&lt;/p>
&lt;p>Because buckets exist in a flat namespace, the OCS documentation
recommends always using &lt;code>generateName&lt;/code> in the claim, rather than
explicitly setting &lt;code>bucketName&lt;/code>, in order to avoid unexpected
conflicts. This means that the generated buckets will have a named
prefixed by the value in &lt;code>generateName&lt;/code>, followed by a random string:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ oc get objectbucketclaim example-rgw -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;{.spec.bucketName}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>example-rgw-425d7193-ae3a-41d9-98e3-9d07b82c9661
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ oc get objectbucketclaim example-noobaa -o jsonpath&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;{.spec.bucketName}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>example-noobaa-2e087028-b3a4-475b-ae83-a4fa80d9e3ef
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Along with the bucket itself, OpenShift will create a &lt;code>Secret&lt;/code> and a
&lt;code>ConfigMap&lt;/code> resource &amp;ndash; named after your &lt;code>OBC&lt;/code> &amp;ndash; with the metadata
necessary to access the bucket.&lt;/p>
&lt;p>The &lt;code>Secret&lt;/code> contains AWS-style credentials for authenticating to the
S3 API:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ oc get secret example-rgw -o yaml | oc neat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> AWS_ACCESS_KEY_ID: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> AWS_SECRET_ACCESS_KEY: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bucket-provisioner: openshift-storage.ceph.rook.io-bucket
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: example-rgw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: oddbit-ocs-example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(I&amp;rsquo;m using the &lt;a href="https://github.com/itaysk/kubectl-neat">neat&lt;/a> filter here to remove extraneous metadata that
OpenShift returns when you request a resource.)&lt;/p>
&lt;p>The &lt;code>ConfigMap&lt;/code> contains a number of keys that provide you (or your code)
with the information necessary to access the bucket. For the RGW
bucket:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ oc get configmap example-rgw -o yaml | oc neat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> BUCKET_HOST: rook-ceph-rgw-ocs-storagecluster-cephobjectstore.openshift-storage.svc.cluster.local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> BUCKET_NAME: example-rgw-425d7193-ae3a-41d9-98e3-9d07b82c9661
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> BUCKET_PORT: &lt;span style="color:#e6db74">&amp;#34;80&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> BUCKET_REGION: us-east-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ConfigMap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bucket-provisioner: openshift-storage.ceph.rook.io-bucket
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: example-rgw
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: oddbit-ocs-example
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And for the Noobaa bucket:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ oc get configmap example-noobaa -o yaml | oc neat
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> BUCKET_HOST: s3.openshift-storage.svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> BUCKET_NAME: example-noobaa-2e087028-b3a4-475b-ae83-a4fa80d9e3ef
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> BUCKET_PORT: &lt;span style="color:#e6db74">&amp;#34;443&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ConfigMap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: noobaa
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bucket-provisioner: openshift-storage.noobaa.io-obc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> noobaa-domain: openshift-storage.noobaa.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: example-noobaa
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: oddbit-ocs-example
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that &lt;code>BUCKET_HOST&lt;/code> contains the internal S3 API endpoint. You won&amp;rsquo;t be
able to reach this from outside the cluster. We&amp;rsquo;ll tackle that in just a
bit.&lt;/p>
&lt;h2 id="accessing-a-bucket-from-a-pod">Accessing a bucket from a pod&lt;/h2>
&lt;p>The easiest way to expose the credentials in a pod is to map the keys
from both the &lt;code>ConfigMap&lt;/code> and &lt;code>Secret&lt;/code> as environment variables using
the &lt;code>envFrom&lt;/code> directive, like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">apiVersion&lt;/span>: &lt;span style="color:#ae81ff">v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">kind&lt;/span>: &lt;span style="color:#ae81ff">Pod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">metadata&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">bucket-example&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">spec&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">containers&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">image&lt;/span>: &lt;span style="color:#ae81ff">myimage&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">env&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">AWS_CA_BUNDLE&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">value&lt;/span>: &lt;span style="color:#ae81ff">/run/secrets/kubernetes.io/serviceaccount/service-ca.crt&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">envFrom&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">configMapRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">example-rgw&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">secretRef&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">example-rgw&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [&lt;span style="color:#ae81ff">...]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that we&amp;rsquo;re also setting &lt;code>AWS_CA_BUNDLE&lt;/code> here, which you&amp;rsquo;ll need
if the internal endpoint referenced by &lt;code>$BUCKET_HOST&lt;/code> is using SSL.&lt;/p>
&lt;p>Inside the pod, we can run, for example, &lt;code>aws&lt;/code> commands as long as we
provide an appropriate s3 endpoint. We can inspect the value of
&lt;code>BUCKET_PORT&lt;/code> to determine if we need &lt;code>http&lt;/code> or &lt;code>https&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ &lt;span style="color:#f92672">[&lt;/span> &lt;span style="color:#e6db74">&amp;#34;&lt;/span>$BUCKET_PORT&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">80&lt;/span> &lt;span style="color:#f92672">]&lt;/span> &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> schema&lt;span style="color:#f92672">=&lt;/span>http &lt;span style="color:#f92672">||&lt;/span> schema&lt;span style="color:#f92672">=&lt;/span>https
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ aws s3 --endpoint $schema://$BUCKET_HOST ls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-02-10 04:30:31 example-rgw-8710aa46-a47a-4a8b-8edd-7dabb7d55469
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Python&amp;rsquo;s &lt;code>boto3&lt;/code> module can also make use of the same environment
variables:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#f92672">import&lt;/span> boto3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#f92672">import&lt;/span> os
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> bucket_host &lt;span style="color:#f92672">=&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>environ[&lt;span style="color:#e6db74">&amp;#39;BUCKET_HOST&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> schema &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#39;http&amp;#39;&lt;/span> &lt;span style="color:#66d9ef">if&lt;/span> os&lt;span style="color:#f92672">.&lt;/span>environ[&lt;span style="color:#e6db74">&amp;#39;BUCKET_PORT&amp;#39;&lt;/span>] &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#39;80&amp;#39;&lt;/span> &lt;span style="color:#66d9ef">else&lt;/span> &lt;span style="color:#e6db74">&amp;#39;https&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> s3 &lt;span style="color:#f92672">=&lt;/span> boto3&lt;span style="color:#f92672">.&lt;/span>client(&lt;span style="color:#e6db74">&amp;#39;s3&amp;#39;&lt;/span>, endpoint_url&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>schema&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">://&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>bucket_host&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> s3&lt;span style="color:#f92672">.&lt;/span>list_buckets()[&lt;span style="color:#e6db74">&amp;#39;Buckets&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[{&lt;span style="color:#e6db74">&amp;#39;Name&amp;#39;&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;example-noobaa-...&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;CreationDate&amp;#39;&lt;/span>: datetime&lt;span style="color:#f92672">.&lt;/span>datetime(&lt;span style="color:#f92672">...&lt;/span>)}]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="external-connections-to-s3-endpoints">External connections to S3 endpoints&lt;/h2>
&lt;p>External access to services in OpenShift is often managed via
&lt;a href="https://docs.openshift.com/enterprise/3.0/architecture/core_concepts/routes.html">routes&lt;/a>. If you look at the routes available in your
&lt;code>openshift-storage&lt;/code> namespace, you&amp;rsquo;ll find the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ oc -n openshift-storage get route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>noobaa-mgmt noobaa-mgmt-openshift-storage.apps.example.com noobaa-mgmt mgmt-https reencrypt None
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>s3 s3-openshift-storage.apps.example.com s3 s3-https reencrypt None
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>s3&lt;/code> route provides external access to your Noobaa S3 endpoint.
You&amp;rsquo;ll note that in the list above there is no route registered for
radosgw&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. There is a service registered for Radosgw named
&lt;code>rook-ceph-rgw-ocs-storagecluster-cephobjectstore&lt;/code>, so we
can expose that service to create an external route by running
something like:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>oc create route edge rgw --service rook-ceph-rgw-ocs-storagecluster-cephobjectstore
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will create a route with &amp;ldquo;edge&amp;rdquo; encryption (TLS termination is
handled by the default ingress router):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ oc -n openshift storage get route
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>noobaa-mgmt noobaa-mgmt-openshift-storage.apps.example.com noobaa-mgmt mgmt-https reencrypt None
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rgw rgw-openshift-storage.apps.example.com rook-ceph-rgw-ocs-storagecluster-cephobjectstore http edge None
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>s3 s3-openshift-storage.apps.example.com s3 s3-https reencrypt None
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="accessing-a-bucket-from-outside-the-cluster">Accessing a bucket from outside the cluster&lt;/h2>
&lt;p>Once we know the &lt;code>Route&lt;/code> to our S3 endpoint, we can use the
information in the &lt;code>Secret&lt;/code> and &lt;code>ConfigMap&lt;/code> created for us when we
provisioned the storage. We just need to replace the &lt;code>BUCKET_HOST&lt;/code>
with the hostname in the route, and we need to use SSL over port 443
regardless of what &lt;code>BUCKET_PORT&lt;/code> tells us.&lt;/p>
&lt;p>We can extract the values into variables using something like the
following shell script, which takes care of getting the appropriate
route from the &lt;code>openshift-storage&lt;/code> namespace, base64-decoding the values
in the &lt;code>Secret&lt;/code>, and replacing the &lt;code>BUCKET_HOST&lt;/code> value:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#!/bin/sh
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>bucket_host&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>oc get configmap $1 -o json | jq -r .data.BUCKET_HOST&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service_name&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>cut -f1 -d. &lt;span style="color:#f92672">&amp;lt;&amp;lt;&amp;lt;&lt;/span>$bucket_host&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service_ns&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>cut -f2 -d. &lt;span style="color:#f92672">&amp;lt;&amp;lt;&amp;lt;&lt;/span>$bucket_host&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># get the externally visible hostname provided by the route&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>public_bucket_host&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">$(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oc -n $service_ns get route -o json |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> jq -r &lt;span style="color:#e6db74">&amp;#39;.items[]|select(.spec.to.name==&amp;#34;&amp;#39;&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span>$service_name&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&amp;#34;)|.spec.host&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># dump configmap and secret as shell variables, replacing the&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># value of BUCKET_HOST in the process.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oc get configmap $1 -o json |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> jq -r &lt;span style="color:#e6db74">&amp;#39;.data as $data|.data|keys[]|&amp;#34;\(.)=\($data[.])&amp;#34;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oc get secret $1 -o json |
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> jq -r &lt;span style="color:#e6db74">&amp;#39;.data as $data|.data|keys[]|&amp;#34;\(.)=\($data[.]|@base64d)&amp;#34;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">)&lt;/span> | sed -e &lt;span style="color:#e6db74">&amp;#39;s/^/export /&amp;#39;&lt;/span> -e &lt;span style="color:#e6db74">&amp;#39;/BUCKET_HOST/ s/=.*/=&amp;#39;&lt;/span>$public_bucket_host&lt;span style="color:#e6db74">&amp;#39;/&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we call the script &lt;code>getenv.sh&lt;/code> and run it like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ sh getenv.sh example-rgw
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It will produce output like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>export BUCKET_HOST&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;s3-openshift-storage.apps.cnv.massopen.cloud&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export BUCKET_NAME&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;example-noobaa-2e1bca2f-ff49-431a-99b8-d7d63a8168b0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export BUCKET_PORT&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;443&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export BUCKET_REGION&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export BUCKET_SUBREGION&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export AWS_ACCESS_KEY_ID&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export AWS_SECRET_ACCESS_KEY&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We could accomplish something similar in Python with the following,
which shows how to use the OpenShift dynamic client to interact with
OpenShift:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> argparse
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> base64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> kubernetes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">import&lt;/span> openshift.dynamic
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">parse_args&lt;/span>():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> p &lt;span style="color:#f92672">=&lt;/span> argparse&lt;span style="color:#f92672">.&lt;/span>ArgumentParser()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> p&lt;span style="color:#f92672">.&lt;/span>add_argument(&lt;span style="color:#e6db74">&amp;#39;-n&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;--namespace&amp;#39;&lt;/span>, required&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#66d9ef">True&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> p&lt;span style="color:#f92672">.&lt;/span>add_argument(&lt;span style="color:#e6db74">&amp;#39;obcname&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> p&lt;span style="color:#f92672">.&lt;/span>parse_args()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>args &lt;span style="color:#f92672">=&lt;/span> parse_args()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k8s_client &lt;span style="color:#f92672">=&lt;/span> kubernetes&lt;span style="color:#f92672">.&lt;/span>config&lt;span style="color:#f92672">.&lt;/span>new_client_from_config()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dyn_client &lt;span style="color:#f92672">=&lt;/span> openshift&lt;span style="color:#f92672">.&lt;/span>dynamic&lt;span style="color:#f92672">.&lt;/span>DynamicClient(k8s_client)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>v1_configmap &lt;span style="color:#f92672">=&lt;/span> dyn_client&lt;span style="color:#f92672">.&lt;/span>resources&lt;span style="color:#f92672">.&lt;/span>get(api_version&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;v1&amp;#39;&lt;/span>, kind&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;ConfigMap&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>v1_secret &lt;span style="color:#f92672">=&lt;/span> dyn_client&lt;span style="color:#f92672">.&lt;/span>resources&lt;span style="color:#f92672">.&lt;/span>get(api_version&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;v1&amp;#39;&lt;/span>, kind&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;Secret&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>v1_service &lt;span style="color:#f92672">=&lt;/span> dyn_client&lt;span style="color:#f92672">.&lt;/span>resources&lt;span style="color:#f92672">.&lt;/span>get(api_version&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;v1&amp;#39;&lt;/span>, kind&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;Service&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>v1_route &lt;span style="color:#f92672">=&lt;/span> dyn_client&lt;span style="color:#f92672">.&lt;/span>resources&lt;span style="color:#f92672">.&lt;/span>get(api_version&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;route.openshift.io/v1&amp;#39;&lt;/span>, kind&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;Route&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>configmap &lt;span style="color:#f92672">=&lt;/span> v1_configmap&lt;span style="color:#f92672">.&lt;/span>get(name&lt;span style="color:#f92672">=&lt;/span>args&lt;span style="color:#f92672">.&lt;/span>obcname, namespace&lt;span style="color:#f92672">=&lt;/span>args&lt;span style="color:#f92672">.&lt;/span>namespace)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>secret &lt;span style="color:#f92672">=&lt;/span> v1_secret&lt;span style="color:#f92672">.&lt;/span>get(name&lt;span style="color:#f92672">=&lt;/span>args&lt;span style="color:#f92672">.&lt;/span>obcname, namespace&lt;span style="color:#f92672">=&lt;/span>args&lt;span style="color:#f92672">.&lt;/span>namespace)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>env &lt;span style="color:#f92672">=&lt;/span> dict(configmap&lt;span style="color:#f92672">.&lt;/span>data)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>env&lt;span style="color:#f92672">.&lt;/span>update({k: base64&lt;span style="color:#f92672">.&lt;/span>b64decode(v)&lt;span style="color:#f92672">.&lt;/span>decode() &lt;span style="color:#66d9ef">for&lt;/span> k, v &lt;span style="color:#f92672">in&lt;/span> secret&lt;span style="color:#f92672">.&lt;/span>data&lt;span style="color:#f92672">.&lt;/span>items()})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>svc_name, svc_ns &lt;span style="color:#f92672">=&lt;/span> env[&lt;span style="color:#e6db74">&amp;#39;BUCKET_HOST&amp;#39;&lt;/span>]&lt;span style="color:#f92672">.&lt;/span>split(&lt;span style="color:#e6db74">&amp;#39;.&amp;#39;&lt;/span>)[:&lt;span style="color:#ae81ff">2&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>routes &lt;span style="color:#f92672">=&lt;/span> v1_route&lt;span style="color:#f92672">.&lt;/span>get(namespace&lt;span style="color:#f92672">=&lt;/span>svc_ns)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> route &lt;span style="color:#f92672">in&lt;/span> routes&lt;span style="color:#f92672">.&lt;/span>items:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> route&lt;span style="color:#f92672">.&lt;/span>spec&lt;span style="color:#f92672">.&lt;/span>to&lt;span style="color:#f92672">.&lt;/span>name &lt;span style="color:#f92672">==&lt;/span> svc_name:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">break&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>env[&lt;span style="color:#e6db74">&amp;#39;BUCKET_PORT&amp;#39;&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#ae81ff">443&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>env[&lt;span style="color:#e6db74">&amp;#39;BUCKET_HOST&amp;#39;&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> route[&lt;span style="color:#e6db74">&amp;#39;spec&amp;#39;&lt;/span>][&lt;span style="color:#e6db74">&amp;#39;host&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">for&lt;/span> k, v &lt;span style="color:#f92672">in&lt;/span> env&lt;span style="color:#f92672">.&lt;/span>items():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> print(&lt;span style="color:#e6db74">f&lt;/span>&lt;span style="color:#e6db74">&amp;#39;export &lt;/span>&lt;span style="color:#e6db74">{&lt;/span>k&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">=&amp;#34;&lt;/span>&lt;span style="color:#e6db74">{&lt;/span>v&lt;span style="color:#e6db74">}&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we run it like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>python genenv.py -n oddbit-ocs-example example-noobaa
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It will produce output largely identical to what we saw above with the
shell script.&lt;/p>
&lt;p>If we load those variables into the environment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ eval &lt;span style="color:#66d9ef">$(&lt;/span>sh getenv.sh example-rgw&lt;span style="color:#66d9ef">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can perform the same operations we executed earlier from inside the
pod:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>$ aws s3 --endpoint https://$BUCKET_HOST ls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-02-10 14:34:12 example-rgw-425d7193-ae3a-41d9-98e3-9d07b82c9661
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>note that this may have changed in the recent OCS 4.6
release&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></content></item><item><title>Remediating poor PyPi performance with DevPi</title><link>https://blog.oddbit.com/post/2021-02-08-remediating-poor-pypi-performa/</link><pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-02-08-remediating-poor-pypi-performa/</guid><description>Performance of the primary PyPi service has been so bad lately that it&amp;rsquo;s become very disruptive. Tasks that used to take a few seconds will now churn along for 15-20 minutes or longer before completing, which is incredibly frustrating.
I first went looking to see if there was a PyPi mirror infrastructure, like we see with CPAN for Perl or CTAN for Tex (and similarly for most Linux distributions). There is apparently no such beast,</description><content>&lt;p>Performance of the primary PyPi service has been so bad lately that
it&amp;rsquo;s become very disruptive. Tasks that used to take a few seconds
will now churn along for 15-20 minutes or longer before completing,
which is incredibly frustrating.&lt;/p>
&lt;p>I first went looking to see if there was a PyPi mirror infrastructure,
like we see with &lt;a href="https://www.cpan.org/">CPAN&lt;/a> for Perl or &lt;a href="https://ctan.org/">CTAN&lt;/a> for Tex (and similarly
for most Linux distributions). There is apparently no such beast,&lt;/p>
&lt;p>I didn&amp;rsquo;t really want to set up a PyPi mirror locally, since the number
of packages I actually use is small vs. the number of packages
available. I figured there must be some sort of caching proxy
available that would act as a shim between me and PyPi, fetching
packages from PyPi and caching them if they weren&amp;rsquo;t already available
locally.&lt;/p>
&lt;p>I was previously aware of &lt;a href="https://www.jfrog.com/confluence/display/JFROG/PyPI+Repositories">Artifactory&lt;/a>, which I suspected (and
confirmed) was capable of this, but while looking around I came across
&lt;a href="https://www.devpi.net/">DevPi&lt;/a>, which unlike Artifactory is written exclusively for
managing Python packages. DevPi itself is hosted on PyPi, and the
documentation made things look easy to configure.&lt;/p>
&lt;p>After reading through their &lt;a href="https://devpi.net/docs/devpi/devpi/stable/+doc/quickstart-pypimirror.html">Quickstart: running a pypi mirror on your
laptop&lt;/a> documentation, I built a containerized service that would
be easy for me to run on my desktop, laptop, work computer, etc. You
can find the complete configuration at
&lt;a href="https://github.com/oddbit-dot-com/docker-devpi-server">https://github.com/oddbit-dot-com/docker-devpi-server&lt;/a>.&lt;/p>
&lt;p>I started with the following &lt;code>Dockerfile&lt;/code> (note I&amp;rsquo;m using
&lt;a href="https://podman.io/">podman&lt;/a> rather than Docker as my container runtime, but the
resulting image will work fine for either environment):&lt;/p>
&lt;pre tabindex="0">&lt;code>FROM python:3.9
RUN pip install devpi-server devpi-web
WORKDIR /root
VOLUME /root/.devpi
COPY docker-entrypoint.sh /docker-entrypoint.sh
ENTRYPOINT [&amp;#34;sh&amp;#34;, &amp;#34;/docker-entrypoint.sh&amp;#34;]
CMD [&amp;#34;devpi-server&amp;#34;, &amp;#34;--host&amp;#34;, &amp;#34;0.0.0.0&amp;#34;]
&lt;/code>&lt;/pre>&lt;p>This installs both &lt;code>devpi-server&lt;/code>, which provides the basic caching
for &lt;code>pip install&lt;/code>, as well as &lt;code>devpi-web&lt;/code>, which provides support for
&lt;code>pip search&lt;/code>.&lt;/p>
&lt;p>To ensure that things are initialized correctly when the container
start up, I&amp;rsquo;ve set the &lt;code>ENYTRYPOINT&lt;/code> to the following script:&lt;/p>
&lt;pre tabindex="0">&lt;code>#!/bin/sh
if ! [ -f /root/.devpi/server ]; then
devpi-init
fi
exec &amp;#34;$@&amp;#34;
&lt;/code>&lt;/pre>&lt;p>This will run &lt;code>devpi-init&lt;/code> if the target directory hasn&amp;rsquo;t already been
initialized.&lt;/p>
&lt;p>The repository includes a &lt;a href="https://github.com/oddbit-dot-com/docker-devpi-server/blob/master/.github/workflows/build_docker_image.yml">GitHub workflow&lt;/a> that builds a new image on each commit
and pushes the result to the &lt;code>oddbit/devpi-server&lt;/code> repository on
Docker Hub.&lt;/p>
&lt;p>Once the image was available on Docker Hub, I created the following
systemd unit to run the service locally:&lt;/p>
&lt;pre tabindex="0">&lt;code>[Service]
Restart=on-failure
ExecStartPre=/usr/bin/rm -f %t/%n-pid
ExecStart=/usr/bin/podman run --replace \
--conmon-pidfile %t/%n-pid --cgroups=no-conmon \
--name %n -d -p 127.0.0.1:3141:3141 \
-v devpi:/root/.devpi oddbit/devpi-server
ExecStopPost=/usr/bin/rm -f %t/%n-pid
PIDFile=%t/%n-pid
Type=forking
[Install]
WantedBy=multi-user.target default.target
&lt;/code>&lt;/pre>&lt;p>There are a couple items of note in this unitfile:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The service is exposed only on &lt;code>localhost&lt;/code> using &lt;code>-p 127.0.0.1:3141:3141&lt;/code>. I don&amp;rsquo;t want this service exposed on
externally visible addresses since I haven&amp;rsquo;t bothered setting up any
sort of authentication.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The service mounts a named volume for use by &lt;code>devpi-server&lt;/code> via the
&lt;code>-v devpi:/root/.devpi&lt;/code> command line option.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>This unit file gets installed into
&lt;code>~/.config/systemd/user/devpi.service&lt;/code>. Running &lt;code>systemctl --user enable --now devpi.service&lt;/code> both enables the service to start at boot
and actually starts it up immediately.&lt;/p>
&lt;p>With the service running, the last thing to do is configure &lt;code>pip&lt;/code> to
utilize it. The following configuration, placed in
&lt;code>~/.config/pip/pip.conf&lt;/code>, does the trick:&lt;/p>
&lt;pre tabindex="0">&lt;code>[install]
index-url = http://localhost:3141/root/pypi/+simple/
[search]
index = http://localhost:3141/root/pypi/
&lt;/code>&lt;/pre>&lt;p>Now both &lt;code>pip install&lt;/code> and &lt;code>pip search&lt;/code> hit the local cache instead of
the upstream PyPi server, and things are generally much, much faster.&lt;/p>
&lt;h2 id="for-poetry-users">For Poetry Users&lt;/h2>
&lt;p>&lt;a href="https://python-poetry.org/">Poetry&lt;/a> respects the &lt;code>pip&lt;/code> configuration and will Just Work.&lt;/p>
&lt;h2 id="for-pipenv-users">For Pipenv Users&lt;/h2>
&lt;p>&lt;a href="https://github.com/pypa/pipenv">Pipenv&lt;/a> does not respect the pip configuration [&lt;a href="https://github.com/pypa/pipenv/issues/1451">1&lt;/a>,
&lt;a href="https://github.com/pypa/pipenv/issues/2075">2&lt;/a>], so you will
need to set the &lt;code>PIPENV_PYPI_MIRROR&lt;/code> environment variable. E.g:&lt;/p>
&lt;pre tabindex="0">&lt;code>export PIPENV_PYPI_MIRROR=http://localhost:3141/root/pypi/+simple/
&lt;/code>&lt;/pre></content></item><item><title>symtool: a tool for interacting with your SYM-1</title><link>https://blog.oddbit.com/post/2021-02-06-symtool-a-tool-for-interacting/</link><pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-02-06-symtool-a-tool-for-interacting/</guid><description>The SYM-1 is a 6502-based single-board computer produced by Synertek Systems Corp in the mid 1970&amp;rsquo;s. I&amp;rsquo;ve had one floating around in a box for many, many years, and after a recent foray into the world of 6502 assembly language programming I decided to pull it out, dust it off, and see if it still works.
The board I have has a whopping 8KB of memory, and in addition to the standard SUPERMON monitor it has the expansion ROMs for the Synertek BASIC interpreter (yet another Microsoft BASIC) and RAE (the &amp;ldquo;Resident Assembler Editor&amp;rdquo;).</description><content>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/SYM-1">SYM-1&lt;/a> is a &lt;a href="https://en.wikipedia.org/wiki/MOS_Technology_6502">6502&lt;/a>-based single-board computer produced by
&lt;a href="https://en.wikipedia.org/wiki/Synertek">Synertek Systems Corp&lt;/a> in the mid 1970&amp;rsquo;s. I&amp;rsquo;ve had one
floating around in a box for many, many years, and after a recent
foray into the world of 6502 assembly language programming I decided
to pull it out, dust it off, and see if it still works.&lt;/p>
&lt;p>The board I have has a whopping 8KB of memory, and in addition to the
standard SUPERMON monitor it has the expansion ROMs for the Synertek
BASIC interpreter (yet another Microsoft BASIC) and RAE (the &amp;ldquo;Resident
Assembler Editor&amp;rdquo;). One interacts with the board either through the
onboard hex keypad and six-digit display, or via a serial connection
at 4800bps (or lower).&lt;/p>
&lt;p>[If you&amp;rsquo;re interested in Microsoft BASIC, the &lt;a href="https://github.com/mist64/msbasic">mist64/msbasic&lt;/a>
repository on GitHub is a trove of information, containing the source
for multiple versions of Microsoft BASIC including the Synertek
version.]&lt;/p>
&lt;p>Fiddling around with the BASIC interpreter and the onboard assembler
was fun, but I wanted to use a &lt;a href="https://www.vim.org/">real editor&lt;/a> for writing source
files, assemble them on my Linux system, and then transfer the
compiled binary to the SYM-1. The first two tasks are easy; there are
lots of editors and there are a variety of 6502 assemblers that will
run under Linux. I&amp;rsquo;m partial to &lt;a href="https://cc65.github.io/doc/ca65.html">ca65&lt;/a>, part of the &lt;a href="https://cc65.github.io/">cc65&lt;/a>
project (which is an incredible project that implements a C compiler
that cross-compiles C for 6502 processors). But what&amp;rsquo;s the best way to
get compiled code over to the SYM-1?&lt;/p>
&lt;h2 id="symtool">Symtool&lt;/h2>
&lt;p>That&amp;rsquo;s where &lt;a href="https://github.com/larsks/symtool">symtool&lt;/a> comes in. Symtool runs on your host and
talks to the SUPERMON monitor on the SYM-1 over a serial connection.
It allows you to view registers, dump and load memory, fill memory,
and execute code.&lt;/p>
&lt;h3 id="configuration">Configuration&lt;/h3>
&lt;p>Symtool needs to know to what serial device your SYM-1 is attached.
You can specify this using the &lt;code>-d &amp;lt;device&amp;gt;&lt;/code> command line option, but
this quickly gets old. To save typing, you can instead set the
&lt;code>SYMTOOL_DEVICE&lt;/code> environment variable:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ export SYMTOOL_DEVICE=/dev/ttyUSB0
$ symtool load ...
$ symtool dump ...
&lt;/code>&lt;/pre>&lt;p>The baud rate defaults to 4800bps. If for some reason you want to use
a slower speed (maybe you&amp;rsquo;d like to relive the good old days of 300bps
modems), you can use the &lt;code>-s&lt;/code> command line option or the
&lt;code>SYMTOOL_SPEED&lt;/code> environment variable.&lt;/p>
&lt;h3 id="loading-code-into-memory">Loading code into memory&lt;/h3>
&lt;p>After compiling your code (I&amp;rsquo;ve included the examples from the SYM-1
Technical Notes &lt;a href="https://github.com/larsks/symtool/tree/master/asm">in the repository&lt;/a>), use the &lt;code>load&lt;/code> command to
load the code into the memory of the SYM-1:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ make -C asm
[...]
$ symtool -v load 0x200 asm/countdown.bin
INFO:symtool.symtool:using port /dev/ttyUSB0, speed 4800
INFO:symtool.symtool:connecting to sym1...
INFO:symtool.symtool:connected
INFO:symtool.symtool:loading 214 bytes of data at $200
&lt;/code>&lt;/pre>&lt;p>(Note the &lt;code>-v&lt;/code> on the command line there; without that, &lt;code>symtool&lt;/code>
won&amp;rsquo;t produce any output unless there&amp;rsquo;s an error.)&lt;/p>
&lt;p>[A note on compiling code: the build logic in the &lt;a href="https://github.com/larsks/symtool/tree/master/asm">&lt;code>asm/&lt;/code>&lt;/a>
directory is configured to load code at address &lt;code>0x200&lt;/code>. If you want
to load code at a different address, you will need to add the
appropriate &lt;code>--start-addr&lt;/code> option to &lt;code>LD65FLAGS&lt;/code> when building, or
modify the linker configuration in &lt;code>sym1.cfg&lt;/code>.]&lt;/p>
&lt;h3 id="examining-memory">Examining memory&lt;/h3>
&lt;p>The above command loads the code into memory but doesn&amp;rsquo;t execute it.
We can use the &lt;code>dump&lt;/code> command to examine memory. By default, &lt;code>dump&lt;/code>
produces binary output. We can use that to extract code from the SYM-1
ROM or to verify that the code we just loaded was transferred
correctly:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ symtool dump 0x200 $(wc -c &amp;lt; asm/countdown.bin) -o check.bin
$ sha1sum check.bin asm/countdown.bin
5851c40bed8cc8b2a132163234b68a7fc0e434c0 check.bin
5851c40bed8cc8b2a132163234b68a7fc0e434c0 asm/countdown.bin
&lt;/code>&lt;/pre>&lt;p>We can also produce a hexdump:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ symtool dump 0x200 $(wc -c &amp;lt; asm/countdown.bin) -h
00000000: 20 86 8B A9 20 85 03 A9 55 8D 7E A6 A9 02 8D 7F ... ...U.~.....
00000010: A6 A9 40 8D 0B AC A9 4E 8D 06 AC A9 C0 8D 0E AC ..@....N........
00000020: A9 00 85 02 A9 20 8D 05 AC 18 58 A9 00 8D 40 A6 ..... ....X...@.
00000030: 8D 41 A6 8D 44 A6 8D 45 A6 A5 04 29 0F 20 73 02 .A..D..E...). s.
00000040: 8D 43 A6 A5 04 4A 4A 4A 4A 20 73 02 8D 42 A6 20 .C...JJJJ s..B.
00000050: 06 89 4C 2B 02 48 8A 48 98 48 AD 0D AC 8D 0D AC ..L+.H.H.H......
00000060: E6 02 A5 02 C9 05 F0 02 50 66 A9 00 85 02 20 78 ........Pf.... x
00000070: 02 50 5D AA BD 29 8C 60 18 A5 04 69 01 18 B8 85 .P]..).`...i....
00000080: 04 C9 FF F0 01 60 A9 7C 8D 41 A6 A9 79 8D 42 A6 .....`.|.A..y.B.
00000090: 8D 43 A6 A9 73 8D 44 A6 A9 00 85 04 20 72 89 20 .C..s.D..... r.
000000A0: 06 89 20 06 89 20 06 89 20 06 89 20 06 89 20 06 .. .. .. .. .. .
000000B0: 89 C6 03 20 06 89 20 06 89 20 06 89 20 06 89 20 ... .. .. .. ..
000000C0: 06 89 20 06 89 A5 03 C9 00 D0 D1 A9 20 85 03 60 .. ......... ..`
000000D0: 68 A8 68 AA 68 40 h.h.h@
&lt;/code>&lt;/pre>&lt;p>Or a disassembly:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ symtool dump 0x200 $(wc -c &amp;lt; asm/countdown.bin) -d
$0200 20 86 8b JSR $8B86
$0203 a9 20 LDA #$20
$0205 85 03 STA $03
$0207 a9 55 LDA #$55
$0209 8d 7e a6 STA $A67E
$020c a9 02 LDA #$02
$020e 8d 7f a6 STA $A67F
$0211 a9 40 LDA #$40
$0213 8d 0b ac STA $AC0B
$0216 a9 4e LDA #$4E
$0218 8d 06 ac STA $AC06
$021b a9 c0 LDA #$C0
$021d 8d 0e ac STA $AC0E
$0220 a9 00 LDA #$00
$0222 85 02 STA $02
$0224 a9 20 LDA #$20
$0226 8d 05 ac STA $AC05
$0229 18 CLC
$022a 58 CLI
$022b a9 00 LDA #$00
$022d 8d 40 a6 STA $A640
$0230 8d 41 a6 STA $A641
$0233 8d 44 a6 STA $A644
$0236 8d 45 a6 STA $A645
$0239 a5 04 LDA $04
$023b 29 0f AND #$0F
$023d 20 73 02 JSR $0273
$0240 8d 43 a6 STA $A643
$0243 a5 04 LDA $04
$0245 4a LSR
$0246 4a LSR
$0247 4a LSR
$0248 4a LSR
$0249 20 73 02 JSR $0273
$024c 8d 42 a6 STA $A642
$024f 20 06 89 JSR $8906
$0252 4c 2b 02 JMP $022B
$0255 48 PHA
$0256 8a TXA
$0257 48 PHA
$0258 98 TYA
$0259 48 PHA
$025a ad 0d ac LDA $AC0D
$025d 8d 0d ac STA $AC0D
$0260 e6 02 INC $02
$0262 a5 02 LDA $02
$0264 c9 05 CMP #$05
$0266 f0 02 BEQ $02
$0268 50 66 BVC $66
$026a a9 00 LDA #$00
$026c 85 02 STA $02
$026e 20 78 02 JSR $0278
$0271 50 5d BVC $5D
$0273 aa TAX
$0274 bd 29 8c LDA $8C29,X
$0277 60 RTS
$0278 18 CLC
$0279 a5 04 LDA $04
$027b 69 01 ADC #$01
$027d 18 CLC
$027e b8 CLV
$027f 85 04 STA $04
$0281 c9 ff CMP #$FF
$0283 f0 01 BEQ $01
$0285 60 RTS
$0286 a9 7c LDA #$7C
$0288 8d 41 a6 STA $A641
$028b a9 79 LDA #$79
$028d 8d 42 a6 STA $A642
$0290 8d 43 a6 STA $A643
$0293 a9 73 LDA #$73
$0295 8d 44 a6 STA $A644
$0298 a9 00 LDA #$00
$029a 85 04 STA $04
$029c 20 72 89 JSR $8972
$029f 20 06 89 JSR $8906
$02a2 20 06 89 JSR $8906
$02a5 20 06 89 JSR $8906
$02a8 20 06 89 JSR $8906
$02ab 20 06 89 JSR $8906
$02ae 20 06 89 JSR $8906
$02b1 c6 03 DEC $03
$02b3 20 06 89 JSR $8906
$02b6 20 06 89 JSR $8906
$02b9 20 06 89 JSR $8906
$02bc 20 06 89 JSR $8906
$02bf 20 06 89 JSR $8906
$02c2 20 06 89 JSR $8906
$02c5 a5 03 LDA $03
$02c7 c9 00 CMP #$00
$02c9 d0 d1 bNE $D1
$02cb a9 20 LDA #$20
$02cd 85 03 STA $03
$02cf 60 RTS
$02d0 68 PLA
$02d1 a8 TAY
$02d2 68 PLA
$02d3 aa TAX
$02d4 68 PLA
$02d5 40 RTI
&lt;/code>&lt;/pre>&lt;h3 id="executing-code">Executing code&lt;/h3>
&lt;p>There are two ways to run your code using &lt;code>symtool&lt;/code>. If you provide
the &lt;code>-g&lt;/code> option to the &lt;code>load&lt;/code> command, &lt;code>symtool&lt;/code> will execute your
code as soon as the load has finished:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ symtool load -g 0x200 asm/countdown.bin
&lt;/code>&lt;/pre>&lt;p>Alternatively, you can use the &lt;code>go&lt;/code> command to run code that has
already been loaded onto the SYM-1:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ symtool go 0x200
&lt;/code>&lt;/pre>&lt;h3 id="examining-registers">Examining registers&lt;/h3>
&lt;p>The &lt;code>registers&lt;/code> command allows you to examine the contents of the 6502
registers:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ symtool registers
s ff (11111111)
f b1 (10110001) +carry -zero -intr -dec -oflow +neg
a 80 (10000000)
x 00 (00000000)
y 50 (01010000)
p b0ac (1011000010101100)
&lt;/code>&lt;/pre>&lt;h3 id="filling-memory">Filling memory&lt;/h3>
&lt;p>If you want to clear a block of memory, you can use the &lt;code>fill&lt;/code>
command. For example, to wipe out the code we loaded in the earlier
example:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ symtool fill 0x200 0 $(wc -c &amp;lt; asm/countdown.bin)
$ symtool dump -h 0x200 $(wc -c &amp;lt; asm/countdown.bin)
00000000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
00000010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................
[...]
&lt;/code>&lt;/pre>&lt;h3 id="notes-on-the-code">Notes on the code&lt;/h3>
&lt;p>The &lt;code>symtool&lt;/code> repository includes both &lt;a href="https://github.com/larsks/symtool/tree/master/tests/unit">unit&lt;/a> and &lt;a href="https://github.com/larsks/symtool/tree/master/tests/functional">functional&lt;/a> tests. The
functional tests require an actual SYM-1 to be attached to your system
(with the device name in the &lt;code>SYMTOOL_DEVICE&lt;/code> environment variable).
The unit tests will run anywhere.&lt;/p>
&lt;h2 id="wrapping-up">Wrapping up&lt;/h2>
&lt;p>No lie, this is a pretty niche project. I&amp;rsquo;m not sure how many people
out there own a SYM-1 these days, but this has been fun to work with
and if maybe one other person finds it useful, I would consider that
a success :).&lt;/p>
&lt;h2 id="see-also">See Also&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>The symtool repository includes an &lt;a href="https://github.com/larsks/symtool/blob/master/reference/synmon11.asm">assembly listing for the
monitor&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://www.6502.org/">6502.org&lt;/a> hosts just about &lt;a href="http://www.6502.org/trainers/synertek/">all the SYM-1 documentation&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ul></content></item><item><title>To sleep or not to sleep?</title><link>https://blog.oddbit.com/post/2020-12-18-to-sleep-or-not-to-sleep/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-12-18-to-sleep-or-not-to-sleep/</guid><description>Let&amp;rsquo;s say you have a couple of sensors attached to an ESP8266 running MicroPython. You&amp;rsquo;d like to sample them at different frequencies (say, one every 60 seconds and one every five minutes), and you&amp;rsquo;d like to do it as efficiently as possible in terms of power consumption. What are your options?
If we don&amp;rsquo;t care about power efficiency, the simplest solution is probably a loop like this:
import machine lastrun_1 = 0 lastrun_2 = 0 while True: now = time.</description><content>&lt;p>Let&amp;rsquo;s say you have a couple of sensors attached to an ESP8266 running
&lt;a href="https://micropython.org/">MicroPython&lt;/a>. You&amp;rsquo;d like to sample them at different frequencies
(say, one every 60 seconds and one every five minutes), and you&amp;rsquo;d like
to do it as efficiently as possible in terms of power consumption.
What are your options?&lt;/p>
&lt;p>If we don&amp;rsquo;t care about power efficiency, the simplest solution is
probably a loop like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>import machine
lastrun_1 = 0
lastrun_2 = 0
while True:
now = time.time()
if (lastrun_1 == 0) or (now - lastrun_1 &amp;gt;= 60):
read_sensor_1()
lastrun_1 = now
if (lastrun_2 == 0) or (now - lastrun_2 &amp;gt;= 300):
read_sensor_2()
lastrun_2 = now
machine.idle()
&lt;/code>&lt;/pre>&lt;p>If we were only reading a single sensor (or multiple sensors at the
same interval), we could drop the loop and juse use the ESP8266&amp;rsquo;s deep
sleep mode (assuming we have &lt;a href="http://docs.micropython.org/en/latest/esp8266/tutorial/powerctrl.html#deep-sleep-mode">wired things properly&lt;/a>):&lt;/p>
&lt;pre tabindex="0">&lt;code>import machine
def deepsleep(duration):
rtc = machine.RTC()
rtc.irq(trigger=rtc.ALARM0, wake=machine.DEEPSLEEP)
rtc.alarm(rtc.ALARM0, duration)
read_sensor_1()
deepsleep(60000)
&lt;/code>&lt;/pre>&lt;p>This will wake up, read the sensor, then sleep for 60 seconds, at
which point the device will reboot and repeat the process.&lt;/p>
&lt;p>If we want both use deep sleep &lt;em>and&lt;/em> run tasks at different intervals,
we can effectively combine the above two methods. This requires a
little help from the RTC, which in addition to keeping time also
provides us with a small amount of memory (492 bytes when using
MicroPython) that will persist across a deepsleep/reset cycle.&lt;/p>
&lt;p>The &lt;code>machine.RTC&lt;/code> class includes a &lt;code>memory&lt;/code> method that provides
access to the RTC memory. We can read the memory like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>import machine
rtc = machine.RTC()
bytes = rtc.memory()
&lt;/code>&lt;/pre>&lt;p>Note that &lt;code>rtc.memory()&lt;/code> will always return a byte string.&lt;/p>
&lt;p>We write to it like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>rtc.memory(&amp;#39;somevalue&amp;#39;)
&lt;/code>&lt;/pre>&lt;p>Lastly, note that the time maintained by the RTC also persists across
a deepsleep/reset cycle, so that if we call &lt;code>time.time()&lt;/code> and then
deepsleep for 10 seconds, when the module boots back up &lt;code>time.time()&lt;/code>
will show that 10 seconds have elapsed.&lt;/p>
&lt;p>We&amp;rsquo;re going to implement a solution similar to the loop presented at
the beginning of this article in that we will store the time at which
at task was last run. Because we need to maintain two different
values, and because the RTC memory operates on bytes, we need a way to
serialize and deserialize a pair of integers. We could use functions
like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>import json
def store_time(t1, t2):
rtc.memory(json.dumps([t1, t2]))
def load_time():
data = rtc.memory()
if not data:
return [0, 0]
try:
return json.loads(data)
except ValueError:
return [0, 0]
&lt;/code>&lt;/pre>&lt;p>The &lt;code>load_time&lt;/code> method returns &lt;code>[0, 0]&lt;/code> if either (a) the RTC memory
was unset or (b) we were unable to decode the value stored in memory
(which might happen if you had previously stored something else
there).&lt;/p>
&lt;p>You don&amp;rsquo;t have to use &lt;code>json&lt;/code> for serializing the data we&amp;rsquo;re storing in
the RTC; you could just as easily use the &lt;code>struct&lt;/code> module:&lt;/p>
&lt;pre tabindex="0">&lt;code>import struct
def store_time(t1, t2):
rtc.memory(struct.pack(&amp;#39;ll&amp;#39;, t1, t2))
def load_time():
data = rtc.memory()
if not data:
return [0, 0]
try:
return struct.unpack(&amp;#39;ll&amp;#39;, data)
except ValueError:
return [0, 0]
&lt;/code>&lt;/pre>&lt;p>Once we&amp;rsquo;re able to store and retrieve data from the RTC, the main part
of our code ends up looking something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>lastrun_1, lastrun_2 = load_time()
now = time.time()
something_happened = False
if lastrun_1 == 0 or (now - lastrun_1 &amp;gt; 60):
read_sensor_1()
lastrun_1 = now
something_happened = True
if lastrun_2 == 0 or (now - lastrun_2 &amp;gt; 300):
read_sensor_2()
lastrun_2 = now
something_happened = True
if something_happened:
store_time(lastrun_1, lastrun_2)
deepsleep(60000)
&lt;/code>&lt;/pre>&lt;p>This code will wake up every 60 seconds. That means it will always run
the &lt;code>read_sensor_1&lt;/code> task, and it will run the &lt;code>read_sensor_2&lt;/code> task
every five minutes. In between, the ESP8266 will be in deep sleep
mode, consuming around 20µA. In order to avoid too many unnecessary
writes to RTC memory, we only store values when &lt;code>lastrun_1&lt;/code> or
&lt;code>lastrun_2&lt;/code> has changed.&lt;/p>
&lt;p>While developing your code, it can be inconvenient to have the device
enter deep sleep mode (because you can&amp;rsquo;t just &lt;code>^C&lt;/code> to return to the
REPL). You can make the deep sleep behavior optional by wrapping
everything in a loop, and optionally calling &lt;code>deepsleep&lt;/code> at the end of
the loop, like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>lastrun_1, lastrun_2 = load_time()
while True:
now = time.time()
something_happened = False
if lastrun_1 == 0 or (now - lastrun_1 &amp;gt; 60):
read_sensor_1()
lastrun_1 = now
something_happened = True
if lastrun_2 == 0 or (now - lastrun_2 &amp;gt; 300):
read_sensor_2()
lastrun_2 = now
something_happened = True
if something_happened:
store_time(lastrun_1, lastrun_2)
if use_deep_sleep:
deepsleep(60000)
else:
machine.idle()
&lt;/code>&lt;/pre>&lt;p>If the variable &lt;code>use_deepsleep&lt;/code> is &lt;code>True&lt;/code>, this code will perform as
described in the previous section, waking once every 60 seconds. If
&lt;code>use_deepsleep&lt;/code> is &lt;code>False&lt;/code>, this will use a busy loop.&lt;/p></content></item><item><title>Animating a map of Covid in the Northeast US</title><link>https://blog.oddbit.com/post/2020-12-13-animating-a-map-of-covid-in-th/</link><pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-12-13-animating-a-map-of-covid-in-th/</guid><description>I recently put together a short animation showing the spread of Covid throughout the Northeast United States:
I thought it might be interesting to walk through the process I used to create the video. The steps described in this article aren&amp;rsquo;t exactly what I used (I was dealing with data in a PostGIS database, and in the interests of simplicity I wanted instructions that can be accomplished with just QGIS), but they end up in the same place.</description><content>&lt;p>I recently put together a short animation showing the spread of Covid
throughout the Northeast United States:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/zGN_zEzd_TE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>I thought it might be interesting to walk through the process I used to
create the video. The steps described in this article aren&amp;rsquo;t exactly
what I used (I was dealing with data in a &lt;a href="https://postgis.net/">PostGIS&lt;/a> database, and in
the interests of simplicity I wanted instructions that can be
accomplished with just QGIS), but they end up in the same place.&lt;/p>
&lt;h2 id="data-sources">Data sources&lt;/h2>
&lt;p>Before creating the map, I had to find appropriate sources of data. I
needed three key pieces of information:&lt;/p>
&lt;ol>
&lt;li>State and county outlines&lt;/li>
&lt;li>Information about population by county&lt;/li>
&lt;li>Information about Covid cases over time by county&lt;/li>
&lt;/ol>
&lt;h3 id="us-census-data">US Census Data&lt;/h3>
&lt;p>I was able to obtain much of the data from the US Census website,
&lt;a href="https://data.census.gov">https://data.census.gov&lt;/a>. Here I was able to find both tabular
demographic data (population information) and geographic data (state
and county cartographic borders):&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/totals/co-est2019-alldata.csv">Population estimates&lt;/a>&lt;/p>
&lt;p>This dataset contains population estimates by county from 2010
through 2019. This comes from the US Census &amp;ldquo;&lt;a href="https://www.census.gov/programs-surveys/popest.html">Population Estimates
Program&lt;/a>&amp;rdquo; (PEP).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_county_5m.zip">County outlines&lt;/a>&lt;/p>
&lt;p>This dataset contains US county outlines provided by the US Census.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_state_5m.zip">State outlines&lt;/a>&lt;/p>
&lt;p>This dataset contains US state outlines provided by the US Census.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The tabular data is provided in &lt;a href="https://en.wikipedia.org/wiki/Comma-separated_values">CSV&lt;/a> (comma-separated value)
format, which is a simple text-only format that can be read by a
variety of software (including spreadsheet software such as Excel or
Google Sheets).&lt;/p>
&lt;p>The geographic data is available as both a &lt;a href="https://en.wikipedia.org/wiki/Shapefile">shapefile&lt;/a> and as a
&lt;a href="https://en.wikipedia.org/wiki/Keyhole_Markup_Language">KML&lt;/a> file. A &lt;em>shapefile&lt;/em> is a relatively standard format for
exchanging geographic data. You generally need some sort of &lt;a href="https://en.wikipedia.org/wiki/Geographic_information_system">GIS
software&lt;/a> in order to open and manipulate a shapefile (a topic that
I will cover later on in this article). KML is another format for
sharing geographic data that was developed by Google as part of Google
Earth.&lt;/p>
&lt;h3 id="new-york-times-covid-data">New York Times Covid Data&lt;/h3>
&lt;p>The New York Times maintains a &lt;a href="https://github.com/nytimes/covid-19-data">Covid dataset&lt;/a> (because our
government is both unable and unwilling to perform this basic public
service) in CSV format that tracks Covid cases and deaths in the
United States, broken down both by state and by county.&lt;/p>
&lt;h2 id="software">Software&lt;/h2>
&lt;p>In order to build something like this map you need a Geographic
Information System (GIS) software package. The 800 pound gorilla of
GIS software is &lt;a href="https://www.esri.com/en-us/arcgis/about-arcgis/overview">ArcGIS&lt;/a>, a capable but pricey commercial package
that may cost more than the casual GIS user is willing to pay.
Fortunately, there are some free alternatives available.&lt;/p>
&lt;p>Google&amp;rsquo;s &lt;a href="https://www.google.com/earth/versions/#earth-pro">Google Earth Pro&lt;/a> has a different focus from most other
GIS software (it is designed more for exploration/educational use than
actual GIS work), but it is able to open and display a variety of GIS
data formats, including the shapefiles used in this project.&lt;/p>
&lt;p>&lt;a href="https://qgis.org/en/site/">QGIS&lt;/a> is a highly capable &lt;a href="https://www.redhat.com/en/topics/open-source/what-is-open-source">open source&lt;/a> GIS package, available
for free for a variety of platforms including MacOS, Windows, and
Linux. This is the software that I used to create the animated map,
and the software we&amp;rsquo;ll be working with in the rest of this article.&lt;/p>
&lt;h2 id="preparing-the-data">Preparing the data&lt;/h2>
&lt;h3 id="geographic-filtering">Geographic filtering&lt;/h3>
&lt;p>I was initially planning on creating a map for the entire United
States, but I immediately ran into a problem: with over 3,200 counties
in the US and upwards of 320 data points per county in the Covid
dataset, that was going to result in over 1,000,000 geographic
features. On my computer, QGIS wasn&amp;rsquo;t able to handle a dataset of
that size. So the first step is limiting the data we&amp;rsquo;re manipulating
to something smaller; I chose New York and New England.&lt;/p>
&lt;p>We start by adding the &lt;code>cb_2018_us_state_5m&lt;/code> map to QGIS. This gives
us all 50 states (and a few territories):&lt;/p>
&lt;figure class="left" >
&lt;img src="states-unfiltered.png" />
&lt;/figure>
&lt;p>To limit this to our target geography, we can select &amp;ldquo;Filter&amp;hellip;&amp;rdquo; from
the layer context menu and apply the following filter:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;#34;NAME&amp;#34; in (
&amp;#39;New York&amp;#39;,
&amp;#39;Massachusetts&amp;#39;,
&amp;#39;Rhode Island&amp;#39;,
&amp;#39;Connecticut&amp;#39;,
&amp;#39;New Hampshire&amp;#39;,
&amp;#39;Vermont&amp;#39;,
&amp;#39;Maine&amp;#39;
)
&lt;/code>&lt;/pre>&lt;p>This gives us:&lt;/p>
&lt;figure class="left" >
&lt;img src="states-filtered.png" />
&lt;/figure>
&lt;p>Next, we need to load in the county outlines that cover the same
geographic area. We start by adding the &lt;code>cb_2018_us_county_5m&lt;/code>
dataset to QGIS, which gets us:&lt;/p>
&lt;figure class="left" >
&lt;img src="counties-unfiltered.png" />
&lt;/figure>
&lt;p>There are several ways we could limit the counties to just those in
our target geography. One method is to use the &amp;ldquo;Clip&amp;hellip;&amp;rdquo; feature in
the &amp;ldquo;Vector-&amp;gt;Geoprocessing Tools&amp;rdquo; menu. This allows to &amp;ldquo;clip&amp;rdquo; one
vector layer (such as our county outlines) using another layer (our
filtered state layer).&lt;/p>
&lt;p>We select &amp;ldquo;Vector-&amp;gt;Geoprocessing Tools-&amp;gt;Clip&amp;hellip;&amp;rdquo;, and then fill
in in the resulting dialog as follows:&lt;/p>
&lt;ul>
&lt;li>For &amp;ldquo;Input layer&amp;rdquo;, select &lt;code>cb_2018_us_county_5m&lt;/code>.&lt;/li>
&lt;li>For &amp;ldquo;Overlay layer&amp;rdquo;, select &lt;code>cb_2018_us_state_5m&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Now select the &amp;ldquo;Run&amp;rdquo; button. You should end up with a new layer named
&lt;code>Clipped&lt;/code>. Hide the original &lt;code>cb_2018_us_county_5m&lt;/code> layer, and rename
&lt;code>Clipped&lt;/code> to &lt;code>cb_2018_us_county_5m_clipped&lt;/code>. This gives us:&lt;/p>
&lt;figure class="left" >
&lt;img src="counties-clipped.png" />
&lt;/figure>
&lt;p>Instead of using the &amp;ldquo;Clip&amp;hellip;&amp;rdquo; algorithm, we could have created a
&lt;a href="https://docs.qgis.org/3.16/en/docs/user_manual/managing_data_source/create_layers.html#creating-virtual-layers">virtual layer&lt;/a> and performed a &lt;a href="http://wiki.gis.com/wiki/index.php/Spatial_Join#:~:text=A%20Spatial%20join%20is%20a,spatially%20to%20other%20feature%20layers.">spatial join&lt;/a> between the state
and county layers; unfortunately, due to issue &lt;a href="https://github.com/qgis/QGIS/issues/40503">#40503&lt;/a>, it&amp;rsquo;s not
possible to use virtual layers with this dataset (or really any
dataset, if you have numeric data you care about).&lt;/p>
&lt;h3 id="merging-population-data-with-our-geographic-data">Merging population data with our geographic data&lt;/h3>
&lt;p>Add the population estimates to our project. Select &amp;ldquo;Layer-&amp;gt;Add
Layer-&amp;gt;Add Delimited Text Layer&amp;hellip;&amp;rdquo;, find the
&lt;code>co-est2019-alldata.csv&lt;/code> dataset and add it to the project. This layer
doesn&amp;rsquo;t have any geographic data of its own; we need to associate it
with one of our other layers in order to make use of it. We can this
by using a &lt;a href="https://www.qgistutorials.com/en/docs/3/performing_table_joins.html">table join&lt;/a>.&lt;/p>
&lt;p>In order to perform a table join, we need a single field in each layer
that corresponds to a field value in the other layer. The counties
dataset has a &lt;code>GEOID&lt;/code> field that combines the state and county &lt;a href="https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt">FIPS
codes&lt;/a>, but the population dataset has only individual state and
county codes. We can create a new &lt;a href="https://docs.qgis.org/3.16/en/docs/user_manual/working_with_vector/attribute_table.html#creating-a-virtual-field">virtual field&lt;/a> in the population
layer that combines these two values in order to provide an
appropriate target field for the table join.&lt;/p>
&lt;p>Open the attribute table for population layer, and click on the &amp;ldquo;Open
field calculator&amp;rdquo; button (it looks like an abacus). Enter &lt;code>geoid&lt;/code> for
the field name, select the &amp;ldquo;Create virtual field&amp;rdquo; checkbox, and select
&amp;ldquo;Text (string)&amp;rdquo; for the field type. In the &amp;ldquo;Expression&amp;rdquo; field, enter:&lt;/p>
&lt;pre tabindex="0">&lt;code>lpad(to_string(&amp;#34;STATE&amp;#34;), 2, &amp;#39;0&amp;#39;) || lpad(to_string(&amp;#34;COUNTY&amp;#34;), 3, &amp;#39;0&amp;#39;)
&lt;/code>&lt;/pre>
&lt;figure class="left" >
&lt;img src="create-virtual-field.png" />
&lt;/figure>
&lt;p>When you return the to attribute table, you will see a new &lt;code>geoid&lt;/code>
field that contains our desired value. We can now perform the table
join.&lt;/p>
&lt;p>Open the properties for the &lt;code>cb_2018_us_county_5m_clipped&lt;/code> layer we
created earlier, and select the &amp;ldquo;Joins&amp;rdquo; tab. Click on the &amp;ldquo;+&amp;rdquo; button.
For &amp;ldquo;Join layer&amp;rdquo;, select &lt;code>co-est2019-alldata&lt;/code>. Select &lt;code>geoid&lt;/code> for
&amp;ldquo;Join field&amp;rdquo; and &lt;code>GEOID&lt;/code> for target field. Lastly, select the &amp;ldquo;Custom
field name prefix&amp;rdquo; checkbox and enter &lt;code>pop_&lt;/code> in the field, then click
&amp;ldquo;OK&amp;rdquo;.&lt;/p>
&lt;figure class="left" >
&lt;img src="county-join-population.png" />
&lt;/figure>
&lt;p>If you examine the attribute table for the layer, you will see the
each county feature is now linked to the appropriate population
data for that county.&lt;/p>
&lt;h3 id="merging-covid-data-with-our-geographic-data">Merging Covid data with our geographic data&lt;/h3>
&lt;p>This is another table join operation, but the process is going to be a
little different. The previous process assumes a 1-1 mapping between
features in the layers being joined, but the Covid dataset has many
data points for each county. We need a solution that will produce the
desired 1-many mapping.&lt;/p>
&lt;p>We can achieve this using the &amp;ldquo;Join attributes by field value&amp;rdquo;
action in the &amp;ldquo;Processing&amp;rdquo; toolbox.&lt;/p>
&lt;p>Start by adding the &lt;code>us-counties.csv&lt;/code> file from the NYT covid dataset
to the project.&lt;/p>
&lt;p>Select &amp;ldquo;Toolbox-&amp;gt;Processing&amp;rdquo; to show the Processing toolbox, if
it&amp;rsquo;s not already visible. In the &amp;ldquo;Search&amp;rdquo; field, enter &amp;ldquo;join&amp;rdquo;, and
then look for &amp;ldquo;Join attributes by field value&amp;rdquo; in the &amp;ldquo;Vector general&amp;rdquo;
section.&lt;/p>
&lt;p>Double click on this to open the input dialog. For &amp;ldquo;Input layer&amp;rdquo;,
select &lt;code>cb_2018_us_county_5m_clipped&lt;/code>, and &amp;ldquo;Table field&amp;rdquo; select
&lt;code>GEOID&lt;/code>. For &amp;ldquo;Input layer 2&amp;rdquo;, select &lt;code>us-counties&lt;/code>, and for &amp;ldquo;Table
field 2&amp;rdquo; select &lt;code>fips&lt;/code>. In the &amp;ldquo;Join type&amp;rdquo; menu, select &amp;ldquo;Create
separate feature for each matching feature (one-to-many)&amp;rdquo;. Ensure the
&amp;ldquo;Discard records which could not be joined&amp;rdquo; is checked. Enter &lt;code>covid_&lt;/code>
in the &amp;ldquo;Joined field prefix [optional]&amp;rdquo; field (this will cause the
fields in the resulting layer to have names like &lt;code>covid_date&lt;/code>,
&lt;code>covid_cases&lt;/code>, etc). Click the &amp;ldquo;Run&amp;rdquo; button to create the new layer.&lt;/p>
&lt;figure class="left" >
&lt;img src="county-join-covid.png" />
&lt;/figure>
&lt;p>You will end up with a new layer named &amp;ldquo;Joined layer&amp;rdquo;. I suggest
renaming this to &lt;code>cb_2018_us_county_5m_covid&lt;/code>. If you enable the &amp;ldquo;show
feature count&amp;rdquo; checkbox for your layers, you will see that while the
&lt;code>cb_2018_us_county_5m_clipped&lt;/code> has 129 features, the new
&lt;code>cb_2018_us_county_5m_covid&lt;/code> layer has over 32,000 features. That&amp;rsquo;s because
for each county, there are around 320 data points tracking Covid cases
(etc) over time.&lt;/p>
&lt;figure class="left" >
&lt;img src="layers-feature-count.png" />
&lt;/figure>
&lt;h2 id="styling">Styling&lt;/h2>
&lt;h3 id="creating-outlines">Creating outlines&lt;/h3>
&lt;p>The only layer on our map that should have filled features will be the
covid data layer. We want to configure our other layers to only
display outlines.&lt;/p>
&lt;p>First, arrange the layers in the following order (from top to bottom):&lt;/p>
&lt;ol>
&lt;li>cb_2018_us_state_5m&lt;/li>
&lt;li>cb_2018_us_county_5m_clipped&lt;/li>
&lt;li>cb_2018_us_county_5m_covid&lt;/li>
&lt;/ol>
&lt;p>The order of the csv layers doesn&amp;rsquo;t matter, and if you still have the
original &lt;code>cb_2018_us_county_5m&lt;/code> layer in your project it should be
hidden.&lt;/p>
&lt;p>Configure the state layer to display outlines. Right click on the
layer and select &amp;ldquo;Properties&amp;rdquo;, then select the &amp;ldquo;Symbology&amp;rdquo; tab. Click
on the &amp;ldquo;Simple Fill&amp;rdquo; item at the top, then in the &amp;ldquo;Symbol layer type&amp;rdquo;
menu select &amp;ldquo;Simple Line&amp;rdquo;. Set the stroke width to 0.66mm.&lt;/p>
&lt;p>As long as we&amp;rsquo;re here, let&amp;rsquo;s also enable labels for the state layer.
Select the &amp;ldquo;Labels&amp;rdquo; tab, then set the menu at the top to &amp;ldquo;Single
Labels&amp;rdquo;. Set the &amp;ldquo;Value&amp;rdquo; field to &amp;ldquo;Name&amp;rdquo;. Click the &amp;ldquo;Apply&amp;rdquo; button to
show the labels on the map without closing the window; now adjust the
font size (and click &amp;ldquo;Apply&amp;rdquo; again) until things look the way you
want. To make the labels a bit easier to read, select the &amp;ldquo;Buffer&amp;rdquo;
panel, and check the &amp;ldquo;Draw text buffer&amp;rdquo; checkbox.&lt;/p>
&lt;p>Now do the same thing (except don&amp;rsquo;t enable labels) with the
&lt;code>cb_2018_us_county_5m_clipped&lt;/code> layer, but set the stroke width to
0.46mm.&lt;/p>
&lt;p>If you hide the the Covid layer, your map should look like this (don&amp;rsquo;t
forget to unhide the Covid layer for the next step):&lt;/p>
&lt;figure class="left" >
&lt;img src="map-outlines.png" />
&lt;/figure>
&lt;h3 id="creating-graduated-colors">Creating graduated colors&lt;/h3>
&lt;p>Open the properties for the &lt;code>cb_2018_us_county_5m_covid&lt;/code> layer, and
select the &amp;ldquo;Symbology&amp;rdquo; tab. At the top of the symbology panel is a
menu currently set to &amp;ldquo;Single Symbol&amp;rdquo;. Set this to &amp;ldquo;Graduated&amp;rdquo;.&lt;/p>
&lt;p>Open the expression editor for the &amp;ldquo;Value&amp;rdquo; field, and set it to:&lt;/p>
&lt;pre tabindex="0">&lt;code>(to_int(&amp;#34;cases&amp;#34;) / &amp;#34;pop_POPESTIMATE2019&amp;#34;) * 1000000
&lt;/code>&lt;/pre>&lt;p>Set the &amp;ldquo;Color ramp&amp;rdquo; to &amp;ldquo;Spectral&amp;rdquo;, and then select &amp;ldquo;Invert color
ramp&amp;rdquo;.&lt;/p>
&lt;p>Ensure the &amp;ldquo;Mode&amp;rdquo; menu is set to &amp;ldquo;Equal count (Quantile)&amp;rdquo;, and then
set &amp;ldquo;Classes&amp;rdquo; to 15. This will give a set of graduated categories that
looks like this:&lt;/p>
&lt;figure class="left" >
&lt;img src="graduated-categories.png" />
&lt;/figure>
&lt;p>Close the properties window. Your map should look something like this:&lt;/p>
&lt;figure class="left" >
&lt;img src="map-graduated-1.png" />
&lt;/figure>
&lt;p>That&amp;rsquo;s not very exciting yet, is it? Let&amp;rsquo;s move on to the final
section of this article.&lt;/p>
&lt;h2 id="animating-the-data">Animating the data&lt;/h2>
&lt;p>For this final step, we need to enable the QGIS &lt;a href="https://plugins.qgis.org/plugins/timemanager/">TimeManager&lt;/a>
plugin. Install the TimeManager plugin if it&amp;rsquo;s not already installed:
open the plugin manager (&amp;ldquo;Plugins-&amp;gt;Manage and Install Plugins&amp;hellip;&amp;rdquo;),
and ensure both that TimeManager is installed and that it is enabled
(the checkbox to the left of the plugin name is checked).&lt;/p>
&lt;p>Return to the project and open the TimeManger panel: select
&amp;ldquo;Plugins-&amp;gt;TimeManager-&amp;gt;Toggle visbility&amp;rdquo;. This will display the
following panel below the map:&lt;/p>
&lt;figure class="left" >
&lt;img src="timemanager-panel-initial.png" />
&lt;/figure>
&lt;p>Make sure that the &amp;ldquo;Time frame size&amp;rdquo; is set to &amp;ldquo;1 days&amp;rdquo;.&lt;/p>
&lt;p>Click the &amp;ldquo;Settings&amp;rdquo; button to open the TimeManager settings window,
then select the &amp;ldquo;Add layer&amp;rdquo; button. In the resulting window, select
the &lt;code>cb_2018_us_county_5m_covid&lt;/code> layer in the &amp;ldquo;Layer&amp;rdquo; menu, the select
the &lt;code>covid_date&lt;/code> column in the &amp;ldquo;Start time&amp;rdquo; menu. Leave all other
values at their defaults and click &amp;ldquo;OK&amp;rdquo; to return to the TimeManager
settings.&lt;/p>
&lt;figure class="left" >
&lt;img src="timemanager-add-layer.png" />
&lt;/figure>
&lt;p>You will see the layer we just added listed in the &amp;ldquo;Layers&amp;rdquo; list. Look
for the &amp;ldquo;Time Format&amp;rdquo; column in this list, which will say &amp;ldquo;TO BE
INFERRED&amp;rdquo;. Click in this column and change the value to &lt;code>%Y-%m-%d&lt;/code> to
match the format of the dates in the &lt;code>covid_date&lt;/code> field.&lt;/p>
&lt;figure class="left" >
&lt;img src="timemanager-settings-final.png" />
&lt;/figure>
&lt;p>You may want to change &amp;ldquo;Show frame for&amp;rdquo; setting from the default to
something like 50 milliseconds. Leave everything else at the defaults
and click the &amp;ldquo;OK&amp;rdquo; button.&lt;/p>
&lt;p>Ensure that the TimeManager is enabled by clicking on the &amp;ldquo;power
button&amp;rdquo; in the TimeManager panel. TimeManager is enabled when the
power button is green.&lt;/p>
&lt;p>Disabled:&lt;/p>
&lt;figure class="left" >
&lt;img src="timemanager-disabled.png" />
&lt;/figure>
&lt;p>Enabled:&lt;/p>
&lt;figure class="left" >
&lt;img src="timemanager-enabled.png" />
&lt;/figure>
&lt;p>Once TimeManager is enabled, you should be able to use the slider to
view the map at different times. For example, here&amp;rsquo;s the map in early
May:&lt;/p>
&lt;figure class="left" >
&lt;img src="timemanager-early-may.png" />
&lt;/figure>
&lt;p>And here it is in early November:&lt;/p>
&lt;figure class="left" >
&lt;img src="timemanager-early-november.png" />
&lt;/figure>
&lt;p>To animate the map, click the play button in the bottom left of the
TimeManager panel.&lt;/p>
&lt;p>You can export the animation to a video using the &amp;ldquo;Export Video&amp;rdquo;
button. Assuming that you have &lt;a href="https://ffmpeg.org/">ffmpeg&lt;/a> installed, you can select an
output directory, select the &amp;ldquo;Video (required ffmpeg &amp;hellip;)&amp;rdquo; button,
then click &amp;ldquo;OK&amp;rdquo;. You&amp;rsquo;ll end up with (a) a PNG format image file for
each frame and (b) a file named &lt;code>out.mp4&lt;/code> containing the exported
video.&lt;/p>
&lt;h2 id="datasets">Datasets&lt;/h2>
&lt;p>I have made all the data referenced in this post available at
&lt;a href="https://github.com/larsks/ne-covid-map">https://github.com/larsks/ne-covid-map&lt;/a>.&lt;/p></content></item><item><title>A note about running gpgv</title><link>https://blog.oddbit.com/post/2020-10-05-a-note-about-running-gpgv/</link><pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-10-05-a-note-about-running-gpgv/</guid><description>I found the following error from gpgv to be a little opaque:
gpgv: unknown type of key resource &amp;#39;trustedkeys.kbx&amp;#39; gpgv: keyblock resource &amp;#39;/home/lars/.gnupg/trustedkeys.kbx&amp;#39;: General error gpgv: Can&amp;#39;t check signature: No public key It turns out that&amp;rsquo;s gpg-speak for &amp;ldquo;your trustedkeys.kbx keyring doesn&amp;rsquo;t exist&amp;rdquo;. That took longer to figure out than I care to admit. To get a key from your regular public keyring into your trusted keyring, you can run something like the following:</description><content>&lt;p>I found the following error from &lt;code>gpgv&lt;/code> to be a little opaque:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpgv: unknown type of key resource &amp;#39;trustedkeys.kbx&amp;#39;
gpgv: keyblock resource &amp;#39;/home/lars/.gnupg/trustedkeys.kbx&amp;#39;: General error
gpgv: Can&amp;#39;t check signature: No public key
&lt;/code>&lt;/pre>&lt;p>It turns out that&amp;rsquo;s gpg-speak for &amp;ldquo;your &lt;code>trustedkeys.kbx&lt;/code> keyring doesn&amp;rsquo;t
exist&amp;rdquo;. That took longer to figure out than I care to admit. To get a key
from your regular public keyring into your trusted keyring, you can run
something like the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>gpg --export -a lars@oddbit.com |
gpg --no-default-keyring --keyring ~/.gnupg/trustedkeys.kbx --import
&lt;/code>&lt;/pre>&lt;p>After which &lt;code>gpgv&lt;/code> works as expected:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ echo hello world | gpg -s -u lars@oddbit.com | gpgv
gpgv: Signature made Mon 05 Oct 2020 07:44:22 PM EDT
gpgv: using RSA key FDE8364F7FEA3848EF7AD3A6042DF6CF74E4B84C
gpgv: issuer &amp;#34;lars@oddbit.com&amp;#34;
gpgv: Good signature from &amp;#34;Lars Kellogg-Stedman &amp;lt;lars@oddbit.com&amp;gt;&amp;#34;
gpgv: aka &amp;#34;keybase.io/larsks &amp;lt;larsks@keybase.io&amp;gt;&amp;#34;
&lt;/code>&lt;/pre></content></item><item><title>Installing metallb on OpenShift with Kustomize</title><link>https://blog.oddbit.com/post/2020-09-27-installing-metallb-on-openshif/</link><pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-09-27-installing-metallb-on-openshif/</guid><description>Out of the box, OpenShift (4.x) on bare metal doesn&amp;rsquo;t come with any integrated load balancer support (when installed in a cloud environment, OpenShift typically makes use of the load balancing features available from the cloud provider). Fortunately, there are third party solutions available that are designed to work in bare metal environments. MetalLB is a popular choice, but requires some minor fiddling to get it to run properly on OpenShift.</description><content>&lt;p>Out of the box, OpenShift (4.x) on bare metal doesn&amp;rsquo;t come with any
integrated load balancer support (when installed in a cloud environment,
OpenShift typically makes use of the load balancing features available from
the cloud provider). Fortunately, there are third party solutions available
that are designed to work in bare metal environments. &lt;a href="https://metallb.universe.tf/">MetalLB&lt;/a> is a
popular choice, but requires some minor fiddling to get it to run properly
on OpenShift.&lt;/p>
&lt;p>If you read through the &lt;a href="https://metallb.universe.tf/installation/">installation instructions&lt;/a>, you will see &lt;a href="https://metallb.universe.tf/installation/clouds/#metallb-on-openshift-ocp">this
note&lt;/a> about installation on OpenShift:&lt;/p>
&lt;blockquote>
&lt;p>To run MetalLB on Openshift, two changes are required: changing the pod
UIDs, and granting MetalLB additional networking privileges.&lt;/p>
&lt;p>Pods get UIDs automatically assigned based on an OpenShift-managed UID
range, so you have to remove the hardcoded unprivileged UID from the
MetalLB manifests. You can do this by removing the
spec.template.spec.securityContext.runAsUser field from both the
controller Deployment and the speaker DaemonSet.&lt;/p>
&lt;p>Additionally, you have to grant the speaker DaemonSet elevated
privileges, so that it can do the raw networking required to make
LoadBalancers work. You can do this with:&lt;/p>
&lt;/blockquote>
&lt;p>The docs here suggest some manual changes you can make, but it&amp;rsquo;s possible
to get everything installed correctly using &lt;a href="https://github.com/kubernetes-sigs/kustomize">Kustomize&lt;/a> (which makes
sense especially given that the MetalLB docs already include instructions
&lt;a href="https://metallb.universe.tf/installation/#installation-with-kustomize">on using Kustomize&lt;/a>).&lt;/p>
&lt;p>A vanilla installation of MetalLB with Kustomize uses a &lt;code>kustomization.yml&lt;/code>
file that looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>namespace: metallb-system
resources:
- github.com/metallb/metallb//manifests?ref=v0.9.3
- configmap.yml
- secret.yml
&lt;/code>&lt;/pre>&lt;p>(Where &lt;code>configmap.yml&lt;/code> and &lt;code>secret.yml&lt;/code> are files you create locally
containing, respectively, the MetalLB configuration and a secret used to
authenticate cluster members.)&lt;/p>
&lt;h2 id="fixing-the-security-context">Fixing the security context&lt;/h2>
&lt;p>In order to remove the &lt;code>runAsUser&lt;/code> directive form the template
&lt;code>securityContext&lt;/code> setting, we can use the &lt;a href="https://kubectl.docs.kubernetes.io/pages/reference/kustomize.html#patchesstrategicmerge">patchesStrategicMerge&lt;/a>
feature. In our &lt;code>kustomization.yml&lt;/code> file we add:&lt;/p>
&lt;pre tabindex="0">&lt;code>patches:
- |-
apiVersion: apps/v1
kind: Deployment
metadata:
name: controller
namespace: metallb-system
spec:
template:
spec:
securityContext:
$patch: replace
runAsNonRoot: true
&lt;/code>&lt;/pre>&lt;p>This instructs &lt;code>kustomize&lt;/code> to replace the contents of the &lt;code>securityContext&lt;/code>
key with the value included in the patch (without the &lt;code>$patch: replace&lt;/code>
directive, the default behavior is to merge the contents, which in this
situation would effectively be a no-op).&lt;/p>
&lt;p>We can accomplish the same thing using &lt;a href="https://tools.ietf.org/html/rfc6902">jsonpatch&lt;/a> syntax. In this case,
we would write:&lt;/p>
&lt;pre tabindex="0">&lt;code>patches:
- target:
kind: Deployment
name: controller
namespace: metallb-system
patch: |-
- op: remove
path: /spec/template/spec/securityContext/runAsUser
&lt;/code>&lt;/pre>&lt;p>With either solution, the final output includes a &lt;code>securityContext&lt;/code> setting
that looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>spec:
template:
spec:
securityContext:
runAsNonRoot: true
&lt;/code>&lt;/pre>&lt;h2 id="granting-elevated-privileges">Granting elevated privileges&lt;/h2>
&lt;p>The MetaLB docs suggest running:&lt;/p>
&lt;pre tabindex="0">&lt;code>oc adm policy add-scc-to-user privileged -n metallb-system -z speaker
&lt;/code>&lt;/pre>&lt;p>But we can configure the same privilege level by setting up an appropriate
role binding as part of our Kustomize manifests.&lt;/p>
&lt;p>First, we create an &lt;code>allow-privileged&lt;/code> cluster role by adding the following
manifest in &lt;code>clusterrole.yml&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: allow-privileged
rules:
- apiGroups:
- security.openshift.io
resourceNames:
- privileged
resources:
- securitycontextconstraints
verbs:
- use
&lt;/code>&lt;/pre>&lt;p>Then we bind the &lt;code>speaker&lt;/code> service account to the &lt;code>allow-privileged&lt;/code> role
by adding a &lt;code>ClusterRoleBinding&lt;/code> in &lt;code>rolebinding.yml&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: metallb-allow-privileged
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: allow-privileged
subjects:
- kind: ServiceAccount
name: speaker
namespace: metallb-system
&lt;/code>&lt;/pre>&lt;p>You will need to add these new manifests to your &lt;code>kustomization.yml&lt;/code>, which
should now look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>namespace: metallb-system
resources:
- github.com/metallb/metallb//manifests?ref=v0.9.3
- configmap.yml
- secret.yml
- clusterole.yml
- rolebinding.yml
patches:
- target:
kind: Deployment
name: controller
namespace: metallb-system
patch: |-
- op: remove
path: /spec/template/spec/securityContext/runAsUser
&lt;/code>&lt;/pre>&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>The changes described here will result in a successful MetalLB deployment
into your OpenShift environment.&lt;/p></content></item><item><title>Vortex Core Keyboard Review</title><link>https://blog.oddbit.com/post/2020-09-26-vortex-core-keyboard-review/</link><pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-09-26-vortex-core-keyboard-review/</guid><description>I&amp;rsquo;ve had my eye on the Vortex Core keyboard for a few months now, and this past week I finally broke down and bought one (with Cherry MX Brown switches). The Vortex Core is a 40% keyboard, which means it consists primarily of letter keys, a few lonely bits of punctuation, and several modifier keys to activate different layers on the keyboard.
Physical impressions It&amp;rsquo;s a really cute keyboard. I&amp;rsquo;m a big fan of MX brown switches, and this keyboard is really a joy to type on, at least when you&amp;rsquo;re working primarily with the alpha keys.</description><content>&lt;p>I&amp;rsquo;ve had my eye on the &lt;a href="http://www.vortexgear.tw/vortex2_2.asp?kind=47&amp;amp;kind2=224&amp;amp;kind3=&amp;amp;kind4=1033">Vortex Core&lt;/a> keyboard for a few months now, and this
past week I finally broke down and bought one (with Cherry MX Brown switches).
The Vortex Core is a 40% keyboard, which means it consists primarily of letter
keys, a few lonely bits of punctuation, and several modifier keys to activate
different layers on the keyboard.&lt;/p>
&lt;h2 id="physical-impressions">Physical impressions&lt;/h2>
&lt;p>It&amp;rsquo;s a really cute keyboard. I&amp;rsquo;m a big fan of MX brown switches, and this
keyboard is really a joy to type on, at least when you&amp;rsquo;re working primarily
with the alpha keys. I&amp;rsquo;m still figuring out where some of the punctuation
is, and with a few exceptions I haven&amp;rsquo;t yet spent time trying to remap
things into more convenient positions.&lt;/p>
&lt;p>The keyboard feels solid. I&amp;rsquo;m a little suspicious of the micro-usb
connector; it feels a little wobbly. I wish that it was USB-C and I wish it
felt a little more stable.&lt;/p>
&lt;p>Here&amp;rsquo;s a picture of my Core next to my Durgod &lt;a href="https://www.amazon.com/DURGOD-Mechanical-Interface-Tenkeyless-Anti-Ghosting/dp/B078H3WPHM">K320&lt;/a>:&lt;/p>
&lt;figure class="left" >
&lt;img src="core-vs-k320.jpg" />
&lt;/figure>
&lt;h2 id="programming">Programming&lt;/h2>
&lt;p>The keyboard first came out in 2017, and if you read reviews that came out
around that time you&amp;rsquo;ll find several complaints around limitations in the
keyboard&amp;rsquo;s programming features, in particular:&lt;/p>
&lt;ul>
&lt;li>you can&amp;rsquo;t map the left and right spacebars differently&lt;/li>
&lt;li>you can&amp;rsquo;t remap layer 0&lt;/li>
&lt;li>you can&amp;rsquo;t remap the Fn1 key&lt;/li>
&lt;/ul>
&lt;p>And so forth. Fortunately, at some point (maybe 2018) Vortexgear released
updated firmware that resolves all of the above issues, and introduces a
completely new way of programming the keyboard.&lt;/p>
&lt;p>Originally, the keyboard was programmed entirely via the keyboard itself: there
was a key combination to activate programming mode in each of the three
programmable layers, and this allowed you to freely remap keys. Unfortunately,
this made it well difficult to share layouts, and made extensive remapping
rather unwieldy.&lt;/p>
&lt;p>The &lt;a href="http://www.vortexgear.tw/db/upload/webdata4/6vortex_201861271445393.exe">updated firmware&lt;/a> (&amp;quot;&lt;code>CORE_MPC&lt;/code>&amp;quot;) does away with the hardware
programming, and instead introduces both a web UI for generating keyboard
layouts and a simple mechanism for pushing those layouts to the keyboard that
is completely operating system independent (which is nice if you&amp;rsquo;re a Linux
user and are tired of having to spin up a Windows VM just to run someone&amp;rsquo;s
firmware programming tool). With the new firmware, you hold down &lt;code>Fn-d&lt;/code> when
booting the keyboard and it will present a FAT-format volume to the operating
system. Drag your layout to the volume, unmount it, and reboot the keyboard and
you&amp;rsquo;re all set (note that you will still need to spin up that Windows VM
one last time in order to install the firmware update).&lt;/p>
&lt;p>The Vortexgear keyboard configurator is available at
&lt;a href="http://www.vortexgear.tw/mpc/index.html">http://www.vortexgear.tw/mpc/index.html&lt;/a>, but you&amp;rsquo;re going to want to use
&lt;a href="https://tsfreddie.github.io/much-programming-core/">https://tsfreddie.github.io/much-programming-core/&lt;/a> instead, which removes
several limitations that are present in the official tool.&lt;/p>
&lt;p>Because the new configurator (a) allows you to remap all layers, including
layer 0, and (b) allows to create mappings for the &lt;code>Pn&lt;/code> key, you have a lot
of flexibility in how you set up your mappings.&lt;/p>
&lt;h3 id="how-ive-configured-things">How I&amp;rsquo;ve configured things&lt;/h3>
&lt;p>I performed some limited remapping of layer 0:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>I&amp;rsquo;ve moved the &lt;code>Fn1&lt;/code> key to the right space bar, and turned the original
&lt;code>Fn1&lt;/code> key into the quote key. I use that enough in general writing that
it&amp;rsquo;s convenient to be able to access it without using a modifier.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I&amp;rsquo;ve set up a cursor cluster using the &lt;code>Pn&lt;/code> key. This gets me the
standard &lt;code>WASD&lt;/code> keys for arrows, and &lt;code>Q&lt;/code> and &lt;code>E&lt;/code> for page up and page
down.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Holding down the &lt;code>Pn&lt;/code> key also gets me a numeric keypad on the right side
of the keyboard.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="final-thoughts">Final thoughts&lt;/h2>
&lt;p>It&amp;rsquo;s a fun keyboard. I&amp;rsquo;m not sure it&amp;rsquo;s going to become my primary keyboard,
especially for writing code, but I&amp;rsquo;m definitely happy with it.&lt;/p></content></item><item><title>Building multi-architecture images with GitHub Actions</title><link>https://blog.oddbit.com/post/2020-09-25-building-multi-architecture-im/</link><pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-09-25-building-multi-architecture-im/</guid><description>At work we have a cluster of IBM Power 9 systems running OpenShift. The problem with this environment is that nobody runs Power 9 on their desktop, and Docker Hub only offers automatic build support for the x86 architecture. This means there&amp;rsquo;s no convenient options for building Power 9 Docker images&amp;hellip;or so I thought.
It turns out that Docker provides GitHub actions that make the process of producing multi-architecture images quite simple.</description><content>&lt;p>At work we have a cluster of IBM Power 9 systems running OpenShift. The
problem with this environment is that nobody runs Power 9 on their desktop,
and Docker Hub only offers automatic build support for the x86
architecture. This means there&amp;rsquo;s no convenient options for building Power 9
Docker images&amp;hellip;or so I thought.&lt;/p>
&lt;p>It turns out that &lt;a href="https://github.com/docker">Docker&lt;/a> provides &lt;a href="https://github.com/features/actions">GitHub actions&lt;/a> that make the process
of producing multi-architecture images quite simple.&lt;/p>
&lt;p>The code demonstrated in this post can be found in my &lt;a href="https://github.com/larsks/hello-flask">hello-flask&lt;/a>
GitHub repository.&lt;/p>
&lt;h2 id="configuring-secrets">Configuring secrets&lt;/h2>
&lt;p>There is some information we need to provide to our workflow that we don&amp;rsquo;t
want to hardcode into configuration files, both for reasons of security (we
don&amp;rsquo;t want to expose passwords in the repository) and convenience (we want
other people to be able to fork this repository and run the workflow
without needing to make any changes to the code).&lt;/p>
&lt;p>We can do this by configuring &amp;ldquo;secrets&amp;rdquo; in the repository on GitHub. You
can configure secrets by visiting the &amp;ldquo;Secrets&amp;rdquo; tab in your repository
settings (&lt;code>https://github.com/&amp;lt;USERNAME&amp;gt;/&amp;lt;REPOSITORY&amp;gt;/settings/secrets&lt;/code>),&lt;/p>
&lt;p>For this workflow, we&amp;rsquo;re going to need two secrets:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>DOCKER_USERNAME&lt;/code> &amp;ndash; this is our Docker Hub username; we&amp;rsquo;ll need this
both for authentication and to set the namespace for the images we&amp;rsquo;re
building.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>DOCKER_PASSWORD&lt;/code> &amp;ndash; this is our Docker Hub password, used for
authentication.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Within a workflow, we can refer to these secrets using syntax like &lt;code>${{ secrets.DOCKER_USERNAME }}&lt;/code> (you&amp;rsquo;ll see example of this later on).&lt;/p>
&lt;h2 id="creating-a-workflow">Creating a workflow&lt;/h2>
&lt;p>In the repository containing your &lt;code>Dockerfile&lt;/code>, create a
&lt;code>.github/workflows&lt;/code> directory. This is where we will place the files that
configure GitHub actions. In this directory, create a file called
&lt;code>build_images.yml&lt;/code> (the particular name isn&amp;rsquo;t important, but it&amp;rsquo;s nice to
make names descriptive).&lt;/p>
&lt;p>We&amp;rsquo;ll first give this workflow a name and configure it to run for pushes on
our &lt;code>master&lt;/code> branch by adding the following to our &lt;code>build_images.yml&lt;/code> file:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
name: &amp;#39;build images&amp;#39;
on:
push:
branches:
- master
&lt;/code>&lt;/pre>&lt;h2 id="setting-up-jobs">Setting up jobs&lt;/h2>
&lt;p>With that boilerplate out of the way, we can start configuring the jobs
that will comprise our workflow. Jobs are defined in the &lt;code>jobs&lt;/code> section of
the configuration file, which is a dictionary that maps job names to their
definition. A job can have multiple actions. For this example, we&amp;rsquo;re going
to set up a &lt;code>docker&lt;/code> job that will perform the following steps:&lt;/p>
&lt;ul>
&lt;li>check out the repository&lt;/li>
&lt;li>prepare some parameters&lt;/li>
&lt;li>set up qemu, which is used to provide emulated environments for
building on architecture other than the host arch&lt;/li>
&lt;li>configure the docker builders&lt;/li>
&lt;li>authenticate to docker hub&lt;/li>
&lt;li>build and push the images to docker hub&lt;/li>
&lt;/ul>
&lt;p>We start by providing a name for our job and configuring the machine on
which the jobs will run. In this example, we&amp;rsquo;re using &lt;code>ubuntu-latest&lt;/code>;
other options include some other Ubuntu variants, Windows, and MacOS (and
you are able to host your own custom builders, but that&amp;rsquo;s outside the scope
of this article).&lt;/p>
&lt;pre tabindex="0">&lt;code>jobs:
docker:
runs-on: ubuntu-latest
steps:
&lt;/code>&lt;/pre>&lt;h3 id="checking-out-the-repository">Checking out the repository&lt;/h3>
&lt;p>In our first step, we use the standard &lt;a href="https://github.com/actions/checkout">actions/checkout&lt;/a>
action to check out the repository:&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Checkout
uses: actions/checkout@v2
&lt;/code>&lt;/pre>&lt;h3 id="preparing-parameters">Preparing parameters&lt;/h3>
&lt;p>The next step is a simple shell script that sets some output parameters we
will be able to consume in subsequent steps. A script can set parameters by
generating output in the form:&lt;/p>
&lt;pre tabindex="0">&lt;code>::set-output name=&amp;lt;name&amp;gt;::&amp;lt;value&amp;gt;
&lt;/code>&lt;/pre>&lt;p>In other steps, we can refer to these parameters using the syntax
&lt;code>${{ steps.&amp;lt;step_name&amp;gt;.output.&amp;lt;name&amp;gt; }}&lt;/code> (e.g. &lt;code>${{ steps.prep.output.tags }}&lt;/code>).&lt;/p>
&lt;p>We&amp;rsquo;re going to use this step to set things like the image name (using our
&lt;code>DOCKER_USERNAME&lt;/code> secret to set the namespace), and to set up several tags
for the image:&lt;/p>
&lt;ul>
&lt;li>By default, we tag it &lt;code>latest&lt;/code>&lt;/li>
&lt;li>If we&amp;rsquo;re building from a git tag, use the tag name instead of &lt;code>latest&lt;/code>.
Note that here we&amp;rsquo;re assuming that git tags are of the form &lt;code>v1.0&lt;/code>, so we
strip off that initial &lt;code>v&lt;/code> to get a Docker tag that is just the version
number.&lt;/li>
&lt;li>We also tag the image with the short commit id&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> - name: Prepare
id: prep
run: |
DOCKER_IMAGE=${{ secrets.DOCKER_USERNAME }}/${GITHUB_REPOSITORY#*/}
VERSION=latest
SHORTREF=${GITHUB_SHA::8}
# If this is git tag, use the tag name as a docker tag
if [[ $GITHUB_REF == refs/tags/* ]]; then
VERSION=${GITHUB_REF#refs/tags/v}
fi
TAGS=&amp;#34;${DOCKER_IMAGE}:${VERSION},${DOCKER_IMAGE}:${SHORTREF}&amp;#34;
# If the VERSION looks like a version number, assume that
# this is the most recent version of the image and also
# tag it &amp;#39;latest&amp;#39;.
if [[ $VERSION =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]]; then
TAGS=&amp;#34;$TAGS,${DOCKER_IMAGE}:latest&amp;#34;
fi
# Set output parameters.
echo ::set-output name=tags::${TAGS}
echo ::set-output name=docker_image::${DOCKER_IMAGE}
&lt;/code>&lt;/pre>&lt;h3 id="set-up-qemu">Set up QEMU&lt;/h3>
&lt;p>The &lt;a href="https://github.com/docker/setup-qemu-action">docker/setup-qemu&lt;/a> action installs QEMU &lt;a href="https://wiki.debian.org/QemuUserEmulation">static binaries&lt;/a>, which
are used to run builders for architectures other than the host.&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Set up QEMU
uses: docker/setup-qemu-action@master
with:
platforms: all
&lt;/code>&lt;/pre>&lt;h3 id="set-up-docker-builders">Set up Docker builders&lt;/h3>
&lt;p>The &lt;a href="https://github.com/docker/setup-buildx-action">docker/setup-buildx&lt;/a> action configures &lt;a href="https://github.com/docker/buildx">buildx&lt;/a>, which is a Docker
CLI plugin that provides enhanced build capabilities. This is the
infrastructure that the following step will use for actually building
images.&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Set up Docker Buildx
id: buildx
uses: docker/setup-buildx-action@master
&lt;/code>&lt;/pre>&lt;h3 id="authenticate-to-docker-hub">Authenticate to Docker Hub&lt;/h3>
&lt;p>In order to push images to Docker Hub, we use the &lt;a href="https://github.com/docker/login-action">docker/login-action&lt;/a>
action to authenticate. This uses the &lt;code>DOCKER_USERNAME&lt;/code> and
&lt;code>DOCKER_PASSWORD&lt;/code> secrets we created earlier in order to establish
credentials for use in subsequent steps.&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Login to DockerHub
if: github.event_name != &amp;#39;pull_request&amp;#39;
uses: docker/login-action@v1
with:
username: ${{ secrets.DOCKER_USERNAME }}
password: ${{ secrets.DOCKER_PASSWORD }}
&lt;/code>&lt;/pre>&lt;h3 id="build-and-push-the-images">Build and push the images&lt;/h3>
&lt;p>This final step uses the [docker/build-push-action][] to build the images
and push them to Docker Hub using the tags we defined in the &lt;code>prep&lt;/code> step.
In this example, we&amp;rsquo;re building images for &lt;code>amd64&lt;/code>, &lt;code>arm64&lt;/code>, and &lt;code>ppc64le&lt;/code>
architectures.&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: Build
uses: docker/build-push-action@v2
with:
builder: ${{ steps.buildx.outputs.name }}
context: .
file: ./Dockerfile
platforms: linux/amd64,linux/arm64,linux/ppc64le
push: true
tags: ${{ steps.prep.outputs.tags }}
&lt;/code>&lt;/pre>&lt;h2 id="the-complete-workflow">The complete workflow&lt;/h2>
&lt;p>When we put all of the above together, we get:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
name: &amp;#39;build images&amp;#39;
on:
push:
branches:
- master
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Checkout
uses: actions/checkout@v2
- name: Prepare
id: prep
run: |
DOCKER_IMAGE=${{ secrets.DOCKER_USERNAME }}/${GITHUB_REPOSITORY#*/}
VERSION=latest
SHORTREF=${GITHUB_SHA::8}
# If this is git tag, use the tag name as a docker tag
if [[ $GITHUB_REF == refs/tags/* ]]; then
VERSION=${GITHUB_REF#refs/tags/v}
fi
TAGS=&amp;#34;${DOCKER_IMAGE}:${VERSION},${DOCKER_IMAGE}:${SHORTREF}&amp;#34;
# If the VERSION looks like a version number, assume that
# this is the most recent version of the image and also
# tag it &amp;#39;latest&amp;#39;.
if [[ $VERSION =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$ ]]; then
TAGS=&amp;#34;$TAGS,${DOCKER_IMAGE}:latest&amp;#34;
fi
# Set output parameters.
echo ::set-output name=tags::${TAGS}
echo ::set-output name=docker_image::${DOCKER_IMAGE}
- name: Set up QEMU
uses: docker/setup-qemu-action@master
with:
platforms: all
- name: Set up Docker Buildx
id: buildx
uses: docker/setup-buildx-action@master
- name: Login to DockerHub
if: github.event_name != &amp;#39;pull_request&amp;#39;
uses: docker/login-action@v1
with:
username: ${{ secrets.DOCKER_USERNAME }}
password: ${{ secrets.DOCKER_PASSWORD }}
- name: Build
uses: docker/build-push-action@v2
with:
builder: ${{ steps.buildx.outputs.name }}
context: .
file: ./Dockerfile
platforms: linux/amd64,linux/arm64,linux/ppc64le
push: true
tags: ${{ steps.prep.outputs.tags }}
&lt;/code>&lt;/pre>&lt;p>You can grab the &lt;a href="https://github.com/larsks/hello-flask">hello-flask&lt;/a> repository and try this out yourself.
You&amp;rsquo;ll need to set up the secrets described earlier in this article, but
then for each commit to the &lt;code>master&lt;/code> branch you will end up a new image,
tagged both as &lt;code>latest&lt;/code> and with the short git commit id.&lt;/p>
&lt;h2 id="the-results">The results&lt;/h2>
&lt;p>We can use the &lt;code>docker manifest inspect&lt;/code> command to inspect the output of
the build step. In the output below, you can see the images build for our
three target architectures:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ docker manifest inspect !$
docker manifest inspect larsks/hello-flask
{
&amp;#34;schemaVersion&amp;#34;: 2,
&amp;#34;mediaType&amp;#34;: &amp;#34;application/vnd.docker.distribution.manifest.list.v2+json&amp;#34;,
&amp;#34;manifests&amp;#34;: [
{
&amp;#34;mediaType&amp;#34;: &amp;#34;application/vnd.docker.distribution.manifest.v2+json&amp;#34;,
&amp;#34;size&amp;#34;: 3261,
&amp;#34;digest&amp;#34;: &amp;#34;sha256:c6bab778a9fd0dc7bf167a5a49281bcd5ebc5e762ceeb06791aff8f0fbd15325&amp;#34;,
&amp;#34;platform&amp;#34;: {
&amp;#34;architecture&amp;#34;: &amp;#34;amd64&amp;#34;,
&amp;#34;os&amp;#34;: &amp;#34;linux&amp;#34;
}
},
{
&amp;#34;mediaType&amp;#34;: &amp;#34;application/vnd.docker.distribution.manifest.v2+json&amp;#34;,
&amp;#34;size&amp;#34;: 3261,
&amp;#34;digest&amp;#34;: &amp;#34;sha256:3c02f36562fcf8718a369a78054750382aba5706e1e9164b76bdc214591024c4&amp;#34;,
&amp;#34;platform&amp;#34;: {
&amp;#34;architecture&amp;#34;: &amp;#34;arm64&amp;#34;,
&amp;#34;os&amp;#34;: &amp;#34;linux&amp;#34;
}
},
{
&amp;#34;mediaType&amp;#34;: &amp;#34;application/vnd.docker.distribution.manifest.v2+json&amp;#34;,
&amp;#34;size&amp;#34;: 3262,
&amp;#34;digest&amp;#34;: &amp;#34;sha256:192fc9acd658edd6b7f2726f921cba2582fb1101d929800dff7fb53de951dd76&amp;#34;,
&amp;#34;platform&amp;#34;: {
&amp;#34;architecture&amp;#34;: &amp;#34;ppc64le&amp;#34;,
&amp;#34;os&amp;#34;: &amp;#34;linux&amp;#34;
}
}
]
}
&lt;/code>&lt;/pre>&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>This process assumes, of course, that your base image of choice is available for your selected architectures. &lt;a href="https://docs.docker.com/docker-for-mac/multi-arch/">According to Docker&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Most of the official images on Docker Hub provide a variety of architectures.
For example, the busybox image supports amd64, arm32v5, arm32v6, arm32v7,
arm64v8, i386, ppc64le, and s390x.&lt;/p>
&lt;/blockquote>
&lt;p>So if you are starting from one of the official images, you&amp;rsquo;ll probably be in good shape. On the other hand, if you&amp;rsquo;re attempting to use a community image as a starting point, you might find that it&amp;rsquo;s only available for a single architecture.&lt;/p></content></item><item><title>OpenShift and CNV: MAC address management in CNV 2.4</title><link>https://blog.oddbit.com/post/2020-08-10-mac-address-management-in-cnv/</link><pubDate>Mon, 10 Aug 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-08-10-mac-address-management-in-cnv/</guid><description>This is part of a series of posts about my experience working with OpenShift and CNV. In this post, I&amp;rsquo;ll look at how the recently released CNV 2.4 resolves some issues in managing virtual machines that are attached directly to local layer 2 networks
In an earlier post, I discussed some issues around the management of virtual machine MAC addresses in CNV 2.3: in particular, that virtual machines are assigned a random MAC address not just at creation time but every time they boot.</description><content>&lt;p>This is part of a &lt;a href="https://blog.oddbit.com/tag/openshift-and-cnv">series of posts&lt;/a> about my experience working with
&lt;a href="https://www.openshift.com/">OpenShift&lt;/a> and &lt;a href="https://www.redhat.com/en/topics/containers/what-is-container-native-virtualization">CNV&lt;/a>. In this post, I&amp;rsquo;ll look at how the
recently released CNV 2.4 resolves some issues in managing virtual
machines that are attached directly to local layer 2 networks&lt;/p>
&lt;p>In &lt;a href="https://blog.oddbit.com/post/2020-07-30-openshift-and-cnv-part-2-expos/">an earlier post&lt;/a>, I discussed some issues around the
management of virtual machine MAC addresses in CNV 2.3: in particular,
that virtual machines are assigned a random MAC address not just at
creation time but every time they boot. CNV 2.4 (re-)introduces &lt;a href="https://docs.openshift.com/container-platform/4.5/virt/virtual_machines/vm_networking/virt-using-mac-address-pool-for-vms.html">MAC
address pools&lt;/a> to alleviate these issues. The high level description
reads:&lt;/p>
&lt;blockquote>
&lt;p>The KubeMacPool component provides a MAC address pool service for
virtual machine NICs in designated namespaces.&lt;/p>
&lt;/blockquote>
&lt;p>In more specific terms, that means that if you enable MAC address
pools on a namespace, when you create create virtual machine network
interfaces they will receive a MAC address from the pool. This is
associated with the &lt;code>VirtualMachine&lt;/code> resource, &lt;strong>not&lt;/strong> the
&lt;code>VirtualMachineInstance&lt;/code> resource, which means that the MAC address
will persist across reboots.&lt;/p>
&lt;p>This solves one of the major pain points of using CNV-managed virtual
machines attached to host networks.&lt;/p>
&lt;p>To enable MAC address pools for a given namespace, set the
&lt;code>mutatevirtualmachines.kubemacpool.io&lt;/code> label to &lt;code>allocate&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>oc label namespace &amp;lt;namespace&amp;gt; mutatevirtualmachines.kubemacpool.io=allocate
&lt;/code>&lt;/pre></content></item><item><title>OpenShift and CNV: Exposing virtualized services</title><link>https://blog.oddbit.com/post/2020-07-30-openshift-and-cnv-part-2-expos/</link><pubDate>Thu, 30 Jul 2020 01:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-07-30-openshift-and-cnv-part-2-expos/</guid><description>This is the second in a series of posts about my experience working with OpenShift and CNV. In this post, I&amp;rsquo;ll be taking a look at how to expose services on a virtual machine once you&amp;rsquo;ve git it up and running.
TL;DR Overview Connectivity options Direct attachment Using an OpenShift Service Exposing services on NodePorts Exposing services on cluster external IPso Exposing services using a LoadBalancer TL;DR Networking seems to be a weak area for CNV right now.</description><content>&lt;p>This is the second in a &lt;a href="https://blog.oddbit.com/tag/openshift-and-cnv">series of posts&lt;/a> about my experience working
with &lt;a href="https://www.openshift.com/">OpenShift&lt;/a> and &lt;a href="https://www.redhat.com/en/topics/containers/what-is-container-native-virtualization">CNV&lt;/a>. In this post, I&amp;rsquo;ll be taking a look
at how to expose services on a virtual machine once you&amp;rsquo;ve git it up
and running.&lt;/p>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#tldr">TL;DR&lt;/a>&lt;/li>
&lt;li>&lt;a href="#overview">Overview&lt;/a>&lt;/li>
&lt;li>&lt;a href="#connectivity-options">Connectivity options&lt;/a>&lt;/li>
&lt;li>&lt;a href="#direct-attachment">Direct attachment&lt;/a>&lt;/li>
&lt;li>&lt;a href="#using-an-openshift-service">Using an OpenShift Service&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#exposing-services-on-nodeports">Exposing services on NodePorts&lt;/a>&lt;/li>
&lt;li>&lt;a href="#exposing-services-on-cluster-external-ipso">Exposing services on cluster external IPso&lt;/a>&lt;/li>
&lt;li>&lt;a href="#exposing-services-using-a-loadbalancer">Exposing services using a LoadBalancer&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>Networking seems to be a weak area for CNV right now. Out of the box,
your options for exposing a service on a virtual machine on a public
address at a well known port are slim.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>We&amp;rsquo;re hoping to use OpenShift + CNV as an alternative to existing
hypervisor platforms, primarily to reduce the number of complex,
distributed projects we need to manage. If we can have a single
control plane for both containerized and virtualized workloads, it
seems like a win for everyone.&lt;/p>
&lt;p>In order to support the most common use case for our virtualization
platforms, consumers of this service need to be able to:&lt;/p>
&lt;ul>
&lt;li>Start a virtual machine using an image of their choice&lt;/li>
&lt;li>Expose services on that virtual machine using well-known ports
on a routeable ip address&lt;/li>
&lt;/ul>
&lt;p>All of the above should be self service (that is, none of those steps
should requiring opening a support ticket or otherwise require
administrative assistance).&lt;/p>
&lt;h2 id="connectivity-options">Connectivity options&lt;/h2>
&lt;p>There are broadly two major connectivity models available to CNV
managed virtual machines:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="#direct-attachment">Direct attachment to a host network&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#using-an-openshift-service">Using an OpenShift Service&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>We&amp;rsquo;re going to start with the direct attachment model, since this may
be familiar to people coming to CNV from other hypervisor platforms.&lt;/p>
&lt;h2 id="direct-attachment">Direct attachment&lt;/h2>
&lt;p>With a little configuration, it is possible to attach virtual machines
directly to an existing layer two network.&lt;/p>
&lt;p>When running CNV, you can affect the network configuration of your
OpenShift hosts by creating &lt;code>NodeNetworkConfigurationPolicy&lt;/code>
objects. Support for this is provided by &lt;code>nmstate&lt;/code>, which is packaged
with CNV. For details, see &amp;ldquo;&lt;a href="https://docs.openshift.com/container-platform/4.4/cnv/cnv_node_network/cnv-updating-node-network-config.html">Updating node network configuration&lt;/a>&amp;rdquo; in
the OpenShift documentation.&lt;/p>
&lt;p>For example, if we want to create a bridge interface on our nodes to
permit CNV managed virtual machines to attach to the network
associated with interface &lt;code>eth1&lt;/code>, we might submit the following
configuration:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: nmstate.io/v1alpha1
kind: NodeNetworkConfigurationPolicy
metadata:
name: br-example-policy
spec:
nodeSelector:
node-role.kubernetes.io/worker: &amp;#34;&amp;#34;
desiredState:
interfaces:
- name: br-example
type: linux-bridge
state: up
ipv4:
dhcp: true
enabled: true
bridge:
options:
stp:
enabled: false
port:
- name: eth1
&lt;/code>&lt;/pre>&lt;p>This would create a Linux bridge device &lt;code>br-example&lt;/code> with interface
&lt;code>eth1&lt;/code> as a member. In order to expose this bridge to virtual
machines, we need to create a &lt;code>NetworkAttachmentDefinition&lt;/code> (which can
be abbreviated as &lt;code>net-attach-def&lt;/code>, but not as &lt;code>nad&lt;/code> for reasons that
may be obvious to English speakers or readers of Urban Dictionary).&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
name: example
namespace: default
spec:
config: &amp;gt;-
{
&amp;#34;name&amp;#34;: &amp;#34;example&amp;#34;,
&amp;#34;cniVersion&amp;#34;: &amp;#34;0.3.1&amp;#34;,
&amp;#34;plugins&amp;#34;: [
{
&amp;#34;type&amp;#34;: &amp;#34;cnv-bridge&amp;#34;,
&amp;#34;bridge&amp;#34;: &amp;#34;br-example&amp;#34;,
&amp;#34;ipam&amp;#34;: {}
},
{
&amp;#34;type&amp;#34;: &amp;#34;cnv-tuning&amp;#34;
}
]
}
&lt;/code>&lt;/pre>&lt;p>Once you have the above definitions in place, it&amp;rsquo;s easy to select this
network when adding interfaces to a virtual machine. Actually making
use of these connections can be a little difficult.&lt;/p>
&lt;p>In a situation that may remind of you of &lt;a href="https://blog.oddbit.com/post/2020-07-30-openshift-and-cnv-part-1-worki/">some issues we had with the
installer&lt;/a>, your virtual machine will boot with a randomly
generated MAC address. Under CNV, generated MAC addresses are
associated with &lt;code>VirtualMachineInstance&lt;/code> resources, which represents
currently running virtual machines. Your &lt;code>VirtualMachine&lt;/code> object is
effectively a template used to generate a new &lt;code>VirtualMachineInstance&lt;/code>
each time it boots. Because the address is associated with the
&lt;em>instance&lt;/em>, you get a new MAC address every time you boot the virtual
machine. That makes it very difficult to associate a static IP address
with your CNV managed virtual machine.&lt;/p>
&lt;p>It is possible to manually assign a MAC address to the virtual machine
when you create, but now you have a bevy of new problems:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Anybody who wants to deploy a virtual machine needs to know what a
MAC address looks like (you laugh, but this isn&amp;rsquo;t something people
generally have to think about).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You probably need some way to track MAC address allocation to avoid
conflicts when everyone chooses &lt;code>DE:AD:BE:EF:CA:FE&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="using-an-openshift-service">Using an OpenShift Service&lt;/h2>
&lt;p>Out of the box, your virtual machines can attach to the default pod
network, which is private network that provides masqueraded outbound
access and no direct inbound access. In this situation, your virtual
machine behaves much more like a container from a network perspective,
and you have access to many of the same network primitives available
to pods. You access these mechanisms by creating an OpenShift
&lt;code>Service&lt;/code> resource.&lt;/p>
&lt;p>Under OpenShift, a &lt;code>Service&lt;/code> is used to &amp;ldquo;expose an application running
on a set of &lt;code>Pods&lt;/code> as a network service (from &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">the Kubernetes
documentation&lt;/a>&amp;rdquo;. From the perspective of OpenShift, your
virtual machine is just another application running in a Pod, so we
can use Service resources to expose applications running on your
virtual machine.&lt;/p>
&lt;p>In order to manage these options, you&amp;rsquo;ll want to install the
&lt;code>virtctl&lt;/code> client. You can grab an &lt;a href="https://github.com/kubevirt/kubevirt/releases">upstream release&lt;/a> from the
&lt;a href="https://github.com/kubevirt/kubevirt">kubevirt&lt;/a> project, or you can &lt;a href="https://docs.openshift.com/container-platform/4.2/cnv/cnv_install/cnv-installing-virtctl.html">enable the appropriate
repositories&lt;/a> and install the &lt;code>kubevirt-virtctl&lt;/code> package.&lt;/p>
&lt;h3 id="exposing-services-on-nodeports">Exposing services on NodePorts&lt;/h3>
&lt;p>A &lt;code>NodePort&lt;/code> lets you expose a service on a random port associated
with the ip addresses of your OpenShift nodes. If you have a virtual
machine named &lt;code>test-vm-1&lt;/code> and you want to access the SSH service on
port 22, you can use the &lt;code>virtctl&lt;/code> command like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>virtctl expose vm test-vm-1 --port=22 --name=myvm-ssh-np --type=NodePort
&lt;/code>&lt;/pre>&lt;p>This will result in &lt;code>Service&lt;/code> that looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ oc get service myvm-ssh-np
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
myvm-ssh-np NodePort 172.30.4.25 &amp;lt;none&amp;gt; 22:31424/TCP 42s
&lt;/code>&lt;/pre>&lt;p>The &lt;code>CLUSTER-IP&lt;/code> in the above output is a cluster internal IP address
that can be used to connect to your server from other containers or
virtual machines. The &lt;code>22:31424/TCP&lt;/code> entry tells us that port &lt;code>31424&lt;/code>
on our OpenShift hosts now maps to port &lt;code>22&lt;/code> in our virtual machine.&lt;/p>
&lt;p>You can connect to your virtual machine with an &lt;code>ssh&lt;/code> command line
along the lines of:&lt;/p>
&lt;pre tabindex="0">&lt;code>ssh -p 31424 someuser@hostname.of.a.node
&lt;/code>&lt;/pre>&lt;p>You can use the hostname of any node in your OpenShift cluster.&lt;/p>
&lt;p>This is fine for testing things out, but it doesn&amp;rsquo;t allow you to
expose services on a well known port, and the cluster administrator
may be uncomfortable with services like this using the ip addresses of
cluster hosts.&lt;/p>
&lt;h3 id="exposing-services-on-cluster-external-ipso">Exposing services on cluster external IPso&lt;/h3>
&lt;p>It is possible to manually assign an external ip address to an
OpenShift service. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>virtctl expose vm test-vm-1 --port 22 --name myvm-ssh-ext --external-ip 192.168.185.18
&lt;/code>&lt;/pre>&lt;p>Which results in the follow service:&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
myvm-ssh-ext ClusterIP 172.30.224.127 192.168.185.18 22/TCP 47s
&lt;/code>&lt;/pre>&lt;p>While this sounds promising at first, there are several caveats:&lt;/p>
&lt;ul>
&lt;li>We once again find ourselves needing to manually manage a pool of
addresses.&lt;/li>
&lt;li>By default, assigning an external ip address requires cluster-admin
privileges.&lt;/li>
&lt;li>Once an external ip is assigned to a service, OpenShift doesn&amp;rsquo;t
actually take care of configuring that address on any host
interfaces: it is up to the local administrator to arrange for
traffic to that address to arrive at the cluster.&lt;/li>
&lt;/ul>
&lt;p>The practical impact of setting an external ip on a service is to
instantiate netfilter rules equivalent to the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>-d 192.168.185.18/32 -p tcp --dport 22 -j DNAT --to-destination 10.129.2.11:22
&lt;/code>&lt;/pre>&lt;p>If you configure the address &lt;code>192.168.185.18&lt;/code> on a host interface (or
otherwise arrange for traffic to that address to reach your host),
these rules take care of directing the connection to your virtual
machine.&lt;/p>
&lt;h3 id="exposing-services-using-a-loadbalancer">Exposing services using a LoadBalancer&lt;/h3>
&lt;p>Historically, OpenShift was designed to run in cloud environments such
as OpenStack, AWS, Google Cloud Engine, and so forth. These platforms
provide integrated load balancer mechanisms that OpenShift was able to
leverage to expose services. Creating a &lt;code>LoadBalancer&lt;/code> service would
instruct the platform to (a) allocate an address, (b) create a load
balancer, and (c) direct traffic from the load balancer to the target
of your service.&lt;/p>
&lt;p>We can request a &lt;code>LoadBalancer&lt;/code> using &lt;code>virtctl&lt;/code> like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>virtctl expose vm test-vm-1 --port=22 --name=myvm-ssh-np --type=LoadBalancer
&lt;/code>&lt;/pre>&lt;p>Unfortunately, OpenShift for baremetal hosts does not include a load
balancer out of the box. This is a shame, because the &lt;code>LoadBalancer&lt;/code>
solution hits just about all of our requirements:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It automatically assigns ip addresses from a configured pool, so
consumers of the environment don&amp;rsquo;t need to manage either ip- or
MAC-address assignment on their own.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It doesn&amp;rsquo;t require special privileges or administrator intervention
(other than for the initial configuration).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It lets you expose services on ports of your choice, rather than
random ports.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>There are some solutions out there that will provide an integrated
load balancer implementation for your baremetal cluster. I&amp;rsquo;ve looked
at:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/redhat-cop/keepalived-operator">keepalived-operator&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://metallb.universe.tf/">metallb&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>I hope we see an integrated LoadBalancer mechanism available for OpenShift on
baremetal in a near-future release.&lt;/p></content></item><item><title>OpenShift and CNV: Installer network requirements</title><link>https://blog.oddbit.com/post/2020-07-30-openshift-and-cnv-part-1-worki/</link><pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-07-30-openshift-and-cnv-part-1-worki/</guid><description>This is the first in a series of posts about my experience working with OpenShift and CNV (&amp;ldquo;Container Native Virtualization&amp;rdquo;, a technology that allows you to use OpenShift to manage virtualized workloads in addition to the containerized workloads for which OpenShift is known). In this post, I&amp;rsquo;ll be taking a look at the installation experience, and in particular at how restrictions in our local environment interacted with the network requirements of the installer.</description><content>&lt;p>This is the first in a &lt;a href="https://blog.oddbit.com/tag/openshift-and-cnv">series of posts&lt;/a> about my experience working
with &lt;a href="https://www.openshift.com/">OpenShift&lt;/a> and &lt;a href="https://www.redhat.com/en/topics/containers/what-is-container-native-virtualization">CNV&lt;/a> (&amp;ldquo;Container Native Virtualization&amp;rdquo;, a
technology that allows you to use OpenShift to manage virtualized
workloads in addition to the containerized workloads for which
OpenShift is known). In this post, I&amp;rsquo;ll be taking a look at the
installation experience, and in particular at how restrictions in our
local environment interacted with the network requirements of the installer.&lt;/p>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#overview">Overview&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-problem">The problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#attempted-solution-1">Attempted solution #1&lt;/a>&lt;/li>
&lt;li>&lt;a href="#attempted-solution-2">Attempted solution #2&lt;/a>&lt;/li>
&lt;li>&lt;a href="#how-we-actually-solved-the-problem">How we actually solved the problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#what-i-would-like-to-see">What I would like to see&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>We&amp;rsquo;re installing OpenShift on baremetal hosts using the IPI installer.
&amp;ldquo;IPI&amp;rdquo; stands for &amp;ldquo;Installer Provisioned Infrastructure&amp;rdquo;, which means
that the OpenShift installer is responsible for provisioning an
operating system onto your hardware and managing the system
configuration. This is in contrast to UPI (&amp;ldquo;User Provisioned
Infrastructure&amp;rdquo;), in which you pre-provision the hosts using whatever
tools you&amp;rsquo;re comfortable with and then point the installer and the
hardware once things are up and running.&lt;/p>
&lt;p>In the environment I&amp;rsquo;m working with, we had a few restrictions that I
suspect are relatively common:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The network we were using as our &amp;ldquo;baremetal&amp;rdquo; network (for the
purposes of this article you can read that as &amp;ldquo;public&amp;rdquo; network) does
not have a dynamic pool of leases. There is DHCP, but all addresses
are statically assigned.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Both the installer and the &lt;a href="https://metal3.io/">Metal3&lt;/a> service use &lt;a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface">IPMI&lt;/a> to manage
the power of the OpenShift nodes. Access to our IPMI network
requires that a static route exist on the host.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Access to the IPMI network also requires a firewall exception for
the host IP address.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>When you&amp;rsquo;re reading through the installer documentation, the above
requirements don&amp;rsquo;t seem problematic at first. Looking at the
&lt;a href="https://openshift-kni.github.io/baremetal-deploy/4.4/Deployment.html#network-requirements_ipi-install-prerequisites">network requirements&lt;/a>, you&amp;rsquo;ll see that the install calls for static
addressing of all the hardware involved in the install:&lt;/p>
&lt;blockquote>
&lt;p>Reserving IP Addresses for Nodes with the DHCP Server&lt;/p>
&lt;p>For the baremetal network, a network administrator must reserve a
number of IP addresses, including:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Three virtual IP addresses.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>1 IP address for the API endpoint&lt;/p>
&lt;/li>
&lt;li>
&lt;p>1 IP address for the wildcard ingress endpoint&lt;/p>
&lt;/li>
&lt;li>
&lt;p>1 IP address for the name server&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>One IP Address for the Provisioning node.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>One IP address for each Control Plane (Master) node.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>One IP address for each worker node.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>The &amp;ldquo;provisioning node&amp;rdquo; is the host on which you run the OpenShift
installer. What the documentation fails to mention is that the
services that manage the install don&amp;rsquo;t actually run on the
provisioning node itself: instead, the installer starts up a
&amp;ldquo;bootstrap virtual machine&amp;rdquo; on the provisioning node, and manages the
install from there.&lt;/p>
&lt;h2 id="the-problem">The problem&lt;/h2>
&lt;p>The bootstrap vm is directly attached to both the baremetal and the
provisioning networks. It is created with a random MAC address, and
relies on DHCP for configuring the baremetal interface. This means
that:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It&amp;rsquo;s not possible to create a static DHCP lease for it, since you
don&amp;rsquo;t know the MAC address ahead of time.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Since you can&amp;rsquo;t create a static DHCP lease, you can&amp;rsquo;t give it a
static IP address.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Since you can&amp;rsquo;t give it a static IP address, you can&amp;rsquo;t create a
firewall exception for access to the IPMI network.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>And lastly, since you can&amp;rsquo;t create a static DHCP lease, you can&amp;rsquo;t
conveniently use DHCP to assign the static route to the IPMI
network.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>This design decision &amp;ndash; the use of a bootstrap vm with a random MAC
address and no facility for assigning a static ip address &amp;ndash; is what
complicated our lives when we first set out to install OpenShift.&lt;/p>
&lt;p>I&amp;rsquo;d like to emphasize that other than the issues discussed in the
remainder of this article, the install process has been relatively
smooth. We&amp;rsquo;re able to go from zero to a completely installed OpenShift
cluster in just a few hours. There were some documentation issues
early on, but I think most of those have already been resolved.&lt;/p>
&lt;h2 id="attempted-solution-1">Attempted solution #1&lt;/h2>
&lt;p>OpenShift uses &lt;a href="https://github.com/coreos/ignition">Ignition&lt;/a> for performing host configuration tasks.
If you&amp;rsquo;re familiar with &lt;a href="https://cloudinit.readthedocs.io/en/latest/">cloud-init&lt;/a>, Ignition is doing something
very similar. One of the first things we tried was passing in a static
network configuration using Ignition. By running
&lt;code>openshift-baremetal-install create ignition-configs&lt;/code>, it&amp;rsquo;s possible
to modify the ignition configuration passed into the bootstrap vm.
Unfortunately, it turns out that prior to loading the ignition
configuration, the bootstrap vm image will attempt to configure all
system interfaces using DHCP&amp;hellip;and if it fails to acquire any
addresses, it just gives up.&lt;/p>
&lt;p>In that case, it never gets as far as attempting to apply the ignition
configuration, so this option didn&amp;rsquo;t work out.&lt;/p>
&lt;h2 id="attempted-solution-2">Attempted solution #2&lt;/h2>
&lt;p>It is possible to pass a static ip configuration into the bootstrap vm
by modifying the kernel command line parameters. There are several
steps involved in creating a custom image:&lt;/p>
&lt;ul>
&lt;li>Parse through a JSON file to get URLs for the relevant images&lt;/li>
&lt;li>Download the images&lt;/li>
&lt;li>Uncompress the bootstrap image&lt;/li>
&lt;li>Use &lt;code>virt-edit&lt;/code> to modify the grub configuration&lt;/li>
&lt;li>Calculate the uncompressed image checksum&lt;/li>
&lt;li>Re-compress the image&lt;/li>
&lt;/ul>
&lt;p>This also requires configuring your &lt;code>install-config.yaml&lt;/code> to use the
new image, and finding an appropriate place to host it.&lt;/p>
&lt;p>This mechanism &lt;em>does&lt;/em> work, but there are a lot of moving parts and in
particular it seems like modifying the grub configuration could be a
little tricky if the command line in the original image were to change
in unexpected ways.&lt;/p>
&lt;h2 id="how-we-actually-solved-the-problem">How we actually solved the problem&lt;/h2>
&lt;p>We ended up taking advantage of the fact that while we didn&amp;rsquo;t know the
MAC address ahead of time, we &lt;em>did&lt;/em> know the MAC address &lt;em>prefix&lt;/em>
ahead of time, so we created a small dynamic range (6 addresses)
limited to that MAC prefix (which would match pretty much anything
started by libvirt, but the only libvirt managed virtual machines
attached to this network were OpenShift bootstrap vms). We were able
to (a) attach the static route declaration to this small dynamic
range, and (b) grant firewall exceptions for these specific addresses.
The relevant lines in our &lt;a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq&lt;/a> configuration look something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>dhcp-host=52:54:00:*:*:*,set:libvirt,set:ocp
dhcp-range=tag:libvirt,10.1.2.130,10.1.2.135,255.255.255.0
dhcp-option=tag:ocp,option:classless-static-route,10.0.0.0/19,10.1.2.101
&lt;/code>&lt;/pre>&lt;p>It&amp;rsquo;s not perfect, but it&amp;rsquo;s working fine.&lt;/p>
&lt;h2 id="what-i-would-like-to-see">What I would like to see&lt;/h2>
&lt;p>The baremetal installer should allow the deployer to pass in a
static address configuration for the bootstrap vm using the
&lt;code>install-config.yaml&lt;/code> file. The bootstrap vm should continue to boot
even if it can&amp;rsquo;t initially configure an interface using DHCP (one
should be able to disable that initial DHCP attempt).&lt;/p></content></item><item><title>Grove Beginner Kit for Arduino (part 2): First look</title><link>https://blog.oddbit.com/post/2020-06-07-first-look-seed-grove-beginner/</link><pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-06-07-first-look-seed-grove-beginner/</guid><description>The folks at Seeed Studio were kind enough to send me a Grove Beginner Kit for Arduino for review. That&amp;rsquo;s a mouthful of a name for a compact little kit!
The Grove Beginner Kit for Arduino (henceforth &amp;ldquo;the Kit&amp;rdquo;, because ain&amp;rsquo;t nobody got time to type that out more than a few times in a single article) is about 8.5 x 5 x 1 inches. Closed, you could fit two of them on a piece of 8.</description><content>&lt;p>The folks at &lt;a href="https://seeedstudio.com">Seeed Studio&lt;/a> were kind enough to send me a &lt;a href="https://www.seeedstudio.com/Grove-Beginner-Kit-for-Arduino-p-4549.html">Grove
Beginner Kit for Arduino&lt;/a> for review. That&amp;rsquo;s a mouthful of a name
for a compact little kit!&lt;/p>
&lt;p>The Grove Beginner Kit for Arduino (henceforth &amp;ldquo;the Kit&amp;rdquo;, because ain&amp;rsquo;t
nobody got time to type that out more than a few times in a single
article) is about 8.5 x 5 x 1 inches. Closed, you could fit two of
them on a piece of 8.5x11 paper with a little room leftover.&lt;/p>
&lt;figure class="left" >
&lt;img src="grove-closed.jpg" />
&lt;/figure>
&lt;p>Opening the top of the box, we see the board itself. The kit is targeted
straight at the STEM market, and as we&amp;rsquo;ll in the following paragraphs there are
a number of features that make it particularly appropriate for this niche.&lt;/p>
&lt;figure class="left" >
&lt;img src="grove-open.jpg" />
&lt;/figure>
&lt;p>The kit comes with a wide variety of sensors, inputs, and outputs (see
the &lt;a href="#sensors">Sensors&lt;/a> and &lt;a href="#inputoutput">Input/Output&lt;/a> sections,
below, for an overview). To make it easy to get started, everything
you see is pre-wired (via traces on the PCB) to the microcontroller.
That means you don&amp;rsquo;t need soldering or connection cables to use
anything on the board.&lt;/p>
&lt;p>Every sensor also has a &lt;a href="https://wiki.seeedstudio.com/Grove_System/">Grove&lt;/a> connector on it, and the main board
&amp;ndash; in addition to Arduino-compatible headers &amp;ndash; has 12 Grove
connectors. The individual sensor boards are designed so that you can
cut them off the PCB and use them on their own. If you open the
left-hand side of the kit, you&amp;rsquo;ll find a collection of six Grove
connection cables if you choose to go this route.&lt;/p>
&lt;p>Using the Grove connectors makes it very easy to add additional
sensors to your projects (it looks like there are about &lt;a href="https://www.seeedstudio.com/category/Sensor-for-Grove-c-24.html.">150 sensor
modules available&lt;/a> in the Seeed Studio store).&lt;/p>
&lt;figure class="left" >
&lt;img src="grove-lh.jpg" />
&lt;/figure>
&lt;p>The right-hand side of the case contains a micro USB cable for power
and for connecting the kit to your computer for programming using the
Arduino IDE.&lt;/p>
&lt;figure class="left" >
&lt;img src="grove-rh.jpg" />
&lt;/figure>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h2 id="sensors">Sensors&lt;/h2>
&lt;p>The Kit ships with a variety of sensors. In addition to simple light
and sound sensors, the kit also has:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>DHT11 Temperature and Humidity Sensor&lt;/p>
&lt;p>The &lt;a href="DHT11-Technical-Data-Sheet.pdf">DHT11&lt;/a> is the less expensive cousin of the DHT22. The
DHT11 can measure temperature from 0-50 °C with an accuracy of ±2 °C,
and humidity from 20-80% with an accuracy of ±5% (the DHT22, in
comparison, has a slightly wider ranger for both temperature and
humidity, and substantially better accuracy).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BMP280 Air Pressure Sensor&lt;/p>
&lt;p>The &lt;a href="Grove-Barometer_Sensor-BMP280-BMP280-DS001-12_Datasheet.pdf">BMP280&lt;/a> can measure pressure from -500 m below sea level to 9000 m
above sea level, with a relative accuracy of ±1 m (that&amp;rsquo;s 300-1100 hPa
with a relative accuracy of ±0.12 hPa). &amp;ldquo;Relative
accuracy&amp;rdquo; means that it&amp;rsquo;s good at detecting pressure &lt;em>changes&lt;/em>, but
it&amp;rsquo;s less accurate if your goal is to read the absolute air pressure
(the accuracy in that case is ±1 hPa, or roughly ±8 m).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LIS3DHTR 3-Axis Accelerometer&lt;/p>
&lt;p>The &lt;a href="LIS3DHTR_datasheet.pdf">LIS3DHTR&lt;/a> has user-selectable scales of ±2 g/±4 g/±8 g/±16 g and
is capable of measuring accelerations with output data rates from 1
Hz to 5.3 kHz.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="inputoutput">Input/Output&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>OLED Display&lt;/p>
&lt;p>There&amp;rsquo;s a 0.96&amp;quot; I2C addressable &lt;a href="OLED_Display_Module.pdf">OLED Display Module&lt;/a> built around
the SSD1315 chip. You can see this display in action &lt;a href="#video">at the bottom
of this post&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LED&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Buzzer&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Button&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Potentiometer&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="lcd-demo">LCD Demo&lt;/h2>
&lt;p>In the next couple of weeks I hope to do something more interesting
with this board, but as a first step I thought it would be fun to
scroll something across the LCD screen. I slapped the following code
together after reading the &lt;a href="https://github.com/olikraus/u8g2/wiki/u8g2reference">u8g2 docs&lt;/a> for a couple of minutes, so I
can guarantee that it&amp;rsquo;s not optimal, but it&amp;rsquo;s a start:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;Arduino.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;U8g2lib.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#ifdef U8X8_HAVE_HW_SPI
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;SPI.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#endif
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#ifdef U8X8_HAVE_HW_I2C
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#include&lt;/span> &lt;span style="color:#75715e">&amp;lt;Wire.h&amp;gt;&lt;/span>&lt;span style="color:#75715e">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#endif
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>U8G2_SSD1306_128X64_NONAME_F_SW_I2C &lt;span style="color:#a6e22e">u8g2&lt;/span>(U8G2_R2, &lt;span style="color:#75715e">/* clock=*/&lt;/span> SCL, &lt;span style="color:#75715e">/* data=*/&lt;/span> SDA, &lt;span style="color:#75715e">/* reset=*/&lt;/span> U8X8_PIN_NONE);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">char&lt;/span> &lt;span style="color:#f92672">*&lt;/span>text &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#e6db74">&amp;#34;blog.oddbit.com&amp;#34;&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">setup&lt;/span>(&lt;span style="color:#66d9ef">void&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u8g2.&lt;span style="color:#a6e22e">setBusClock&lt;/span>(&lt;span style="color:#ae81ff">4000000&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u8g2.&lt;span style="color:#a6e22e">begin&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u8g2.&lt;span style="color:#a6e22e">setFontMode&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">loop&lt;/span>(&lt;span style="color:#66d9ef">void&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">u8g2_uint_t&lt;/span> x;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u8g2.&lt;span style="color:#a6e22e">firstPage&lt;/span>();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">do&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u8g2.&lt;span style="color:#a6e22e">setFont&lt;/span>(u8g2_font_luBIS18_tf);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> u8g2.&lt;span style="color:#a6e22e">drawStr&lt;/span>(x, &lt;span style="color:#ae81ff">40&lt;/span>, text);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> x &lt;span style="color:#f92672">-=&lt;/span> &lt;span style="color:#ae81ff">10&lt;/span>;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> } &lt;span style="color:#66d9ef">while&lt;/span> (u8g2.&lt;span style="color:#a6e22e">nextPage&lt;/span>());
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- raw HTML omitted --></content></item><item><title>Grove Beginner Kit for Arduino (part 1)</title><link>https://blog.oddbit.com/post/2020-04-15-grove-beginner-kit-for-arduino/</link><pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-04-15-grove-beginner-kit-for-arduino/</guid><description>The folks at Seeed Studio have just released the Grove Beginner Kit for Arduino, and they asked if I would be willing to take a look at it in exchange for a free kit. At first glance it reminds me of the Radio Shack (remember when they were cool?) electronics kit I had when I was a kid &amp;ndash; but somewhat more advanced. I&amp;rsquo;m excited to take a closer look, but given shipping these days means it&amp;rsquo;s probably a month away at least.</description><content>&lt;p>The folks at &lt;a href="https://www.seeedstudio.com/">Seeed Studio&lt;/a> have just released the &lt;a href="https://www.seeedstudio.com/Grove-Beginner-Kit-for-Arduino-p-4549.html">Grove Beginner Kit for
Arduino&lt;/a>, and they asked if I would be willing to take a look at it in
exchange for a free kit. At first glance it reminds me of the Radio Shack
(remember when they were cool?) electronics kit I had when I was a kid &amp;ndash; but
somewhat more advanced. I&amp;rsquo;m excited to take a closer look, but given shipping
these days means it&amp;rsquo;s probably a month away at least.&lt;/p>
&lt;figure class="left" >
&lt;img src="grove-beginner-kit.png" />
&lt;/figure>
&lt;p>An interesting feature of the kit is that while everything is hardwired
together (so you can use it &amp;ldquo;out of the box&amp;rdquo; without any jumper wires, etc),
most of the components can be snapped out so that they can be used on their
own. Once separated, components can be connected using a Grove connector on
each piece.&lt;/p>
&lt;p>The kit includes:&lt;/p>
&lt;ul>
&lt;li>A LED&lt;/li>
&lt;li>A buzzer&lt;/li>
&lt;li>A 0.96&amp;quot; OLED Display 0.96&amp;quot;&lt;/li>
&lt;li>A button&lt;/li>
&lt;li>A rotary potentiometer&lt;/li>
&lt;li>A light&lt;/li>
&lt;li>A sound sensor&lt;/li>
&lt;li>A DHT11 temperature and humidity sensor&lt;/li>
&lt;li>A BMP280 air pressure sensor&lt;/li>
&lt;li>A 3LIS3DHTR 3-axis accelerometer&lt;/li>
&lt;li>A &lt;a href="http://wiki.seeedstudio.com/Seeeduino_Lotus/">Seeduino Lotus&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>It seems like an interesting mix of sensors for someone just getting started with Arduino programming. I&amp;rsquo;ll let you know how it looks in practice as soon as it arrives.&lt;/p></content></item><item><title>Some thoughts on Mechanical Keyboards</title><link>https://blog.oddbit.com/post/2020-04-15-some-thoughts-on-mechanical-ke/</link><pubDate>Wed, 15 Apr 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-04-15-some-thoughts-on-mechanical-ke/</guid><description>Since we&amp;rsquo;re all stuck in the house and working from home these days, I&amp;rsquo;ve had to make some changes to my home office. One change in particular was requested by my wife, who now shares our rather small home office space with me: after a week or so of calls with me clattering away on my old Das Keyboard 3 Professional in the background, she asked if I could get something that was maybe a little bit quieter.</description><content>&lt;p>Since we&amp;rsquo;re all stuck in the house and working from home these days, I&amp;rsquo;ve had to make some changes to my home office. One change in particular was requested by my wife, who now shares our rather small home office space with me: after a week or so of calls with me clattering away on my old Das Keyboard 3 Professional in the background, she asked if I could get something that was maybe a little bit quieter.&lt;/p>
&lt;p>I didn&amp;rsquo;t really need much of a push: I have a &lt;a href="https://www.coolermaster.com/catalog/peripheral/keyboards/masterkeys-s-with-superior-pbt-keycaps/">Cooler Master MasterKeys S&lt;/a> at $work with Cherry MX brown switches that I like a lot, and I would have just bought that one but Cooler Master has stopped manufacturing it. I thought about buying a new Das Keyboard, but their support is awful: while my Das Keyboard 3 Professional has worked fine for many years, I also owned a 4C that stopped working after about a year, and they wanted to charge me $80 just to look at it. You&amp;rsquo;re better off putting that money towards a nicer keyboard from somewhere else.&lt;/p>
&lt;p>I&amp;rsquo;ve been interested in the &lt;a href="https://qmk.fm/">QMK&lt;/a> project &amp;ndash; an open source keyboard firmware with all sorts of nifty features &amp;ndash; for a while, but it turns out that there aren&amp;rsquo;t many &amp;ldquo;off the shelf&amp;rdquo; mechanical keyboards with standard layouts that can be used with QMK firmware. The folks at &lt;a href="https://drop.com/">Drop&lt;/a> market a few models that &lt;em>do&lt;/em> support QMK, so I gave the &lt;a href="https://drop.com/buy/drop-ctrl-mechanical-keyboard">Drop CTRL&lt;/a> a shot. TL;DR: It&amp;rsquo;s a fine keyboard, but it turns out I really have no need for RGB lighting. The translucent key labels just mean I can&amp;rsquo;t see them well in bright light, and mostly the lights were just distracting. I also found that with the low profile case I was constantly hitting the &amp;ldquo;windows&amp;rdquo; key with my palm, which was annoying. Playing with QMK was fun, but I ended up sending the CTRL back.&lt;/p>
&lt;figure class="left" >
&lt;img src="https://blog.oddbit.com/assets/2020/04/15/drop-ctrl.png" />
&lt;/figure>
&lt;p>I&amp;rsquo;m now using a &lt;a href="https://amazon.com/DURGOD-Mechanical-Interface-Tenkeyless-Anti-Ghosting/dp/B078H3WPHM?">Durgod K320 Taurus&lt;/a>, also with Cherry MX brown switches. I&amp;rsquo;m much happier with this keyboard: it is as expected very similar to the Masterkeys S I&amp;rsquo;ve been using. It feels very solid and it has a detachable cable, which seems trivial until your cats gnaws on it and your keyboard stops working. While the keyboard does support some customization, the software to take advantage of that tragically only runs under Windows, so I&amp;rsquo;m using it without any bells and whistles right now. It&amp;rsquo;s been great so far, and the price is very reasonable (especially compared to the CTRL).&lt;/p>
&lt;figure class="left" >
&lt;img src="https://blog.oddbit.com/assets/2020/04/15/durgod-k320.png" />
&lt;/figure></content></item><item><title>I see you have the machine that goes ping...</title><link>https://blog.oddbit.com/post/2020-03-20-i-see-you-have-the-machine-tha/</link><pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-03-20-i-see-you-have-the-machine-tha/</guid><description>We&amp;rsquo;re all looking for ways to keep ourselves occupied these days, and for me that means leaping at the chance to turn a small problem into a slightly ridiculous electronics project. For reasons that I won&amp;rsquo;t go into here I wanted to generate an alert when a certain WiFi BSSID becomes visible. A simple solution to this problem would have been a few lines of shell script to send me an email&amp;hellip;but this article isn&amp;rsquo;t about simple solutions!</description><content>&lt;p>We&amp;rsquo;re all looking for ways to keep ourselves occupied these days, and
for me that means leaping at the chance to turn a small problem into a
slightly ridiculous electronics project. For reasons that I won&amp;rsquo;t go
into here I wanted to generate an alert when a certain WiFi BSSID
becomes visible. A simple solution to this problem would have been a
few lines of shell script to send me an email&amp;hellip;but this article isn&amp;rsquo;t
about simple solutions!&lt;/p>
&lt;p>I thought it would be fun to put together a physical device of some
sort that would sound an alarm when the network in question was
visible. There weren&amp;rsquo;t too many options floating around the house &amp;ndash; I
found a &lt;a href="https://www.amazon.com/RuiLing-Decibels-Continuous-Sounder-Electronic/dp/B07NK8MGL9">small buzzer&lt;/a>, but it wasn&amp;rsquo;t very loud so wasn&amp;rsquo;t much use
unless I was right next to it. I needed something a little more
dramatic, and found it in the old chime doorbell I had floating
around the basement. This means the problem statement became:&lt;/p>
&lt;blockquote>
&lt;p>Design a device that will ring the doorbell chime when a given BSSID
becomes visible.&lt;/p>
&lt;/blockquote>
&lt;p>(Why a BSSID? The BSSID is the hardware address of the access point.
In most cases, it&amp;rsquo;s easy to change the name of a WiFi network &amp;ndash; the
SSID &amp;ndash; but somewhat more difficult to change the BSSID.)&lt;/p>
&lt;h1 id="tldr">TL;DR&lt;/h1>
&lt;p>Before looking at the implementation in more detail, let&amp;rsquo;s take a look
at the finished project. When the device detects a target BSSID, it
rings the bell twice and lights the &lt;code>ALARM&lt;/code> LED:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/cT2JB-aDhTQ" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>After the initial alarm, the bell will ring once every five minutes
while the alarm persists. Once the BSSID goes offline, the device
cancels the alarm and extinguishes the &lt;code>ALARM&lt;/code> LED:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/XY6YKFK2qv4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>If the doorbell proves annoying, there&amp;rsquo;s a switch that activates
silent mode:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/bgM7Asc4FD4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>When silent mode is active, the device will illuminate the &lt;code>ALARM&lt;/code> LED
without sounding the bell:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/xRxxqKiiYVc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="but-wait-theres-more">But wait, there&amp;rsquo;s more!&lt;/h2>
&lt;p>There&amp;rsquo;s also a web interface that allows one to monitor and configure
the device. The web interface allows one to:&lt;/p>
&lt;ul>
&lt;li>See a list of visible networks&lt;/li>
&lt;li>Add a network to the list of targets&lt;/li>
&lt;li>Remove a network from the list of targets&lt;/li>
&lt;li>See whether or not the scanning &amp;ldquo;thread&amp;rdquo; is active&lt;/li>
&lt;li>See whether or not there is currently an active alarm&lt;/li>
&lt;/ul>
&lt;p>Here&amp;rsquo;s a video of it in action:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/TtDwYMXy-b8" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h2 id="and-thats-not-all">And that&amp;rsquo;s not all!&lt;/h2>
&lt;p>In order to support the UI, there&amp;rsquo;s a simple HTTP API that permits
programmatic interaction with the device. The API supports the
following endpoints:&lt;/p>
&lt;ul>
&lt;li>&lt;code>GET /api/target&lt;/code> &amp;ndash; get a list of targets&lt;/li>
&lt;li>`POST /api/target&amp;rsquo; &amp;ndash; add a BSSID to the list of targets&lt;/li>
&lt;li>&lt;code>DELETE /api/target/&amp;lt;bssid&amp;gt;&lt;/code> &amp;ndash; remove a BSSID from the list of
targets&lt;/li>
&lt;li>&lt;code>GET /api/status&lt;/code> &amp;ndash; get the current alarm status and whether or not
the scan is running&lt;/li>
&lt;li>&lt;code>GET /api/scan/result&lt;/code> &amp;ndash; get list of visible networks&lt;/li>
&lt;li>&lt;code>GET /api/scan/start&lt;/code> &amp;ndash; start the scan&lt;/li>
&lt;li>&lt;code>GET /api/scan/stop&lt;/code> &amp;ndash; stop the scan&lt;/li>
&lt;/ul>
&lt;p>There are a couple of other methods, too, but they&amp;rsquo;re more for
debugging than anything else.&lt;/p>
&lt;h2 id="show-me-the-code">Show me the code!&lt;/h2>
&lt;p>The code for this project is all online at
&lt;a href="https://github.com/larsks/maxdetector">https://github.com/larsks/maxdetector&lt;/a>.&lt;/p>
&lt;h1 id="implementation-details">Implementation details&lt;/h1>
&lt;h2 id="software-notes">Software notes&lt;/h2>
&lt;p>My initial inclination was to implement the entire solution in
&lt;a href="https://micropython.org/">MicroPython&lt;/a> on an &lt;a href="https://docs.wemos.cc/en/latest/d1/d1_mini.html">Wemos D1 mini&lt;/a> (an &lt;a href="https://en.wikipedia.org/wiki/ESP8266">esp8266&lt;/a> development
board), but this proved problematic: MicroPython&amp;rsquo;s &lt;code>network.WLAN.scan&lt;/code>
method is a blocking operation, by which I mean it blocks
&lt;em>everything&lt;/em>, including interrupt handling, timer tasks, etc. This
made it difficult to handle some physical UI aspects, such as button
debouncing, in a reliable fashion.&lt;/p>
&lt;p>I ended up moving the physical UI aspects to an &lt;a href="https://store.arduino.cc/usa/arduino-uno-rev3">Arduino Uno&lt;/a>. The
ESP8266 handles scanning for WiFi networks, and raises a signal to the
Uno when an alarm is active. The Uno handles the silent mode button,
the LEDs, and the relay attached to the doorbell.&lt;/p>
&lt;p>After the initial implementation, I realized that it really need a web
interface (because of course it does), so in addition to the WiFi
scanning the ESP8266 now hosts a simple web server. Because of the
blocking nature of the WiFi scan, this means the web server may
occasionally pause for a few seconds, but this hasn&amp;rsquo;t proven to be a
problem.&lt;/p>
&lt;p>In the end, I have four major blocks of code in three different languages:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/maxdetector/blob/master/maxdetector.py">maxdetector.py&lt;/a> implements the WiFi scanning&lt;/li>
&lt;li>&lt;a href="https://github.com/larsks/maxdetector/blob/master/server.py">server.py&lt;/a> implements the server side of the web interface&lt;/li>
&lt;li>&lt;a href="https://github.com/larsks/maxdetector/blob/master/static/md.js">md.js&lt;/a> implements the dynamic portion of the web interface&lt;/li>
&lt;li>&lt;a href="https://github.com/larsks/maxdetector/blob/master/src/maxdetector.cpp">maxdetector.cpp&lt;/a> implements the physical UI and operates the doorbell&lt;/li>
&lt;/ul>
&lt;h3 id="wifi-scanning">Wifi scanning&lt;/h3>
&lt;p>The WiFi scanning operation is implemented as a &amp;ldquo;background task&amp;rdquo;
driven by a MicroPython &lt;a href="https://docs.micropython.org/en/latest/esp8266/quickref.html#timers">virtual timer&lt;/a>. The scanning task triggers
once every 10 seconds (and takes a little over 2 seconds to complete).&lt;/p>
&lt;h3 id="web-server">Web server&lt;/h3>
&lt;p>The web server is a simple &lt;code>select.poll()&lt;/code> based server capable of
servicing multiple clients (very, very slowly). I was interested in an
&lt;code>asyncio&lt;/code> implementation, but at the time the only &lt;code>asyncio&lt;/code>
module for MicroPython was the one in &lt;a href="https://github.com/micropython/micropython-lib">micropython-lib&lt;/a>, which
hadn&amp;rsquo;t been touched in several years. A new &lt;code>asyncio&lt;/code> module has
recently been &lt;a href="https://github.com/micropython/micropython/commit/1d4d688b3b251120f5827a3605ec232d977eaa0f">added to micropython&lt;/a>, but that post-dates the
implementation of this project.&lt;/p>
&lt;p>The server uses a very simple route-registration mechanism that should
be familiar if you&amp;rsquo;ve worked with various other Python web frameworks.
It would be relatively easy to repurpose it for something other than
this project.&lt;/p>
&lt;h2 id="the-hardware">The hardware&lt;/h2>
&lt;p>Everything is bundled &amp;ldquo;neatly&amp;rdquo; (whereby &amp;ldquo;neatly&amp;rdquo; I mean &amp;ldquo;haphazardly&amp;rdquo;)
into an old shoe box. On the outside, you can see the three LEDs (for
the ACTIVE, SILENT, and ALARM signals), the SILENT switch, and the
doorbell itself:&lt;/p>
&lt;figure class="left" >
&lt;img src="detector-outside-labelled.png" />
&lt;/figure>
&lt;p>On the inside, you&amp;rsquo;ll find the Arduino Uno, the Wemos D1 mini, the
relay, and a step-down converter:&lt;/p>
&lt;figure class="left" >
&lt;img src="detector-inside-labelled.png" />
&lt;/figure>
&lt;p>The step-down converter isn&amp;rsquo;t actually necessary: when I put things
together, I didn&amp;rsquo;t realize that the Uno would accept up to 12V into
its regulator. Since I already had the step-down converter in place,
I&amp;rsquo;m feeding about 7.5v to the Uno. The doorbell gets 12V.&lt;/p>
&lt;p>I initially prototyped the circuit in &lt;a href="https://www.tinkercad.com/things/cpRuevAoV5L-max-detector">Tinkercad Circuits&lt;/a>, where
everything worked just fine. But after wiring things up and testing out
the device, it would start ringing endlessly. Upon inspection, this
was because the Uno was resetting every time the doorbell chimed. This
was due to flyback voltage from the relay, which is simple to fix if
you happen to have an appropriate diode handy&amp;hellip;but if you don&amp;rsquo;t, it
means calling around to all your aquaintenances to find someone who
happens to have some lying around. With a diode in place, everything
worked swimmingly.&lt;/p></content></item><item><title>A passwordless serial console for your Raspberry Pi</title><link>https://blog.oddbit.com/post/2020-02-24-a-passwordless-serial-console/</link><pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-02-24-a-passwordless-serial-console/</guid><description>legendre on #raspbian asked:
How can i config rasp lite to open a shell on the serial uart on boot? Params are 1200-8-N-1 Dont want login running, just straight to sh
In this article, we&amp;rsquo;ll walk through one way of implementing this configuration.
Activate the serial port Raspbian automatically starts a getty on the serial port if one is available. You should see an agetty process associated with your serial port when you run ps -ef.</description><content>&lt;p>&lt;code>legendre&lt;/code> on &lt;code>#raspbian&lt;/code> asked:&lt;/p>
&lt;blockquote>
&lt;p>How can i config rasp lite to open a shell on the serial uart on boot? Params
are 1200-8-N-1 Dont want login running, just straight to sh&lt;/p>
&lt;/blockquote>
&lt;p>In this article, we&amp;rsquo;ll walk through one way of implementing this configuration.&lt;/p>
&lt;h2 id="activate-the-serial-port">Activate the serial port&lt;/h2>
&lt;p>Raspbian automatically starts a getty on the serial port if one is available. You should see an &lt;code>agetty&lt;/code> process associated with your serial port when you run &lt;code>ps -ef&lt;/code>. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>root@raspberrypi:/etc/systemd/system# ps -fe | grep agetty | grep ttyS0
root 1138 1 0 00:24 ttyS0 00:00:00 /sbin/agetty -o -p -- \u --keep-baud 115200,38400,9600 ttyS0 vt220
&lt;/code>&lt;/pre>&lt;p>If you don&amp;rsquo;t see this process and you&amp;rsquo;re on a Raspberry Pi 3 (or later), you may need to &lt;a href="https://www.raspberrypi.org/forums/viewtopic.php?f=28&amp;amp;t=141195">explicitly enable the serial port&lt;/a> by adding &lt;code>enable_uart=1&lt;/code> to &lt;code>/boot/config.txt&lt;/code>. If you make this change, reboot your Pi before continuing, then repeat the above test to make sure things are working as expected.&lt;/p>
&lt;p>Note that your serial port may not always be named &lt;code>ttyS0&lt;/code>. I&amp;rsquo;m going to use the value &lt;code>ttyS0&lt;/code> throughout this article to represent the appropriate device name. The correct device name is the penultimate argument in the above &lt;code>agetty&lt;/code> command.&lt;/p>
&lt;h2 id="modify-the-serial-gettyttys0-unit">Modify the serial-getty@ttyS0 unit&lt;/h2>
&lt;p>The &lt;code>agetty&lt;/code> process we saw in the previous section is started by the &lt;code>serial-getty@ttyS0.service&lt;/code> service unit (which is an instance of the &lt;code>serial-getty@.service&lt;/code> &lt;a href="https://fedoramagazine.org/systemd-template-unit-files/">template unit&lt;/a>). We need to modify that service so that it will call &lt;code>agetty&lt;/code> with the &lt;code>--autologin root&lt;/code> option.&lt;/p>
&lt;p>Rather than editing the unit file included in Raspbian, we&amp;rsquo;re going to make the changes by creating a systemd &amp;ldquo;drop-in&amp;rdquo; configuration to override the stock service unit. From the &lt;a href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html">systemd.unit man page&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Along with a unit file foo.service, a &amp;ldquo;drop-in&amp;rdquo; directory foo.service.d/ may exist. All files with the suffix &amp;ldquo;.conf&amp;rdquo; from this directory will be parsed after the unit file itself is parsed. This is useful to alter or add configuration settings for a unit, without having to modify unit files.&lt;/p>
&lt;/blockquote>
&lt;p>The easiest way to creating a drop-in unit is with the &lt;code>systemctl edit&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl edit serial-getty@ttyS0
&lt;/code>&lt;/pre>&lt;p>This will bring up an editor (&lt;code>nano&lt;/code> by default, unless you have set &lt;code>VISUAL&lt;/code> in your environment to point at a different editor) in which you will create &lt;code>/etc/systemd/system/serial-getty@ttyS0.d/override.conf&lt;/code>.&lt;/p>
&lt;p>Enter the following content:&lt;/p>
&lt;pre tabindex="0">&lt;code>[Service]
ExecStart=
ExecStart=/sbin/agetty -o &amp;#39;-p -- \u&amp;#39; --keep-baud 115200,38400,9600 --noclear --autologin root ttyS0 vt220
&lt;/code>&lt;/pre>&lt;p>(That while the original request was for a getty running as 1200 bos, the above configuration is more generally useful. To allow connectoins at 1200 bps, modify the list of rates above to looking something like &lt;code>115200,38400,9600,1200&lt;/code> (if you want to permit connections at higher speeds) or just &lt;code>1200&lt;/code> (if you really want to permit only 1200 bps connections).&lt;/p>
&lt;p>Save the file, then reload &lt;code>systemd&lt;/code> by running &lt;code>systemctl daemon-reload&lt;/code>. This tells &lt;code>systemd&lt;/code> to re-read its unit files.&lt;/p>
&lt;p>Finally, restart the &lt;code>serial-getty@ttys0&lt;/code> service:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl restart serial-getty@ttyS0
&lt;/code>&lt;/pre>&lt;h2 id="configure-passwordless-root-login-on-the-console">Configure passwordless root login on the console&lt;/h2>
&lt;p>With the above change to the service unit, &lt;code>agetty&lt;/code> will attempt to log in the &lt;code>root&lt;/code> user on the console but will prompt for a password. That looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>Raspbian GNU/Linux 10 raspberrypi ttyS0
raspberrypi login: root (automatic login)
Password:
&lt;/code>&lt;/pre>&lt;p>We need to configure things such that the &lt;code>root&lt;/code> user does not require a password when logging on the serial console. We&amp;rsquo;ll do this by modifying the &lt;a href="http://www.linux-pam.org/">PAM&lt;/a> configuration for the &lt;code>login&lt;/code> program.&lt;/p>
&lt;p>Add the following to the top of &lt;code>/etc/pam.d/login&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>auth sufficient pam_listfile.so item=tty sense=allow file=/etc/rootshelltty onerr=fail apply=root
&lt;/code>&lt;/pre>&lt;p>This configures &lt;code>login&lt;/code> to permit a login for the &lt;code>root&lt;/code> user if it finds the login tty in the file &lt;code>/etc/rootshelltty&lt;/code>.&lt;/p>
&lt;p>Now, add the serial port device to &lt;code>/etc/rootshelltty&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>root@raspberrypi:/etc# echo /dev/ttyS0 &amp;gt; /etc/rootshelltty
&lt;/code>&lt;/pre>&lt;p>These changes will take affect as soon as &lt;code>agetty&lt;/code> restarts. You can wait for the &lt;code>Password:&lt;/code> prompt to timeout, or just restart the service by running &lt;code>systemctl restart serial-getty@ttyS0&lt;/code>.&lt;/p>
&lt;hr>
&lt;p>With these changes, the Pi will now automatically start a &lt;code>root&lt;/code> shell on the serial port without prompting for a password:&lt;/p>
&lt;pre tabindex="0">&lt;code>Raspbian GNU/Linux 10 raspberrypi ttyS0
raspberrypi login: root (automatic login)
Last login: Mon Feb 24 00:29:00 EST 2020 on ttyS0
Linux raspberrypi 4.19.97-v7+ #1294 SMP Thu Jan 30 13:15:58 GMT 2020 armv7l
[...]
root@raspberrypi:~#
&lt;/code>&lt;/pre></content></item><item><title>Configuring Open vSwitch with nmcli</title><link>https://blog.oddbit.com/post/2020-02-15-configuring-open-vswitch-with/</link><pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-02-15-configuring-open-vswitch-with/</guid><description>I recently acquired a managed switch for my home office in order to segment a few devices off onto their own isolated vlan. As part of this, I want to expose these vlans on my desktop using Open vSwitch (OVS), and I wanted to implement the configuration using NetworkManager rather than either relying on the legacy /etc/sysconfig/network-scripts scripts or rolling my own set of services. These are my notes in case I ever have to do this again.</description><content>&lt;p>I recently acquired a managed switch for my home office in order to segment a few devices off onto their own isolated vlan. As part of this, I want to expose these vlans on my desktop using Open vSwitch (OVS), and I wanted to implement the configuration using NetworkManager rather than either relying on the legacy &lt;code>/etc/sysconfig/network-scripts&lt;/code> scripts or rolling my own set of services. These are my notes in case I ever have to do this again.&lt;/p>
&lt;p>First, we need the openvswitch plugin for NetworkManager:&lt;/p>
&lt;pre tabindex="0">&lt;code>yum install NetworkManager-ovs
&lt;/code>&lt;/pre>&lt;p>Without the plugin, &lt;code>nmcli&lt;/code> will happily accept all your configuration commands, but you&amp;rsquo;ll get an error when you try to bring an interface up.&lt;/p>
&lt;h2 id="target-configuration">Target configuration&lt;/h2>
&lt;p>This is what I want when we&amp;rsquo;re done:&lt;/p>
&lt;pre tabindex="0">&lt;code>1e668de8-c2ac-4dd7-9824-95e1cade31ce
Bridge br-house
Port &amp;#34;vlan1&amp;#34;
tag: 1
Interface &amp;#34;vlan1&amp;#34;
type: internal
Port &amp;#34;vlan102&amp;#34;
tag: 102
Interface &amp;#34;vlan102&amp;#34;
type: internal
Port br-house
Interface br-house
type: internal
Port &amp;#34;eth0&amp;#34;
Interface &amp;#34;eth0&amp;#34;
type: system
Port &amp;#34;vlan101&amp;#34;
tag: 101
Interface &amp;#34;vlan101&amp;#34;
type: internal
ovs_version: &amp;#34;2.12.0&amp;#34;
&lt;/code>&lt;/pre>&lt;h2 id="nmcli-commands">NMCLI commands&lt;/h2>
&lt;p>To create the ovs bridge:&lt;/p>
&lt;pre tabindex="0">&lt;code>nmcli c add type ovs-bridge conn.interface br-house con-name br-house
nmcli c add type ovs-port conn.interface br-house master br-house con-name ovs-port-br-house
nmcli c add type ovs-interface slave-type ovs-port conn.interface br-house master ovs-port-br-house con-name ovs-if-br-house
&lt;/code>&lt;/pre>&lt;p>Unlike &lt;code>ovs-vsctl&lt;/code>, creating the bridge won&amp;rsquo;t automatically create an interface for you. The two additional commands above get us an actual interface named &lt;code>br-house&lt;/code> (configured using DHCP, because we didn&amp;rsquo;t explicitly set &lt;code>ipv4.method&lt;/code> on the interface).&lt;/p>
&lt;p>Next, we add &lt;code>eth0&lt;/code> to the bridge:&lt;/p>
&lt;pre tabindex="0">&lt;code>nmcli c add type ovs-port conn.interface eth0 master br-house con-name ovs-port-eth0
nmcli c add type ethernet conn.interface eth0 master ovs-port-eth0 con-name ovs-if-eth0
&lt;/code>&lt;/pre>&lt;p>And finally, we create some ports to expose specific vlans:&lt;/p>
&lt;pre tabindex="0">&lt;code>nmcli c add type ovs-port conn.interface vlan1 master br-house ovs-port.tag 1 con-name ovs-port-vlan1
nmcli c add type ovs-interface slave-type ovs-port conn.interface vlan1 master ovs-port-vlan1 con-name ovs-if-vlan1 ipv4.method static ipv4.address 192.168.7.1/24
nmcli c add type ovs-port conn.interface vlan101 master br-house ovs-port.tag 101 con-name ovs-port-vlan101
nmcli c add type ovs-interface slave-type ovs-port conn.interface vlan101 master ovs-port-vlan101 con-name ovs-if-vlan101 ipv4.method static ipv4.address 192.168.11.1/24
nmcli c add type ovs-port conn.interface vlan102 master br-house ovs-port.tag 102 con-name ovs-port-vlan102
nmcli c add type ovs-interface slave-type ovs-port conn.interface vlan102 master ovs-port-vlan102 con-name ovs-if-vlan102 ipv4.method static ipv4.address 192.168.13.1/24
&lt;/code>&lt;/pre></content></item><item><title>How long is a cold spell in Boston?</title><link>https://blog.oddbit.com/post/2020-01-23-how-long-is-a-cold-spell/</link><pubDate>Thu, 23 Jan 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-01-23-how-long-is-a-cold-spell/</guid><description>We&amp;rsquo;ve had some wacky weather recently. In the space of a week, the temperature went from a high of about 75°F to a low around 15°F. This got me to thinking about what constitutes &amp;ldquo;normal&amp;rdquo; weather here in the Boston area, and in particular, how common it is to have a string of consecutive days in which the high temperature stays below freezing. While this was an interesting question in itself, it also seemed like a great opportunity to learn a little about Pandas, the Python data analysis framework.</description><content>&lt;p>We&amp;rsquo;ve had some wacky weather recently. In the space of a week, the temperature went from a high of about 75°F to a low around 15°F. This got me to thinking about what constitutes &amp;ldquo;normal&amp;rdquo; weather here in the Boston area, and in particular, how common it is to have a string of consecutive days in which the high temperature stays below freezing. While this was an interesting question in itself, it also seemed like a great opportunity to learn a little about &lt;a href="https://pandas.pydata.org">Pandas&lt;/a>, the Python data analysis framework.&lt;/p>
&lt;p>The first step was finding an appropriate dataset. &lt;a href="https://www.noaa.gov/">NOAA&lt;/a> provides a &lt;a href="https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND">daily summaries&lt;/a> dataset that includes daily high and low temperature; for Boston, this data extends back to about 1936.&lt;/p>
&lt;p>The next step was figuring how to solve the problem. To be explicit, the question I&amp;rsquo;m trying to answer is:&lt;/p>
&lt;blockquote>
&lt;p>For any given winter, what was the longest consecutive string of days in which the temperature stayed below freezing?&lt;/p>
&lt;/blockquote>
&lt;p>There are several parts to this problem.&lt;/p>
&lt;h2 id="reading-the-data">Reading the data&lt;/h2>
&lt;p>We can read the data using Pandas&amp;rsquo; &lt;code>read_csv&lt;/code> method:&lt;/p>
&lt;pre tabindex="0">&lt;code>df = pandas.read_csv(&amp;#39;boston.csv&amp;#39;)
&lt;/code>&lt;/pre>&lt;p>This assumes of course that we have previously &lt;code>import&lt;/code>ed the Pandas library:&lt;/p>
&lt;pre tabindex="0">&lt;code>import pandas
&lt;/code>&lt;/pre>&lt;p>Now we have a dataframe in &lt;code>df&lt;/code>, but it&amp;rsquo;s using a positional index (i.e., the first item is at index &lt;code>0&lt;/code>, the second at &lt;code>1&lt;/code>, etc), whereas we want it to use a date-based index. The data has a &lt;code>DATE&lt;/code> column that we can turn into an appropriate index like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>df[&amp;#39;DATE&amp;#39;] = pandas.to_datetime(df[&amp;#39;DATE&amp;#39;])
df.set_index(df[&amp;#39;DATE&amp;#39;], inplace=True)
&lt;/code>&lt;/pre>&lt;h2 id="which-winter">Which winter?&lt;/h2>
&lt;p>I need to be able to group the data by &amp;ldquo;winter&amp;rdquo;. For example, dates from December 21, 2018 through March 20, 2019 would all be associated with &amp;ldquo;winter 2018&amp;rdquo;. It would be easy to group the data by &lt;em>year&lt;/em> using Pandas&amp;rsquo; &lt;code>groupby&lt;/code> method:&lt;/p>
&lt;pre tabindex="0">&lt;code>df.groupby(df[&amp;#39;DATE&amp;#39;].dt.year)...
&lt;/code>&lt;/pre>&lt;p>But what&amp;rsquo;s the equivalent for grouping by winter? My first attempt was a naive iterative solution:&lt;/p>
&lt;pre tabindex="0">&lt;code>def get_winter_start(val):
if (val.month == 10 and val.day &amp;gt;= 20) or val.month &amp;gt; 10:
winter = val.year
elif (val.month == 3 and val.day &amp;lt;= 20) or val.month &amp;lt; 3:
winter = val.year-1
else:
winter = 0
return winter
df[&amp;#39;winter_start&amp;#39;] = df[&amp;#39;DATE&amp;#39;].apply(get_winter_start)
&lt;/code>&lt;/pre>&lt;p>This works, but it&amp;rsquo;s not particular graceful and doesn&amp;rsquo;t take advantage of any of the vector operations supported by Pandas. I eventually came up with a different solution. First, create a boolean series that indicates whether a given date is in winter or not:&lt;/p>
&lt;pre tabindex="0">&lt;code>df[&amp;#39;winter&amp;#39;] = (
((df[&amp;#39;DATE&amp;#39;].dt.month == 12) &amp;amp; (df[&amp;#39;DATE&amp;#39;].dt.day &amp;gt;= 20)) |
(df[&amp;#39;DATE&amp;#39;].dt.month &amp;lt; 3) |
((df[&amp;#39;DATE&amp;#39;].dt.month == 3) &amp;amp; (df[&amp;#39;DATE&amp;#39;].dt.day &amp;lt;= 20))
)
&lt;/code>&lt;/pre>&lt;p>Next, use this boolean series to create a new dataframe that contains &lt;em>only&lt;/em> dates in winter. Given this new data, the winter year is the current year for the month of December, or (the current year - 1) for months in Janurary, February, and March:&lt;/p>
&lt;pre tabindex="0">&lt;code>winter = df[df[&amp;#39;winter&amp;#39;]].copy()
winter[&amp;#39;winter_start&amp;#39;] = (
winter[&amp;#39;DATE&amp;#39;].dt.year - (winter[&amp;#39;DATE&amp;#39;].dt.month &amp;lt;= 3))
&lt;/code>&lt;/pre>&lt;p>This seems to do the job. You&amp;rsquo;ll note that in the above expression I&amp;rsquo;m subtracting a boolean from an integer, which is in fact totally legal and I talk about that in more detail &lt;a href="#bool">later on&lt;/a> in this article.&lt;/p>
&lt;h2 id="finding-a-sequence-of-consecutive-days-an-iterative-solution">Finding a sequence of consecutive days: an iterative solution&lt;/h2>
&lt;p>To find the longest sequence of days below freezing, I again started with an iterative solution:&lt;/p>
&lt;pre tabindex="0">&lt;code>def max_dbf(val):
acc = []
cur = 0
for i, row in val.iterrows():
if row[&amp;#39;TMAX&amp;#39;] &amp;lt;= 32:
cur += 1
else:
if cur:
acc.append(cur)
cur = 0
if cur:
acc.append(cur)
return max(acc)
&lt;/code>&lt;/pre>&lt;p>Which I applied using Pandas&amp;rsquo; &lt;code>apply&lt;/code> method:&lt;/p>
&lt;pre tabindex="0">&lt;code>res = winter.groupby(&amp;#39;winter_start&amp;#39;).apply(max_dbf)
&lt;/code>&lt;/pre>&lt;p>This time it&amp;rsquo;s not just ugly, but it&amp;rsquo;s also noticeably slow. I started doing some research to figure out how to make it faster.&lt;/p>
&lt;h2 id="finding-a-sequence-of-consecutive-days-a-pandas-solution">Finding a sequence of consecutive days: a Pandas solution&lt;/h2>
&lt;p>In an answer to &lt;a href="https://stackoverflow.com/questions/27626542/counting-consecutive-positive-value-in-python-array">this question&lt;/a> on Stack Overflow, user &lt;a href="https://stackoverflow.com/users/487339/dsm">DSM&lt;/a> suggests that given a series, you can find the longest sequence of consecutive items matching a condition by first creating a boolean series &lt;code>y&lt;/code> that is &lt;code>True&lt;/code> (or &lt;code>1&lt;/code>) for items that match the condition (and &lt;code>False&lt;/code> or &lt;code>0&lt;/code> otherwise), and then running:&lt;/p>
&lt;pre tabindex="0">&lt;code>result = y * (y.groupby((y != y.shift()).cumsum()).cumcount() + 1)
&lt;/code>&lt;/pre>&lt;p>Using that suggestion, I rewrote the &lt;code>max_dbf&lt;/code> method to look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>def max_dbf(val):
y = val[&amp;#39;TMAX&amp;#39;] &amp;lt;= 32
res = y * (y.groupby((y != y.shift()).cumsum()).cumcount() + 1)
return max(res)
&lt;/code>&lt;/pre>&lt;p>&amp;hellip;and you know what, it works! But what exactly is going on there? There&amp;rsquo;s a reasonable explanation in &lt;a href="https://stackoverflow.com/a/27626699/147356">the answer&lt;/a>, but I&amp;rsquo;m new enough to Pandas that I wanted to work it out for myself.&lt;/p>
&lt;h2 id="setting-the-stage">Setting the stage&lt;/h2>
&lt;p>In order to explore the operation of this expression, let&amp;rsquo;s start with some sample data. This is the value of &lt;code>TMAX&lt;/code> for the month of Janurary, 2018:&lt;/p>
&lt;pre tabindex="0">&lt;code>data = [
[&amp;#39;2018-01-01&amp;#39;, 13],
[&amp;#39;2018-01-02&amp;#39;, 19],
[&amp;#39;2018-01-03&amp;#39;, 29],
[&amp;#39;2018-01-04&amp;#39;, 30],
[&amp;#39;2018-01-05&amp;#39;, 24],
[&amp;#39;2018-01-06&amp;#39;, 12],
[&amp;#39;2018-01-07&amp;#39;, 17],
[&amp;#39;2018-01-08&amp;#39;, 35],
[&amp;#39;2018-01-09&amp;#39;, 43],
[&amp;#39;2018-01-10&amp;#39;, 36],
[&amp;#39;2018-01-11&amp;#39;, 51],
[&amp;#39;2018-01-12&amp;#39;, 60],
[&amp;#39;2018-01-13&amp;#39;, 61],
[&amp;#39;2018-01-14&amp;#39;, 23],
[&amp;#39;2018-01-15&amp;#39;, 21],
[&amp;#39;2018-01-16&amp;#39;, 33],
[&amp;#39;2018-01-17&amp;#39;, 34],
[&amp;#39;2018-01-18&amp;#39;, 32],
[&amp;#39;2018-01-19&amp;#39;, 34],
[&amp;#39;2018-01-20&amp;#39;, 47],
[&amp;#39;2018-01-21&amp;#39;, 49],
[&amp;#39;2018-01-22&amp;#39;, 39],
[&amp;#39;2018-01-23&amp;#39;, 55],
[&amp;#39;2018-01-24&amp;#39;, 42],
[&amp;#39;2018-01-25&amp;#39;, 30],
[&amp;#39;2018-01-26&amp;#39;, 34],
[&amp;#39;2018-01-27&amp;#39;, 53],
[&amp;#39;2018-01-28&amp;#39;, 52],
[&amp;#39;2018-01-29&amp;#39;, 43],
[&amp;#39;2018-01-30&amp;#39;, 31],
[&amp;#39;2018-01-31&amp;#39;, 30],
]
data_unzipped = list(zip(*data))
df = pandas.DataFrame({&amp;#39;DATE&amp;#39;: data_unzipped[0], &amp;#39;TMAX&amp;#39;: data_unzipped[1]})
&lt;/code>&lt;/pre>&lt;p>Our initial dataframe looks like this:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>DATE&lt;/th>
&lt;th>TMAX&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>2018-01-01&lt;/td>
&lt;td>13&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>2018-01-02&lt;/td>
&lt;td>19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2018-01-03&lt;/td>
&lt;td>29&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>2018-01-04&lt;/td>
&lt;td>30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>2018-01-05&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>2018-01-06&lt;/td>
&lt;td>12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>2018-01-07&lt;/td>
&lt;td>17&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>2018-01-08&lt;/td>
&lt;td>35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>2018-01-09&lt;/td>
&lt;td>43&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>2018-01-10&lt;/td>
&lt;td>36&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>2018-01-11&lt;/td>
&lt;td>51&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>2018-01-12&lt;/td>
&lt;td>60&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>2018-01-13&lt;/td>
&lt;td>61&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>2018-01-14&lt;/td>
&lt;td>23&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2018-01-15&lt;/td>
&lt;td>21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>2018-01-16&lt;/td>
&lt;td>33&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>2018-01-17&lt;/td>
&lt;td>34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>2018-01-18&lt;/td>
&lt;td>32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>2018-01-19&lt;/td>
&lt;td>34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>2018-01-20&lt;/td>
&lt;td>47&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>2018-01-21&lt;/td>
&lt;td>49&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>2018-01-22&lt;/td>
&lt;td>39&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>22&lt;/td>
&lt;td>2018-01-23&lt;/td>
&lt;td>55&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>23&lt;/td>
&lt;td>2018-01-24&lt;/td>
&lt;td>42&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24&lt;/td>
&lt;td>2018-01-25&lt;/td>
&lt;td>30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>2018-01-26&lt;/td>
&lt;td>34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26&lt;/td>
&lt;td>2018-01-27&lt;/td>
&lt;td>53&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27&lt;/td>
&lt;td>2018-01-28&lt;/td>
&lt;td>52&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>28&lt;/td>
&lt;td>2018-01-29&lt;/td>
&lt;td>43&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>29&lt;/td>
&lt;td>2018-01-30&lt;/td>
&lt;td>31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>30&lt;/td>
&lt;td>2018-01-31&lt;/td>
&lt;td>30&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="step-1">Step 1&lt;/h3>
&lt;p>We first need to create a boolean series indicating whether or not the temperature is below freezing. We&amp;rsquo;ll put this into the dataframe as series &lt;code>freezing&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>df[&amp;#39;freezing&amp;#39;] = df[&amp;#39;TMAX&amp;#39;] &amp;lt;= 32
&lt;/code>&lt;/pre>&lt;p>Our dataframe now looks like this:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>DATE&lt;/th>
&lt;th>TMAX&lt;/th>
&lt;th>freezing&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>2018-01-01&lt;/td>
&lt;td>13&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>2018-01-02&lt;/td>
&lt;td>19&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2018-01-03&lt;/td>
&lt;td>29&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>2018-01-04&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>2018-01-05&lt;/td>
&lt;td>24&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>2018-01-06&lt;/td>
&lt;td>12&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>2018-01-07&lt;/td>
&lt;td>17&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>2018-01-08&lt;/td>
&lt;td>35&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>2018-01-09&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>2018-01-10&lt;/td>
&lt;td>36&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>2018-01-11&lt;/td>
&lt;td>51&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>2018-01-12&lt;/td>
&lt;td>60&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>2018-01-13&lt;/td>
&lt;td>61&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>2018-01-14&lt;/td>
&lt;td>23&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2018-01-15&lt;/td>
&lt;td>21&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>2018-01-16&lt;/td>
&lt;td>33&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>2018-01-17&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>2018-01-18&lt;/td>
&lt;td>32&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>2018-01-19&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>2018-01-20&lt;/td>
&lt;td>47&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>2018-01-21&lt;/td>
&lt;td>49&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>2018-01-22&lt;/td>
&lt;td>39&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>22&lt;/td>
&lt;td>2018-01-23&lt;/td>
&lt;td>55&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>23&lt;/td>
&lt;td>2018-01-24&lt;/td>
&lt;td>42&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24&lt;/td>
&lt;td>2018-01-25&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>2018-01-26&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26&lt;/td>
&lt;td>2018-01-27&lt;/td>
&lt;td>53&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27&lt;/td>
&lt;td>2018-01-28&lt;/td>
&lt;td>52&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>28&lt;/td>
&lt;td>2018-01-29&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>29&lt;/td>
&lt;td>2018-01-30&lt;/td>
&lt;td>31&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>30&lt;/td>
&lt;td>2018-01-31&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="step-2">Step 2&lt;/h3>
&lt;p>Now we start looking at the various components in our expression of interest. In this step, we are looking at the highlighted part below:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Instead of &lt;code>y&lt;/code>, we&amp;rsquo;re operating on the result of the previous step, &lt;code>df['freezing']&lt;/code>. We&amp;rsquo;ll place the result of this step into a new series named &lt;code>step2&lt;/code> in the dataframe:&lt;/p>
&lt;pre tabindex="0">&lt;code>df[&amp;#39;step2&amp;#39;] = df[&amp;#39;freezing&amp;#39;] != df[&amp;#39;freezing&amp;#39;].shift()
&lt;/code>&lt;/pre>&lt;p>This gives us the following:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>DATE&lt;/th>
&lt;th>TMAX&lt;/th>
&lt;th>freezing&lt;/th>
&lt;th>step2&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>2018-01-01&lt;/td>
&lt;td>13&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>2018-01-02&lt;/td>
&lt;td>19&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2018-01-03&lt;/td>
&lt;td>29&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>2018-01-04&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>2018-01-05&lt;/td>
&lt;td>24&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>2018-01-06&lt;/td>
&lt;td>12&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>2018-01-07&lt;/td>
&lt;td>17&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>2018-01-08&lt;/td>
&lt;td>35&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>2018-01-09&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>2018-01-10&lt;/td>
&lt;td>36&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>2018-01-11&lt;/td>
&lt;td>51&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>2018-01-12&lt;/td>
&lt;td>60&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>2018-01-13&lt;/td>
&lt;td>61&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>2018-01-14&lt;/td>
&lt;td>23&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2018-01-15&lt;/td>
&lt;td>21&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>2018-01-16&lt;/td>
&lt;td>33&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>2018-01-17&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>2018-01-18&lt;/td>
&lt;td>32&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>2018-01-19&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>2018-01-20&lt;/td>
&lt;td>47&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>2018-01-21&lt;/td>
&lt;td>49&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>2018-01-22&lt;/td>
&lt;td>39&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>22&lt;/td>
&lt;td>2018-01-23&lt;/td>
&lt;td>55&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>23&lt;/td>
&lt;td>2018-01-24&lt;/td>
&lt;td>42&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24&lt;/td>
&lt;td>2018-01-25&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>2018-01-26&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26&lt;/td>
&lt;td>2018-01-27&lt;/td>
&lt;td>53&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27&lt;/td>
&lt;td>2018-01-28&lt;/td>
&lt;td>52&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>28&lt;/td>
&lt;td>2018-01-29&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>29&lt;/td>
&lt;td>2018-01-30&lt;/td>
&lt;td>31&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>30&lt;/td>
&lt;td>2018-01-31&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Looking at the values of &lt;code>step2&lt;/code> in this table, we can see an interesting property: &lt;code>step2&lt;/code> is &lt;code>True&lt;/code> only in cases where the value of &lt;code>df['freezing']&lt;/code> changes.&lt;/p>
&lt;h3 id="step-3">Step 3&lt;/h3>
&lt;!-- raw HTML omitted -->
&lt;p>In this step, we apply the &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.cumsum.html">cumsum&lt;/a> method (&amp;ldquo;cumulative sum&amp;rdquo;) to the result of step 2. We store the result in a new series &lt;code>step3&lt;/code> in the dataframe:&lt;/p>
&lt;pre tabindex="0">&lt;code>df[&amp;#39;step3&amp;#39;] = df[&amp;#39;step2&amp;#39;].cumsum()
&lt;/code>&lt;/pre>&lt;p>The result looks like this:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>DATE&lt;/th>
&lt;th>TMAX&lt;/th>
&lt;th>freezing&lt;/th>
&lt;th>step2&lt;/th>
&lt;th>step3&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>2018-01-01&lt;/td>
&lt;td>13&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>2018-01-02&lt;/td>
&lt;td>19&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2018-01-03&lt;/td>
&lt;td>29&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>2018-01-04&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>2018-01-05&lt;/td>
&lt;td>24&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>2018-01-06&lt;/td>
&lt;td>12&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>2018-01-07&lt;/td>
&lt;td>17&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>2018-01-08&lt;/td>
&lt;td>35&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>2018-01-09&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>2018-01-10&lt;/td>
&lt;td>36&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>2018-01-11&lt;/td>
&lt;td>51&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>2018-01-12&lt;/td>
&lt;td>60&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>2018-01-13&lt;/td>
&lt;td>61&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>2018-01-14&lt;/td>
&lt;td>23&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2018-01-15&lt;/td>
&lt;td>21&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>2018-01-16&lt;/td>
&lt;td>33&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>2018-01-17&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>2018-01-18&lt;/td>
&lt;td>32&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>2018-01-19&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>2018-01-20&lt;/td>
&lt;td>47&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>2018-01-21&lt;/td>
&lt;td>49&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>2018-01-22&lt;/td>
&lt;td>39&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>22&lt;/td>
&lt;td>2018-01-23&lt;/td>
&lt;td>55&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>23&lt;/td>
&lt;td>2018-01-24&lt;/td>
&lt;td>42&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24&lt;/td>
&lt;td>2018-01-25&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>2018-01-26&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26&lt;/td>
&lt;td>2018-01-27&lt;/td>
&lt;td>53&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27&lt;/td>
&lt;td>2018-01-28&lt;/td>
&lt;td>52&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>28&lt;/td>
&lt;td>2018-01-29&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>29&lt;/td>
&lt;td>2018-01-30&lt;/td>
&lt;td>31&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>9&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>30&lt;/td>
&lt;td>2018-01-31&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>9&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;!-- raw HTML omitted -->&lt;!-- raw HTML omitted -->&lt;/p>
&lt;p>We&amp;rsquo;re applying the &lt;code>cumsum&lt;/code> method to a boolean series. By doing so, we&amp;rsquo;re taking advantage of the fact that in Python &lt;a href="https://docs.python.org/release/3.0.1/reference/datamodel.html#the-standard-type-hierarchy">we can treat boolean values as integers&lt;/a>: a &lt;code>True&lt;/code> value evaluates to &lt;code>1&lt;/code>, and a &lt;code>False&lt;/code> value to &lt;code>0&lt;/code>. What we get with this operation is effectively a &amp;ldquo;sequence id&amp;rdquo;: because &lt;code>step2&lt;/code> is only &lt;code>True&lt;/code> when the value of &lt;code>freezing&lt;/code> changes, the value calculated in this step only increments when we start a new sequence of values for which &lt;code>freezing&lt;/code> has the same value.&lt;/p>
&lt;h3 id="step-4">Step 4&lt;/h3>
&lt;!-- raw HTML omitted -->
&lt;p>In the previous step, we calculated what I called a &amp;ldquo;sequence id&amp;rdquo;. We can take advantage of this to group the data into consecutive stretches for which the temperature was either below freezing or not by using the value as an argument to Pandas&amp;rsquo; &lt;code>groupby&lt;/code> method, and then applying the &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.cumcount.html">cumcount&lt;/a> method:&lt;/p>
&lt;pre tabindex="0">&lt;code>df[&amp;#39;step4&amp;#39;] = df[&amp;#39;freezing&amp;#39;].groupby(df[&amp;#39;step3&amp;#39;]).cumcount() + 1
&lt;/code>&lt;/pre>&lt;p>The &lt;code>cumcount&lt;/code> method simply numbers the items in each group, starting at 0. This gives us:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>DATE&lt;/th>
&lt;th>TMAX&lt;/th>
&lt;th>freezing&lt;/th>
&lt;th>step2&lt;/th>
&lt;th>step3&lt;/th>
&lt;th>step4&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>2018-01-01&lt;/td>
&lt;td>13&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>2018-01-02&lt;/td>
&lt;td>19&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2018-01-03&lt;/td>
&lt;td>29&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>2018-01-04&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>2018-01-05&lt;/td>
&lt;td>24&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>2018-01-06&lt;/td>
&lt;td>12&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>2018-01-07&lt;/td>
&lt;td>17&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>2018-01-08&lt;/td>
&lt;td>35&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>2018-01-09&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>2018-01-10&lt;/td>
&lt;td>36&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>2018-01-11&lt;/td>
&lt;td>51&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>2018-01-12&lt;/td>
&lt;td>60&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>2018-01-13&lt;/td>
&lt;td>61&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>2018-01-14&lt;/td>
&lt;td>23&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2018-01-15&lt;/td>
&lt;td>21&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>3&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>2018-01-16&lt;/td>
&lt;td>33&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>4&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>2018-01-17&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>4&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>2018-01-18&lt;/td>
&lt;td>32&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>2018-01-19&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>6&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>2018-01-20&lt;/td>
&lt;td>47&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>2018-01-21&lt;/td>
&lt;td>49&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>2018-01-22&lt;/td>
&lt;td>39&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>22&lt;/td>
&lt;td>2018-01-23&lt;/td>
&lt;td>55&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>23&lt;/td>
&lt;td>2018-01-24&lt;/td>
&lt;td>42&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24&lt;/td>
&lt;td>2018-01-25&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>7&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>2018-01-26&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26&lt;/td>
&lt;td>2018-01-27&lt;/td>
&lt;td>53&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27&lt;/td>
&lt;td>2018-01-28&lt;/td>
&lt;td>52&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>28&lt;/td>
&lt;td>2018-01-29&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>29&lt;/td>
&lt;td>2018-01-30&lt;/td>
&lt;td>31&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>9&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>30&lt;/td>
&lt;td>2018-01-31&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>9&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="step-5">Step 5&lt;/h3>
&lt;!-- raw HTML omitted -->
&lt;p>Looking at the results of the previous step, we can see the simply asking for &lt;code>df['step5'].max()&lt;/code> would give us the longest sequence of days for which the value of &lt;code>freezing&lt;/code> remained constant. How do we limit that to only consider sequences in which &lt;code>freezing&lt;/code> is &lt;code>True&lt;/code>? We again take advantage of the fact that a boolean is just an integer, and we multiply the result of the previous step by the value of the &lt;code>freezing&lt;/code> series:&lt;/p>
&lt;pre tabindex="0">&lt;code>df[&amp;#39;step5&amp;#39;] = df[&amp;#39;freezing&amp;#39;] * df[&amp;#39;step4&amp;#39;]
&lt;/code>&lt;/pre>&lt;p>This will zero out all the values from the previous step in which &lt;code>freezing&lt;/code> is &lt;code>False&lt;/code>, because &lt;code>False * x&lt;/code> is the same as &lt;code>0 * x&lt;/code>. This gives us our final result:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>DATE&lt;/th>
&lt;th>TMAX&lt;/th>
&lt;th>freezing&lt;/th>
&lt;th>step2&lt;/th>
&lt;th>step3&lt;/th>
&lt;th>step4&lt;/th>
&lt;th>step5&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>2018-01-01&lt;/td>
&lt;td>13&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>2018-01-02&lt;/td>
&lt;td>19&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>2018-01-03&lt;/td>
&lt;td>29&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>3&lt;/td>
&lt;td>3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>2018-01-04&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>4&lt;/td>
&lt;td>4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>2018-01-05&lt;/td>
&lt;td>24&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>5&lt;/td>
&lt;td>5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>2018-01-06&lt;/td>
&lt;td>12&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>6&lt;/td>
&lt;td>6&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>2018-01-07&lt;/td>
&lt;td>17&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>1&lt;/td>
&lt;td>7&lt;/td>
&lt;td>7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>2018-01-08&lt;/td>
&lt;td>35&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>2&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>2018-01-09&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>2018-01-10&lt;/td>
&lt;td>36&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>3&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>2018-01-11&lt;/td>
&lt;td>51&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>4&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>2018-01-12&lt;/td>
&lt;td>60&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>5&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>2018-01-13&lt;/td>
&lt;td>61&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>2&lt;/td>
&lt;td>6&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>2018-01-14&lt;/td>
&lt;td>23&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>3&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>2018-01-15&lt;/td>
&lt;td>21&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>3&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>2018-01-16&lt;/td>
&lt;td>33&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>4&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>2018-01-17&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>4&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>2018-01-18&lt;/td>
&lt;td>32&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>5&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>2018-01-19&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>6&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>2018-01-20&lt;/td>
&lt;td>47&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>2018-01-21&lt;/td>
&lt;td>49&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>3&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>2018-01-22&lt;/td>
&lt;td>39&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>4&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>22&lt;/td>
&lt;td>2018-01-23&lt;/td>
&lt;td>55&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>5&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>23&lt;/td>
&lt;td>2018-01-24&lt;/td>
&lt;td>42&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>6&lt;/td>
&lt;td>6&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24&lt;/td>
&lt;td>2018-01-25&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>7&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>2018-01-26&lt;/td>
&lt;td>34&lt;/td>
&lt;td>False&lt;/td>
&lt;td>True&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26&lt;/td>
&lt;td>2018-01-27&lt;/td>
&lt;td>53&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27&lt;/td>
&lt;td>2018-01-28&lt;/td>
&lt;td>52&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;td>3&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>28&lt;/td>
&lt;td>2018-01-29&lt;/td>
&lt;td>43&lt;/td>
&lt;td>False&lt;/td>
&lt;td>False&lt;/td>
&lt;td>8&lt;/td>
&lt;td>4&lt;/td>
&lt;td>0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>29&lt;/td>
&lt;td>2018-01-30&lt;/td>
&lt;td>31&lt;/td>
&lt;td>True&lt;/td>
&lt;td>True&lt;/td>
&lt;td>9&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>30&lt;/td>
&lt;td>2018-01-31&lt;/td>
&lt;td>30&lt;/td>
&lt;td>True&lt;/td>
&lt;td>False&lt;/td>
&lt;td>9&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="step-6">Step 6&lt;/h3>
&lt;p>Now the answer to our question is as simple as asking for the maximum value from the previous step:&lt;/p>
&lt;pre tabindex="0">&lt;code>max_consecutive_dbf = df[&amp;#39;step5&amp;#39;].max()
&lt;/code>&lt;/pre>&lt;p>And if everything worked as expected, we should find that the longest consecutive sequence of days on which the temperature stayed below freezing was 7 days, from 2018-01-01 through 2018-01-07:&lt;/p>
&lt;pre tabindex="0">&lt;code>assert max_consecutive_dbf == 7
&lt;/code>&lt;/pre>&lt;h2 id="results">Results&lt;/h2>
&lt;p>If we look at the results for the past 20 years, we see the following:&lt;/p>
&lt;figure class="left" >
&lt;img src="sample-results.png" />
&lt;/figure>
&lt;p>For data used in the above chart, the average stretch in which the temperature stays below freezing is 6.45 days (the average for the entire dataset is 6.33 days).&lt;/p></content></item><item><title>Snarl: A tool for literate blogging</title><link>https://blog.oddbit.com/post/2020-01-15-snarl-a-tool-for-literate-blog/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-01-15-snarl-a-tool-for-literate-blog/</guid><description>Literate programming is a programming paradigm introduced by Donald Knuth in which a program is combined with its documentation to form a single document. Tools are then used to extract the documentation for viewing or typesetting or to extract the program code so it can be compiled and/or run. While I have never been very enthusiastic about literate programming as a development methodology, I was recently inspired to explore these ideas as they relate to the sort of technical writing I do for this blog.</description><content>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Literate_programming">Literate programming&lt;/a> is a programming paradigm introduced by Donald Knuth in which a program is combined with its documentation to form a single document. Tools are then used to extract the documentation for viewing or typesetting or to extract the program code so it can be compiled and/or run. While I have never been very enthusiastic about literate programming as a development methodology, I was recently inspired to explore these ideas as they relate to the sort of technical writing I do for this blog.&lt;/p>
&lt;p>My previous post was about &lt;a href="https://blog.oddbit.com/post/2019-12-19-ovn-and-dhcp/">OVN and DHCP&lt;/a>. While I had tested out configuration in question prior to writing the article, I introduced some code changes in the article without testing them, and that resulted in some &lt;a href="https://github.com/larsks/blog.oddbit.com/issues/8#issuecomment-572824924">dumb errors&lt;/a>. It occurred to me that some of the same tooling that had been designed for literate programming may offer a mechanism by which I could extract and test code in my blog posts to ensure that the code I was presenting operates as described.&lt;/p>
&lt;p>Unfortunately, many of the &lt;a href="http://www.literateprogramming.com/tools.html">existing literate programming tools&lt;/a> weren&amp;rsquo;t going to work out. Several of them assume you want to write documentation in LaTeX. Others are designed primarily for a particular programming language, and just about all of them use syntax that doesn&amp;rsquo;t really play well with Markdown documents. The tool that came closest was &lt;a href="https://github.com/driusan/lmt">lmt&lt;/a>, but that tool only addresses the &amp;ldquo;tangling&amp;rdquo; aspect of document processing (extracting code), whereas I explicitly want the ability to exclude some code from the finished documentation, which requires support for &amp;ldquo;weaving&amp;rdquo; (documentation extraction).&lt;/p>
&lt;p>Since I couldn&amp;rsquo;t find a tool that did exactly what I wanted, I ended up writing my own.&lt;/p>
&lt;h2 id="introducing-snarl">Introducing Snarl&lt;/h2>
&lt;p>&lt;a href="https://github.com/larsks/snarl">Snarl&lt;/a> is a tool for writing literate blog posts. It&amp;rsquo;s primary purpose is to permit you to extract code from a Markdown document in order to test it and ensure its accuracy. It has feature similar to many other literate programming tools:&lt;/p>
&lt;ul>
&lt;li>You can present code in an order that differs from that which you output for testing. This allows you to present code in the way that makes the most sense for your readers, rather than that required by the particular programming language you&amp;rsquo;re using.&lt;/li>
&lt;li>Code blocks can refer to other code blocks. These references are expanded when writing the content to files.&lt;/li>
&lt;li>Snarl supports an &amp;ldquo;include&amp;rdquo; feature that permits you to split up a large document across multiple files.&lt;/li>
&lt;/ul>
&lt;h2 id="code-blocks">Code blocks&lt;/h2>
&lt;p>The heart of Snarl is the code block, which uses an extended form of the standard Markdown fenced code block:&lt;/p>
&lt;pre>&lt;code>```[&amp;lt;language&amp;gt;]=&amp;lt;label&amp;gt; [--file] [--hide] [--tag tag [...]] [--replace
&amp;lt;pattern&amp;gt; &amp;lt;substitution&amp;gt;]
...code goes here...
```
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>&amp;lt;language&amp;gt;&lt;/code> information is optional and is ignored by Snarl; this is standard syntax for providing syntax coloring hints for fenced code blocks. The value is passed on to the rendered Markdown when running &lt;code>snarl weave&lt;/code>.&lt;/p>
&lt;p>Everything after the &lt;code>=&lt;/code> is interpreted using a standard command-line option parser (Python&amp;rsquo;s &lt;a href="https://docs.python.org/3/library/argparse.html">argparse&lt;/a> module), which means that &lt;code>&amp;lt;label&amp;gt;&lt;/code> may contain whitespace as long as you quote it.&lt;/p>
&lt;p>The options are:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>--file&lt;/code> (&lt;code>-f&lt;/code>) &amp;ndash; mark a code block as a file. Blocks marked as files will be written out by default when running &lt;code>snarl tangle&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>--hide&lt;/code> (&lt;code>-h&lt;/code>) &amp;ndash; Elide this block when running &lt;code>snarl weave&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>--tag &amp;lt;tag&amp;gt;&lt;/code> (&lt;code>-t tag&lt;/code>) &amp;ndash; Apply a tag to the code block. You can elect to write out only certain code blocks using the &lt;code>--tag&lt;/code> option to &lt;code>snarl tangle&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>--replace &amp;lt;pattern&amp;gt; &amp;lt;subsitution&amp;gt;&lt;/code> &amp;ndash; Replace regular expression &lt;code>&amp;lt;pattern&amp;gt;&lt;/code> with &lt;code>&amp;lt;substitution&amp;gt;&lt;/code> when tangling the document.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="a-simple-example">A simple example&lt;/h2>
&lt;p>Let&amp;rsquo;s say we were going to write a post about a &amp;ldquo;Hello world!&amp;rdquo; program in C. Our document might look something like this:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h3 id="tangling">Tangling&lt;/h3>
&lt;p>If we were to run &lt;code>snarl tangle&lt;/code> on this file, we would get as output a single file, &lt;code>hello.c&lt;/code>, with the following content:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>This file contains the contents of the &lt;code>include header files&lt;/code> block and the &lt;code>main function&lt;/code> block, along with some literal content from the &lt;code>hello.c&lt;/code> block itself.&lt;/p>
&lt;p>It would be trivial to have a simple shell script compile and execute this code to ensure that it behaves as expected.&lt;/p>
&lt;h3 id="weaving">Weaving&lt;/h3>
&lt;p>Running &lt;code>snarl weave&lt;/code> on the document source would result in:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>You can see that the Snarl-annotated code blocks have been rendered as standard Markdown fenced code blocks without the additional metadata. You can also see the effect of the &lt;code>--hide&lt;/code> option on code blocks: the contents of &lt;code>hello.c&lt;/code> are excluded in the final Markdown output.&lt;/p>
&lt;h2 id="a-longer-example">A longer example&lt;/h2>
&lt;p>The &lt;a href="https://raw.githubusercontent.com/larsks/blog.oddbit.com-snarl/master/ovn-and-dhcp/2019-12-19-ovn-and-dhcp.snarl.md">source&lt;/a> to my earlier blog post on &lt;a href="https://blog.oddbit.com/post/2019-12-19-ovn-and-dhcp/">OVN and DHCP&lt;/a> provides a less contrived example. In addition to the document source itself, you can see the framework I used for running the scripts presented in the post and testing the environment afterwards.&lt;/p>
&lt;h3 id="using-replace">Using replace&lt;/h3>
&lt;p>That post provides an example of using the &lt;code>--replace&lt;/code> flag on a code block. For the purposes of the article, I was using fixed IP addresses for the nodes involved, but when setting up a virtual environment for testing it was easier to just let the nodes pick up addresses dynamically. In order to test the code that uses a static ip address, I replace that address with a variable reference when generating the script files:&lt;/p>
&lt;pre>&lt;code>```=configure_ovs_external_ids --replace 192.168.122.100 ${OVN0_ADDRESS}
ovs-vsctl set open_vswitch . \
external_ids:ovn-remote=tcp:192.168.122.100:6642 \
external_ids:ovn-encap-ip=$(ip addr show eth0 | awk '$1 == &amp;quot;inet&amp;quot; {print $2}' | cut -f1 -d/) \
external_ids:ovn-encap-type=geneve \
external_ids:system-id=$(hostname)
```
&lt;/code>&lt;/pre>
&lt;p>When running &lt;code>snarl tangle&lt;/code> on that document, the above text renders as:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl set open_vswitch . \
external_ids:ovn-remote=tcp:${OVN0_ADDRESS}:6642 \
external_ids:ovn-encap-ip=$(ip addr show eth0 | awk &amp;#39;$1 == &amp;#34;inet&amp;#34; {print $2}&amp;#39; | cut -f1 -d/) \
external_ids:ovn-encap-type=geneve \
external_ids:system-id=$(hostname)
&lt;/code>&lt;/pre>&lt;h3 id="using-tags">Using tags&lt;/h3>
&lt;p>If you look at the files embedded in that post, you will find that they are tagged using the &lt;code>--tag&lt;/code> (&lt;code>-t&lt;/code>) option:&lt;/p>
&lt;pre>&lt;code>```=configure-common.sh --file --hide -t setup
&amp;lt;&amp;lt;enable_common_services&amp;gt;&amp;gt;
&amp;lt;&amp;lt;add_br_int&amp;gt;&amp;gt;
```
&lt;/code>&lt;/pre>
&lt;p>This permits me to extract a subset of files. For example, to extract just the files tagged &lt;code>setup&lt;/code>, I would run:&lt;/p>
&lt;pre tabindex="0">&lt;code>snarl tangle -t setup 2019-12-19-ovn-and-dhcp.snarl.md
&lt;/code>&lt;/pre>&lt;h2 id="including-files">Including files&lt;/h2>
&lt;p>While the main focus of Snarl is extracting code from documentation, sometimes you want to go in the other direction. Snarl supports an &lt;code>include&lt;/code> directive that will include the content of another file in the current document. The &lt;code>include&lt;/code> directive is written as an HTML comment:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;!-- include &amp;lt;path&amp;gt; [--escape-html] [--verbatim] --&amp;gt;
&lt;/code>&lt;/pre>&lt;p>A simple example might look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;!-- include anotherfile.md --&amp;gt;
&lt;/code>&lt;/pre>&lt;p>By default, the file contents will be processed as if they were part of the existing document. That means that your included file may itself contain Snarl directives. This isn&amp;rsquo;t always the behavior that you want, so there are a couple of options available that modify the behavior of &lt;code>include&lt;/code>.&lt;/p>
&lt;p>In order to embed Snarl samples in this post, I am using the HTML &lt;code>&amp;lt;pre&amp;gt;&lt;/code> element to wrap the sample text, which I am including in this document via the &lt;code>include&lt;/code> directive. I don&amp;rsquo;t want to interpret Snarl directives in these included files. The &lt;code>--verbatim&lt;/code> (or &lt;code>-v&lt;/code>) option will include the literal content of the named file without looking for Snarl directives.&lt;/p>
&lt;p>An unfortunate side effect of using the &lt;code>&amp;lt;pre&amp;gt;&lt;/code> element is that I have to escape any &lt;code>&amp;lt;&lt;/code> characters that appear in the included content. The &lt;code>--escape-html&lt;/code> (or &lt;code>-e&lt;/code>) option performs the necessary HTML escaping on the included file so that it will display as intended.&lt;/p>
&lt;p>For example, the example Snarl source presented earlier in this document was included using the following syntax:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;!-- include hello.snarl.md -ve --&amp;gt;
&lt;/code>&lt;/pre>&lt;h2 id="to-infinity-and-beyond">To infinity and beyond&lt;/h2>
&lt;p>Being able to extract and verify code in technical articles is incredibly useful. There are other tools out there that will do something similar, but I&amp;rsquo;m happy with how Snarl operates. I will absolutely be using this going forward to avoid unfortunate errors in my blogs posts, and I hope others will find it useful as well.&lt;/p></content></item><item><title>OVN and DHCP: A minimal example</title><link>https://blog.oddbit.com/post/2019-12-19-ovn-and-dhcp/</link><pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-12-19-ovn-and-dhcp/</guid><description>Introduction A long time ago, I wrote an article all about OpenStack Neutron (which at that time was called Quantum). That served as an excellent reference for a number of years, but if you&amp;rsquo;ve deployed a recent version of OpenStack you may have noticed that the network architecture looks completely different. The network namespaces previously used to implement routers and dhcp servers are gone (along with iptables rules and other features), and have been replaced by OVN (&amp;ldquo;Open Virtual Network&amp;rdquo;).</description><content>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>A long time ago, I wrote an article &lt;a href="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/">all about OpenStack Neutron&lt;/a> (which at that time was called Quantum). That served as an excellent reference for a number of years, but if you&amp;rsquo;ve deployed a recent version of OpenStack you may have noticed that the network architecture looks completely different. The network namespaces previously used to implement routers and dhcp servers are gone (along with iptables rules and other features), and have been replaced by OVN (&amp;ldquo;Open Virtual Network&amp;rdquo;). What is OVN? How does it work? In this article, I&amp;rsquo;d like to explore a minimal OVN installation to help answer these questions.&lt;/p>
&lt;h2 id="goals">Goals&lt;/h2>
&lt;p>We&amp;rsquo;re going to create a single OVN logical switch to which we will attach a few ports. We will demonstrate how we can realize a port on a physical node and configure it using DHCP, using a virtual DHCP server provided by OVN.&lt;/p>
&lt;h2 id="so-what-is-ovn-anyway">So what is OVN anyway?&lt;/h2>
&lt;p>If you&amp;rsquo;re just getting started with OVN, you&amp;rsquo;ll find that&amp;rsquo;s a hard question to answer: there is no dedicated OVN website; there&amp;rsquo;s no OVN landing page at &lt;a href="http://openvswitch.org">http://openvswitch.org&lt;/a>; in fact, there&amp;rsquo;s really no documentation for OVN at all other than the man pages. The only high-level description you&amp;rsquo;ll find comes from the &lt;code>ovn-architecture(7)&lt;/code> man page:&lt;/p>
&lt;blockquote>
&lt;p>OVN, the Open Virtual Network, is a system to support virtual network
abstraction. OVN complements the existing capabilities of OVS to add
native support for virtual network abstractions, such as virtual L2 and L3
overlays and security groups.&lt;/p>
&lt;/blockquote>
&lt;p>Where Open vSwitch (OVS) provides a virtual switch on a single host, OVN extends this abstraction to span multiple hosts. You can create virtual switches that span many physical nodes, and OVN will take care of creating overlay networks to support this abstraction. While OVS is primarily just a layer 2 device, OVN also operates at layer 3: you can create virtual routers to connect your virtual networks as well a variety of access control mechanisms such as security groups and ACLs.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>You&amp;rsquo;re going to need a recent version of OVN. Packages are available for &lt;a href="http://docs.openvswitch.org/en/latest/intro/install/distributions/">most major distributions&lt;/a>. I used &lt;a href="https://getfedora.org/">Fedora 31&lt;/a> for my testing, which includes OVS and OVN version 2.12.0. You can of course also &lt;a href="http://docs.openvswitch.org/en/latest/intro/install/">install from source&lt;/a>.&lt;/p>
&lt;p>This post assumes that you are logged in to your system as the &lt;code>root&lt;/code> user. Most of the commands require root access in order to function correctly.&lt;/p>
&lt;h2 id="concepts">Concepts&lt;/h2>
&lt;p>OVN operates with a pair of databases. The &lt;em>Northbound&lt;/em> database contains the &lt;em>logical&lt;/em> structure of your networks: this is where you define switches, routers, ports, and so on.&lt;/p>
&lt;p>The &lt;em>Southbound&lt;/em> database is concerned with the &lt;em>physical&lt;/em> structure of your network. This database maintains information about which ports are realized on which hosts.&lt;/p>
&lt;p>The &lt;a href="http://www.openvswitch.org/support/dist-docs/ovn-northd.8.html">&lt;code>ovn-northd&lt;/code>&lt;/a> service &amp;ldquo;translates the logical network configuration in terms of conventional network concepts, taken from the OVN North‐ bound Database, into logical datapath flows in the OVN Southbound Database below it.&amp;rdquo; (&lt;a href="http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html">ovn-architecture(7)&lt;/a>)&lt;/p>
&lt;p>The &lt;a href="http://www.openvswitch.org/support/dist-docs/ovn-controller.8.html">&lt;code>ovn-controller&lt;/code>&lt;/a> service running on each host connects to the Southbound database and is responsible for configuring OVS as instructed by the database configuration.&lt;/p>
&lt;h2 id="test-environment">Test environment&lt;/h2>
&lt;p>This article assumes a test environment with three nodes running Fedora 31. All nodes have a single interface connecting to a shared layer 2 network:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>MAC address&lt;/th>
&lt;th>IP address&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ovn0&lt;/td>
&lt;td>de:ca:ff:00:00:64&lt;/td>
&lt;td>192.168.122.100&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ovn1&lt;/td>
&lt;td>de:ca:ff:00:00:65&lt;/td>
&lt;td>192.168.122.101&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ovn2&lt;/td>
&lt;td>de:ca:ff:00:00:66&lt;/td>
&lt;td>192.168.122.102&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="setting-up-ovn">Setting up OVN&lt;/h1>
&lt;h2 id="initial-configuration-steps">Initial configuration steps&lt;/h2>
&lt;p>Our first step will be to activate &lt;code>openvswitch&lt;/code> and &lt;code>ovn-controller&lt;/code> on all of the nodes in our test environment. On all nodes, run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl enable --now openvswitch ovn-controller
&lt;/code>&lt;/pre>&lt;p>The &lt;code>--now&lt;/code> flag causes &lt;code>systemd&lt;/code> to start the service as well as enabling it in future boots.&lt;/p>
&lt;p>By default, OVN manages an &lt;code>openvswitch&lt;/code> bridge named &lt;code>br-int&lt;/code> (for &amp;ldquo;integration&amp;rdquo;). We&amp;rsquo;ll need to create this on all of our nodes. On all nodes, run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-br br-int
&lt;/code>&lt;/pre>&lt;h2 id="configuring-the-controller">Configuring the controller&lt;/h2>
&lt;p>We will designate the node &lt;code>ovn0&lt;/code> as our controller (which simply means &amp;ldquo;this node will run &lt;code>ovn-northd&lt;/code>). The first thing we need to do is enable the &lt;code>ovn-northd&lt;/code> service. On node &lt;code>ovn0&lt;/code>, run:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl enable --now ovn-northd
&lt;/code>&lt;/pre>&lt;p>In addition to starting the &lt;code>ovn-northd&lt;/code> service itself, this will also starts two instances of &lt;a href="http://www.openvswitch.org/support/dist-docs/ovsdb-server.1.html">&lt;code>ovsdb-server&lt;/code>&lt;/a>: one serving the Northbound database, listening on &lt;code>/run/ovn/ovnnb_db.sock&lt;/code>, and the second for the Southbound database, listening on &lt;code>/run/ovn/ovnsb_db.sock&lt;/code>. In order for the &lt;code>ovn-controller&lt;/code> service on the other nodes to connect to the Southbound database, we will need to configure that instance of &lt;code>ovsdb-server&lt;/code> to listen for tcp connections. We can do that using the &lt;code>ovn-sbctl set-connection&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-sbctl set-connection ptcp:6642
&lt;/code>&lt;/pre>&lt;p>The &lt;code>ptcp&lt;/code> in the above setting means &amp;ldquo;passive tcp&amp;rdquo;, which means &amp;ldquo;listen on port 6642 for connections&amp;rdquo;. After running the above command, we see that there is now an &lt;code>ovsdb-server&lt;/code> instance listening on port 6642:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ss -tlnp | grep 6642
LISTEN 0 10 0.0.0.0:6642 0.0.0.0:* users:((&amp;#34;ovsdb-server&amp;#34;,pid=1798,fd=21))
&lt;/code>&lt;/pre>&lt;h2 id="connecting-nodes-to-the-controller">Connecting nodes to the controller&lt;/h2>
&lt;p>Now that we have our controller configured, we have to connect the &lt;code>ovn-controller&lt;/code> service on our nodes to the Southbound database. We do this by creating several entries in the &lt;code>external_ids&lt;/code> column of the OVS &lt;code>open_vswitch&lt;/code> database on each host:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ovn-remote&lt;/code> &amp;ndash; this is the address of the controller&lt;/li>
&lt;li>&lt;code>ovn-encap-ip&lt;/code> &amp;ndash; this is the local address that will be used for tunnel endpoints&lt;/li>
&lt;li>&lt;code>ovn-encap-type&lt;/code> &amp;ndash; the encapsulation mechanism to use for tunnels&lt;/li>
&lt;li>&lt;code>system-id&lt;/code> &amp;ndash; a unique identifier for the local host&lt;/li>
&lt;/ul>
&lt;p>On all nodes, run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl set open_vswitch . \
external_ids:ovn-remote=tcp:192.168.122.100:6642 \
external_ids:ovn-encap-ip=$(ip addr show eth0 | awk &amp;#39;$1 == &amp;#34;inet&amp;#34; {print $2}&amp;#39; | cut -f1 -d/) \
external_ids:ovn-encap-type=geneve \
external_ids:system-id=$(hostname)
&lt;/code>&lt;/pre>&lt;p>This points &lt;code>ovn-remote&lt;/code> at the address of the controller, sets &lt;code>ovn-encap-ip&lt;/code> to the address of &lt;code>eth0&lt;/code> on the local host, sets &lt;code>systemd-id&lt;/code> to the local hostname, and selects &lt;a href="https://tools.ietf.org/html/draft-ietf-nvo3-geneve-08">geneve&lt;/a> encapsulation for tunnels (see &lt;a href="https://blog.russellbryant.net/2017/05/30/ovn-geneve-vs-vxlan-does-it-matter/">this post&lt;/a> for information on why OVN prefers Geneve encapsulation).&lt;/p>
&lt;p>We can verify these settings by using the &lt;code>ovs-vsctl list&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn1 ~]# ovs-vsctl --columns external_ids list open_vswitch
external_ids : {hostname=&amp;#34;ovn1.virt&amp;#34;, ovn-encap-ip=&amp;#34;192.168.122.101&amp;#34;, ovn-encap-type=geneve, ovn-remote=&amp;#34;192.168.122.100&amp;#34;, rundir=&amp;#34;/var/run/openvswitch&amp;#34;, system-id=&amp;#34;ovn1&amp;#34;}
&lt;/code>&lt;/pre>&lt;p>After running the above commands, each node should now have tunnels interfaces connecting to the other nodes in the test environment. For example, running &lt;code>ovs-vsctl show&lt;/code> on node &lt;code>ovn1&lt;/code> looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>f0087676-7f93-419c-9da0-32321d2d3668
Bridge br-int
fail_mode: secure
Port &amp;#34;ovn-ovn0-0&amp;#34;
Interface &amp;#34;ovn-ovn0-0&amp;#34;
type: geneve
options: {csum=&amp;#34;true&amp;#34;, key=flow, remote_ip=&amp;#34;192.168.122.100&amp;#34;}
Port br-int
Interface br-int
type: internal
Port &amp;#34;ovn-ovn2-0&amp;#34;
Interface &amp;#34;ovn-ovn2-0&amp;#34;
type: geneve
options: {csum=&amp;#34;true&amp;#34;, key=flow, remote_ip=&amp;#34;192.168.122.102&amp;#34;}
ovs_version: &amp;#34;2.12.0&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Due to what appears to be &lt;a href="https://mail.openvswitch.org/pipermail/ovs-discuss/2020-January/049692.html">some sort of race condition in OVN&lt;/a>, you may not see the geneve tunnels in the &lt;code>ovs-vsctl show&lt;/code> output. If this is the case, restart &lt;code>ovn-controller&lt;/code> on all your ovn nodes:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl restart ovn-controller
&lt;/code>&lt;/pre>&lt;p>The issue with the geneve tunnels appears to be resolved by &lt;a href="https://patchwork.ozlabs.org/patch/1222380/">this patch&lt;/a>, which will hopefully land in OVN in the near future.&lt;/p>
&lt;h1 id="creating-a-virtual-network">Creating a virtual network&lt;/h1>
&lt;p>Now that we have a functioning OVN environment, we&amp;rsquo;re ready to create our virtual network.&lt;/p>
&lt;h2 id="create-a-logical-switch">Create a logical switch&lt;/h2>
&lt;p>We&amp;rsquo;ll start by creating a logical switch, which we will call &lt;code>net0&lt;/code>. We create that using the &lt;code>ovn-nbctl ls-add&lt;/code> command. Run the following on &lt;code>ovn0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl ls-add net0
&lt;/code>&lt;/pre>&lt;p>After running the above command, the output of &lt;code>ovn-nbctl show&lt;/code> will look something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-nbctl show
switch d8d96fb2-e1e7-469d-8c72-b7e891fb16ba (net0)
&lt;/code>&lt;/pre>&lt;p>Next, we need to set some configuration options on the switch that will be used to set the range from which we allocate addresses via DHCP. We&amp;rsquo;re going to have OVN manage the &lt;code>10.0.0.0/24&lt;/code> network, which means we need to set &lt;code>other_config:subnet&lt;/code> to &lt;code>10.0.0.0/24&lt;/code>. I generally like to reserve some addresses from the DHCP range to use for static allocations, so I have also set &lt;code>other_config:exclude_ips&lt;/code> to &lt;code>10.0.0.1..10.0.0.10&lt;/code>. This means that DHCP allocations will come from the range &lt;code>10.0.0.11&lt;/code> - &lt;code>10.0.0.254&lt;/code>.&lt;/p>
&lt;p>To apply these settings, run the following commands on &lt;code>ovn0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl set logical_switch net0 \
other_config:subnet=&amp;#34;10.0.0.0/24&amp;#34; \
other_config:exclude_ips=&amp;#34;10.0.0.1..10.0.0.10&amp;#34;
&lt;/code>&lt;/pre>&lt;h2 id="create-dhcp-options">Create DHCP options&lt;/h2>
&lt;p>Each port that we want to configure using DHCP needs to be associated with a set of DHCP options. We accomplish this by creating a new entry in the Northbound &lt;code>dhcp_options&lt;/code> table, and then set the &lt;code>dhcp_options&lt;/code> column of the port to the id of the object we created in the &lt;code>dhcp_options&lt;/code> table.&lt;/p>
&lt;p>Looking at &lt;a href="https://github.com/ovn-org/ovn/blob/master/northd/ovn-northd.c#L4113">the source&lt;/a>, there are three required options that must be set in order for DHCP to operate:&lt;/p>
&lt;ul>
&lt;li>&lt;code>server_id&lt;/code> &amp;ndash; the ip address of the virtual dhcp server&lt;/li>
&lt;li>&lt;code>server_mac&lt;/code> &amp;ndash; the MAC address of the virtual dhcp server&lt;/li>
&lt;li>&lt;code>lease_time&lt;/code> &amp;ndash; the lifetime of DHCP leases&lt;/li>
&lt;/ul>
&lt;p>While not actually required, we can also set the &lt;code>router&lt;/code> key to provide information about the default gateway. We&amp;rsquo;re not going to make use of it in this example, but in practice you will probably want to set the &lt;code>router&lt;/code> option.&lt;/p>
&lt;p>We also need to set the CIDR range that will be served by the DHCP server.&lt;/p>
&lt;p>We can create the appropriate options using the &lt;code>ovn-nbctl dhcp-options-create&lt;/code> command. Run the following on &lt;code>ovn0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl dhcp-options-create 10.0.0.0/24
&lt;/code>&lt;/pre>&lt;p>Despite the name of that command, it doesn&amp;rsquo;t actually let us set DHCP options. For that, we need to first look up the uuid of our newly created entry in the &lt;code>dhcp_options&lt;/code> table. Let&amp;rsquo;s store that in the &lt;code>CIDR_UUID&lt;/code> variable, which we will use in a few places in the remainder of this post:&lt;/p>
&lt;pre tabindex="0">&lt;code>CIDR_UUID=$(ovn-nbctl --bare --columns=_uuid find dhcp_options cidr=&amp;#34;10.0.0.0/24&amp;#34;)
&lt;/code>&lt;/pre>&lt;p>With that uuid in hand, we can now set the required options:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl dhcp-options-set-options ${CIDR_UUID} \
lease_time=3600 \
router=10.0.0.1 \
server_id=10.0.0.1 \
server_mac=c0:ff:ee:00:00:01
&lt;/code>&lt;/pre>&lt;p>We can use the database &lt;code>list&lt;/code> command to inspect the &lt;code>dhcp_options&lt;/code> table to verify that things look as we expect:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-nbctl list dhcp_options
_uuid : f8a6abc5-b8e4-4209-8809-b95435b4d48b
cidr : &amp;#34;10.0.0.0/24&amp;#34;
external_ids : {lease_time=&amp;#34;3600&amp;#34;, router=&amp;#34;10.0.0.1&amp;#34;, server_id=&amp;#34;10.0.0.1&amp;#34;, server_mac=&amp;#34;c0:ff:ee:00:00:01&amp;#34;}
options : {}
&lt;/code>&lt;/pre>&lt;p>Instead of using the &lt;code>dhcp-options-create&lt;/code> command, as we did in this section, we could instead have used the database &lt;code>create&lt;/code> command. The quoting requirements for that command are a little more complex, but unlike the &lt;code>dhcp-options-create&lt;/code> command the &lt;code>create&lt;/code> command returns the id of the row it creates. This can be useful if you&amp;rsquo;re using the command as part of a script. The equivalent &lt;code>create&lt;/code> command would look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>CIDR_UUID=$(ovn-nbctl create dhcp_options \
cidr=10.0.0.0/24 \
options=&amp;#39;&amp;#34;lease_time&amp;#34;=&amp;#34;3600&amp;#34; &amp;#34;router&amp;#34;=&amp;#34;10.0.0.1&amp;#34; &amp;#34;server_id&amp;#34;=&amp;#34;10.0.0.1&amp;#34; &amp;#34;server_mac&amp;#34;=&amp;#34;c0:ff:ee:00:00:01&amp;#34;&amp;#39;)
&lt;/code>&lt;/pre>&lt;h2 id="create-logical-ports">Create logical ports&lt;/h2>
&lt;p>Let&amp;rsquo;s add the following three logical ports to the switch:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>MAC Address&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>port1&lt;/td>
&lt;td>c0:ff:ee:00:00:11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>port2&lt;/td>
&lt;td>c0:ff:ee:00:00:12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>port3&lt;/td>
&lt;td>c0:ff:ee:00:00:13&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For each port, we&amp;rsquo;ll need to run three commands. First, we create the port on the switch:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-add net0 port1
&lt;/code>&lt;/pre>&lt;p>Next, we set the port addresses. For this example, I&amp;rsquo;m using static MAC addresses and dynamic (assigned by DHCP) IP addresses, so the command will look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-set-addresses port1 &amp;#34;c0:ff:ee:00:00:11 dynamic&amp;#34;
&lt;/code>&lt;/pre>&lt;p>If you want OVN to set MAC addresses for the ports as well, you would instead run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-set-addresses port1 &amp;#34;dynamic&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Finally, we associate the port with the DHCP options we created in the previous section:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-set-dhcpv4-options port1 $CIDR_UUID
&lt;/code>&lt;/pre>&lt;p>Repeat the above sequence for &lt;code>port2&lt;/code> and &lt;code>port3&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-add net0 port2
ovn-nbctl lsp-set-addresses port2 &amp;#34;c0:ff:ee:00:00:12 dynamic&amp;#34;
ovn-nbctl lsp-set-dhcpv4-options port2 $CIDR_UUID
ovn-nbctl lsp-add net0 port3
ovn-nbctl lsp-set-addresses port3 &amp;#34;c0:ff:ee:00:00:13 dynamic&amp;#34;
ovn-nbctl lsp-set-dhcpv4-options port3 $CIDR_UUID
&lt;/code>&lt;/pre>&lt;p>When you&amp;rsquo;re done, &lt;code>ovn-nbctl show&lt;/code> should return output similar to the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>switch 3c03342f-f762-410b-9f4e-572d266c8ff7 (net0)
port port2
addresses: [&amp;#34;c0:ff:ee:00:00:12 dynamic&amp;#34;]
port port3
addresses: [&amp;#34;c0:ff:ee:00:00:13 dynamic&amp;#34;]
port port1
addresses: [&amp;#34;c0:ff:ee:00:00:11 dynamic&amp;#34;]
&lt;/code>&lt;/pre>&lt;p>We can see additional details using the database command &lt;code>ovn-nbctl list logical_switch_port&lt;/code>. The entry for &lt;code>port1&lt;/code> might look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>_uuid : 8ad6a4c0-4c7b-4817-bf13-8e7b1a86bab1
addresses : [&amp;#34;c0:ff:ee:00:00:11 dynamic&amp;#34;]
dhcpv4_options : f8a6abc5-b8e4-4209-8809-b95435b4d48b
dhcpv6_options : []
dynamic_addresses : &amp;#34;c0:ff:ee:00:00:11 10.0.0.11&amp;#34;
enabled : []
external_ids : {}
ha_chassis_group : []
name : port1
options : {}
parent_name : []
port_security : []
tag : []
tag_request : []
type : &amp;#34;&amp;#34;
up : false
&lt;/code>&lt;/pre>&lt;p>Looking at the &lt;code>dynamic_addresses&lt;/code> column we can see that &lt;code>port1&lt;/code> has been assigned the ip address &lt;code>10.0.0.11&lt;/code>. We can see the assigned addresses for all of our ports like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-nbctl --columns dynamic_addresses list logical_switch_port
dynamic_addresses : &amp;#34;c0:ff:ee:00:00:13 10.0.0.13&amp;#34;
dynamic_addresses : &amp;#34;c0:ff:ee:00:00:11 10.0.0.11&amp;#34;
dynamic_addresses : &amp;#34;c0:ff:ee:00:00:12 10.0.0.12&amp;#34;
&lt;/code>&lt;/pre>&lt;h2 id="simulating-a-dhcp-request-with-ovn-trace">Simulating a DHCP request with ovn-trace&lt;/h2>
&lt;p>At this point, we have a functioning switch, although we haven&amp;rsquo;t actually realized the ports anywhere yet. This is the perfect time to introduce the &lt;code>ovn-trace&lt;/code> tool, which can be used to simulate how your OVN network will handle a packet of data.&lt;/p>
&lt;p>We can show how OVN will respond to a DHCP &lt;code>DISCOVER&lt;/code> message with the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-trace --summary net0 &amp;#39;
inport==&amp;#34;port1&amp;#34; &amp;amp;&amp;amp;
eth.src==c0:ff:ee:00:00:11 &amp;amp;&amp;amp;
ip4.src==0.0.0.0 &amp;amp;&amp;amp;
ip.ttl==1 &amp;amp;&amp;amp;
ip4.dst==255.255.255.255 &amp;amp;&amp;amp;
udp.src==68 &amp;amp;&amp;amp;
udp.dst==67&amp;#39;
&lt;/code>&lt;/pre>&lt;p>The above command simulates a packet originating on &lt;code>port1&lt;/code> with the appropriate MAC address (&lt;code>eth.src&lt;/code>, &lt;code>c0:ff:ee:00:00:11&lt;/code>) and a source address (&lt;code>ip4.src&lt;/code>) of &lt;code>0.0.0.0&lt;/code> (port 68 (&lt;code>udp.src&lt;/code>)), targeting (&lt;code>ip4.dst&lt;/code>) the broadcast address &lt;code>255.255.255.255&lt;/code> (port 67 (&lt;code>udp.dst&lt;/code>)).&lt;/p>
&lt;p>Assuming everything is functioning correctly, this should produce the following output:&lt;/p>
&lt;pre tabindex="0">&lt;code># udp,reg14=0x2,vlan_tci=0x0000,dl_src=c0:ff:ee:00:00:11,dl_dst=c0:ff:ee:00:00:01,nw_src=0.0.0.0,nw_dst=255.255.255.255,nw_tos=0,nw_ecn=0,nw_ttl=1,tp_src=68,tp_dst=67
ingress(dp=&amp;#34;net0&amp;#34;, inport=&amp;#34;port1&amp;#34;) {
next;
reg0[3] = put_dhcp_opts(offerip = 10.0.0.11, lease_time = 3600, netmask = 255.255.255.0, router = 10.0.0.1, server_id = 10.0.0.1);
/* We assume that this packet is DHCPDISCOVER or DHCPREQUEST. */;
next;
eth.dst = eth.src;
eth.src = c0:ff:ee:00:00:01;
ip4.dst = 10.0.0.11;
ip4.src = 10.0.0.1;
udp.src = 67;
udp.dst = 68;
outport = inport;
flags.loopback = 1;
output;
egress(dp=&amp;#34;net0&amp;#34;, inport=&amp;#34;port1&amp;#34;, outport=&amp;#34;port1&amp;#34;) {
next;
output;
/* output to &amp;#34;port1&amp;#34;, type &amp;#34;&amp;#34; */;
};
};
&lt;/code>&lt;/pre>&lt;p>In the above output, you can see that OVN is filling in the details of the DHCP lease (that&amp;rsquo;s the &lt;code>put_dhcp_options&lt;/code> command), and then sending the packet back out &lt;code>port1&lt;/code> with the ethernet source and destination addresses reversed (so that the destination address is now the MAC address of &lt;code>port1&lt;/code>).&lt;/p>
&lt;p>It looks like everything is working in theory. Let&amp;rsquo;s attach some actual network interfaces and see what happens!&lt;/p>
&lt;h1 id="attaching-network-interfaces">Attaching network interfaces&lt;/h1>
&lt;p>In this section, we will attach network interfaces to our logical switch and demonstrate that they can be properly configured using DHCP.&lt;/p>
&lt;h2 id="create-an-ovs-port">Create an OVS port&lt;/h2>
&lt;p>On host &lt;code>ovn1&lt;/code>, let&amp;rsquo;s create port &lt;code>port1&lt;/code>. We&amp;rsquo;ll want to ensure that (a) the MAC address of this port matches the MAC address we configured earlier (&lt;code>c0:ff:ee:00:00:11&lt;/code>), and we need to make sure that the &lt;code>iface-id&lt;/code> external id matches the port name we registered in the Northbound database. We can do that with the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br-int port1 -- \
set interface port1 \
type=internal \
mac=&amp;#39;[&amp;#34;c0:ff:ee:00:00:11&amp;#34;]&amp;#39; \
external_ids:iface-id=port1
&lt;/code>&lt;/pre>&lt;p>After running this command, running &lt;code>ovs-vsctl show&lt;/code> on &lt;code>ovn1&lt;/code> should produce:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn1 ~]# ovs-vsctl show
f359ad7a-5fcd-49b3-8557-e61be3a0b130
Bridge br-int
fail_mode: secure
Port br-int
Interface br-int
type: internal
Port &amp;#34;port1&amp;#34;
Interface &amp;#34;port1&amp;#34;
type: internal
Port &amp;#34;ovn-ovn2-0&amp;#34;
Interface &amp;#34;ovn-ovn2-0&amp;#34;
type: geneve
options: {csum=&amp;#34;true&amp;#34;, key=flow, remote_ip=&amp;#34;192.168.122.102&amp;#34;}
Port &amp;#34;ovn-ovn0-0&amp;#34;
Interface &amp;#34;ovn-ovn0-0&amp;#34;
type: geneve
options: {csum=&amp;#34;true&amp;#34;, key=flow, remote_ip=&amp;#34;192.168.122.100&amp;#34;}
ovs_version: &amp;#34;2.12.0&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Furthermore, OVN should also be aware of this port. If we run &lt;code>ovn-sbctl show&lt;/code> on &lt;code>ovn0&lt;/code>, we see a binding for host &lt;code>ovn1&lt;/code> (look for the &lt;code>Port_Binding port1&lt;/code> line under &lt;code>Chassis ovn1&lt;/code>):&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-sbctl show
Chassis ovn0
hostname: ovn0.virt
Encap geneve
ip: &amp;#34;192.168.122.100&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Chassis ovn1
hostname: ovn1.virt
Encap geneve
ip: &amp;#34;192.168.122.101&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Port_Binding port1
Chassis ovn2
hostname: ovn2.virt
Encap geneve
ip: &amp;#34;192.168.122.102&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
&lt;/code>&lt;/pre>&lt;h2 id="configure-the-port-using-dhcp">Configure the port using DHCP&lt;/h2>
&lt;p>We can now try to configure this interface with DHCP. Let&amp;rsquo;s first move the interface into a network namespace; this means we don&amp;rsquo;t need to worry about messing up routing on the host. We&amp;rsquo;ll create a namespace named &lt;code>vm1&lt;/code> and make &lt;code>port1&lt;/code> part of that namespace:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip netns add vm1
ip link set netns vm1 port1
ip -n vm1 addr add 127.0.0.1/8 dev lo
ip -n vm1 link set lo up
&lt;/code>&lt;/pre>&lt;p>We can now configure the interface using DHCP by running the &lt;code>dhclient&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip netns exec vm1 dhclient -v -i port1 --no-pid
&lt;/code>&lt;/pre>&lt;p>After &lt;code>dhclient&lt;/code> goes to the background, we see that it was able to successfully request an address:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn1 ~]# ip netns exec vm1 dhclient -v -i port1 --no-pid
Internet Systems Consortium DHCP Client 4.4.1
Copyright 2004-2018 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/
Listening on LPF/port1/c0:ff:ee:00:00:11
Sending on LPF/port1/c0:ff:ee:00:00:11
Sending on Socket/fallback
Created duid &amp;#34;\000\004\344J\012\236\007\033AF\261\354\246\273\206\011\226g&amp;#34;.
DHCPDISCOVER on port1 to 255.255.255.255 port 67 interval 7 (xid=0xffc0820a)
DHCPOFFER of 10.0.0.11 from 10.0.0.1
DHCPREQUEST for 10.0.0.11 on port1 to 255.255.255.255 port 67 (xid=0xffc0820a)
DHCPACK of 10.0.0.11 from 10.0.0.1 (xid=0xffc0820a)
bound to 10.0.0.11 -- renewal in 1378 seconds.
&lt;/code>&lt;/pre>&lt;p>And it has correctly configured the interface:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn1 ~]# ip netns exec vm1 ip addr show port1
6: port1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
link/ether c0:ff:ee:00:00:11 brd ff:ff:ff:ff:ff:ff
inet 10.0.0.11/24 brd 10.0.0.255 scope global dynamic port1
valid_lft 577sec preferred_lft 577sec
inet6 fe80::c2ff:eeff:fe00:11/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;h2 id="configuring-port2-on-ovn1">Configuring port2 on ovn1&lt;/h2>
&lt;p>Let&amp;rsquo;s repeat the above process with &lt;code>port2&lt;/code>, again using host &lt;code>ovn1&lt;/code>. First we add the port:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br-int port2 -- \
set interface port2 \
type=internal \
mac=&amp;#39;[&amp;#34;c0:ff:ee:00:00:12&amp;#34;]&amp;#39; \
external_ids:iface-id=port2
&lt;/code>&lt;/pre>&lt;p>Add it to a namespace:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip netns add vm2
ip link set netns vm2 port2
ip -n vm2 addr add 127.0.0.1/8 dev lo
ip -n vm2 link set lo up
&lt;/code>&lt;/pre>&lt;p>Configure it using &lt;code>dhclient&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip netns exec vm2 dhclient -v -i port2 --no-pid
&lt;/code>&lt;/pre>&lt;p>And finally look at the OVN port bindings on &lt;code>ovn0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-sbctl show
Chassis ovn1
hostname: ovn1.virt
Encap geneve
ip: &amp;#34;192.168.122.101&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Port_Binding port2
Port_Binding port1
Chassis ovn0
hostname: ovn0.virt
Encap geneve
ip: &amp;#34;192.168.122.100&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Chassis ovn2
hostname: ovn2.virt
Encap geneve
ip: &amp;#34;192.168.122.102&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
&lt;/code>&lt;/pre>&lt;h2 id="configuring-port3-on-ovn2">Configuring port3 on ovn2&lt;/h2>
&lt;p>Lastly, let&amp;rsquo;s repeat the above process for &lt;code>port3&lt;/code> on host &lt;code>ovn2&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br-int port3 -- \
set interface port3 \
type=internal \
mac=&amp;#39;[&amp;#34;c0:ff:ee:00:00:13&amp;#34;]&amp;#39; \
external_ids:iface-id=port3
ip netns add vm3
ip link set netns vm3 port3
ip -n vm3 addr add 127.0.0.1/8 dev lo
ip -n vm3 link set lo up
ip netns exec vm3 dhclient -v -i port3 --no-pid
&lt;/code>&lt;/pre>&lt;p>When we&amp;rsquo;re done, &lt;code>ovn-sbctl show&lt;/code> looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-sbctl show
Chassis ovn1
hostname: ovn1.virt
Encap geneve
ip: &amp;#34;192.168.122.101&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Port_Binding port2
Port_Binding port1
Chassis ovn0
hostname: ovn0.virt
Encap geneve
ip: &amp;#34;192.168.122.100&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Chassis ovn2
hostname: ovn2.virt
Encap geneve
ip: &amp;#34;192.168.122.102&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Port_Binding port3
&lt;/code>&lt;/pre>&lt;h2 id="verify-connectivity">Verify connectivity&lt;/h2>
&lt;p>We can verify that the network namespaces we&amp;rsquo;ve created in the above examples are able to communicate with each other regardless of the host on which they have been created. For example, if we log into &lt;code>ovn2&lt;/code> we can show that we are able to reach the address of &lt;code>port1&lt;/code> (&lt;code>10.0.0.11&lt;/code>) from &lt;code>port3&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn2 ~]# ip netns exec vm3 ping -c1 10.0.0.11
PING 10.0.0.11 (10.0.0.11) 56(84) bytes of data.
64 bytes from 10.0.0.11: icmp_seq=1 ttl=64 time=0.266 ms
--- 10.0.0.11 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.266/0.266/0.266/0.000 ms
&lt;/code>&lt;/pre>&lt;h1 id="thats-all-folks">That&amp;rsquo;s all folks!&lt;/h1>
&lt;p>I hope this post helps you understand how to set up a simple OVN environment with DHCP. Please feel free to leave comments and questions!&lt;/p>
&lt;h2 id="thanks-to">Thanks to&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/LorenzoBianconi">Lorenzo Bianconi&lt;/a> for helping sort this out over email.&lt;/li>
&lt;li>&lt;a href="https://twitter.com/zhouhanok">Han Zhou&lt;/a> for helping solve the issue around Geneve tunnels coming up appropriately.&lt;/li>
&lt;/ul>
&lt;h2 id="see-also">See also&lt;/h2>
&lt;p>Below are some of the resources to which I referred while figuring out how to put this all together:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/09/03/ovn-dynamic-ip-address-management/">Dynamic IP address management in Open Virtual Network (OVN): Part One&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/">Dynamic IP address management in Open Virtual Network (OVN): Part Two&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://qyx.me/2018/07/10/run-and-test-ovn/">Run Open Virtual Network (OVN) in Ubuntu&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://dani.foroselectronica.es/simple-ovn-setup-in-5-minutes-491/">Simple OVN setup in 5 minutes&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>TM-V71A and Linux, part 1: Programming mode</title><link>https://blog.oddbit.com/post/2019-10-03-tm-v71a-linux-part-1/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-10-03-tm-v71a-linux-part-1/</guid><description>I recently acquired my Technician amateur radio license, and like many folks my first radio purchase was a Baofeng UV-5R. Due to its low cost, this is a very popular radio, and there is excellent open source software available for programming it in the form of the CHIRP project. After futzing around with the UV-5R for a while, I wanted to get something a little nicer for use at home, so I purchased a Kenwood TM-V71A.</description><content>&lt;p>I recently acquired my Technician amateur radio license, and like many folks my first radio purchase was a &lt;a href="https://baofengtech.com/uv-5r?PageSpeed=noscript">Baofeng UV-5R&lt;/a>. Due to its low cost, this is a very popular radio, and there is excellent open source software available for programming it in the form of the &lt;a href="https://chirp.danplanet.com/projects/chirp/wiki/Home">CHIRP&lt;/a> project. After futzing around with the UV-5R for a while, I wanted to get something a little nicer for use at home, so I purchased a &lt;a href="https://www.kenwood.com/usa/com/amateur/tm-v71a/">Kenwood TM-V71A&lt;/a>. CHIRP claims to have support for this radio as well, but it turns out it&amp;rsquo;s not very good: it uses a &amp;ldquo;live&amp;rdquo; connection so every time you edit a channel it tries to update the radio. This result in a slow and flaky UI, especially when trying to make bulk changes (like relocating a block of channels). I ended up using Kenwood&amp;rsquo;s official &lt;a href="https://www.kenwood.com/i/products/info/amateur/mcp_2a.html">MCP-2A&lt;/a> software running on a Windows guest on my system, which works but isn&amp;rsquo;t ideal. I decided to learn more about how the radio interacts with the computer to see if I could improve the situation.&lt;/p>
&lt;hr>
&lt;h2 id="existing-solutions">Existing solutions&lt;/h2>
&lt;p>The &lt;a href="https://hamlib.github.io/">Hamlib&lt;/a> project has an &lt;a href="https://github.com/Hamlib/Hamlib/blob/master/kenwood/tmd710.c">existing TM-D710 driver&lt;/a> that also support the TM-V71. Using either the &lt;code>rigctl&lt;/code> command line tool or using the Hamlib API, it is possible to&amp;hellip;&lt;/p>
&lt;ul>
&lt;li>Control PTT on the radio&lt;/li>
&lt;li>Tune to a specific frequency&lt;/li>
&lt;li>Tune to a specific memory channel&lt;/li>
&lt;li>Change the current modulation (AM/FM/NFM)&lt;/li>
&lt;/ul>
&lt;p>&amp;hellip;and perform a few other actions. However, neither the command line tool nor the API appear to provide facilities for importing/exporting channels, editing channels, performing a memory backup and restore, and so forth. Additionaly, I do most of development these days in Python. While there exists a Python binding for Hamlib, it&amp;rsquo;s not terrible intuitive.&lt;/p>
&lt;h2 id="programming-mode">Programming mode&lt;/h2>
&lt;p>The way the MCP-2A program interacts with the TM-V71 is via a memory dump and restore process (which is much like how CHIRP interacts with Baofeng radios). This makes use of the radio&amp;rsquo;s &amp;ldquo;programming mode&amp;rdquo;, and support for that in Hamlib doesn&amp;rsquo;t exist. In fact, while the control commands for the radio have been documented in various places (such as LA3QMA&amp;rsquo;s &lt;a href="https://github.com/LA3QMA/TM-V71_TM-D710-Kenwood/blob/master/commands/MR.md">TM-V71_TM-D710-Kenwood repository&lt;/a>), I was not able to find any documentation that described how to interact with the radio in programming mode. In order to figure out how things work, I would need to trace the interaction between MCP-2A and the radio.&lt;/p>
&lt;p>I run MCP-2A in a Windows 10 guest on my Linux host. Normally, when configuring the radio, I expose the USB serial adapter to the Windows guest using the USB redirection feature offered by libvirt. In order to trace the interaction between the software and the radio, I need to stick something in between that will log the data being sent back and forth.&lt;/p>
&lt;p>I started by configuring a serial port on the Windows guest that was connected to a Unix socket:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-xml" data-lang="xml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;lt;serial&lt;/span> &lt;span style="color:#a6e22e">type=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;unix&amp;#39;&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;source&lt;/span> &lt;span style="color:#a6e22e">mode=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;bind&amp;#39;&lt;/span> &lt;span style="color:#a6e22e">path=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;/tmp/win10-serial&amp;#39;&lt;/span>&lt;span style="color:#f92672">/&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;target&lt;/span> &lt;span style="color:#a6e22e">type=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;isa-serial&amp;#39;&lt;/span> &lt;span style="color:#a6e22e">port=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;1&amp;#39;&lt;/span>&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;model&lt;/span> &lt;span style="color:#a6e22e">name=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;isa-serial&amp;#39;&lt;/span>&lt;span style="color:#f92672">/&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">&amp;lt;/target&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;lt;/serial&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This exposes a Unix socket at &lt;code>/tmp/win10-serial&lt;/code> when the guest is running. On the host, I use &lt;a href="http://www.dest-unreach.org/socat/">socat&lt;/a> as a proxy between that socket and the actual serial port. I ended up running &lt;code>socat&lt;/code> like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>socat -x -v \
/dev/ttyUSB1,b57600,crtscts=1,raw,echo=0 \
unix-connect:/tmp/win10-serial
&lt;/code>&lt;/pre>&lt;p>The &lt;code>-x&lt;/code> and &lt;code>-v&lt;/code> options instruct &lt;code>socat&lt;/code> to log on stdout data passing through the proxy, in both raw and hexadecimal format. With the proxy running, I started MCP-2A on the Windows guest and performed a &amp;ldquo;Read data from the transceiver&amp;rdquo; operation. This resulted in a log that looks like the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt; 2019/09/28 14:42:32.732010 length=1 from=0 to=0
49 I
--
&amp;lt; 2019/09/28 14:42:32.834799 length=1 from=1 to=1
44 D
--
&amp;lt; 2019/09/28 14:42:33.043626 length=1 from=2 to=2
0d .
--
&amp;gt; 2019/09/28 14:42:33.069589 length=10 from=0 to=9
49 44 20 54 4d 2d 56 37 31 0d ID TM-V71.
[...]
&lt;/code>&lt;/pre>&lt;p>In this output, chunks marked with &lt;code>&amp;lt;&lt;/code> represent data sent FROM the software TO the radio, and chunks marked with &lt;code>&amp;gt;&lt;/code> represent data sent FROM the radio TO the software. The information we need is there, but the output is a little too cluttered, making it hard to interpret. I wrote a quick script to clean it up; after processing, the beginning of the transaction look like this (where &lt;code>&amp;lt;&amp;lt;&amp;lt;&lt;/code> represents date being sent to the radio, and &lt;code>&amp;gt;&amp;gt;&amp;gt;&lt;/code> represents data received from the radio):&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;&amp;lt;&amp;lt; 00000000: 49 I
&amp;lt;&amp;lt;&amp;lt; 00000000: 44 D
&amp;lt;&amp;lt;&amp;lt; 00000000: 0D .
&amp;gt;&amp;gt;&amp;gt; 00000000: 49 44 20 54 4D 2D 56 37 31 0D ID TM-V71.
&amp;lt;&amp;lt;&amp;lt; 00000000: 54 43 20 31 0D TC 1.
&amp;gt;&amp;gt;&amp;gt; 00000000: 3F 0D ?.
&amp;lt;&amp;lt;&amp;lt; 00000000: 49 44 0D ID.
&amp;gt;&amp;gt;&amp;gt; 00000000: 49 44 20 54 4D 2D 56 37 31 0D ID TM-V71.
&amp;lt;&amp;lt;&amp;lt; 00000000: 54 59 0D TY.
&amp;gt;&amp;gt;&amp;gt; 00000000: 54 59 20 4B 2C 30 2C 30 2C 31 2C 30 0D TY K,0,0,1,0.
&amp;lt;&amp;lt;&amp;lt; 00000000: 46 56 20 30 FV 0
&amp;lt;&amp;lt;&amp;lt; 00000000: 0D .
&amp;gt;&amp;gt;&amp;gt; 00000000: 46 56 20 30 2C 31 2E 30 30 2C 32 2E 31 30 2C 41 FV 0,1.00,2.10,A
&amp;gt;&amp;gt;&amp;gt; 00000000: 2C 31 0D ,1.
&amp;lt;&amp;lt;&amp;lt; 00000000: 30 4D 20 50 0M P
&amp;lt;&amp;lt;&amp;lt; 00000000: 52 4F 47 52 41 4D 0D ROGRAM.
&amp;gt;&amp;gt;&amp;gt; 00000000: 30 4D 0D 0M.
&amp;lt;&amp;lt;&amp;lt; 00000000: 52 00 R.
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 00 ..
&amp;gt;&amp;gt;&amp;gt; 00000000: 57 00 00 00 00 4B 01 FF FF FF FF FF FF FF FF FF W....K..........
&amp;gt;&amp;gt;&amp;gt; 00000000: FF 00 FF FF 00 00 39 31 35 01 00 00 00 00 01 01 ......915.......
&amp;gt;&amp;gt;&amp;gt; 00000000: 00 00 00 01 02 03 00 00 FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF ....
&amp;lt;&amp;lt;&amp;lt; 00000000: 06 .
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 .
&amp;lt;&amp;lt;&amp;lt; 00000000: 52 01 R.
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 00 ..
&amp;gt;&amp;gt;&amp;gt; 00000000: 57 01 00 00 FF FF FF FF FF FF FF FF FF FF FF FF W...............
[...]
&lt;/code>&lt;/pre>&lt;p>And the end of the transaction looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>[...]
&amp;lt;&amp;lt;&amp;lt; 00000000: 52 7E 00 R~.
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 .
&amp;gt;&amp;gt;&amp;gt; 00000000: 57 7E 00 00 FF FF FF FF FF FF FF FF FF FF FF FF W~..............
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;gt;&amp;gt;&amp;gt; 00000000: FF FF FF FF ....
&amp;lt;&amp;lt;&amp;lt; 00000000: 06 .
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 .
&amp;lt;&amp;lt;&amp;lt; 00000000: 45 E
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 0D ..
&amp;gt;&amp;gt;&amp;gt; 00000000: 00 .
&lt;/code>&lt;/pre>&lt;p>This is a lot easier to read. We can see the software initially sends some commands to identify the radio:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ID&lt;/code> to get the radio model&lt;/li>
&lt;li>&lt;code>TY&lt;/code> to get the radio type&lt;/li>
&lt;li>&lt;code>FV 0&lt;/code> to get the firmware version&lt;/li>
&lt;/ul>
&lt;p>And then enters programming mode by sending &lt;code>0M PROGRAM&lt;/code>. Inspecting the remainder of the dump shows us that the software makes a series of read requests that look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;&amp;lt;&amp;lt; 00000000: 52 00 R.
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 00 ..
&lt;/code>&lt;/pre>&lt;p>That is, a four byte request of the form:&lt;/p>
&lt;ol>
&lt;li>&lt;code>R&lt;/code>&lt;/li>
&lt;li>Address (2 bytes)&lt;/li>
&lt;li>Size&lt;/li>
&lt;/ol>
&lt;p>When a read request has a size of &lt;code>0&lt;/code>, that means &lt;code>read 256 bytes&lt;/code>.&lt;/p>
&lt;p>The response to a read request looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; 00000000: 57 00 00 00 00 4B 01 FF FF FF FF FF FF FF FF FF W....K..........
&amp;gt;&amp;gt;&amp;gt; 00000000: FF 00 FF FF 00 00 39 31 35 01 00 00 00 00 01 01 ......915.......
&amp;gt;&amp;gt;&amp;gt; ...
&lt;/code>&lt;/pre>&lt;p>That is:&lt;/p>
&lt;ol>
&lt;li>&lt;code>W&lt;/code>&lt;/li>
&lt;li>Address (2 bytes)&lt;/li>
&lt;li>Size&lt;/li>
&lt;li>&lt;code>&amp;lt;Size&amp;gt;&lt;/code> bytes of data&lt;/li>
&lt;/ol>
&lt;p>It turns out that the response to the read request is exactly the syntax used to perform a write request. The log of the write operation starts like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;&amp;lt;&amp;lt; 00000000: 54 43 20 TC
&amp;lt;&amp;lt;&amp;lt; 00000000: 31 0D 1.
&amp;gt;&amp;gt;&amp;gt; 00000000: 3F 0D ?.
&amp;lt;&amp;lt;&amp;lt; 00000000: 49 I
&amp;lt;&amp;lt;&amp;lt; 00000000: 44 0D D.
&amp;gt;&amp;gt;&amp;gt; 00000000: 49 44 20 54 4D 2D 56 37 31 0D ID TM-V71.
&amp;lt;&amp;lt;&amp;lt; 00000000: 54 59 0D TY.
&amp;gt;&amp;gt;&amp;gt; 00000000: 54 59 20 4B 2C 30 2C 30 2C 31 2C 30 0D TY K,0,0,1,0.
&amp;lt;&amp;lt;&amp;lt; 00000000: 46 56 FV
&amp;lt;&amp;lt;&amp;lt; 00000000: 20 30 0D 0.
&amp;gt;&amp;gt;&amp;gt; 00000000: 46 56 20 30 2C 31 2E 30 30 2C 32 2E 31 30 2C 41 FV 0,1.00,2.10,A
&amp;gt;&amp;gt;&amp;gt; 00000000: 2C 31 0D ,1.
&amp;lt;&amp;lt;&amp;lt; 00000000: 30 4D 0M
&amp;lt;&amp;lt;&amp;lt; 00000000: 20 50 52 4F 47 52 41 4D 0D PROGRAM.
&amp;gt;&amp;gt;&amp;gt; 00000000: 30 4D 0D 0M.
&amp;lt;&amp;lt;&amp;lt; 00000000: 52 R
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 00 04 ...
&amp;gt;&amp;gt;&amp;gt; 00000000: 57 00 00 04 00 4B 01 FF W....K..
&amp;lt;&amp;lt;&amp;lt; 00000000: 06 .
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 .
&amp;lt;&amp;lt;&amp;lt; 00000000: 57 00 W.
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 01 FF ...
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 .
&amp;lt;&amp;lt;&amp;lt; 00000000: 57 00 W.
&amp;lt;&amp;lt;&amp;lt; 00000000: 04 FC FF FF FF FF FF FF FF FF FF 00 ............
&amp;lt;&amp;lt;&amp;lt; 00000000: FF FF 00 00 39 31 35 01 00 00 00 00 ....915.....
&amp;lt;&amp;lt;&amp;lt; 00000000: 01 01 00 00 00 01 02 .......
&amp;lt;&amp;lt;&amp;lt; 00000000: 03 00 00 FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000010: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000020: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000030: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000040: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000050: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000060: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000070: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000080: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000000: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000010: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000020: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000030: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ................
&amp;lt;&amp;lt;&amp;lt; 00000040: FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF ...............
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 .
[...]
&lt;/code>&lt;/pre>&lt;p>And concludes with:&lt;/p>
&lt;pre tabindex="0">&lt;code>[...]
&amp;lt;&amp;lt;&amp;lt; 00000000: 57 00 W.
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 04 00 4B 01 FF ...K..
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 .
&amp;lt;&amp;lt;&amp;lt; 00000000: 45 E
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 0D ..
&amp;gt;&amp;gt;&amp;gt; 00000000: 00 .
&lt;/code>&lt;/pre>&lt;p>In this log, we see a series of write requests; for example:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;&amp;lt;&amp;lt; 00000000: 57 00 W.
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 01 FF ...
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 .
&lt;/code>&lt;/pre>&lt;p>In the above trace, the software is first writing one byte (&lt;code>0xFF&lt;/code>) to address &lt;code>0x0&lt;/code>. The radio acknowledges a write request with a response byte (&lt;code>0x06&lt;/code>).&lt;/p>
&lt;p>By experimenting a bit, it seems as if writing &lt;code>0xFF&lt;/code> to &lt;code>0x0&lt;/code>, will cause the radio to reset to defaults when you exit programming mode. That&amp;rsquo;s why at the conclusion of the write operation, we see the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>&amp;lt;&amp;lt;&amp;lt; 00000000: 57 00 W.
&amp;lt;&amp;lt;&amp;lt; 00000000: 00 04 00 4B 01 FF ...K..
&amp;gt;&amp;gt;&amp;gt; 00000000: 06 .
&lt;/code>&lt;/pre>&lt;p>That is, the software is rewriting the first four bytes of memory with their original contents.&lt;/p>
&lt;h3 id="a-warning-about-manual-interaction">A warning about manual interaction&lt;/h3>
&lt;p>If you manually place the radio into programming mode, you will see that after entering &lt;code>0M PROGRAM&lt;/code> the radio displays &lt;code>PROG MCP&lt;/code>. Unless you are a preternaturally fast typist, the radio display will shortly switch to &lt;code>PROG ERR&lt;/code> because of the delay between entering programming mode and any read or write transactions. In this state, you will still be able to enter read or write commands, but the status byte in response to a successful write command will be &lt;code>0x15&lt;/code> instead of &lt;code>0x06&lt;/code>.&lt;/p>
&lt;hr>
&lt;p>I&amp;rsquo;ve summarized this information in a document that I submitted as a pull request to &lt;a href="https://github.com/LA3QMA/TM-V71_TM-D710-Kenwood/">https://github.com/LA3QMA/TM-V71_TM-D710-Kenwood/&lt;/a>. You can read it &lt;a href="https://github.com/LA3QMA/TM-V71_TM-D710-Kenwood/blob/master/PROGRAMMING_MODE.md">here&lt;/a>.&lt;/p></content></item><item><title>Avoid rebase hell: squashing without rebasing</title><link>https://blog.oddbit.com/post/2019-06-17-avoid-rebase-hell-squashing-wi/</link><pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-06-17-avoid-rebase-hell-squashing-wi/</guid><description>You&amp;rsquo;re working on a pull request. You&amp;rsquo;ve been working on a pull request for a while, and due to lack of sleep or inebriation you&amp;rsquo;ve been merging changes into your feature branch rather than rebasing. You now have a pull request that looks like this (I&amp;rsquo;ve marked merge commits with the text [merge]):
7e181479 Adds methods for widget sales 0487162 [merge] Merge remote-tracking branch &amp;#39;origin/master&amp;#39; into my_feature 76ee81c [merge] Merge branch &amp;#39;my_feature&amp;#39; of https://github.</description><content>&lt;p>You&amp;rsquo;re working on a pull request. You&amp;rsquo;ve been working on a pull request for a while, and due to lack of sleep or inebriation you&amp;rsquo;ve been merging changes into your feature branch rather than rebasing. You now have a pull request that looks like this (I&amp;rsquo;ve marked merge commits with the text &lt;code>[merge]&lt;/code>):&lt;/p>
&lt;pre tabindex="0">&lt;code>7e181479 Adds methods for widget sales
0487162 [merge] Merge remote-tracking branch &amp;#39;origin/master&amp;#39; into my_feature
76ee81c [merge] Merge branch &amp;#39;my_feature&amp;#39; of https://github.com/my_user_name/widgets into my_feature
981aab4 Adds api for the widget service.
b048836 Includes fixes suggested by reviewer.
3dd0c22 adds changes requested by reviewer
5891db2 [merge] fixing merge conflicts
2e226e4 fixes suggestions given by the reviewer
da1e85c Adds gadget related API spec
c555cc1 Adds doodad related API spec
e5beb3e Adds api for update and delete of widgets
c43bade Adds api for creating widgets
deaa962 Adds all get methods for listing widgets
9de79ab Adds api for showing a widget and simple data model
8288ab1 Adds api framework for widget service
&lt;/code>&lt;/pre>&lt;p>You know that&amp;rsquo;s a mess, so you try to fix it by running &lt;code>git rebase -i master&lt;/code> and squashing everything together&amp;hellip;and you find yourself stuck in an endless maze of merge conflicts. There has to be a better way!&lt;/p>
&lt;p>&lt;em>(voiceover: there is a better way&amp;hellip;)&lt;/em>&lt;/p>
&lt;h2 id="option-1-merge---squash">Option 1: merge &amp;ndash;squash&lt;/h2>
&lt;p>In this method, you will create a temporary branch and use &lt;code>git merge --squash&lt;/code> to squash together the changes in your pull request.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Check out a new branch based on &lt;code>master&lt;/code> (or the appropriate base branch if your feature branch isn&amp;rsquo;t based on &lt;code>master&lt;/code>):&lt;/p>
&lt;pre tabindex="0">&lt;code>git checkout -b work master
&lt;/code>&lt;/pre>&lt;p>(This creates a new branch called &lt;code>work&lt;/code> and makes that your current branch.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Bring in the changes from your messy pull request using &lt;code>git merge --squash&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>git merge --squash my_feature
&lt;/code>&lt;/pre>&lt;p>This brings in all the changes from your &lt;code>my_feature&lt;/code> branch and stages them, but does not create &lt;em>any&lt;/em> commits.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Commit the changes with an appropriate commit message:&lt;/p>
&lt;pre tabindex="0">&lt;code>git commit
&lt;/code>&lt;/pre>&lt;p>At this point, your &lt;code>work&lt;/code> branch should be identical to the original &lt;code>my_feature&lt;/code> branch (running &lt;code>git diff my_feature_branch&lt;/code> should not show any changes), but it will have only a single commit after &lt;code>master&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Return to your feature branch and &lt;code>reset&lt;/code> it to the squashed version:&lt;/p>
&lt;pre tabindex="0">&lt;code>git checkout my_feature
git reset --hard work
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Update your pull request:&lt;/p>
&lt;pre tabindex="0">&lt;code>git push -f
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Optionally clean up your work branch:&lt;/p>
&lt;pre tabindex="0">&lt;code>git branch -D work
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h2 id="option-2-using-git-commit-tree">Option 2: Using &lt;code>git commit-tree&lt;/code>&lt;/h2>
&lt;p>In this method, you will use &lt;code>git commit-tree&lt;/code> to create a new commit without requiring a temporary branch.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Use &lt;code>git commit-tree&lt;/code> to create new commit that reproduces the current state of your &lt;code>my_feature&lt;/code> branch but&lt;/p>
&lt;pre tabindex="0">&lt;code>git commit-tree -p master -m &amp;#39;this implements my_feature&amp;#39; my_feature^{tree}
&lt;/code>&lt;/pre>&lt;p>This uses the current state of the &lt;code>my_feature&lt;/code> branch as the source of a new commit whose parent is &lt;code>master&lt;/code>. This will print out a commit hash:&lt;/p>
&lt;pre tabindex="0">&lt;code>1d3917a3b7c43f4585084e626303c9eeee59c6d6
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Reset your &lt;code>my_feature&lt;/code> branch to this new commit hash:&lt;/p>
&lt;pre tabindex="0">&lt;code>git reset --hard 1d3917a3b7c43f4585084e626303c9eeee59c6d6
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Consider editing your commit message to meet &lt;a href="https://blog.oddbit.com/post/2019-06-14-git-etiquette-commit-messages/">best practices&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Update your pull request:&lt;/p>
&lt;pre tabindex="0">&lt;code>git push -f
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol></content></item><item><title>Git Etiquette: Commit messages and pull requests</title><link>https://blog.oddbit.com/post/2019-06-14-git-etiquette-commit-messages/</link><pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-06-14-git-etiquette-commit-messages/</guid><description>Always work on a branch (never commit on master) When working with an upstream codebase, always make your changes on a feature branch rather than your local master branch. This will make it easier to keep your local master branch current with respect to upstream, and can help avoid situations in which you accidentally overwrite your local changes or introduce unnecessary merge commits into your history.
Rebase instead of merge If you need to incorporate changes from the upstream master branch in the feature branch on which you are currently doing, bring in those changes using git rebase rather than git merge.</description><content>&lt;h2 id="always-work-on-a-branch-never-commit-on-master">Always work on a branch (never commit on master)&lt;/h2>
&lt;p>When working with an upstream codebase, always make your changes on a feature branch rather than your local &lt;code>master&lt;/code> branch. This will make it easier to keep your local &lt;code>master&lt;/code> branch current with respect to upstream, and can help avoid situations in which you accidentally overwrite your local changes or introduce unnecessary merge commits into your history.&lt;/p>
&lt;h2 id="rebase-instead-of-merge">Rebase instead of merge&lt;/h2>
&lt;p>If you need to incorporate changes from the upstream &lt;code>master&lt;/code> branch in the feature branch on which you are currently doing, bring in those changes using &lt;code>git rebase&lt;/code> rather than &lt;code>git merge&lt;/code>. This process will generally start by ensuring that your local copy of the upstream &lt;code>master&lt;/code> is current:&lt;/p>
&lt;pre tabindex="0">&lt;code>git remote update
&lt;/code>&lt;/pre>&lt;p>Followed by rebasing your changes on top of that branch:&lt;/p>
&lt;pre tabindex="0">&lt;code>git rebase origin/master
&lt;/code>&lt;/pre>&lt;h2 id="make-your-commit-messages-meaningful">Make your commit messages meaningful&lt;/h2>
&lt;p>Your commit messages should follow common best practices, such as &lt;a href="https://wiki.openstack.org/wiki/GitCommitMessages">those documented for OpenStack&lt;/a>. In general, this means:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Your commit message should have the subject on the first line, then a blank line, then the message body.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The message body should be wrapped at around 75 characters.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Your commit message should succinctly describe &lt;em>what&lt;/em> was changed and &lt;em>why&lt;/em> it was changed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If your commit is associated with a bug, user story, task, or other tracking mechanism, include a reference to the appropriate item in your commit message.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="example-of-bad-commit-messages">Example of bad commit messages&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>This should probably have been squashed with the commit that introduced the typo:&lt;/p>
&lt;pre tabindex="0">&lt;code>Fixed a typo.
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>This doesn&amp;rsquo;t provide sufficient details:&lt;/p>
&lt;pre tabindex="0">&lt;code>Trying that thing again
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Changes necessary to resolve merge conflicts should generally be squashed into the commits that introduce the conflict:&lt;/p>
&lt;pre tabindex="0">&lt;code>Fixing merge conflicts
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h3 id="example-of-good-commit-messages">Example of good commit messages&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>A constructed example:&lt;/p>
&lt;pre tabindex="0">&lt;code>Support new &amp;#34;description&amp;#34; field on all widgets (TG-100)
Update both the database model and the web UI to support an extended
&amp;#34;description&amp;#34; field on all objects. This allows us to generate more useful
product listings.
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>from &lt;a href="https://github.com/openstack/ironic/commit/6ca99d67302d7e1b639ad9b745631e65a4b2f25c">https://github.com/openstack/ironic/commit/6ca99d67302d7e1b639ad9b745631e65a4b2f25c&lt;/a>:&lt;/p>
&lt;pre tabindex="0">&lt;code>Add release note updating status of smartnics
Smartnic support merged into Neutron a few weeks ago,
and as we downplayed the functionality during the Stein
cycle, we should highlight it during Train since it should
now be available.
Change-Id: I19372a0ede703f62940bbb2cc3a80618560ebc93
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>from &lt;a href="https://github.com/ansible/ansible/commit/88a1fc28d894575d48edc3817cc1a8ef4dca3cae">https://github.com/ansible/ansible/commit/88a1fc28d894575d48edc3817cc1a8ef4dca3cae&lt;/a>:&lt;/p>
&lt;pre tabindex="0">&lt;code>Clean up iosxr get_config_diff function (#57589)
This fixes an index error issue when running tests on zuul.ansible.com
for iosxr. We can fix this by getting the last element in the list.
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;h2 id="fix-typos-by-amending-or-rebasing">Fix typos by amending or rebasing&lt;/h2>
&lt;p>If before your pull request has merged you realize you need to fix a typo or other error, do not create a new commit with the correction.&lt;/p>
&lt;p>If the typo is in the most recent commit, simply fix it, &lt;code>git add&lt;/code> the change, and then use &lt;code>git commit --amend&lt;/code> to update the latest commit.&lt;/p>
&lt;p>If the typo is &lt;strong>not&lt;/strong> in the latest commit, then use &lt;code>git rebase&lt;/code> to edit the commit. The procedure will look something like this:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Assuming that your branch is based on &lt;code>origin/master&lt;/code>, run &lt;code>git rebase -i origin/master&lt;/code>. This will bring up the &amp;ldquo;pick list&amp;rdquo;, which allows you to select among various actions for each commit between &lt;code>origin/master&lt;/code> and your current branch. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>pick 6ca99d673 Add release note updating status of smartnics
pick c2ab34a8c Do not log an exception if Allocation is deleted during handling.
pick 87464fbbc Change constraints opendev.org to release.openstack.org
pick 43f7bf9f0 Fix :param: in docstring
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Change &lt;code>pick&lt;/code> to &lt;code>edit&lt;/code> for the commit you wish to edit, then exit your editor.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Make the necessary change, then &lt;code>git add&lt;/code> your modified files and then &lt;code>git rebase --continue&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>In either of the above situations, when you have committed the changes locally, run &lt;code>git push -f&lt;/code> to update the remote branch on GitHub (which will in turn update your pull request).&lt;/p>
&lt;h2 id="have-one-commit-per-logical-change-and-one-major-feature-per-pull-request">Have one commit per logical change and one major feature per pull request&lt;/h2>
&lt;p>When you submit a pull request, all the commits associated with that pull request should be related to the same major feature. For example, if you made the follow changes:&lt;/p>
&lt;ul>
&lt;li>Fixed bug #1234&lt;/li>
&lt;li>Wrote new feature discussed at last planning session&lt;/li>
&lt;/ul>
&lt;p>Those should be two separate pull requests. On the other hand, if you have instead made these changes:&lt;/p>
&lt;ul>
&lt;li>Wrote new feature discussed at last planning session&lt;/li>
&lt;li>Wrote documentation for new feature&lt;/li>
&lt;li>Wrote tests for new feature&lt;/li>
&lt;/ul>
&lt;p>Those could all be made part of the same pull request.&lt;/p>
&lt;p>Within your pull request, there should be a single commit for each logical change. For example rather than:&lt;/p>
&lt;ul>
&lt;li>Started documentation for new feature&lt;/li>
&lt;li>Made changes to documentation based on review&lt;/li>
&lt;li>Reformatted documentation to fix syntax error&lt;/li>
&lt;/ul>
&lt;p>You should have a single commit:&lt;/p>
&lt;ul>
&lt;li>Write documentation for new feature&lt;/li>
&lt;/ul>
&lt;p>You should use &lt;code>git commit --amend&lt;/code> or &lt;code>git rebase&lt;/code>, as discussed earlier, to keep your commits topical and organized.&lt;/p></content></item><item><title>Running Keystone with Docker Compose</title><link>https://blog.oddbit.com/post/2019-06-07-running-keystone-with-docker-c/</link><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-06-07-running-keystone-with-docker-c/</guid><description>In this article, we will look at what is necessary to run OpenStack&amp;rsquo;s Keystone service (and the requisite database server) in containers using Docker Compose.
Running MariaDB The standard mariadb docker image can be configured via a number of environment variables. It also benefits from persistent volume storage, since in most situations you don&amp;rsquo;t want to lose your data when you remove a container. A simple docker command line for starting MariaDB might look something like:</description><content>&lt;p>In this article, we will look at what is necessary to run OpenStack&amp;rsquo;s &lt;a href="https://docs.openstack.org/keystone/latest/">Keystone&lt;/a> service (and the requisite database server) in containers using &lt;a href="https://docs.docker.com/compose/">Docker Compose&lt;/a>.&lt;/p>
&lt;h2 id="running-mariadb">Running MariaDB&lt;/h2>
&lt;p>The standard &lt;a href="https://hub.docker.com/_/mariadb/">mariadb docker image&lt;/a> can be configured via a number of environment variables. It also benefits from persistent volume storage, since in most situations you don&amp;rsquo;t want to lose your data when you remove a container. A simple &lt;code>docker&lt;/code> command line for starting MariaDB might look something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run \
-v mariadb_data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=secret.password
mariadb
&lt;/code>&lt;/pre>&lt;p>The above assumes that we have previously created a Docker volume named &lt;code>mariadb_data&lt;/code> by running:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker volume create mariadb_data
&lt;/code>&lt;/pre>&lt;p>An equivalent &lt;code>docker-compose.yml&lt;/code> would look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb
environment:
MYSQL_ROOT_PASSWORD: secret.password
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;p>Now, rather than typing a long &lt;code>docker run&lt;/code> command line (and possibly forgetting something), you can simply run:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker-compose up
&lt;/code>&lt;/pre>&lt;h3 id="pre-creating-a-database">Pre-creating a database&lt;/h3>
&lt;p>For the purposes of setting up Keystone in Docker, we will need to make a few changes. In particular, we will need to have the &lt;code>mariadb&lt;/code> container create the &lt;code>keystone&lt;/code> database (and user) for us, and as a matter of best practice we will want to specify an explicit tag for the &lt;code>mariadb&lt;/code> image rather than relying on the default &lt;code>latest&lt;/code>.&lt;/p>
&lt;p>We can have the &lt;code>mariadb&lt;/code> image create a database for us at startup by setting the &lt;code>MYSQL_DATABASE&lt;/code>, &lt;code>MYSQL_USER&lt;/code>, and &lt;code>MYSQL_PASSWORD&lt;/code> environment variables:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb:10.4.5-bionic
environment:
MYSQL_ROOT_PASSWORD: secret.password
MYSQL_USER: keystone
MYSQL_PASSWORD: another.password
MYSQL_DATABASE: keystone
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;p>When the &lt;code>database&lt;/code> service starts up, it will create a &lt;code>keystone&lt;/code> database accessible by the &lt;code>keystone&lt;/code> user.&lt;/p>
&lt;h3 id="parameterize-all-the-things">Parameterize all the things&lt;/h3>
&lt;p>The above example is pretty much what we want, but there is one problem: we&amp;rsquo;ve hardcoded our passwords (and database name) into the &lt;code>docker-compose.yml&lt;/code> file, which makes it hard to share: it would be unsuitable for hosting on a public git repository, because anybody who wanted to use it would need to modify the file first, which would make it difficult to contribute changes or bring in new changes from the upstream repository. We can solve that problem by using environment variables in our &lt;code>docker-compose.yml&lt;/code>. Much like the shell, &lt;code>docker-compose&lt;/code> will replace an expression of the form &lt;code>${MY_VARIABLE}&lt;/code> with the value of the &lt;code>MY_VARIABLE&lt;/code> environment variable. It is possible to provide a fallback value in the event that an environment variable is undefined by writing &lt;code>${MY_VARIABLE:-some_default_value}&lt;/code>.&lt;/p>
&lt;p>You have a couple options for providing values for this variables. You can of course simply set them in the environment, either like this:&lt;/p>
&lt;pre>&lt;code>export MY_VARIABLE=some_value
&lt;/code>&lt;/pre>
&lt;p>Or as part of the &lt;code>docker-compose&lt;/code> command line, like this:&lt;/p>
&lt;pre>&lt;code>MY_VARIABLE=some_value docker-compose up
&lt;/code>&lt;/pre>
&lt;p>Alternatively, you can also set them in a &lt;code>.env&lt;/code> file in the same directory as your &lt;code>docker-compose.yml&lt;/code> file; &lt;code>docker-compose&lt;/code> reads this file automatically when it runs. A &lt;code>.env&lt;/code> file looks like this:&lt;/p>
&lt;pre>&lt;code>MY_VARIABLE=some_value
&lt;/code>&lt;/pre>
&lt;p>With the above in mind, we can restructure our example &lt;code>docker-compose.yml&lt;/code> so that it looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb:${MARIADB_IMAGE_TAG:-10.4.5-bionic}
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
MYSQL_USER: ${KEYSTONE_DB_USER:-keystone}
MYSQL_PASSWORD: ${KEYSTONE_DB_PASSWORD}
MYSQL_DATABASE: ${KEYSTONE_DB_NAME:-keystone}
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;h2 id="running-keystone">Running Keystone&lt;/h2>
&lt;h3 id="selecting-a-base-image">Selecting a base image&lt;/h3>
&lt;p>While there is an official MariaDB image available in Docker Hub, there is no such thing as an official Keystone image. A search for &lt;code>keystone&lt;/code> yields over 300 results. I have elected to use the Keystone image produced as part of the &lt;a href="https://wiki.openstack.org/wiki/TripleO">TripleO&lt;/a> project, &lt;a href="https://hub.docker.com/r/tripleomaster/centos-binary-keystone">tripleo-master/centos-binary-keystone&lt;/a>. The &lt;code>current-rdo&lt;/code> tag follows the head of the Keystone repository, and the images are produced automatically as part of the CI process. Unlike the MariaDB image, which is designed to pretty much be &amp;ldquo;plug and play&amp;rdquo;, the Keystone image is going to require some configuration before it provides us with a useful service.&lt;/p>
&lt;p>Using the &lt;code>centos-binary-keystone&lt;/code> image, there are two required configuration tasks we will have to complete when starting the container:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>We will need to inject an appropriate configuration file to run Keystone as a &lt;a href="https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface">WSGI&lt;/a> binary under Apache &lt;a href="http://httpd.apache.org/">httpd&lt;/a>. This is certainly not the only way to run Keystone, but the &lt;code>centos-binary-keystone&lt;/code> image has both &lt;code>httpd&lt;/code> and &lt;code>mod_wsgi&lt;/code> installed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We will need to inject a minimal configuration for Keystone (for example, we will need to provide Keystone with connection information for the database instance).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="keystone-wsgi-configuration">Keystone WSGI configuration&lt;/h3>
&lt;p>We need to configure Keystone as a WSGI service running on port 5000. We will do this with the following configuration file:&lt;/p>
&lt;pre tabindex="0">&lt;code>Listen 5000
ErrorLog &amp;#34;/dev/stderr&amp;#34;
CustomLog &amp;#34;/dev/stderr&amp;#34; combined
&amp;lt;VirtualHost *:5000&amp;gt;
ServerName keystone
ServerSignature Off
DocumentRoot &amp;#34;/var/www/cgi-bin/keystone&amp;#34;
&amp;lt;Directory &amp;#34;/var/www/cgi-bin/keystone&amp;#34;&amp;gt;
Options Indexes FollowSymLinks MultiViews
AllowOverride None
Require all granted
&amp;lt;/Directory&amp;gt;
WSGIApplicationGroup %{GLOBAL}
WSGIDaemonProcess keystone_main display-name=keystone-main \
processes=12 threads=1 user=keystone group=keystone
WSGIProcessGroup keystone_main
WSGIScriptAlias / &amp;#34;/var/www/cgi-bin/keystone/main&amp;#34;
WSGIPassAuthorization On
&amp;lt;/VirtualHost&amp;gt;
&lt;/code>&lt;/pre>&lt;p>The easiest way to inject this custom configuration is to bake it into a custom image. Using the &lt;code>tripleomaster/centos-binary-keystone&lt;/code> base image identified earlier, we can start with a custom &lt;code>Dockerfile&lt;/code> that looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>ARG KEYSTONE_IMAGE_TAG=current-tripleo
FROM tripleomaster/centos-binary-keystone:${KEYSTONE_IMAGE_TAG}
COPY keystone-wsgi-main.conf /etc/httpd/conf.d/keystone-wsgi-main.conf
&lt;/code>&lt;/pre>&lt;p>The &lt;code>ARG&lt;/code> directive permits us to select an image tag via a build argument (but defaults to &lt;code>current-tripleo&lt;/code>).&lt;/p>
&lt;p>We can ask &lt;code>docker-compose&lt;/code> to build our custom image for us when we run &lt;code>docker-compose up&lt;/code>. Instead of specifying an &lt;code>image&lt;/code> as we did with the MariaDB container, we use the &lt;code>build&lt;/code> directive:&lt;/p>
&lt;pre tabindex="0">&lt;code>[...]
keystone:
build:
context: .
args:
KEYSTONE_IMAGE_TAG: current-tripleo
[...]
&lt;/code>&lt;/pre>&lt;p>This tells &lt;code>docker-compose&lt;/code> to use the &lt;code>Dockerfile&lt;/code> in the current directory (and to set the &lt;code>KEYSTONE_IMAGE_TAG&lt;/code> build argument to &lt;code>current-tripleo&lt;/code>). Note that &lt;code>docker-compose&lt;/code> will only build this image for us by default if it doesn&amp;rsquo;t already exist; we can ask &lt;code>docker-compose&lt;/code> to build it explicitly by running &lt;code>docker-compose build&lt;/code>, or by providing the &lt;code>--build&lt;/code> option to &lt;code>docker-compose up&lt;/code>.&lt;/p>
&lt;h3 id="configuring-at-build-time-vs-run-time">Configuring at build time vs run time&lt;/h3>
&lt;p>In the previous section, we used a &lt;code>Dockerfile&lt;/code> to build on a selected base image by adding custom content. Other sorts of configuration must happen when the container starts up (for example, we probably want to be able to set passwords at runtime). One way of solving this problem is to embed some scripts into our custom image and then run them when the container starts in order to perform any necessary initialization.&lt;/p>
&lt;p>I have placed some custom scripts and templates into the &lt;code>runtime&lt;/code> directory and arranged to copy that directory into the custom image like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>ARG KEYSTONE_IMAGE_TAG=current-tripleo
FROM tripleomaster/centos-binary-keystone:${KEYSTONE_IMAGE_TAG}
COPY keystone-wsgi-main.conf /etc/httpd/conf.d/keystone-wsgi-main.conf
COPY runtime /runtime
CMD [&amp;#34;/bin/sh&amp;#34;, &amp;#34;/runtime/startup.sh&amp;#34;]
&lt;/code>&lt;/pre>&lt;p>The &lt;code>runtime&lt;/code> directory contains the following files:&lt;/p>
&lt;ul>
&lt;li>&lt;code>runtime/dtu.py&lt;/code> &amp;ndash; a short Python script for generating files from templates.&lt;/li>
&lt;li>&lt;code>runtime/startup.sh&lt;/code> &amp;ndash; a shell script that performs all the necessary initialization tasks before starting Keystone&lt;/li>
&lt;li>&lt;code>runtime/keystone.j2.conf&lt;/code> &amp;ndash; template for the Keystone configuration file&lt;/li>
&lt;li>&lt;code>runtime/clouds.j2.yaml&lt;/code> &amp;ndash; template for a &lt;code>clouds.yaml&lt;/code> for use by the &lt;code>openshift&lt;/code> command line client.&lt;/li>
&lt;/ul>
&lt;h3 id="starting-up">Starting up&lt;/h3>
&lt;p>The &lt;code>startup.sh&lt;/code> script performs the following actions:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Generates &lt;code>/etc/keystone/keystone.conf&lt;/code> from &lt;code>/runtime/keystone.j2.conf&lt;/code>.&lt;/p>
&lt;p>The file &lt;code>/runtime/keystone.j2.conf&lt;/code> is a minimal Keystone configuration template. It ensures that Keystone logs to &lt;code>stderr&lt;/code> (by setting &lt;code>log_file&lt;/code> to an empty value) and configures the database connection using values from the environment.&lt;/p>
&lt;pre tabindex="0">&lt;code>[DEFAULT]
debug = {{ environ.KEYSTONE_DEBUG|default(&amp;#39;false&amp;#39;) }}
log_file =
[database]
{% set keystone_db_user = environ.KEYSTONE_DB_USER|default(&amp;#39;keystone&amp;#39;) %}
{% set keystone_db_host = environ.KEYSTONE_DB_HOST|default(&amp;#39;localhost&amp;#39;) %}
{% set keystone_db_port = environ.KEYSTONE_DB_PORT|default(&amp;#39;3306&amp;#39;) %}
{% set keystone_db_name = environ.KEYSTONE_DB_NAME|default(&amp;#39;keystone&amp;#39;) %}
{% set keystone_db_pass = environ.KEYSTONE_DB_PASSWORD|default(&amp;#39;insert-password-here&amp;#39;) %}
connection = mysql+pymysql://{{ keystone_db_user }}:{{ keystone_db_pass }}@{{ keystone_db_host }}:{{ keystone_db_port }}/{{ keystone_db_name }}
[token]
provider = fernet
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Generates &lt;code>/root/clouds.yaml&lt;/code> from &lt;code>/runtime/clouds.j2.yaml&lt;/code>.&lt;/p>
&lt;p>The &lt;code>clouds.yaml&lt;/code> file can be used with to provide authentication information to the &lt;code>openshift&lt;/code> command line client (and other applications that use the OpenStack Python SDK). We&amp;rsquo;ll see an example of this further on in this article.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Initializes Keystone&amp;rsquo;s fernet token mechanism by running &lt;code>keystone-manage fernet_setup&lt;/code>.&lt;/p>
&lt;p>Keystone supports various token generation mechanisms. Fernet tokens provide some advantages over the older UUID token mechanism. From the &lt;a href="fernet-faq">FAQ&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Even though fernet tokens operate very similarly to UUID tokens, they do not require persistence or leverage the configured token persistence driver in any way. The keystone token database no longer suffers bloat as a side effect of authentication. Pruning expired tokens from the token database is no longer required when using fernet tokens. Because fernet tokens do not require persistence, they do not have to be replicated. As long as each keystone node shares the same key repository, fernet tokens can be created and validated instantly across nodes.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>Initializes the Keystone database schema by running &lt;code>keystone-manage db_sync&lt;/code>.&lt;/p>
&lt;p>The &lt;code>db_sync&lt;/code> command creates the database tables that Keystone requires to operate.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Creates the Keystone &lt;code>admin&lt;/code> user and initial service catalog entries by running &lt;code>keystone-manage bootstrap&lt;/code>&lt;/p>
&lt;p>Before we can authenticate to Keystone, there needs to exist a user with administrative privileges (so that we can create other users, projects, and so forth).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Starts &lt;code>httpd&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="avoiding-race-conditions">Avoiding race conditions&lt;/h3>
&lt;p>When we run &lt;code>docker-compose up&lt;/code>, it will bring up both the &lt;code>keystone&lt;/code> container and the &lt;code>database&lt;/code> container in parallel. This is going to cause problems if we try to initialize the Keystone database schema before the database server is actually up and running. There is a &lt;code>depends_on&lt;/code> keyword that can be used to order the startup of containers in your &lt;code>docker-compose.yml&lt;/code> file, but this isn&amp;rsquo;t useful to us: this only delays the startup of the dependent container until the indicated container is &lt;em>running&lt;/em>. It doesn&amp;rsquo;t know anything about application startup, and so it would not wait for the database to be ready.&lt;/p>
&lt;p>We need to explicitly wait until we can successfully connect to the database before we can complete initializing the Keystone service. It turns out the easiest solution to this problem is to imply run the database schema initialization in a loop until it is successful, like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>echo &amp;#34;* initializing database schema&amp;#34;
while ! keystone-manage db_sync; do
echo &amp;#34;! database schema initialization failed; retrying in 5 seconds...&amp;#34;
sleep 5
done
&lt;/code>&lt;/pre>&lt;p>This will attempt the &lt;code>db_sync&lt;/code> command every five seconds until it is sucessful.&lt;/p>
&lt;h2 id="the-final-docker-compose-file">The final docker-compose file&lt;/h2>
&lt;p>Taking all of the above into account, this is what the final &lt;code>docker-compose.yml&lt;/code> file looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
version: &amp;#34;3&amp;#34;
services:
database:
image: mariadb:10.4.5-bionic
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
MYSQL_USER: ${KEYSTONE_DB_USER:-keystone}
MYSQL_PASSWORD: ${KEYSTONE_DB_PASSWORD}
MYSQL_DATABASE: ${KEYSTONE_DB_NAME:-keystone}
volumes:
- mysql:/var/lib/mysql
keystone:
build:
context: .
args:
KEYSTONE_IMAGE_TAG: current-tripleo
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
KEYSTONE_ADMIN_PASSWORD: ${KEYSTONE_ADMIN_PASSWORD}
KEYSTONE_DB_PASSWORD: ${KEYSTONE_DB_PASSWORD}
KEYSTONE_DB_USER: ${KEYSTONE_DB_USER:-keystone}
KEYSTONE_DB_NAME: ${KEYSTONE_DB_NAME:-keystone}
KEYSTONE_DEBUG: ${KEYSTONE_DEBUG:-&amp;#34;false&amp;#34;}
ports:
- &amp;#34;127.0.0.1:5000:5000&amp;#34;
volumes:
mysql:
&lt;/code>&lt;/pre>&lt;h2 id="interacting-with-keystone">Interacting with Keystone&lt;/h2>
&lt;p>Once Keystone is up and running, we can grab the generated &lt;code>clouds.yaml&lt;/code> file like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker-compose exec keystone cat /root/clouds.yaml &amp;gt; clouds.yaml
&lt;/code>&lt;/pre>&lt;p>Now we can run the &lt;code>openstack&lt;/code> command line client:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ export OS_CLOUD=openstack-public
$ openstack catalog list
+----------+----------+-----------------------------------+
| Name | Type | Endpoints |
+----------+----------+-----------------------------------+
| keystone | identity | RegionOne |
| | | internal: http://localhost:5000 |
| | | RegionOne |
| | | public: http://localhost:5000 |
| | | |
+----------+----------+-----------------------------------+
$ openstack user list
+----------------------------------+-------+
| ID | Name |
+----------------------------------+-------+
| e8f460619a854c849feaf278b8d68e2c | admin |
+----------------------------------+-------+
&lt;/code>&lt;/pre>&lt;h2 id="project-sources">Project sources&lt;/h2>
&lt;p>You can find everything reference in this article in the &lt;a href="https://github.com/cci-moc/flocx-keystone-dev">flocx-keystone-dev&lt;/a> repository.&lt;/p></content></item><item><title>A DIY CPAP Battery Box</title><link>https://blog.oddbit.com/post/2019-05-14-a-diy-cpap-battery-box/</link><pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-05-14-a-diy-cpap-battery-box/</guid><description>A year or so ago I was diagnosed with sleep apnea, and since them I&amp;rsquo;ve been sleeping with a CPAP. This past weekend, I joined my daughter on a scout camping trip to a campground without readily accessible electricity. This would be the first time I found myself in this situation, and as the date approached, I realized I was going to have to build or buy some sort of battery solution for my CPAP.</description><content>&lt;p>A year or so ago I was diagnosed with &lt;a href="https://en.wikipedia.org/wiki/Sleep_apnea">sleep apnea&lt;/a>, and since them I&amp;rsquo;ve been sleeping with a &lt;a href="https://en.wikipedia.org/wiki/Continuous_positive_airway_pressure">CPAP&lt;/a>. This past weekend, I joined my daughter on a &lt;a href="https://www.scouting.org/scoutsbsa/">scout&lt;/a> camping trip to a &lt;a href="https://www.mass.gov/locations/harold-parker-state-forest">campground&lt;/a> without readily accessible electricity. This would be the first time I found myself in this situation, and as the date approached, I realized I was going to have to build or buy some sort of battery solution for my CPAP. I opted for the &amp;ldquo;build&amp;rdquo; option because it seemed like a fun project.&lt;/p>
&lt;p>This is what I ended up with:&lt;/p>
&lt;figure class="left" >
&lt;img src="batterybox-003.jpg" />
&lt;/figure>
&lt;figure class="left" >
&lt;img src="batterybox-005.jpg" />
&lt;/figure>
&lt;figure class="left" >
&lt;img src="batterybox-001.jpg" />
&lt;/figure>
&lt;p>It has a pair of 10Ah SLA batteries, two 12V ports, two USB charging ports, and an integrated 4A charger (just plug in an extension cord and it&amp;rsquo;s all set). There are fuses on all of the output ports.&lt;/p>
&lt;h2 id="batteries">Batteries&lt;/h2>
&lt;figure class="left" >
&lt;img src="12v-10ah-batteries.jpg" />
&lt;/figure>
&lt;p>I spent some time looking at battery options. I wanted something that would easily last a couple of nights. According to the &lt;a href="https://www.resmed.com/us/dam/documents/articles/198103_battery-guide_glo_eng.pdf">documentation from ResMed&lt;/a>, my AirSense CPAP draws around 1A when the heat and humidity are disabled. That means for two nights, with the heat and humidity off, I&amp;rsquo;ll need at least 16Ah.&lt;/p>
&lt;p>I looked at some &lt;a href="https://en.wikipedia.org/wiki/Lithium_iron_phosphate_battery">LiFePO4&lt;/a> batteries, but they were typically two- to three- times the cost of an equivalent &lt;a href="https://en.wikipedia.org/wiki/Sealed_lead-acid_battery">SLA&lt;/a> battery. I ended up purchasing a pair of 10Ah SLA batteries. Testing confirms that it does last for two nights. With the integrated 4A charger, they charge up relatively quickly as well (for a single night of use, it took around an hour for a full charge).&lt;/p>
&lt;p>The batteries are held in place by a roughly U-shaped piece of wood on the bottom of the case. The charger and the foam bad hold the batteries in when the top is closed.&lt;/p>
&lt;figure class="left" >
&lt;img src="resmed-battery-excerpt.png" />
&lt;figcaption class="center" >ResMed battery data&lt;/figcaption>
&lt;/figure>
&lt;h2 id="parts-list">Parts list&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B00009XVKW">Pelican 1300 case&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/dp/B01FG7NW60">Motopower MP00207 4A Charger&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B009ANV81S">NOCO 13A 125V port plug with extension cord&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B07KSGMC39">YonHan 4.8A Dual USB charger&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B012IE1EKA">AutoEC 30A 12V switches&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.amazon.com/gp/product/B07QN2ML4X">12V Sockets&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="panel-design">Panel design&lt;/h2>
&lt;p>I used &lt;a href="https://www.qcad.org/">qcad&lt;/a> to design the panel layout. The cross-hairs are for identifying the center point:&lt;/p>
&lt;figure class="left" >
&lt;img src="panel-layout.png" />
&lt;/figure>
&lt;p>I used a 1 1/8&amp;quot; hole saw to make the holes for the 12V and USB ports and a 2&amp;quot; hole saw for the through-plug on the back. I used a &lt;a href="https://www.amazon.com/gp/product/B001OEPYWK">step bit&lt;/a> to drill the smaller holes for the switches.&lt;/p>
&lt;h2 id="a-few-extra-pictures">A few extra pictures&lt;/h2>
&lt;figure class="left" >
&lt;img src="batterybox-002.jpg" />
&lt;/figure>
&lt;figure class="left" >
&lt;img src="batterybox-004.jpg" />
&lt;/figure></content></item><item><title>Unpacking a Python regular expression</title><link>https://blog.oddbit.com/post/2019-05-07-unpacking-a-python-regular-exp/</link><pubDate>Tue, 07 May 2019 10:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-05-07-unpacking-a-python-regular-exp/</guid><description>I recently answered a question from Harsha Nalore on StackOverflow that involved using Ansible to extract the output of a command sent to a BigIP device of some sort. My solution &amp;ndash; which I claim to be functional, but probably not optimal &amp;ndash; involved writing an Ansible filter module to parse the output. That filter made use of a complex-looking regular expression. Harsha asked for some details on that regular expression works, and the existing StackOverflow answer didn&amp;rsquo;t really seem the write place for that: so, here we are.</description><content>&lt;p>I recently answered &lt;a href="https://stackoverflow.com/q/55965819/147356">a question&lt;/a> from &lt;a href="https://stackoverflow.com/users/7738974/harsha-nalore">Harsha Nalore&lt;/a> on &lt;a href="https://stackoverflow.com/">StackOverflow&lt;/a> that involved using Ansible to extract the output of a command sent to a BigIP device of some sort. My solution &amp;ndash; which I claim to be functional, but probably not optimal &amp;ndash; involved writing an &lt;a href="https://ansible.com/">Ansible&lt;/a> filter module to parse the output. That filter made use of a complex-looking regular expression. Harsha asked for some details on that regular expression works, and the existing StackOverflow answer didn&amp;rsquo;t really seem the write place for that: so, here we are.&lt;/p>
&lt;p>The output in question looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>gtm wideip a wideip {
description wideip
pool-lb-mode topology
pools {
test1-pool {
order 1
}
test2-pool {
order 0
}
}
}
&lt;/code>&lt;/pre>&lt;p>The goal is to return a list of pool names. You can see the complete solution in &lt;a href="https://stackoverflow.com/a/55970019/147356">my answer&lt;/a>; for the purposes of this post we&amp;rsquo;re interesting in the following two regular expressions:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>re_pools &lt;span style="color:#f92672">=&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>compile(&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">gtm \s+ wideip \s+ a \s+ (\S+) \s+ { \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">(?P&amp;lt;parameters&amp;gt;(\S+ \s+ \S+ \s+)*)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">pools \s+ { \s+ (?P&amp;lt;pools&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">(?:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">\S+ \s+ { \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">[^}]* \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">} \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">)+ \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;&lt;/span>, flags&lt;span style="color:#f92672">=&lt;/span>re&lt;span style="color:#f92672">.&lt;/span>VERBOSE)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>re_pool &lt;span style="color:#f92672">=&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>compile(&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">(\S+) \s+ { \s+ [^}]* \s+ } \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;&lt;/span>, flags&lt;span style="color:#f92672">=&lt;/span>re&lt;span style="color:#f92672">.&lt;/span>VERBOSE)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="verbose-mode">VERBOSE mode&lt;/h1>
&lt;p>The first thing to note is that I&amp;rsquo;m using &lt;code>VERBOSE&lt;/code> syntax for both of these expressions. That means that whitespace must be included explicitly in the expression. That&amp;rsquo;s what all of those &lt;code>\s+&lt;/code> markers are &amp;ndash; that means &amp;ldquo;any white space character, one or more times&amp;rdquo;. For example, consider the following simple expression:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#66d9ef">match&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;this is a test&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;this is a test&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The pattern matches the string just fine. But if we were to enable the &lt;code>VERBOSE&lt;/code> flag, the pattern would no longer match:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#66d9ef">match&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;this is a test&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;this is a test&amp;#39;&lt;/span>, flags&lt;span style="color:#f92672">=&lt;/span>re&lt;span style="color:#f92672">.&lt;/span>VERBOSE)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We would instead need to write it like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#66d9ef">match&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;this \s is \s a \s test&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;this is a test&amp;#39;&lt;/span>, flags&lt;span style="color:#f92672">=&lt;/span>re&lt;span style="color:#f92672">.&lt;/span>VERBOSE)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The advantage to &lt;code>VERBOSE&lt;/code> mode is that you can split your regular expression across multiple lines for legibility:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#66d9ef">match&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">... this \s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">... is \s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">... a \s
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">... test&amp;#39;&amp;#39;&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;this is a test&amp;#39;&lt;/span>, flags&lt;span style="color:#f92672">=&lt;/span>re&lt;span style="color:#f92672">.&lt;/span>VERBOSE)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="capture-groups">Capture groups&lt;/h2>
&lt;p>In order to make it easier to extract information from the results of a match, I&amp;rsquo;m using named capture groups. A &amp;ldquo;capture group&amp;rdquo; is a part of the expression inside parentheses that can be extracted from the resulting match object. Unnamed groups can be extracted using their index. If we wanted to match the phrase &lt;code>this is a &amp;lt;noun&amp;gt;&lt;/code>, rather than &lt;code>this is a test&lt;/code>, and we wanted to extract the noun, we might write something like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> re_example &lt;span style="color:#f92672">=&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>compile(&lt;span style="color:#e6db74">&amp;#39;this is a (\S+)&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">match&lt;/span> &lt;span style="color:#f92672">=&lt;/span> re_example&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#66d9ef">match&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;this is a frog&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">match&lt;/span>&lt;span style="color:#f92672">.&lt;/span>groups()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>(&lt;span style="color:#e6db74">&amp;#39;frog&amp;#39;&lt;/span>,)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">match&lt;/span>&lt;span style="color:#f92672">.&lt;/span>group(&lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;frog&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The expression &lt;code>(\S+)&lt;/code> is a capture group that will match any string of non-whitespace characters. This works fine for a simple expression, but keeping the index straight in a complex expression can be difficult. This is where named capture groups become useful. We could rewrite the above like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> re_example &lt;span style="color:#f92672">=&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>compile(&lt;span style="color:#e6db74">&amp;#39;this is a (?P&amp;lt;noun&amp;gt;\S+)&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">match&lt;/span> &lt;span style="color:#f92672">=&lt;/span> re_example&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#66d9ef">match&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;this is a frog&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">match&lt;/span>&lt;span style="color:#f92672">.&lt;/span>groupdict()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&lt;span style="color:#e6db74">&amp;#39;noun&amp;#39;&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;frog&amp;#39;&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">match&lt;/span>&lt;span style="color:#f92672">.&lt;/span>group(&lt;span style="color:#e6db74">&amp;#39;noun&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;frog&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="non-capture-groups">Non-capture groups&lt;/h2>
&lt;p>Sometimes, you want to group part of a regular expression in a way that does not result in another capture group. This is what the &lt;code>(?: ...)&lt;/code> expression is for. For example, we we were to write:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> re_example &lt;span style="color:#f92672">=&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>compile(&lt;span style="color:#e6db74">&amp;#39;this (?:is|was) a (?P&amp;lt;noun&amp;gt;\S+)&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then we could match the phrase &lt;code>this is a test&lt;/code> or &lt;code>this was a test&lt;/code>, but we would still only have a single capture group:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">match&lt;/span> &lt;span style="color:#f92672">=&lt;/span> re_example&lt;span style="color:#f92672">.&lt;/span>&lt;span style="color:#66d9ef">match&lt;/span>(&lt;span style="color:#e6db74">&amp;#39;this is a test&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#66d9ef">match&lt;/span>&lt;span style="color:#f92672">.&lt;/span>groupdict()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&lt;span style="color:#e6db74">&amp;#39;noun&amp;#39;&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;test&amp;#39;&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>With all that in mind, let&amp;rsquo;s take a look at the regular expression in my answer:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>re_pools &lt;span style="color:#f92672">=&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>compile(&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">gtm \s+ wideip \s+ a \s+ (\S+) \s+ { \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">(?P&amp;lt;parameters&amp;gt;(\S+ \s+ \S+ \s+)*)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">pools \s+ { \s+ (?P&amp;lt;pools&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">(?:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">\S+ \s+ { \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">[^}]* \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">} \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">)+ \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;&lt;/span>, flags&lt;span style="color:#f92672">=&lt;/span>re&lt;span style="color:#f92672">.&lt;/span>VERBOSE)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The first line matches &lt;code>gtm wideip a &amp;lt;something&amp;gt; {&lt;/code>:&lt;/p>
&lt;pre>&lt;code>gtm \s+ wideip \s+ a \s+ (\S+) \s+ { \s+
&lt;/code>&lt;/pre>
&lt;p>Next, we match the &lt;code>&amp;lt;key&amp;gt; &amp;lt;value&amp;gt;&lt;/code> part of the output, which looks like this:&lt;/p>
&lt;pre>&lt;code>description wideip
pool-lb-mode topology
&lt;/code>&lt;/pre>
&lt;p>With this expression:&lt;/p>
&lt;pre>&lt;code> (?P&amp;lt;parameters&amp;gt;(\S+ \s+ \S+ \s+)*)
&lt;/code>&lt;/pre>
&lt;p>That is a named capture group (&amp;ldquo;parameters&amp;rdquo;) that matches the expression &lt;code>(\S+ \s+ \S+ \s+)&lt;/code> zero or more times (&lt;code>*&lt;/code>). Since &lt;code>\S+&lt;/code> means &amp;ldquo;a string of non-whitespace characters&amp;rdquo; and &lt;code>\s+&lt;/code> means &amp;ldquo;a string of whitespace characters&amp;rdquo;, this correctly matches that part of the output.&lt;/p>
&lt;p>Next, we match the entire &lt;code>pools {...}&lt;/code> part of the output with this expression:&lt;/p>
&lt;pre>&lt;code>pools \s+ { \s+ (?P&amp;lt;pools&amp;gt;
(?:
\S+ \s+ { \s+
[^}]* \s+
} \s+
)+ \s+
)
&lt;/code>&lt;/pre>
&lt;p>That creates a named capture group (&amp;ldquo;pools&amp;rdquo;) that looks for one or more occurrences of the pattern:&lt;/p>
&lt;pre>&lt;code>\S+ \s+ { \s+
[^}]* \s+
} \s+
&lt;/code>&lt;/pre>
&lt;p>The first line will match a string like &lt;code>test1-pool1 {&lt;/code>. The next line matches any sequence of characters that are not &lt;code>}&lt;/code>, so that gathers up everthing between &lt;code>test1-pool {&lt;/code> and the closing &lt;code>}&lt;/code>. Because we have the entire thing wrapped in &lt;code>(?: ...)+&lt;/code>, we are looking for one or more matches of that sub-expression, which gathers up all of the pool definitions.&lt;/p>
&lt;p>Finally we match the closing brace:&lt;/p>
&lt;pre>&lt;code>}
&lt;/code>&lt;/pre>
&lt;p>When that expression matches, we end up with a match object that has a &lt;code>pools&lt;/code> match group that will look like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> print(&lt;span style="color:#66d9ef">match&lt;/span>&lt;span style="color:#f92672">.&lt;/span>group(&lt;span style="color:#e6db74">&amp;#39;pools&amp;#39;&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test1&lt;span style="color:#f92672">-&lt;/span>pool {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> order &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> test2&lt;span style="color:#f92672">-&lt;/span>pool {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> order &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We now use a much simpler regular expression to extract the pool names from that content:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>re_pool &lt;span style="color:#f92672">=&lt;/span> re&lt;span style="color:#f92672">.&lt;/span>compile(&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">(\S+) \s+ { \s+ [^}]* \s+ } \s+
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;&lt;/span>, flags&lt;span style="color:#f92672">=&lt;/span>re&lt;span style="color:#f92672">.&lt;/span>VERBOSE)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>That has a single capture group (&lt;code>(\S+)&lt;/code>) that will match the pool name; the remainder of the expression takes care of matching the &lt;code>{ &amp;lt;anythingthing not '}'&amp;gt; }&lt;/code> part. We use &lt;code>re.findall&lt;/code> to get &lt;em>all&lt;/em> of the matches in one go:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> re_pool&lt;span style="color:#f92672">.&lt;/span>findall(&lt;span style="color:#66d9ef">match&lt;/span>&lt;span style="color:#f92672">.&lt;/span>group(&lt;span style="color:#e6db74">&amp;#39;pools&amp;#39;&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[&lt;span style="color:#e6db74">&amp;#39;test1-pool&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;test2-pool&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And that&amp;rsquo;s it!&lt;/p>
&lt;h2 id="for-more-information">For more information&lt;/h2>
&lt;p>For more information on Python regular expressions:&lt;/p>
&lt;ul>
&lt;li>The documentation for the &lt;a href="https://docs.python.org/3/library/re.html">re&lt;/a> module.&lt;/li>
&lt;li>The &lt;a href="https://docs.python.org/3/howto/regex.html">Regular expression HOWTO&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>New comment system</title><link>https://blog.oddbit.com/post/2019-05-07-new-comment-system/</link><pubDate>Tue, 07 May 2019 09:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-05-07-new-comment-system/</guid><description>As long as I&amp;rsquo;m switching site generators, it seems like a good idea to refresh the comment system as well. I&amp;rsquo;ve been using Disqus for a while, since when I started it was one of the only games in town. There are now alternatives of different sorts, and one in particular caught my eye: Utterances uses GitHub issues for storing comments, which seems like a fantastic idea.
That means that comments will finally be stored in the same place as the blog content, which I think is a happy state of affairs.</description><content>&lt;p>As long as I&amp;rsquo;m switching site generators, it seems like a good idea to refresh the comment system as well. I&amp;rsquo;ve been using &lt;a href="https://disqus.com">Disqus&lt;/a> for a while, since when I started it was one of the only games in town. There are now alternatives of different sorts, and one in particular caught my eye: &lt;a href="https://utteranc.es/">Utterances&lt;/a> uses GitHub issues for storing comments, which seems like a fantastic idea.&lt;/p>
&lt;p>That means that comments will finally be stored in the same place as the blog content, which I think is a happy state of affairs.&lt;/p></content></item><item><title>New static site generator</title><link>https://blog.oddbit.com/post/2019-05-06-new-static-site-generator/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-05-06-new-static-site-generator/</guid><description>I&amp;rsquo;ve switched my static site generator from Pelican to Hugo. I&amp;rsquo;ve tried to ensure that all the old links continue to work correctly, but if you notice anything missing or otherwise not working as intended, please let me know by opening an issue. Thanks!</description><content>&lt;p>I&amp;rsquo;ve switched my static site generator from &lt;a href="https://blog.getpelican.com/">Pelican&lt;/a> to &lt;a href="https://gohugo.io/">Hugo&lt;/a>. I&amp;rsquo;ve tried to ensure that all the old links continue to work correctly, but if you notice anything missing or otherwise not working as intended, please let me know by &lt;a href="https://github.com/larsks/blog.oddbit.com/issues">opening an issue&lt;/a>. Thanks!&lt;/p></content></item><item><title>Adding support for privilege escalation to Ansible's docker connection driver</title><link>https://blog.oddbit.com/post/2019-04-26-adding-support-for-privilege-e/</link><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-04-26-adding-support-for-privilege-e/</guid><description>Update 2019-05-09 Pull request #55816 has merged, so you can now use sudo with the docker connection driver even when sudo is configured to require a password.
I often use Docker to test out Ansible playbooks. While normally that works great, I recently ran into an unexpected problem with privilege escalation. Given a simple playbook like this:
--- - hosts: all gather_facts: false become: true tasks: - ping: And an inventory like this:</description><content>&lt;p>&lt;strong>Update 2019-05-09&lt;/strong> Pull request
&lt;a href="https://github.com/ansible/ansible/pull/55816" class="pull-request">#55816&lt;/a>
has merged, so you can now use &lt;code>sudo&lt;/code> with the &lt;code>docker&lt;/code> connection driver even when &lt;code>sudo&lt;/code> is configured to require a password.&lt;/p>
&lt;hr>
&lt;p>I often use Docker to test out Ansible playbooks. While normally that works great, I recently ran into an unexpected problem with privilege escalation. Given a simple playbook like this:&lt;/p>
&lt;pre>&lt;code>---
- hosts: all
gather_facts: false
become: true
tasks:
- ping:
&lt;/code>&lt;/pre>
&lt;p>And an inventory like this:&lt;/p>
&lt;pre>&lt;code>all:
vars:
ansible_user: example
ansible_connection: docker
hosts:
server1:
ansible_host: sudostuff_server1_1
server2:
ansible_host: sudostuff_server2_1
server3:
ansible_host: sudostuff_server3_1
&lt;/code>&lt;/pre>
&lt;p>And containers with &lt;code>sudo&lt;/code> configured to require a password, Ansible would fail like this (note that I&amp;rsquo;ve configured Ansible to use the &lt;code>debug&lt;/code> plugin for &lt;code>stdout_callback&lt;/code>):&lt;/p>
&lt;pre tabindex="0">&lt;code>fatal: [server1]: FAILED! =&amp;gt; {
&amp;#34;changed&amp;#34;: false,
&amp;#34;rc&amp;#34;: 1
}
MSG:
MODULE FAILURE
See stdout/stderr for the exact error
MODULE_STDERR:
We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:
#1) Respect the privacy of others.
#2) Think before you type.
#3) With great power comes great responsibility.
[sudo via ansible, key=rzrfiifcqoggklmehivtcrrlnnbphwbp] password:
&lt;/code>&lt;/pre>&lt;p>In the above output, you&amp;rsquo;ll note that there are no actual errors, but unexpectedly we&amp;rsquo;re seeing the privilege escalation prompt show up in the &lt;code>stderr&lt;/code> of the command. A quick search revealed bugs &lt;a href="https://github.com/ansible/ansible/issues/31759">#31759&lt;/a> and &lt;a href="https://github.com/ansible/ansible/issues/53385">#53385&lt;/a>, both of which confirm that privilege escalation simply doesn&amp;rsquo;t work using the &lt;code>docker&lt;/code> connection plugin.&lt;/p>
&lt;h2 id="use-the-source-luke">Use the source, Luke&lt;/h2>
&lt;figure class="left" >
&lt;img src="sausage.jpg" />
&lt;figcaption class="center" >Discovering how the sausage is made...&lt;/figcaption>
&lt;/figure>
&lt;p>Looking at the source, I was surprised: while Ansible has individual plugins for different privilege escalation methods, it is entirely up to the individual connection plugin to implement the logic necessary to make use of these mechanisms. I had expected privilege escalation support to be implemented in the base connection plugin (&lt;code>ConnectionBase&lt;/code> in &lt;code>lib/ansible/plugins/connection/__init__.py&lt;/code>), but it&amp;rsquo;s not. So while the &lt;a href="https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/connection/ssh.py">ssh plugin&lt;/a> has a fairly complex set of logic for handing the &lt;code>become&lt;/code> prompt, and the &lt;a href="https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/connection/local.py">local plugin&lt;/a> had a relatively simple solution, the &lt;code>docker&lt;/code> connection had none.&lt;/p>
&lt;p>Fortunately, in many ways the &lt;code>docker&lt;/code> plugin is almost identical to the &lt;code>local&lt;/code> plugin, which means that rather than doing actual work I was able to largely cut-and-paste the privilege escalation support from the &lt;code>local&lt;/code> plugin into the &lt;code>docker&lt;/code> plugin. You can find this work in pull request
&lt;a href="https://github.com/ansible/ansible/pull/55816" class="pull-request">#55816&lt;/a>
.&lt;/p></content></item><item><title>Writing Ansible filter plugins</title><link>https://blog.oddbit.com/post/2019-04-25-writing-ansible-filter-plugins/</link><pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-04-25-writing-ansible-filter-plugins/</guid><description>I often see questions from people who are attemping to perform complex text transformations in their Ansible playbooks. While I am a huge fan of Ansible, data transformation is not one of its strong points. For example, this past week someone asked a question on Stack Overflow in which they were attempting to convert the output of the keytool command into a list of dictionaries. The output of the keytool -list -v command looks something like this:</description><content>&lt;p>I often see questions from people who are attemping to perform complex text transformations in their &lt;a href="https://www.ansible.com/">Ansible&lt;/a> playbooks. While I am a huge fan of Ansible, data transformation is not one of its strong points. For example, this past week someone &lt;a href="https://stackoverflow.com/questions/55853384/ansible-build-list-dictionary-with-from-list-of-strings/55854394">asked a question&lt;/a> on Stack Overflow in which they were attempting to convert the output of the &lt;a href="https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html">keytool&lt;/a> command into a list of dictionaries. The output of the &lt;code>keytool -list -v&lt;/code> command looks something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>Keystore type: PKCS12
Keystore provider: SUN
Your keystore contains 2 entries
Alias name: alias2
Creation date: Apr 25, 2019
Entry type: PrivateKeyEntry
Certificate chain length: 1
Certificate[1]:
Owner: CN=Alice McHacker, OU=Unknown, O=Example Corp, L=Boston, ST=MA, C=US
Issuer: CN=Alice McHacker, OU=Unknown, O=Example Corp, L=Boston, ST=MA, C=US
Serial number: 5c017636
Valid from: Thu Apr 25 23:22:37 EDT 2019 until: Wed Jul 24 23:22:37 EDT 2019
Certificate fingerprints:
SHA1: FB:AC:36:08:F6:3C:C0:CF:E1:D7:E6:7D:2F:31:BF:BE:5A:C8:7A:C6
SHA256: 73:F1:EC:61:6B:63:93:F5:BE:78:23:A1:79:14:7D:F0:A3:9A:D8:22:99:6B:38:0F:D6:38:AA:93:B5:58:8E:E0
Signature algorithm name: SHA256withRSA
Subject Public Key Algorithm: 2048-bit RSA key
Version: 3
Extensions:
#1: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: 17 D4 A3 54 E4 0C DB CC 00 3E 1C 4D 74 B4 DE 55 ...T.....&amp;gt;.Mt..U
0010: D6 C9 CB 21 ...!
]
]
*******************************************
*******************************************
Alias name: alias1
Creation date: Apr 25, 2019
Entry type: PrivateKeyEntry
Certificate chain length: 1
Certificate[1]:
Owner: CN=Mallory Root, OU=Unknown, O=Example Corp, L=New York, ST=NY, C=US
Issuer: CN=Mallory Root, OU=Unknown, O=Example Corp, L=New York, ST=NY, C=US
Serial number: 2617e8fb
Valid from: Thu Apr 25 23:22:59 EDT 2019 until: Wed Jul 24 23:22:59 EDT 2019
Certificate fingerprints:
SHA1: DD:83:42:F3:AD:EB:DC:66:50:DA:7D:D7:59:32:9B:31:0C:E0:90:B9
SHA256: D9:3E:42:47:A1:DB:2F:00:46:F7:58:54:30:D1:83:F5:DD:C6:5D:8B:8B:6B:94:4A:34:B0:0D:D8:6F:7A:6E:B6
Signature algorithm name: SHA256withRSA
Subject Public Key Algorithm: 2048-bit RSA key
Version: 3
Extensions:
#1: ObjectId: 2.5.29.14 Criticality=false
SubjectKeyIdentifier [
KeyIdentifier [
0000: 98 53 CF EF 77 36 02 4D 63 83 D7 4F 06 EF 09 CA .S..w6.Mc..O....
0010: 41 92 6D 92 A.m.
]
]
*******************************************
*******************************************
&lt;/code>&lt;/pre>&lt;p>That&amp;rsquo;s a mess. We&amp;rsquo;d like to extract specific information about the keys in the keystore; specifically:&lt;/p>
&lt;ul>
&lt;li>The owner&lt;/li>
&lt;li>The issuer&lt;/li>
&lt;li>The creation date&lt;/li>
&lt;li>The valid from/valid until dates&lt;/li>
&lt;/ul>
&lt;p>There are a few ways of approaching this problem (for example, one could have your playbook call out to &lt;code>awk&lt;/code> to parse the &lt;code>keytool&lt;/code> output and generate JSON data for Ansible to consume), but a more robust, flexible, and often simpler way of dealing with something like this is to write a custom filter plugin in Python.&lt;/p>
&lt;h2 id="what-is-a-filter-plugin">What is a filter plugin?&lt;/h2>
&lt;p>A filter plugin defines one or more Python functions that can be used in Jinja2 templating expressions (using the &lt;code>|&lt;/code> filter operator). A filter function receives one mandatory argument (the value to the left of the &lt;code>|&lt;/code>) and zero or more additional positional and/or keyword arguments, performs some transformation on the input data, and returns the result.&lt;/p>
&lt;p>For example, there is a &lt;code>unique&lt;/code> filter, which takes a list and returns a new list consisting of only unique values. If we had a list of names and wanted to eliminiate duplicates, we might use something like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">set_fact&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">unique_names&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ [&amp;#39;alice&amp;#39;, &amp;#39;bob&amp;#39;, &amp;#39;alice&amp;#39;, &amp;#39;mallory&amp;#39;, &amp;#39;bob&amp;#39;, &amp;#39;mallory&amp;#39;]|unique }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>That would set &lt;code>unique_names&lt;/code> to the list &lt;code>['alice', 'bob', 'mallory']&lt;/code>.&lt;/p>
&lt;h2 id="how-do-you-write-a-filter-plugin">How do you write a filter plugin?&lt;/h2>
&lt;p>A filter plugin doesn&amp;rsquo;t require much. You&amp;rsquo;ll need to create a Python module that defines a &lt;code>FilterModule&lt;/code> class, and that class must have a method named &lt;code>filters&lt;/code> that will return a dictionary that maps filter names to callables implementing the filter. For example, if we want a filter named &lt;code>upper&lt;/code> that would transform a string to upper-case, we could write:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">FilterModule&lt;/span>(object):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">filters&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> {&lt;span style="color:#e6db74">&amp;#39;upper&amp;#39;&lt;/span>: &lt;span style="color:#66d9ef">lambda&lt;/span> x: x&lt;span style="color:#f92672">.&lt;/span>upper()}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we wanted implement a version of the &lt;code>unique&lt;/code> filter, it might look like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">filter_unique&lt;/span>(things):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seen &lt;span style="color:#f92672">=&lt;/span> set()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> unique_things &lt;span style="color:#f92672">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> thing &lt;span style="color:#f92672">in&lt;/span> things:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> thing &lt;span style="color:#f92672">not&lt;/span> &lt;span style="color:#f92672">in&lt;/span> seen:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seen&lt;span style="color:#f92672">.&lt;/span>add(thing)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> unique_things&lt;span style="color:#f92672">.&lt;/span>append(thing)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> unique_things
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">FilterModule&lt;/span>(object):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">filters&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> {&lt;span style="color:#e6db74">&amp;#39;unique&amp;#39;&lt;/span>: filter_unique}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We need to put the new module in a directory named &lt;code>filter_plugins&lt;/code> that is adjacent to our playbook. If we were to place the &lt;code>upper&lt;/code> filter module in, say, &lt;code>filter_plugins/upper.py&lt;/code>, we could then add a task like this to our playbook:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">debug&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ &amp;#39;this is a test&amp;#39;|upper }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And get this output:&lt;/p>
&lt;pre tabindex="0">&lt;code>TASK [debug] **********************************************************************************
ok: [localhost] =&amp;gt; {
&amp;#34;msg&amp;#34;: &amp;#34;THIS IS A TEST&amp;#34;
}
&lt;/code>&lt;/pre>&lt;h2 id="parsing-keytool-output">Parsing keytool output&lt;/h2>
&lt;p>Our &lt;code>keytool&lt;/code> filter is only a little bit more complicated:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#!/usr/bin/python&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">filter_keys_to_list&lt;/span>(v):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key_list &lt;span style="color:#f92672">=&lt;/span> []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key &lt;span style="color:#f92672">=&lt;/span> {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> found_start &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">False&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># iterate over lines of output from keytool&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">for&lt;/span> line &lt;span style="color:#f92672">in&lt;/span> v&lt;span style="color:#f92672">.&lt;/span>splitlines():
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Discard any lines that don&amp;#39;t look like &amp;#34;key: value&amp;#34; lines&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#e6db74">&amp;#39;: &amp;#39;&lt;/span> &lt;span style="color:#f92672">not&lt;/span> &lt;span style="color:#f92672">in&lt;/span> line:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Look for &amp;#34;Alias name&amp;#34; at the beginning of a line to identify&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># the start of a new key.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> line&lt;span style="color:#f92672">.&lt;/span>startswith(&lt;span style="color:#e6db74">&amp;#39;Alias name&amp;#39;&lt;/span>):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> found_start &lt;span style="color:#f92672">=&lt;/span> &lt;span style="color:#66d9ef">True&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># If we have already collected data on a key, append that to&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># the list of keys.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> key:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key_list&lt;span style="color:#f92672">.&lt;/span>append(key)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key &lt;span style="color:#f92672">=&lt;/span> {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Read the next line if we haven&amp;#39;t found the start of a key&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># yet.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> &lt;span style="color:#f92672">not&lt;/span> found_start:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Split fields and values into dictionary items.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> field, value &lt;span style="color:#f92672">=&lt;/span> line&lt;span style="color:#f92672">.&lt;/span>split(&lt;span style="color:#e6db74">&amp;#39;: &amp;#39;&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> field &lt;span style="color:#f92672">in&lt;/span> [&lt;span style="color:#e6db74">&amp;#39;Alias name&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Owner&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Issuer&amp;#39;&lt;/span>, &lt;span style="color:#e6db74">&amp;#39;Creation date&amp;#39;&lt;/span>]:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key[field] &lt;span style="color:#f92672">=&lt;/span> value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">elif&lt;/span> field &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#e6db74">&amp;#39;Valid from&amp;#39;&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key[&lt;span style="color:#e6db74">&amp;#39;Valid from&amp;#39;&lt;/span>], key[&lt;span style="color:#e6db74">&amp;#39;Valid until&amp;#39;&lt;/span>] &lt;span style="color:#f92672">=&lt;/span> value&lt;span style="color:#f92672">.&lt;/span>split(&lt;span style="color:#e6db74">&amp;#39; until: &amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#75715e"># Append the final key.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">if&lt;/span> key:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key_list&lt;span style="color:#f92672">.&lt;/span>append(key)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> key_list
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">class&lt;/span> &lt;span style="color:#a6e22e">FilterModule&lt;/span>(object):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filter_map &lt;span style="color:#f92672">=&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#e6db74">&amp;#39;keys_to_list&amp;#39;&lt;/span>: filter_keys_to_list,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">def&lt;/span> &lt;span style="color:#a6e22e">filters&lt;/span>(self):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#66d9ef">return&lt;/span> self&lt;span style="color:#f92672">.&lt;/span>filter_map
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The logic here is fairly simple:&lt;/p>
&lt;ul>
&lt;li>Iterate over the lines in the output from &lt;code>keytool&lt;/code>.&lt;/li>
&lt;li>Look for &amp;ldquo;Alias name&amp;rdquo; at the beginning of a line to identify
the start of key data.&lt;/li>
&lt;li>Split lines on &lt;code>: &lt;/code> into field names and values.&lt;/li>
&lt;li>Assemble a dictionary from selected fields.&lt;/li>
&lt;li>Append the dictionary to a list and repeat.&lt;/li>
&lt;/ul>
&lt;p>Using it makes for a clear and simple playbook:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">set_fact&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">key_list&lt;/span>: &lt;span style="color:#e6db74">&amp;#34;{{ keytool.stdout|keys_to_list }}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="more-information">More information&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/blog-2019-04-25-filter-plugins">Playbook and filter plugin referenced in this article&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html">Ansible &amp;ldquo;Filters&amp;rdquo; documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ansible/ansible/tree/devel/lib/ansible/plugins/filter">Existing filter plugins in Ansible&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Docker build learns about secrets and ssh agent forwarding</title><link>https://blog.oddbit.com/post/2019-02-24-docker-build-learns-about-secr/</link><pubDate>Sun, 24 Feb 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-02-24-docker-build-learns-about-secr/</guid><description>A common problem for folks working with Docker is accessing resources which require authentication during the image build step. A particularly common use case is getting access to private git repositories using ssh key-based authentication. Until recently there hasn&amp;rsquo;t been a great solution:
you can embed secrets in your image, but now you can&amp;rsquo;t share the image with anybody. you can use build arguments, but this requires passing in an unenecrypted private key on the docker build command line, which is suboptimal for a number of reasons you can perform all the steps requiring authentication at runtime, but this can needlessly complicate your container startup process.</description><content>&lt;p>A common problem for folks working with Docker is accessing resources which require authentication during the image build step. A particularly common use case is getting access to private git repositories using ssh key-based authentication. Until recently there hasn&amp;rsquo;t been a great solution:&lt;/p>
&lt;ul>
&lt;li>you can embed secrets in your image, but now you can&amp;rsquo;t share the image with anybody.&lt;/li>
&lt;li>you can use build arguments, but this requires passing in an unenecrypted private key on the &lt;code>docker build&lt;/code> command line, which is suboptimal for a number of reasons&lt;/li>
&lt;li>you can perform all the steps requiring authentication at runtime, but this can needlessly complicate your container startup process.&lt;/li>
&lt;/ul>
&lt;p>With Docker 18.09, there are some experimental features available that makes this much easier. You can read the official announcement &lt;a href="https://docs.docker.com/develop/develop-images/build_enhancements/">here&lt;/a>, but I wanted to highlight the support for ssh agent forwarding and private keys.&lt;/p>
&lt;h1 id="prerequisites">Prerequisites&lt;/h1>
&lt;p>In order to use the new features, you first need to explicitly enable BuildKit support by setting &lt;code>DOCKER_BUILDKIT=1&lt;/code> in your environment:&lt;/p>
&lt;pre>&lt;code>export DOCKER_BUILDKIT=1
&lt;/code>&lt;/pre>
&lt;p>And to utilize the new &lt;code>Dockerfile&lt;/code> syntax, you need to start your &lt;code>Dockerfile&lt;/code> with this directive:&lt;/p>
&lt;pre>&lt;code># syntax=docker/dockerfile:1.0.0-experimental
&lt;/code>&lt;/pre>
&lt;p>That instructs Docker to use the named image (&lt;code>docker/dockerfile:1.0.0-experimental&lt;/code>) to handle the image build process.&lt;/p>
&lt;h2 id="a-simple-example">A simple example&lt;/h2>
&lt;p>The most common use case will probably be forwarding access to your local ssh agent. In order for the build process to get access to your agent, two things must happen:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The &lt;code>RUN&lt;/code> command that requires credentials must specify &lt;code>--mount=type=ssh&lt;/code> in order to have access to the forwarded agent connection, and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You must pass an appropriate &lt;code>--ssh&lt;/code> option on the &lt;code>docker build&lt;/code> command line. This is to prevent a Dockerfile from unexpectedly gaining access to your ssh credentials.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>We can see this in action if we start with the following &lt;code>Dockerfile&lt;/code>:&lt;/p>
&lt;pre>&lt;code># syntax=docker/dockerfile:1.0.0-experimental
FROM alpine
RUN apk add --update git openssh
# This is necessary to prevent the &amp;quot;git clone&amp;quot; operation from failing
# with an &amp;quot;unknown host key&amp;quot; error.
RUN mkdir -m 700 /root/.ssh; \
touch -m 600 /root/.ssh/known_hosts; \
ssh-keyscan github.com &amp;gt; /root/.ssh/known_hosts
# This command will have access to the forwarded agent (if one is
# available)
RUN --mount=type=ssh git clone git@github.com:moby/buildkit
&lt;/code>&lt;/pre>
&lt;p>If we run build the image like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>export DOCKER_BUILDKIT=1
docker build --ssh default -t buildtest .
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;then our &lt;code>git clone&lt;/code> operation will successfully authenticate with github using our ssh private key, assuming that we had one loaded into our local ssh agent.&lt;/p>
&lt;h2 id="but-wait-theres-more">But wait, there&amp;rsquo;s more&lt;/h2>
&lt;p>In the previous example line, the &lt;code>--ssh default&lt;/code> option requests &lt;code>docker build&lt;/code> to forward your default ssh agent. There may be situations in which this isn&amp;rsquo;t appropriate (for example, maybe you need to use a key that isn&amp;rsquo;t loaded into your default agent). You can provide the &lt;code>--ssh&lt;/code> option with one or more paths to ssh agent sockets or (unencrypted) private key files. Let&amp;rsquo;s say you have two service-specific private keys:&lt;/p>
&lt;ul>
&lt;li>For GitHub, you need to use &lt;code>$HOME/.ssh/github_rsa&lt;/code>&lt;/li>
&lt;li>For BitBucket, you need to use &lt;code>$HOME/.ssh/bitbucket_rsa&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>You can provide the keys on the &lt;code>docker build&lt;/code> command line like this:&lt;/p>
&lt;pre>&lt;code>docker build --ssh github=$HOME/.ssh/github_rsa,bitbucket=$HOME/.ssh/bitbucket_rsa -t buildtest .
&lt;/code>&lt;/pre>
&lt;p>Then inside your &lt;code>Dockerfile&lt;/code>, you can use the &lt;code>id=&amp;lt;name&amp;gt;&lt;/code> parameter to the &lt;code>--mount&lt;/code> option to specify which key should be available to the &lt;code>RUN&lt;/code> command:&lt;/p>
&lt;pre>&lt;code># syntax=docker/dockerfile:1.0.0-experimental
FROM alpine
RUN apk add --update git openssh
# This is necessary to prevent the &amp;quot;git clone&amp;quot; operation from failing
# with an &amp;quot;unknown host key&amp;quot; error.
RUN mkdir -m 700 /root/.ssh; \
touch -m 600 /root/.ssh/known_hosts; \
ssh-keyscan github.com bitbucket.com &amp;gt; /root/.ssh/known_hosts
# This command has access to the &amp;quot;github&amp;quot; key
RUN --mount=type=ssh,id=github git clone git@github.com:some/project
# This command has access to the &amp;quot;bitbucket&amp;quot; key
RUN --mount=type=ssh,id=bitbucket git clone git@bitbucket.com:other/project
&lt;/code>&lt;/pre>
&lt;h2 id="other-secrets">Other secrets&lt;/h2>
&lt;p>In this post I&amp;rsquo;ve looked specfically at providing &lt;code>docker build&lt;/code> with access to your ssh keys. Docker 18.09 also introduces support for exposing other secrets to the build process; see the official announcement (linked above) for details.&lt;/p></content></item><item><title>In which I PEBKAC so you don't have to</title><link>https://blog.oddbit.com/post/2019-02-11-in-which-i-pebkac-so-you-dont-/</link><pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-02-11-in-which-i-pebkac-so-you-dont-/</guid><description>Say you have a simple bit of code:
#include &amp;lt;avr/io.h&amp;gt; #include &amp;lt;util/delay.h&amp;gt; #define LED_BUILTIN _BV(PORTB5) int main(void) { DDRB |= LED_BUILTIN; while (1) { PORTB |= LED_BUILTIN; // turn on led _delay_ms(1000); // delay 1s PORTB &amp;amp;= ~LED_BUILTIN; // turn off led _delay_ms(1000); // delay 1s } } You have a Makefile that compiles that into an object (.o) file like this:
avr-gcc -mmcu=atmega328p -DF_CPU=16000000 -Os -c blink.c If you were to forget to set the device type when compiling your .</description><content>&lt;p>Say you have a simple bit of code:&lt;/p>
&lt;pre>&lt;code>#include &amp;lt;avr/io.h&amp;gt;
#include &amp;lt;util/delay.h&amp;gt;
#define LED_BUILTIN _BV(PORTB5)
int main(void)
{
DDRB |= LED_BUILTIN;
while (1)
{
PORTB |= LED_BUILTIN; // turn on led
_delay_ms(1000); // delay 1s
PORTB &amp;amp;= ~LED_BUILTIN; // turn off led
_delay_ms(1000); // delay 1s
}
}
&lt;/code>&lt;/pre>
&lt;p>You have a Makefile that compiles that into an object (&lt;code>.o&lt;/code>) file like this:&lt;/p>
&lt;pre>&lt;code>avr-gcc -mmcu=atmega328p -DF_CPU=16000000 -Os -c blink.c
&lt;/code>&lt;/pre>
&lt;p>If you were to forget to set the device type when compiling your &lt;code>.c&lt;/code> file into an object file (&lt;code>.o&lt;/code>), you would get a warning:&lt;/p>
&lt;pre>&lt;code>$ avr-gcc -DF_CPU=16000000 -Os -c blink.c
In file included from blink.c:1:0:
/usr/avr/include/avr/io.h:623:6: warning: #warning &amp;quot;device type not defined&amp;quot; [-Wcpp]
# warning &amp;quot;device type not defined&amp;quot;
^~~~~~~
&lt;/code>&lt;/pre>
&lt;p>But if you were to forget to set the device type when linking the final ELF binary, you would not be so lucky:&lt;/p>
&lt;pre>&lt;code>$ avr-gcc -o blink.elf blink.o
&amp;lt;...silence...&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>So you would, perhaps, be surprised when you flash this to your device and it doesn&amp;rsquo;t work. If you take a look at the assembly generated for the above command, it looks like this:&lt;/p>
&lt;pre>&lt;code>$ avr-objdump -d blink.elf
blink.elf: file format elf32-avr
Disassembly of section .text:
00000000 &amp;lt;main&amp;gt;:
0: 25 9a sbi 0x04, 5 ; 4
2: 2d 9a sbi 0x05, 5 ; 5
4: 2f e3 ldi r18, 0x3F ; 63
6: 8d e0 ldi r24, 0x0D ; 13
8: 93 e0 ldi r25, 0x03 ; 3
a: 21 50 subi r18, 0x01 ; 1
c: 80 40 sbci r24, 0x00 ; 0
e: 90 40 sbci r25, 0x00 ; 0
10: e1 f7 brne .-8 ; 0xa &amp;lt;__zero_reg__+0x9&amp;gt;
12: 00 c0 rjmp .+0 ; 0x14 &amp;lt;__zero_reg__+0x13&amp;gt;
14: 00 00 nop
16: 2d 98 cbi 0x05, 5 ; 5
18: 2f e3 ldi r18, 0x3F ; 63
1a: 8d e0 ldi r24, 0x0D ; 13
1c: 93 e0 ldi r25, 0x03 ; 3
1e: 21 50 subi r18, 0x01 ; 1
20: 80 40 sbci r24, 0x00 ; 0
22: 90 40 sbci r25, 0x00 ; 0
24: e1 f7 brne .-8 ; 0x1e &amp;lt;__zero_reg__+0x1d&amp;gt;
26: 00 c0 rjmp .+0 ; 0x28 &amp;lt;__zero_reg__+0x27&amp;gt;
28: 00 00 nop
2a: eb cf rjmp .-42 ; 0x2 &amp;lt;__zero_reg__+0x1&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>If you remember to include the device type:&lt;/p>
&lt;pre>&lt;code>$ avr-gcc -mmcu=atmega328p -o blink.elf blink.o
&lt;/code>&lt;/pre>
&lt;p>You instead get:&lt;/p>
&lt;pre>&lt;code>$ avr-objdump -d blink.elf
blink.elf: file format elf32-avr
Disassembly of section .text:
00000000 &amp;lt;__vectors&amp;gt;:
0: 0c 94 34 00 jmp 0x68 ; 0x68 &amp;lt;__ctors_end&amp;gt;
4: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
8: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
c: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
10: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
14: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
18: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
1c: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
20: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
24: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
28: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
2c: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
30: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
34: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
38: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
3c: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
40: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
44: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
48: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
4c: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
50: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
54: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
58: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
5c: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
60: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
64: 0c 94 3e 00 jmp 0x7c ; 0x7c &amp;lt;__bad_interrupt&amp;gt;
00000068 &amp;lt;__ctors_end&amp;gt;:
68: 11 24 eor r1, r1
6a: 1f be out 0x3f, r1 ; 63
6c: cf ef ldi r28, 0xFF ; 255
6e: d8 e0 ldi r29, 0x08 ; 8
70: de bf out 0x3e, r29 ; 62
72: cd bf out 0x3d, r28 ; 61
74: 0e 94 40 00 call 0x80 ; 0x80 &amp;lt;main&amp;gt;
78: 0c 94 56 00 jmp 0xac ; 0xac &amp;lt;_exit&amp;gt;
0000007c &amp;lt;__bad_interrupt&amp;gt;:
7c: 0c 94 00 00 jmp 0 ; 0x0 &amp;lt;__vectors&amp;gt;
00000080 &amp;lt;main&amp;gt;:
80: 25 9a sbi 0x04, 5 ; 4
82: 2d 9a sbi 0x05, 5 ; 5
84: 2f e3 ldi r18, 0x3F ; 63
86: 8d e0 ldi r24, 0x0D ; 13
88: 93 e0 ldi r25, 0x03 ; 3
8a: 21 50 subi r18, 0x01 ; 1
8c: 80 40 sbci r24, 0x00 ; 0
8e: 90 40 sbci r25, 0x00 ; 0
90: e1 f7 brne .-8 ; 0x8a &amp;lt;main+0xa&amp;gt;
92: 00 c0 rjmp .+0 ; 0x94 &amp;lt;main+0x14&amp;gt;
94: 00 00 nop
96: 2d 98 cbi 0x05, 5 ; 5
98: 2f e3 ldi r18, 0x3F ; 63
9a: 8d e0 ldi r24, 0x0D ; 13
9c: 93 e0 ldi r25, 0x03 ; 3
9e: 21 50 subi r18, 0x01 ; 1
a0: 80 40 sbci r24, 0x00 ; 0
a2: 90 40 sbci r25, 0x00 ; 0
a4: e1 f7 brne .-8 ; 0x9e &amp;lt;main+0x1e&amp;gt;
a6: 00 c0 rjmp .+0 ; 0xa8 &amp;lt;main+0x28&amp;gt;
a8: 00 00 nop
aa: eb cf rjmp .-42 ; 0x82 &amp;lt;main+0x2&amp;gt;
000000ac &amp;lt;_exit&amp;gt;:
ac: f8 94 cli
000000ae &amp;lt;__stop_program&amp;gt;:
ae: ff cf rjmp .-2 ; 0xae &amp;lt;__stop_program&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>You can see that this code includes things like the jump table, without which your code won&amp;rsquo;t run.&lt;/p>
&lt;p>The moral of this story is: don&amp;rsquo;t forget to set the device type.&lt;/p></content></item><item><title>ATOMIC_BLOCK magic in avr-libc</title><link>https://blog.oddbit.com/post/2019-02-01-atomicblock-magic-in-avrlibc/</link><pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-02-01-atomicblock-magic-in-avrlibc/</guid><description>The AVR C library, avr-libc, provide an ATOMIC_BLOCK macro that you can use to wrap critical sections of your code to ensure that interrupts are disabled while the code executes. At high level, the ATOMIC_BLOCK macro (when called using ATOMIC_FORCEON) does something like this:
cli(); ...your code here... seti(); But it&amp;rsquo;s more than that. If you read the documentation for the macro, it says:
Creates a block of code that is guaranteed to be executed atomically.</description><content>&lt;p>The AVR C library, &lt;a href="https://www.nongnu.org/avr-libc/">avr-libc&lt;/a>, provide an &lt;code>ATOMIC_BLOCK&lt;/code> macro that you can use to wrap critical sections of your code to ensure that interrupts are disabled while the code executes. At high level, the &lt;code>ATOMIC_BLOCK&lt;/code> macro (when called using &lt;code>ATOMIC_FORCEON&lt;/code>) does something like this:&lt;/p>
&lt;pre>&lt;code>cli();
...your code here...
seti();
&lt;/code>&lt;/pre>
&lt;p>But it&amp;rsquo;s more than that. If you read &lt;a href="https://www.nongnu.org/avr-libc/user-manual/group__util__atomic.html#gaaaea265b31dabcfb3098bec7685c39e4">the documentation&lt;/a> for the macro, it says:&lt;/p>
&lt;blockquote>
&lt;p>Creates a block of code that is guaranteed to be executed atomically. Upon entering the block the Global Interrupt Status flag in SREG is disabled, and re-enabled upon exiting the block &lt;strong>from any exit path&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;p>I didn&amp;rsquo;t really think much about the bit that I&amp;rsquo;ve highlighted until I wrote some code that looked something like this:&lt;/p>
&lt;pre>&lt;code>ATOMIC_BLOCK(ATOMIC_RESTORESTATE) {
while(some_condition) {
if (something_failed)
goto fail;
do_something_else();
}
}
fail:
return false;
&lt;/code>&lt;/pre>
&lt;p>There&amp;rsquo;s a &lt;code>goto&lt;/code> statement there; that&amp;rsquo;s an unconditional jump to the &lt;code>fail&lt;/code> label outside the &lt;code>ATOMIC_BLOCK&lt;/code> block. There isn&amp;rsquo;t really any opportunity there for anything to re-enable interrupts, and yet, my code worked just fine. What&amp;rsquo;s going on?&lt;/p>
&lt;p>It turns out that this is due to GCC magic. If you look at the expansion of the &lt;code>ATOMIC_BLOCK&lt;/code> macro, it looks like this:&lt;/p>
&lt;pre>&lt;code>#define ATOMIC_BLOCK(type) for ( type, __ToDo = __iCliRetVal(); \
__ToDo ; __ToDo = 0 )
&lt;/code>&lt;/pre>
&lt;p>It accepts a &lt;code>type&lt;/code> parameter which can be one of &lt;code>ATOMIC_RESTORESTATE&lt;/code> or &lt;code>ATOMIC_FORCEON&lt;/code>, which look like this:&lt;/p>
&lt;pre>&lt;code>#define ATOMIC_RESTORESTATE uint8_t sreg_save \
__attribute__((__cleanup__(__iRestore))) = SREG
&lt;/code>&lt;/pre>
&lt;p>And this:&lt;/p>
&lt;pre>&lt;code>#define ATOMIC_FORCEON uint8_t sreg_save \
__attribute__((__cleanup__(__iSeiParam))) = 0
&lt;/code>&lt;/pre>
&lt;p>The magic is the &lt;code>__attribute__&lt;/code> keyword: GCC supports &lt;a href="https://gcc.gnu.org/onlinedocs/gcc/Common-Variable-Attributes.html">custom attributes&lt;/a> on variables that can change the way the variable is stored or used. In this case, the code is using the &lt;code>__cleanup__&lt;/code> attribute, which:&lt;/p>
&lt;blockquote>
&lt;p>&amp;hellip;runs a function when the variable goes out of scope.&lt;/p>
&lt;/blockquote>
&lt;p>So when we write:&lt;/p>
&lt;pre>&lt;code>ATOMIC_BLOCK(ATOMIC_FORCEON) {
do_something_here();
}
&lt;/code>&lt;/pre>
&lt;p>That becomes:&lt;/p>
&lt;pre>&lt;code>for ( uint8_t sreg_save __attribute__((__cleanup__(__iSeiParam))) = 0, __ToDo = __iCliRetVal(); __ToDo ; __ToDo = 0 ) {
do_something_here();
}
&lt;/code>&lt;/pre>
&lt;p>Which instructs GCC to ensure that the &lt;code>iSeiParam&lt;/code> function is run whenever the &lt;code>sreg_save&lt;/code> variable goes out of scope. The &lt;code>iSeiParam()&lt;/code> method looks like:&lt;/p>
&lt;pre>&lt;code>static __inline__ void __iSeiParam(const uint8_t *__s)
{
__asm__ __volatile__ (&amp;quot;sei&amp;quot; ::: &amp;quot;memory&amp;quot;);
__asm__ volatile (&amp;quot;&amp;quot; ::: &amp;quot;memory&amp;quot;);
(void)__s;
}
&lt;/code>&lt;/pre>
&lt;p>In other words, this is very much like a &lt;code>try&lt;/code>/&lt;code>finally&lt;/code> block in Python, although the cleanup action is attached to a particular variable rather than to the block of code itself. I think that&amp;rsquo;s pretty neat.&lt;/p></content></item><item><title>AVR micro-optimization: Avr-gcc and --short-enums</title><link>https://blog.oddbit.com/post/2019-01-28-avr-gcc-short-enums/</link><pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-01-28-avr-gcc-short-enums/</guid><description>How big is an enum? I noticed something odd while browsing through the assembly output of some AVR C code I wrote recently. In the code, I have the following expression:
int main() { setup(); while (state != STATE_QUIT) { loop(); } } Here, state is a variable of type enum STATE, which looks something like this (not exactly like this; there are actually 19 possible values but I didn&amp;rsquo;t want to clutter this post with unnecessary code listings):</description><content>&lt;h2 id="how-big-is-an-enum">How big is an enum?&lt;/h2>
&lt;p>I noticed something odd while browsing through the assembly output of some AVR C code &lt;a href="https://blog.oddbit.com/post/2019-01-19-pipower-a-raspberry-pi-ups/">I wrote recently&lt;/a>. In the code, I have the following expression:&lt;/p>
&lt;pre>&lt;code>int main() {
setup();
while (state != STATE_QUIT) {
loop();
}
}
&lt;/code>&lt;/pre>
&lt;p>Here, &lt;code>state&lt;/code> is a variable of type &lt;code>enum STATE&lt;/code>, which looks something like this (not exactly like this; there are actually &lt;a href="https://github.com/larsks/pipower/blob/master/states.h">19 possible values&lt;/a> but I didn&amp;rsquo;t want to clutter this post with unnecessary code listings):&lt;/p>
&lt;pre>&lt;code>enum STATE {
STATE_0,
STATE_1,
STATE_QUIT
};
&lt;/code>&lt;/pre>
&lt;p>Now, if you do a little research, you&amp;rsquo;ll find that the size of an &lt;code>enum&lt;/code> is unspecified by the C standard: it is implementation dependent. You will also find &lt;a href="https://www.embedded.fm/blog/2016/6/28/how-big-is-an-enum">articles&lt;/a> that say:&lt;/p>
&lt;blockquote>
&lt;p>The GCC C compiler will allocate enough memory for an enum to hold any of the values that you have declared. So, if your code only uses values below 256, your enum should be 8 bits wide.&lt;/p>
&lt;/blockquote>
&lt;p>The boolean expression in the &lt;code>while&lt;/code> loop gets translated as:&lt;/p>
&lt;pre>&lt;code> lds r24,state
lds r25,state+1
sbiw r24,2
brne .L9
&lt;/code>&lt;/pre>
&lt;p>In other words, that statement about the GCC compiler doesn&amp;rsquo;t appear to be true: We can see that the compiler is treating the &lt;code>state&lt;/code> variable as a 16-bit integer despite the &lt;code>enum&lt;/code> have only three values, which means that (a) two &lt;code>lds&lt;/code> operations are required to load the value into registers, and (b) it&amp;rsquo;s using &lt;code>sbiw&lt;/code>, which takes 2 clock cycles, rather than the &lt;code>cpi&lt;/code> operand, which only takes a single clock cycle. We see similar behavior in a &lt;code>switch&lt;/code> statement inside the &lt;code>loop()&lt;/code> function:&lt;/p>
&lt;pre>&lt;code>void loop() {
switch(state) {
case STATE_0:
state = STATE_1;
break;
case STATE_1:
state = STATE_QUIT;
break;
case STATE_QUIT:
break;
}
}
&lt;/code>&lt;/pre>
&lt;p>The generated assembly for this includes the following:&lt;/p>
&lt;pre>&lt;code> lds r24,state
lds r25,state+1
cpi r24,1
cpc r25,__zero_reg__
breq .L3
sbiw r24,1
brsh .L6
ldi r24,lo8(1)
ldi r25,0
sts state+1,r25
sts state,r24
&lt;/code>&lt;/pre>
&lt;p>As before, this requires two &lt;code>lds&lt;/code> instructions to load a value from the &lt;code>state&lt;/code> variable:&lt;/p>
&lt;pre>&lt;code> lds r24,state
lds r25,state+1
&lt;/code>&lt;/pre>
&lt;p>And two &lt;code>ldi&lt;/code> + two &lt;code>sts&lt;/code> instructions to store a new value into the &lt;code>state&lt;/code> variable:&lt;/p>
&lt;pre>&lt;code> ldi r24,lo8(1)
ldi r25,0
sts state+1,r25
sts state,r24
&lt;/code>&lt;/pre>
&lt;p>And either multiple instructions (&lt;code>cpi&lt;/code> + &lt;code>cpc&lt;/code>) or multi-cycle instructions (&lt;code>sbiw&lt;/code>) to compare the value in the &lt;code>state&lt;/code> variable to constant values.&lt;/p>
&lt;p>The code we&amp;rsquo;re looking at here isn&amp;rsquo;t at all performance sensitive, but I figured that there had to be a way to get &lt;code>avr-gcc&lt;/code> to use a smaller data size for this &lt;code>enum&lt;/code>. While searching for a solution I stumbled across Rafael Baptista&amp;rsquo;s &amp;ldquo;&lt;a href="https://oroboro.com/short-enum/">The trouble with GCC&amp;rsquo;s &amp;ndash;short-enums flag&lt;/a>&amp;rdquo;, which is an interesting read all by itself but also introduced me to the &lt;code>--short-enums&lt;/code> flag, which does this:&lt;/p>
&lt;blockquote>
&lt;p>Allocate to an &amp;ldquo;enum&amp;rdquo; type only as many bytes as it needs for the declared range
of possible values. Specifically, the &amp;ldquo;enum&amp;rdquo; type is equivalent to the smallest
integer type that has enough room.&lt;/p>
&lt;/blockquote>
&lt;p>That sure sounds like exactly what I want. After rebuilding the code using &lt;code>--short-enums&lt;/code>, the generated assembly for &lt;code>main()&lt;/code> becomes:&lt;/p>
&lt;pre>&lt;code> lds r24,state
cpi r24,lo8(2)
brne .L10
&lt;/code>&lt;/pre>
&lt;p>The original code required six cycles (&lt;code>lds&lt;/code> + &lt;code>lds&lt;/code> + &lt;code>sbiw&lt;/code>), but this code only takes three (&lt;code>lds&lt;/code> + &lt;code>cpi&lt;/code>). The &lt;code>loop()&lt;/code> function becomes:&lt;/p>
&lt;pre>&lt;code> lds r24,state
mov r24,r24
ldi r25,0
cpi r24,1
cpc r25,__zero_reg__
breq .L3
cpi r24,2
cpc r25,__zero_reg__
breq .L6
or r24,r25
breq .L5
rjmp .L7
.L5:
ldi r24,lo8(1)
sts state,r24
&lt;/code>&lt;/pre>
&lt;p>While the compiler is still performing comparisons on 16 bit values&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cpi r24,1
cpc r25,__zero_reg__
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;it now only requires a single instruction to load or store values from/to the &lt;code>state&lt;/code> variable:&lt;/p>
&lt;pre>&lt;code> ldi r24,lo8(1)
sts state,r24
&lt;/code>&lt;/pre>
&lt;p>So, the tl;dr is that the &lt;code>--short-enums&lt;/code> flag makes a lot of sense when compiling code for an 8-bit device, and arguably makes the compiler generate code that is more intuitive.&lt;/p></content></item><item><title>AVR micro-optimization: Losing malloc</title><link>https://blog.oddbit.com/post/2019-01-28-losing-malloc/</link><pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-01-28-losing-malloc/</guid><description>Pssst! Hey&amp;hellip;hey, buddy, wanna get an extra KB for cheap?
When I write OO-style code in C, I usually start with something like the following, in which I use malloc() to allocate memory for a variable of a particular type, perform some initialization actions, and then return it to the caller:
Button *button_new(uint8_t pin, uint8_t poll_freq) { Button *button = (Button *)malloc(sizeof(Button)); // do some initialization stuff return button; } And when initially writing pipower, that&amp;rsquo;s exactly what I did.</description><content>&lt;p>Pssst! Hey&amp;hellip;hey, buddy, wanna get an extra KB for cheap?&lt;/p>
&lt;p>When I write OO-style code in C, I usually start with something like the following, in which I use &lt;code>malloc()&lt;/code> to allocate memory for a variable of a particular type, perform some initialization actions, and then return it to the caller:&lt;/p>
&lt;pre>&lt;code>Button *button_new(uint8_t pin, uint8_t poll_freq) {
Button *button = (Button *)malloc(sizeof(Button));
// do some initialization stuff
return button;
}
&lt;/code>&lt;/pre>
&lt;p>And when initially writing &lt;a href="https://blog.oddbit.com/post/2019-01-19-pipower-a-raspberry-pi-ups/">pipower&lt;/a>, that&amp;rsquo;s exactly what I did. But while thinking about it after the fact, I realized the following:&lt;/p>
&lt;ul>
&lt;li>I&amp;rsquo;m designing for a fixed piece of hardware. I have a fixed number of inputs; I don&amp;rsquo;t actually need to create new &lt;code>Button&lt;/code> variables dynamically at runtime.&lt;/li>
&lt;li>The ATtiny85 only has 8KB of memory. Do I really need the overhead of &lt;code>malloc()&lt;/code>?&lt;/li>
&lt;/ul>
&lt;p>The answer, of course, is that no, I don&amp;rsquo;t, so I rewrote the code so that it only has statically allocated structures. This reduced the size of the resulting binary from this:&lt;/p>
&lt;pre>&lt;code>AVR Memory Usage
----------------
Device: attiny85
Program: 3916 bytes (47.8% Full)
(.text + .data + .bootloader)
Data: 35 bytes (6.8% Full)
(.data + .bss + .noinit)
&lt;/code>&lt;/pre>
&lt;p>To this:&lt;/p>
&lt;pre>&lt;code>AVR Memory Usage
----------------
Device: attiny85
Program: 3146 bytes (38.4% Full)
(.text + .data + .bootloader)
Data: 29 bytes (5.7% Full)
(.data + .bss + .noinit)
&lt;/code>&lt;/pre>
&lt;p>That&amp;rsquo;s a savings of just under 800 bytes, which on the one hand doesn&amp;rsquo;t seem like it a lot&amp;hellip;but on the other hand saves 10% of the available memory!&lt;/p>
&lt;h2 id="debugging-caveat">Debugging caveat&lt;/h2>
&lt;p>If you remove &lt;code>malloc()&lt;/code> from your code and then try to debug it with &lt;code>gdb&lt;/code>, you may find yourself staring at the following error:&lt;/p>
&lt;pre>&lt;code>evaluation of this expression requires the program to have a function &amp;quot;malloc&amp;quot;.
&lt;/code>&lt;/pre>
&lt;p>This will happen if you ask &lt;code>gdb&lt;/code> to do something that requires allocating memory for e.g., a string buffer. The solution is to ensure that &lt;code>malloc()&lt;/code> is linked into your code when you build for debugging. I use something like the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#ifdef DEBUG
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span>&lt;span style="color:#a6e22e">__attribute__&lt;/span>((&lt;span style="color:#a6e22e">optimize&lt;/span>(&lt;span style="color:#e6db74">&amp;#34;O0&amp;#34;&lt;/span>)))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">void&lt;/span> &lt;span style="color:#a6e22e">_force_malloc&lt;/span>() {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a6e22e">malloc&lt;/span>(&lt;span style="color:#ae81ff">0&lt;/span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#endif
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>__attribute__((optimize(&amp;quot;O0&amp;quot;)))&lt;/code> directive disables all optimizations for this function, which should prevent gcc from optimizing out the reference to &lt;code>malloc()&lt;/code>.&lt;/p></content></item><item><title>Debugging attiny85 code, part 1: simavr and gdb</title><link>https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-1/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-1/</guid><description>In a case of awful timing, after my recent project involving some attiny85 programming I finally got around to learning how to use simavr and gdb to help debug my AVR code. It was too late for me (and I will never get the time back that I spent debugging things with an LED and lots of re-flashing), but maybe this will help someone else!
I&amp;rsquo;ve split this into three posts:</description><content>&lt;p>In a case of awful timing, after my &lt;a href="https://blog.oddbit.com/2019/01/19/pipower-a-raspberry-pi-ups/">recent project involving some attiny85 programming&lt;/a> I finally got around to learning how to use &lt;a href="https://github.com/buserror/simavr">simavr&lt;/a> and &lt;code>gdb&lt;/code> to help debug my AVR code. It was too late for me (and I will never get the time back that I spent debugging things with an LED and lots of re-flashing), but maybe this will help someone else!&lt;/p>
&lt;p>I&amp;rsquo;ve split this into three posts:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-1/">Part 1: Using GDB&lt;/a>&lt;/p>
&lt;p>A walkthrough of using GDB to manually inspect the behavior of our code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-2/">Part 2: Automating GDB with scripts&lt;/a>&lt;/p>
&lt;p>Creating GDB scripts to automatically test the behavior of our code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-3/">Part 3: Tracing with simavr&lt;/a>&lt;/p>
&lt;p>Using &lt;code>simavr&lt;/code> to collect information about the state of microcontroller pins while our code is running.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>This is part 1.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>In these posts, I will be referencing the code from my &lt;a href="https://github.com/larsks/pipower">pipower&lt;/a> project that I discussed in &lt;a href="https://blog.oddbit.com/2019/01/19/pipower-a-raspberry-pi-ups/">an earlier post&lt;/a>. If you want to follow along, start by cloning that repository:&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/larsks/pipower
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll also want to be familiar with the &lt;a href="https://www.microchip.com/wwwproducts/en/ATtiny85">attiny85&lt;/a> or a similar AVR microcontroller, since I&amp;rsquo;ll be referring to register names (like &lt;code>PORTB&lt;/code>) without additional explanation.&lt;/p>
&lt;h2 id="goals">Goals&lt;/h2>
&lt;p>In this walkthrough I won&amp;rsquo;t be attempting to fix a bug, so perhaps the term &amp;ldquo;debugging&amp;rdquo; is, if not a misnomer, than at least only broadly applied. Rather, I am attempting to verify that my code behaves as expected in response to various inputs to the microcontroller pins.&lt;/p>
&lt;p>&lt;a href="https://github.com/larsks/pipower">Pipower&lt;/a> is implemented as a simple state machine. In each operational state, changes to input pins or timer expirations can cause it to transition into another state. The complete set of states look like this:&lt;/p>
&lt;figure class="left" >
&lt;img src="pipower_states.png" />
&lt;/figure>
&lt;p>We&amp;rsquo;re going to walk through a particular set of state transitions.&lt;/p>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>Before we start debugging, we should make sure that the code is built with debugging symbols and without optimizations. If you &lt;code>cd&lt;/code> into the &lt;code>sim&lt;/code> directory of the &lt;code>pipower&lt;/code> project and run &lt;code>make&lt;/code>, that&amp;rsquo;s exactly what you&amp;rsquo;ll get.&lt;/p>
&lt;pre>&lt;code>$ cd sim
$ make
[...]
avr-gcc -I.. -DTIMER_BOOTWAIT=1000 -DTIMER_SHUTDOWN=1000 -DTIMER_POWEROFF=1000 -Wall -g -Og -DF_CPU=1000000 -mmcu=attiny85 -c ../pipower.c -o pipower.o
[...]
&lt;/code>&lt;/pre>
&lt;p>There are several things happening here:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The &lt;code>Makefile&lt;/code> in this directory sets &lt;code>VPATH=..&lt;/code>, which means &lt;code>make&lt;/code> will look in the parent directory to find our sources. We need to tell the compiler to also look for include files in that directory; we do that with &lt;code>-I..&lt;/code>.&lt;/p>
&lt;p>A caveat to the above is that &lt;code>make&lt;/code> will also look in &lt;code>..&lt;/code> for object files to determine whether or not they need to be rebuilt. If you have previously built &lt;code>pipower&lt;/code> from the project root directory, you&amp;rsquo;ll want to run a &lt;code>make clean&lt;/code> in that directory first.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We&amp;rsquo;re enabling debug symbols (with &lt;code>-g&lt;/code>) and disabling most optimizations with &lt;code>-Og&lt;/code>. From the &lt;code>gcc&lt;/code> &lt;a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">manual&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>&lt;code>-Og&lt;/code> should be the optimization level of choice for the standard edit-compile-debug cycle, offering a reasonable level of optimization while maintaining fast compilation and a good debugging experience. It is a better choice than &lt;code>-O0&lt;/code> for producing debuggable code because some compiler passes that collect debug information are disabled at &lt;code>-O0&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>We are redefining several timers to be much shorter so that we don&amp;rsquo;t have to wait around when debugging (&lt;code>-DTIMER_BOOTWAIT=1000&lt;/code>, etc).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="start-simavr">Start simavr&lt;/h2>
&lt;p>We need to start &lt;code>simavr&lt;/code> before we can attach to it with &lt;code>gdb&lt;/code>. We&amp;rsquo;ll need to provide it with a path to our compiled &lt;code>pipower.elf&lt;/code> file, and we also want to make sure that we provide a clock frequency that matches the value of &lt;code>F_CPU&lt;/code> we used when building our code (&lt;code>1000000&lt;/code>):&lt;/p>
&lt;pre>&lt;code>$ simavr -m attiny85 -f 1000000 pipower.elf -g
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-g&lt;/code> flag instructs &lt;code>simavr&lt;/code> to wait for a debugger connection before it starts to execute the code.&lt;/p>
&lt;h2 id="connect-with-gdb">Connect with gdb&lt;/h2>
&lt;p>Now that &lt;code>simavr&lt;/code> is running, we start up &lt;code>avr-gdb&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ avr-gdb
GNU gdb (GDB) 8.1
[...]
&lt;/code>&lt;/pre>
&lt;p>Load our binary:&lt;/p>
&lt;pre>&lt;code>(gdb) file pipower.elf
Reading symbols from pipower.elf...done.
&lt;/code>&lt;/pre>
&lt;p>Attach to the simulator (by default, &lt;code>simavr&lt;/code> listens on port &lt;code>1234&lt;/code>):&lt;/p>
&lt;pre>&lt;code>(gdb) target remote :1234
Remote debugging using :1234
0x00000000 in __vectors ()
&lt;/code>&lt;/pre>
&lt;p>And load the code into the simulator:&lt;/p>
&lt;pre>&lt;code>(gdb) load
Loading section .text, size 0xa42 lma 0x0
Loading section .data, size 0x6 lma 0xa42
Start address 0x0, load size 2632
Transfer rate: 2570 KB/sec, 175 bytes/write.
&lt;/code>&lt;/pre>
&lt;h2 id="run-the-code">Run the code&lt;/h2>
&lt;p>At this point, our &lt;code>main()&lt;/code> function has not started executing. Before we start the code, let&amp;rsquo;s set a breakpoint in the &lt;a href="https://github.com/larsks/pipower/blob/b822b91af88d8baeb4e0e69fa5e69c074b96c32f/pipower.c#L94">loop()&lt;/a> function:&lt;/p>
&lt;pre>&lt;code>(gdb) b loop
Breakpoint 1 at 0xaa: file pipower.c, line 98.
&lt;/code>&lt;/pre>
&lt;p>With this in place, when we start executing our code via the &lt;code>continue&lt;/code> command&amp;hellip;&lt;/p>
&lt;pre>&lt;code>(gdb) c
Continuing.
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;execution will stop as soon as it reaches the top of the &lt;code>loop()&lt;/code> function:&lt;/p>
&lt;pre>&lt;code>Breakpoint 1, loop () at pipower.c:98
98 now = millis();
&lt;/code>&lt;/pre>
&lt;p>While debugging this code, I find it helpful to have the current state of the attiny85 pins displayed whenever we hit a breakpoint, as well as the current value of the &lt;code>state&lt;/code> variable. The following commands will display the current state and the binary contents of the &lt;code>PORTB&lt;/code> and &lt;code>PINB&lt;/code> registers:&lt;/p>
&lt;pre>&lt;code>(gdb) display state
(gdb) display /t PORTB
(gdb) display /t PINB
&lt;/code>&lt;/pre>
&lt;p>Pin assignments used in this project as as follows:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Pin&lt;/th>
&lt;th>Name&lt;/th>
&lt;th>I/O&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0&lt;/td>
&lt;td>&lt;code>POWER&lt;/code>&lt;/td>
&lt;td>I&lt;/td>
&lt;td>Connected to active-low power button&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>&lt;code>USB&lt;/code>&lt;/td>
&lt;td>I&lt;/td>
&lt;td>USB signal from powerboost&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>&lt;code>EN&lt;/code>&lt;/td>
&lt;td>O&lt;/td>
&lt;td>EN signal to powerboost (turns on power)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>&lt;code>SHUTDOWN&lt;/code>&lt;/td>
&lt;td>O&lt;/td>
&lt;td>Signal to Pi requesting shutdown&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>&lt;code>BOOT&lt;/code>&lt;/td>
&lt;td>I&lt;/td>
&lt;td>Signal from Pi that it has booted&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We can see the initial value of everything by running the &lt;code>display&lt;/code> command without any arguments:&lt;/p>
&lt;pre>&lt;code>(gdb) display
1: state = STATE_START
2: /t PORTB = 10001
3: /t PINB = 10001
&lt;/code>&lt;/pre>
&lt;p>Since this is the first iteration of the loop, we&amp;rsquo;re in state &lt;code>STATE_START&lt;/code>. Looking at &lt;code>PINB&lt;/code>, we can see that the &lt;code>USB&lt;/code> signal (pin 1) is low, indicating that external power is not connected.&lt;/p>
&lt;p>Let&amp;rsquo;s run the loop for a few iterations:&lt;/p>
&lt;pre>&lt;code>(gdb) c 10
Will ignore next 9 crossings of breakpoint 1. Continuing.
Breakpoint 1, loop () at pipower.c:98
98 now = millis();
1: state = STATE_IDLE2
2: /t PORTB = 10001
3: /t PINB = 10001
(gdb)
&lt;/code>&lt;/pre>
&lt;p>Since there was no external power available (&lt;code>PIN_USB&lt;/code> is low), the code has entered the &lt;a href="https://github.com/larsks/pipower/blob/b822b91af88d8baeb4e0e69fa5e69c074b96c32f/pipower.c#L232">STATE_IDLE2&lt;/a> state, as expected. Let&amp;rsquo;s see what happens when power becomes available. We start by setting &lt;code>PIN_USB&lt;/code> high:&lt;/p>
&lt;pre>&lt;code>(gdb) set PINB = PINB | 1&amp;lt;&amp;lt;PIN_USB
&lt;/code>&lt;/pre>
&lt;p>And then run the loop a few times:&lt;/p>
&lt;pre>&lt;code>(gdb) c 100
Will ignore next 99 crossings of breakpoint 1. Continuing.
Breakpoint 1, loop () at pipower.c:98
98 now = millis();
1: state = STATE_BOOTWAIT1
2: /t PORTB = 10101
3: /t PINB = 10111
&lt;/code>&lt;/pre>
&lt;p>The code has entered &lt;a href="https://github.com/larsks/pipower/blob/b822b91af88d8baeb4e0e69fa5e69c074b96c32f/pipower.c#L163">STATE_BOOTWAIT1&lt;/a>, which means it is waiting for an attached Raspberry Pi to set &lt;code>PIN_BOOT&lt;/code> low. You can also see the &lt;code>PIN_EN&lt;/code> has been set high, which instructs the &lt;a href="https://www.adafruit.com/product/2465">power supply&lt;/a> to provide power to the Pi.&lt;/p>
&lt;p>In practice, an attached Raspberry Pi would set &lt;code>PIN_BOOT&lt;/code> high to signal that it had successfully booted. Let&amp;rsquo;s simulate that here and run the loop for a few more iterations:&lt;/p>
&lt;pre>&lt;code>(gdb) set PINB = PINB &amp;amp; ~(1&amp;lt;&amp;lt;PIN_BOOT)
(gdb) c 100
Will ignore next 99 crossings of breakpoint 1. Continuing.
Breakpoint 1, loop () at pipower.c:98
98 now = millis();
1: state = STATE_BOOT
2: /t PORTB = 10101
3: /t PINB = 111
&lt;/code>&lt;/pre>
&lt;p>This brings us to the &lt;a href="https://github.com/larsks/pipower/blob/b822b91af88d8baeb4e0e69fa5e69c074b96c32f/pipower.c#L172">STATE_BOOT&lt;/a> state, which means that our Pi has successfully booted. At this point, a button press should trigger the shutdown sequence. Let&amp;rsquo;s see if that actually works! We will simulate a button press by first setting &lt;code>PIN_POWER&lt;/code> low, running the loop a few times, and then setting it high:&lt;/p>
&lt;pre>&lt;code>(gdb) set PINB = PINB &amp;amp; ~(1&amp;lt;&amp;lt;PIN_POWER)
(gdb) c 100
[...]
(gdb) set PINB = PINB | 1&amp;lt;&amp;lt;PIN_POWER
(gdb) c 100
Will ignore next 99 crossings of breakpoint 1. Continuing.
Breakpoint 1, loop () at pipower.c:98
98 now = millis();
1: state = STATE_SHUTDOWN1
2: /t PORTB = 11101
3: /t PINB = 1111
&lt;/code>&lt;/pre>
&lt;p>The code recognized and responded to the button press, and is now in state &lt;a href="https://github.com/larsks/pipower/blob/b822b91af88d8baeb4e0e69fa5e69c074b96c32f/pipower.c#L188">STATE_SHUTDOWN1&lt;/a>, waiting for the Pi to set &lt;code>PIN_BOOT&lt;/code> high. We can raise &lt;code>PIN_BOOT&lt;/code>:&lt;/p>
&lt;pre>&lt;code>(gdb) set PINB = PINB | 1&amp;lt;&amp;lt;PIN_BOOT
(gdb) c 100
Will ignore next 99 crossings of breakpoint 1. Continuing.
Breakpoint 1, loop () at pipower.c:98
98 now = millis();
1: state = STATE_POWEROFF1
2: /t PORTB = 10101
3: /t PINB = 10111
&lt;/code>&lt;/pre>
&lt;p>This brings us to &lt;a href="https://github.com/larsks/pipower/blob/b822b91af88d8baeb4e0e69fa5e69c074b96c32f/pipower.c#L203">STATE_POWEROFF1&lt;/a>, during which the controller will wait some amount of time before cutting power to the Pi. Our debug build has reduced this timer to 1 second, but if you don&amp;rsquo;t have time for that we can simply run until the next state transition like this:&lt;/p>
&lt;pre>&lt;code>(gdb) disable 1
(gdb) tb loop if state != STATE_POWEROFF1
Note: breakpoint 1 (disabled) also set at pc 0xaa.
Temporary breakpoint 2 at 0xaa: file ../pipower.c, line 98.
(gdb) c
Continuing.
Temporary breakpoint 2, loop () at ../pipower.c:98
115 now = millis();
1: state = STATE_POWEROFF2
(gdb) enable1
(gdb)
&lt;/code>&lt;/pre>
&lt;p>This works by disabling the unqualified breakpoint at the top of &lt;code>loop()&lt;/code> and creating a new temporary breakpoint (meaning it will be removed once it triggers) that will only trigger when the global &lt;code>state&lt;/code> value has changed.&lt;/p>
&lt;p>From &lt;a href="https://github.com/larsks/pipower/blob/b822b91af88d8baeb4e0e69fa5e69c074b96c32f/pipower.c#L213">STATE_POWEROFF2&lt;/a>, Pipower will enter the various &lt;code>IDLE*&lt;/code> stages, during which it enters a very low power sleep mode.&lt;/p>
&lt;hr>
&lt;p>That&amp;rsquo;s it for the walk-through! Hopefully that has given you some idea of how you can use &lt;code>simavr&lt;/code> and a debugger to test your AVR code without having to flash it to a device first.&lt;/p></content></item><item><title>Debugging attiny85 code, part 2: Automating GDB with scripts</title><link>https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-2/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-2/</guid><description>This is the second of three posts about using gdb and simavr to debug AVR code. The complete series is:
Part 1: Using GDB
A walkthrough of using GDB to manually inspect the behavior of our code.
Part 2: Automating GDB with scripts
Creating GDB scripts to automatically test the behavior of our code.
Part 3: Tracing with simavr
Using simavr to collect information about the state of microcontroller pins while our code is running.</description><content>&lt;p>This is the second of three posts about using &lt;code>gdb&lt;/code> and &lt;code>simavr&lt;/code> to debug AVR code. The complete series is:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-1/">Part 1: Using GDB&lt;/a>&lt;/p>
&lt;p>A walkthrough of using GDB to manually inspect the behavior of our code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-2/">Part 2: Automating GDB with scripts&lt;/a>&lt;/p>
&lt;p>Creating GDB scripts to automatically test the behavior of our code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-3/">Part 3: Tracing with simavr&lt;/a>&lt;/p>
&lt;p>Using &lt;code>simavr&lt;/code> to collect information about the state of microcontroller pins while our code is running.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>In these posts, I will be referencing the code from my &lt;a href="https://github.com/larsks/pipower">pipower&lt;/a> project that I discussed in [an earlier post][pipower-post]. If you want to follow along, start by cloning that repository:&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/larsks/pipower
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll also want to be familiar with the &lt;a href="https://www.microchip.com/wwwproducts/en/ATtiny85">attiny85&lt;/a> or a similar AVR microcontroller, since I&amp;rsquo;ll be referring to register names (like &lt;code>PORTB&lt;/code>) without additional explanation.&lt;/p>
&lt;h2 id="goals">Goals&lt;/h2>
&lt;p>In &lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-1/">the first post&lt;/a> on this topic, we looked at how one can use &lt;code>gdb&lt;/code> and &lt;a href="https://github.com/buserror/simavr">simavr&lt;/a> to debug your attiny85 (or other AVR code) without flashing it to a device. In this post, I would like to extend that by looking at how we can automate some aspects of the debugging process.&lt;/p>
&lt;h2 id="sending-commands-to-gdb">Sending commands to gdb&lt;/h2>
&lt;p>In the previous post, we were entering commands into &lt;code>gdb&lt;/code> manually. It is also possible to provide &lt;code>gdb&lt;/code> with a script of commands to execute. Let&amp;rsquo;s assume we have a file that contains the following commands:&lt;/p>
&lt;pre>&lt;code>file pipower.elf
target remote :1234
load
&lt;/code>&lt;/pre>
&lt;p>There are a few different mechanisms available for passing these commands to &lt;code>gdb&lt;/code>. Naively we can simply redirect &lt;code>stdin&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ avr-gdb &amp;lt; commands.gdb
GNU gdb (GDB) 8.1
[...]
(gdb) Reading symbols from pipower.elf...done.
(gdb) Remote debugging using :1234
0x00000000 in __vectors ()
(gdb) Loading section .text, size 0xa54 lma 0x0
Loading section .data, size 0x6 lma 0xa54
Start address 0x0, load size 2650
Transfer rate: 1293 KB/sec, 31 bytes/write.
(gdb) quit
A debugging session is active.
Inferior 1 [Remote target] will be detached.
Quit anyway? (y or n) [answered Y; input not from terminal]
Detaching from program: /home/lars/projects/pipower/sim/pipower.elf, Remote target
&lt;/code>&lt;/pre>
&lt;p>This will work fine in situations in which you expect &lt;code>gdb&lt;/code> to run with no user interaction, but in this particular example, that makes our command file useless: while &lt;code>gdb&lt;/code> does connect to &lt;code>simavr&lt;/code>, it then exits immediately. This is where the &lt;code>--command&lt;/code> (or &lt;code>-x&lt;/code>) options comes in handy: that will read commands from a file and then return to the &lt;code>(gdb)&lt;/code> prompt:&lt;/p>
&lt;pre>&lt;code>$ avr-gdb -x commands.gdb
GNU gdb (GDB) 8.1
[...]
0x00000000 in __vectors ()
Loading section .text, size 0xa54 lma 0x0
Loading section .data, size 0x6 lma 0xa54
Start address 0x0, load size 2650
Transfer rate: 1293 KB/sec, 31 bytes/write.
(gdb)
&lt;/code>&lt;/pre>
&lt;p>This allows us to preload our debugging session with commands and then continue with an interactive session. You can achieve something similar using the &lt;code>source&lt;/code> command in &lt;code>gdb&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ avr-gdb
GNU gdb (GDB) 8.1
[...]
(gdb) source commands.gdb
0x00000000 in __vectors ()
Loading section .text, size 0xa54 lma 0x0
Loading section .data, size 0x6 lma 0xa54
Start address 0x0, load size 2650
Transfer rate: 431 KB/sec, 31 bytes/write.
(gdb)
&lt;/code>&lt;/pre>
&lt;h2 id="conditional-and-temporary-breakpoints">Conditional and temporary breakpoints&lt;/h2>
&lt;p>There are several different ways to set breakpoints in &lt;code>gdb&lt;/code>. The simplest is the &lt;code>b&lt;/code> command, which sets a breakpoint at the given location. This simple breakpoint will trigger whenever execution reaches the given line of code. We can influence this behavior by setting a breakpoint condition, such as:&lt;/p>
&lt;pre>&lt;code>b loop if state == STATE_POWEROFF2
&lt;/code>&lt;/pre>
&lt;p>This breakpoint will only trigger if the expression (&lt;code>state == STATE_POWEROFF2&lt;/code>) evaluates to true.&lt;/p>
&lt;p>Sometimes, we don&amp;rsquo;t want a persistent breakpoint: we want the code to stop once at a given point, and then continue executing afterwards without stopping again at the same place. We can accomplish this by setting a temporary breakpoint using the &lt;code>tb&lt;/code> command. If we were to write the previous example like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>tb loop if state == STATE_POWEROFF2
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;then the code would stop &lt;em>once&lt;/em> at the given breakpoint, but subsequently iterations of the loop would continue merrily on their way.&lt;/p>
&lt;h2 id="defining-new-commands">Defining new commands&lt;/h2>
&lt;p>The &lt;code>gdb&lt;/code> scripting language permits us to create new commands with the &lt;code>define&lt;/code> command. In the previous post, I simulated the passage of time by iterating through the main loop using a command such as &lt;code>c 100&lt;/code>. This works, but isn&amp;rsquo;t particularly accurate and may make it difficult if one wants to run for a specific amount of time (for example, to run out a timer). We can define a new &lt;code>wait_for&lt;/code> command that will let us wait for a given number of milliseconds:&lt;/p>
&lt;pre>&lt;code># wait for &amp;lt;n&amp;gt; milliseconds
define wait_for
disable 1
set $start_time = now
tb loop if now == $start_time + $arg0
c
enable 1
end
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>disable 1&lt;/code> at the beginning is disabling breakpoint 1, which we assume is the breakpoint created by running &lt;code>b loop&lt;/code> as in the &lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-1/">previous post&lt;/a>. We re-enable the breakpoint at the end of the definition.&lt;/p>
&lt;p>This takes advantage of the fact that the code in &lt;code>pipower.c&lt;/code> is explicitly updating a variable call &lt;code>now&lt;/code> with the output of the &lt;code>millis()&lt;/code> command, which counts milliseconds since the microprocessor started. We can store the current value of that variable in a &lt;code>gdb&lt;/code> variable by using the &lt;code>set&lt;/code> command:&lt;/p>
&lt;pre>&lt;code>set $start_time = now
&lt;/code>&lt;/pre>
&lt;p>This allows us to create a temporary breakpoint with a break condition that makes use of that value:&lt;/p>
&lt;pre>&lt;code>tb loop if now == $start_time + $arg0
&lt;/code>&lt;/pre>
&lt;p>This breakpoint will activate when the global &lt;code>now&lt;/code> variable is equal to the value we saved in &lt;code>$start_time&lt;/code> + whatever was passed as an argument to the &lt;code>wait_for&lt;/code> command.&lt;/p>
&lt;p>Since commands can call other commands, we can use the new &lt;code>wait_for&lt;/code> command to create a new command that simulates a button press. For our purposes, a &amp;ldquo;button press&amp;rdquo; means that &lt;code>PIN_POWER&lt;/code> goes low for 100ms and then goes high. We can simulate that like this:&lt;/p>
&lt;pre>&lt;code> define short_press
set PINB=PINB &amp;amp; ~(1&amp;lt;&amp;lt;PIN_POWER)
wait_for 100
set PINB=PINB | 1&amp;lt;&amp;lt;PIN_POWER
c
end
&lt;/code>&lt;/pre>
&lt;p>Recall that &lt;code>c&lt;/code> means &lt;code>continue&lt;/code>, which will cause the code to continue running until it hits a breakpoint.&lt;/p>
&lt;h2 id="automated-testing-the-script">Automated testing: the script&lt;/h2>
&lt;p>Using everything discussed above, we can put together something like the &lt;a href="https://github.com/larsks/pipower/tree/master/sim/simulate.gdb">simulate.gdb&lt;/a> script included in the &lt;code>sim&lt;/code> directory of the Pipower project.&lt;/p>
&lt;p>We start by disabling pagination. This prevent &lt;code>gdb&lt;/code> from stopping and asking us to &amp;ldquo;press return to continue&amp;rdquo;.&lt;/p>
&lt;pre>&lt;code>set pagination off
&lt;/code>&lt;/pre>
&lt;p>We load our binary and connect to the simulator:&lt;/p>
&lt;pre>&lt;code>file pipower.elf
target remote :1234
load
&lt;/code>&lt;/pre>
&lt;p>Next, we define a few helper functions to avoid repetitive code in the rest of the script:&lt;/p>
&lt;pre>&lt;code>##
## Helper functions
##
# wait for &amp;lt;n&amp;gt; milliseconds
define wait_for
disable 1
set $start_time = now
tb loop if now == $start_time + $arg0
c
enable 1
end
# simulate a short press of the power button
define short_press
set PINB=PINB &amp;amp; ~(1&amp;lt;&amp;lt;PIN_POWER)
wait_for 100
set PINB=PINB | 1&amp;lt;&amp;lt;PIN_POWER
c
end
# log a message
define log
printf &amp;quot;\n* %s\n&amp;quot;, $arg0
end
# run until we reach the given state
define run_until_state
disable 1
tb loop if $arg0 == state
c
enable 1
end
&lt;/code>&lt;/pre>
&lt;p>Prior to running the code, we a breakpoint on the &lt;code>loop()&lt;/code> function:&lt;/p>
&lt;pre>&lt;code>##
## Execution starts here
##
# set an initial breakpoint at the start of loop() and advance the program
# to that point
b loop
&lt;/code>&lt;/pre>
&lt;p>And then start things running. This will stop at the top of &lt;code>loop()&lt;/code>:&lt;/p>
&lt;pre>&lt;code>c
&lt;/code>&lt;/pre>
&lt;p>In order to see how things are progressing as the script runs, let&amp;rsquo;s arrange to display the current value of the global &lt;code>state&lt;/code> variable as well as the &lt;code>PORTB&lt;/code> and &lt;code>PINB&lt;/code> registers every time we hit a breakpoint:&lt;/p>
&lt;pre>&lt;code># set up some information to display at each breakpoint
display state
display /t PORTB
display /t PINB
display
&lt;/code>&lt;/pre>
&lt;p>Now that our displays are setup, let&amp;rsquo;s run the code for a bit and then set &lt;code>PIN_USB&lt;/code> high (this would indicate that external power is available to our device):&lt;/p>
&lt;pre>&lt;code># let the code advance for 100ms
wait_for 100
# enable external power
log &amp;quot;setting PIN_USB&amp;quot;
set PINB=PINB | 1&amp;lt;&amp;lt;PIN_USB
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ll use the &lt;code>run_until_state&lt;/code> command that we defined earlier in the file to execute until we reach the &lt;code>STATE_BOOTWAIT1&lt;/code> state:&lt;/p>
&lt;pre>&lt;code>run_until_state STATE_BOOTWAIT1
wait_for 100
&lt;/code>&lt;/pre>
&lt;p>At this point, the code expects an attached Raspberry Pi to assert the &lt;code>BOOT&lt;/code> signal by bringing &lt;code>PIN_BOOT&lt;/code> low:&lt;/p>
&lt;pre>&lt;code># assert BOOT
log &amp;quot;resetting PIN_BOOT&amp;quot;
set PINB=PINB &amp;amp; ~(1&amp;lt;&amp;lt;PIN_BOOT)
run_until_state STATE_BOOT
&lt;/code>&lt;/pre>
&lt;p>Once the Pi has booted successfully and provided the &lt;code>BOOT&lt;/code> signal to our code, we enter the &lt;code>STATE_BOOT&lt;/code> state. Let&amp;rsquo;s run in this state for a second&amp;hellip;&lt;/p>
&lt;pre>&lt;code>##
## ...the pi has booted...
##
wait_for 1000
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then simulate a press of the power button:&lt;/p>
&lt;pre>&lt;code># request a shutdown by pressing the power button
log &amp;quot;pressing power button&amp;quot;
short_press
&lt;/code>&lt;/pre>
&lt;p>Our code sets &lt;code>PIN_SHUTDOWN&lt;/code> high, which would signal to an attached Pi that it should begin the shutdown process. The code enters the &lt;code>STATE_SHUTDOWN1&lt;/code> state in which it waits for the Pi to signal successful shutdown by de-asserting &lt;code>BOOT&lt;/code> by bringing &lt;code>PIN_BOOT&lt;/code> high:&lt;/p>
&lt;pre>&lt;code>run_until_state STATE_SHUTDOWN1
# de-assert BOOT
wait_for 100
log &amp;quot;setting PIN_BOOT&amp;quot;
set PINB=PINB | 1&amp;lt;&amp;lt;PIN_BOOT
&lt;/code>&lt;/pre>
&lt;p>Once we receive the successful shutdown signal, the code enters the poweroff phase, during which it will wait &lt;code>TIMER_POWEROFF&lt;/code> milliseconds before cutting the power. Let&amp;rsquo;s walk through the poweroff state transitions:&lt;/p>
&lt;pre>&lt;code># step through state transitions until we reach
# STATE_IDLE2
run_until_state STATE_POWEROFF0
run_until_state STATE_POWEROFF1
run_until_state STATE_POWEROFF2
log &amp;quot;entering idle mode&amp;quot;
run_until_state STATE_IDLE0
run_until_state STATE_IDLE1
run_until_state STATE_IDLE2
wait_for 100
&lt;/code>&lt;/pre>
&lt;p>And finally force the code to exit:&lt;/p>
&lt;pre>&lt;code>log &amp;quot;setting quit flag&amp;quot;
set state=STATE_QUIT
finish
disconnect
quit
&lt;/code>&lt;/pre>
&lt;h2 id="automated-testing-the-output">Automated testing: the output&lt;/h2>
&lt;p>Running that script produces the following output, which lets us see the state transitions and pin values as the code is running:&lt;/p>
&lt;pre>&lt;code>0x00000000 in __vectors ()
Loading section .text, size 0xa74 lma 0x0
Loading section .data, size 0x6 lma 0xa74
Start address 0x0, load size 2682
Transfer rate: 873 KB/sec, 31 bytes/write.
Breakpoint 1 at 0xb0: file ../pipower.c, line 116.
Breakpoint 1, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_START
2: /t PORTB = 10001
3: /t PINB = 10001
Temporary breakpoint 2 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 2, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_IDLE2
2: /t PORTB = 10001
3: /t PINB = 10001
* setting PIN_USB
Temporary breakpoint 3 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 3, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_BOOTWAIT1
2: /t PORTB = 10101
3: /t PINB = 10111
Temporary breakpoint 4 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 4, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_BOOTWAIT1
2: /t PORTB = 10101
3: /t PINB = 10111
* resetting PIN_BOOT
Temporary breakpoint 5 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 5, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_BOOT
2: /t PORTB = 10101
3: /t PINB = 111
Temporary breakpoint 6 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 6, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_BOOT
2: /t PORTB = 10101
3: /t PINB = 111
* pressing power button
Temporary breakpoint 7 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 7, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_BOOT
2: /t PORTB = 10101
3: /t PINB = 110
Breakpoint 1, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_BOOT
2: /t PORTB = 10101
3: /t PINB = 111
Temporary breakpoint 8 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 8, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_SHUTDOWN1
2: /t PORTB = 11101
3: /t PINB = 1111
Temporary breakpoint 9 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 9, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_SHUTDOWN1
2: /t PORTB = 11101
3: /t PINB = 1111
* setting PIN_BOOT
Temporary breakpoint 10 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 10, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_POWEROFF0
2: /t PORTB = 11101
3: /t PINB = 11111
Temporary breakpoint 11 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 11, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_POWEROFF1
2: /t PORTB = 10101
3: /t PINB = 10111
Temporary breakpoint 12 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 12, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_POWEROFF2
2: /t PORTB = 10101
3: /t PINB = 10111
* entering idle mode
Temporary breakpoint 13 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 13, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_IDLE0
2: /t PORTB = 10001
3: /t PINB = 10011
Temporary breakpoint 14 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 14, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_IDLE1
2: /t PORTB = 10001
3: /t PINB = 10011
Temporary breakpoint 15 at 0xb0: file ../pipower.c, line 116.
Temporary breakpoint 15, loop () at ../pipower.c:116
116 now = millis();
1: state = STATE_IDLE2
2: /t PORTB = 10001
3: /t PINB = 10011
* setting quit flag
main () at ../pipower.c:280
280 while (state != STATE_QUIT) {
1: state = STATE_QUIT
2: /t PORTB = 10001
3: /t PINB = 10011
&lt;/code>&lt;/pre></content></item><item><title>Debugging attiny85 code, part 3: Tracing with simavr</title><link>https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-3/</link><pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-3/</guid><description>This is the third of three posts about using gdb and simavr to debug AVR code. The complete series is:
Part 1: Using GDB
A walkthrough of using GDB to manually inspect the behavior of our code.
Part 2: Automating GDB with scripts
Creating GDB scripts to automatically test the behavior of our code.
Part 3: Tracing with simavr
Using simavr to collect information about the state of microcontroller pins while our code is running.</description><content>
&lt;figure class="left" >
&lt;img src="pipower_trace.png" />
&lt;/figure>
&lt;p>This is the third of three posts about using &lt;code>gdb&lt;/code> and &lt;code>simavr&lt;/code> to debug AVR code. The complete series is:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-1/">Part 1: Using GDB&lt;/a>&lt;/p>
&lt;p>A walkthrough of using GDB to manually inspect the behavior of our code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-2/">Part 2: Automating GDB with scripts&lt;/a>&lt;/p>
&lt;p>Creating GDB scripts to automatically test the behavior of our code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-3/">Part 3: Tracing with simavr&lt;/a>&lt;/p>
&lt;p>Using &lt;code>simavr&lt;/code> to collect information about the state of microcontroller pins while our code is running.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>In these posts, I will be referencing the code from my &lt;a href="https://github.com/larsks/pipower">pipower&lt;/a> project that I discussed in [an earlier post][pipower-post]. If you want to follow along, start by cloning that repository:&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/larsks/pipower
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll also want to be familiar with the &lt;a href="https://www.microchip.com/wwwproducts/en/ATtiny85">attiny85&lt;/a> or a similar AVR microcontroller, since I&amp;rsquo;ll be referring to register names (like &lt;code>PORTB&lt;/code>) without additional explanation.&lt;/p>
&lt;h2 id="goals">Goals&lt;/h2>
&lt;p>In the &lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-2/">previous post&lt;/a> we looked at we can automate &lt;code>gdb&lt;/code> using scripts. In this post, we&amp;rsquo;ll combine that with the tracing facilities offered by &lt;code>simavr&lt;/code> in order to generate traces for pin values and state transitions while the code is running.&lt;/p>
&lt;h2 id="preparing-your-code">Preparing your code&lt;/h2>
&lt;p>We need to embed some metadata in our code that provides &lt;code>simavr&lt;/code> with information about what data we want to collect. We do this by adding some special code that will get embedded in the &lt;code>.mmcu&lt;/code> section of the resulting ELF binary. You can see an example of this in &lt;a href="https://github.com/larsks/pipower/blob/master/sim/simavr.c">sim/simavr.c&lt;/a> in the Pipower project:&lt;/p>
&lt;pre>&lt;code>#include &amp;lt;simavr/avr/avr_mcu_section.h&amp;gt;
#include &amp;quot;pins.h&amp;quot;
extern uint8_t state;
/** Trace data to collect.
*
* We collect each bit of PORTB (inputs and outputs) separately. We also keep
* track of the global state variable.
*/
const struct avr_mmcu_vcd_trace_t _mytrace[] _MMCU_ = {
{ AVR_MCU_VCD_SYMBOL(&amp;quot;PIN_POWER&amp;quot;), .mask = (1&amp;lt;&amp;lt;PIN_POWER), .what = (void*)&amp;amp;PINB, },
{ AVR_MCU_VCD_SYMBOL(&amp;quot;PIN_USB&amp;quot;), .mask = (1&amp;lt;&amp;lt;PIN_USB), .what = (void*)&amp;amp;PINB, },
{ AVR_MCU_VCD_SYMBOL(&amp;quot;PIN_EN&amp;quot;), .mask = (1&amp;lt;&amp;lt;PIN_EN), .what = (void*)&amp;amp;PORTB, },
{ AVR_MCU_VCD_SYMBOL(&amp;quot;PIN_SHUTDOWN&amp;quot;), .mask = (1&amp;lt;&amp;lt;PIN_SHUTDOWN), .what = (void*)&amp;amp;PORTB, },
{ AVR_MCU_VCD_SYMBOL(&amp;quot;PIN_BOOT&amp;quot;), .mask = (1&amp;lt;&amp;lt;PIN_BOOT), .what = (void*)&amp;amp;PINB, },
{ AVR_MCU_VCD_SYMBOL(&amp;quot;STATE&amp;quot;), .what = (void*)&amp;amp;state, },
};
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>_mytrace&lt;/code> variable (the name is unimportant) is an array of &lt;code>struct avr_mmcu_vcd_trace_t&lt;/code> records (the &lt;code>_MMCU_&lt;/code> flag sets an attribute that will embed this data in the appropriate section of the ELF binary). Each record has the following fields:&lt;/p>
&lt;ul>
&lt;li>&lt;code>AVR_MCU_VCD_SYMBOL(&amp;quot;...&amp;quot;)&lt;/code> &amp;ndash; this sets the name of a data series to collect.&lt;/li>
&lt;li>&lt;code>.mask&lt;/code> (optional) &amp;ndash; use this if you only want to collect certain bits. In this example, I am masking out all but a single bit for each pin. I&amp;rsquo;m not using a mask for &lt;code>state&lt;/code> because I want the actual integer value in that case.&lt;/li>
&lt;li>&lt;code>.what&lt;/code> &amp;ndash; this is the variable to collect. It can be pretty much anything that is in scope. Here, &lt;code>PORTB&lt;/code> and &lt;code>PINB&lt;/code> are the attiny85 input/output registers, and &lt;code>state&lt;/code> is a global variable from &lt;code>pipower.c&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>You can build a version of the pipower code that includes this metadata by running, in the &lt;code>sim/&lt;/code> directory:&lt;/p>
&lt;pre>&lt;code>make TRACE=1
&lt;/code>&lt;/pre>
&lt;p>You can see the new &lt;code>.mmcu&lt;/code> section of you inspect the section headers of &lt;code>pipower.elf&lt;/code> using, e.g., &lt;code>objdump&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ objdump pipower.elf -h
pipower.elf: file format elf32-little
Sections:
Idx Name Size VMA LMA File off Algn
0 .text 00000a74 00000000 00000000 00000094 2**1
CONTENTS, ALLOC, LOAD, READONLY, CODE
1 .mmcu 000000de 00000a74 00000a74 00000b08 2**0
CONTENTS, ALLOC, LOAD, READONLY, DATA
[...]
&lt;/code>&lt;/pre>
&lt;h2 id="gathering-trace-data">Gathering trace data&lt;/h2>
&lt;p>To gather the trace data with &lt;code>simavr&lt;/code>, we simply repeat the steps from the &lt;a href="https://blog.oddbit.com/post/2019-01-22-debugging-attiny-code-pt-2/">previous post&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>Start &lt;code>simavr&lt;/code>&lt;/li>
&lt;li>Run our automated testing script (&lt;code>simulate.gdb&lt;/code>)&lt;/li>
&lt;li>Stop &lt;code>simavr&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>At the end of this process, we will find a new file in our current directory, &lt;code>gtkwave_trace.vcd&lt;/code>. This is a &lt;a href="https://en.wikipedia.org/wiki/Value_change_dump">value change dump&lt;/a> file, which is a standard format for representing digital signals over time.&lt;/p>
&lt;h2 id="visualizing-the-trace-data">Visualizing the trace data&lt;/h2>
&lt;p>We can view the &lt;code>.vcd&lt;/code> file using &lt;a href="http://gtkwave.sourceforge.net/">gtkwave&lt;/a>. From the command line, you can run:&lt;/p>
&lt;pre>&lt;code>gtkwave gtkwave_trace.vcd
&lt;/code>&lt;/pre>
&lt;p>This will bring up the main &lt;code>gtkwave&lt;/code> window, which look like this:&lt;/p>
&lt;figure class="left" >
&lt;img src="gtkwave-1.png" />
&lt;/figure>
&lt;p>Select &lt;code>logic&lt;/code> in the SST (&amp;ldquo;Signal Search Tree&amp;rdquo;) pane. This will display the available signals in the next pane down. Select all the signals (click on the top one, then shift-click on the last one) and select &amp;ldquo;Append&amp;rdquo;. You should end up with a display that looks like this:&lt;/p>
&lt;figure class="left" >
&lt;img src="gtkwave-2.png" />
&lt;/figure>
&lt;p>The graph is displaying our collected signal data, but the initial time scale isn&amp;rsquo;t particularly useful. Select &amp;ldquo;Time -&amp;gt; Zoom -&amp;gt; Zoom Best Fit&amp;rdquo;; you should end up with something that looks like:&lt;/p>
&lt;figure class="left" >
&lt;img src="gtkwave-3.png" />
&lt;/figure>
&lt;p>(Note that any time you resize the window you&amp;rsquo;ll need to manually adjust the zoom level. You can use the menu item, or just press &lt;code>CTRL&lt;/code>-&lt;code>ALT&lt;/code>-&lt;code>F&lt;/code>.)&lt;/p>
&lt;p>You can now see a graph of how the various pins changed over the runtime of the program. This is a useful way to verify that the code is behaving as expected. In this case, I think the section just past the 1 second mark is interesting; you can see the power button press, followed by &lt;code>PIN_SHUTDOWN&lt;/code> going high, then &lt;code>PIN_BOOT&lt;/code> going high, and finally &lt;code>PIN_EN&lt;/code> disabling power output after the 1 second timer expiry.&lt;/p>
&lt;p>You&amp;rsquo;ll note that the &lt;code>state&lt;/code> information is displayed numerically, which isn&amp;rsquo;t particularly helpful. We can fix that by setting up a &amp;ldquo;Translate Filter File&amp;rdquo;. Right click on the &lt;code>STATE[7:0]&lt;/code> label in the &amp;ldquo;Signals&amp;rdquo; pane, then select &amp;ldquo;Data Format -&amp;gt; Translate Filter File -&amp;gt; Enable and Select&amp;rdquo;. In the &amp;ldquo;Select Signal Filter&amp;rdquo; window that pops up&amp;hellip;&lt;/p>
&lt;figure class="left" >
&lt;img src="gtkwave-4.png" />
&lt;/figure>
&lt;p>&amp;hellip;select &amp;ldquo;Add Filter to List&amp;rdquo;. Browse to the &lt;code>sim&lt;/code> directory of the &lt;code>pipower&lt;/code> project, and select &lt;code>state_filter.txt&lt;/code>. Now select that same file in the &amp;ldquo;Filter Select&amp;rdquo; list, then click OK. You should now see state names displayed in the graph:&lt;/p>
&lt;figure class="left" >
&lt;img src="gtkwave-5.png" />
&lt;/figure>
&lt;hr>
&lt;p>That&amp;rsquo;s the end of this series of posts on debugging AVR code with &lt;code>gdb&lt;/code> and &lt;code>simavr&lt;/code>. I hope you found it useful!&lt;/p></content></item><item><title>PiPower: A Raspberry Pi UPS</title><link>https://blog.oddbit.com/post/2019-01-19-pipower-a-raspberry-pi-ups/</link><pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-01-19-pipower-a-raspberry-pi-ups/</guid><description>I have a Raspberry Pi running RetroPie hooked up to a television. It&amp;rsquo;s powered from a USB port on the TV, which is convenient, but it means that whenever we shut off the TV we&amp;rsquo;re pulling the plug on the Pi. While there haven&amp;rsquo;t been any problems so far, this is a classic recipe for filesystem problems or data loss at some point. I started looking into UPS options to alleviate this issue.</description><content>
&lt;figure class="left" >
&lt;img src="pipower-top.jpg" />
&lt;/figure>
&lt;p>I have a Raspberry Pi running &lt;a href="https://retropie.org.uk/">RetroPie&lt;/a> hooked up to a television. It&amp;rsquo;s powered from a USB port on the TV, which is convenient, but it means that whenever we shut off the TV we&amp;rsquo;re pulling the plug on the Pi. While there haven&amp;rsquo;t been any problems so far, this is a classic recipe for filesystem problems or data loss at some point. I started looking into UPS options to alleviate this issue. I wanted something with the following features:&lt;/p>
&lt;ul>
&lt;li>Notify the Pi when external power is removed so that the Pi can shut down cleanly.&lt;/li>
&lt;li>Power off the Pi after the Pi has shut down.&lt;/li>
&lt;li>Boot the Pi when external power is restored.&lt;/li>
&lt;/ul>
&lt;p>There are several Pi UPS solutions out there, but most of them are designed for a different purpose: they will run your Pi from battery for as long as possible, and will notify your Pi when the battery level goes low. That&amp;rsquo;s great if you want a portable device, but isn&amp;rsquo;t the right solution for my situation. One notable exception is the &lt;a href="https://juice4halt.com/">juice4halt&lt;/a> product, which is a super-capacitor based unit that does pretty much exactly when I want. Unfortunately, it is somewhat pricey.&lt;/p>
&lt;p>While looking at various solutions, I found the Adafruit &lt;a href="https://www.adafruit.com/product/2465">PowerBoost 1000c&lt;/a>. When external power is available, this device will charge a LIPO battery and provide power to your Pi at the same time. When external power is removed, this device will power your Pi from the battery. By itself it doesn&amp;rsquo;t have any facilities for communicating with your Pi, but it does provide several control lines which suggested some interesting possibilities.&lt;/p>
&lt;figure class="left" >
&lt;img src="powerboost1000c.png" />
&lt;/figure>
&lt;p>Getting the Powerboost talking to the Pi seemed like a good job for a small microcontroller. I happen to have a few &lt;a href="https://www.microchip.com/wwwproducts/en/ATtiny85">attiny85&lt;/a>s kicking about, so I decided to use one of those.&lt;/p>
&lt;h2 id="code">Code&lt;/h2>
&lt;p>You can find all the code used in this project in &lt;a href="https://github.com/larsks/pipower">the GitHub repository&lt;/a>. The code is written in C, and can be compiled using &lt;code>avr-gcc&lt;/code>. It requires &lt;a href="https://www.nongnu.org/avr-libc/">avr-libc&lt;/a>.&lt;/p>
&lt;h2 id="theory-of-operation">Theory of operation&lt;/h2>
&lt;p>When everything first boots up, the microcontroller checks the &lt;code>USB&lt;/code> signal from the PowerBoost to see if external power is available. The &lt;code>USB&lt;/code> line must remain high for 1 second before it is considered valid (it turns out that the &lt;code>USB&lt;/code> can go high momentarily when things first come on, so this avoids erroneously powering up the Pi when external power isn&amp;rsquo;t available).&lt;/p>
&lt;p>If power is available, the controller brings the &lt;code>EN&lt;/code> line high, which causes the PowerBoost to start supplying power to the Pi. The controller will wait for up to 30 seconds for the &lt;code>BOOT&lt;/code> line to go low. The Pi boots up, and the &lt;code>pipower-up&lt;/code> service (see below) brings the &lt;code>BOOT&lt;/code> line low to indicate that it has successfully booted. If the &lt;code>BOOT&lt;/code> line does not go low within 30 seconds, the controller assumes the Pi has failed to boot and disconnects the power, then enters the lower-power idle mode.&lt;/p>
&lt;p>If you shut down the Pi manually, the &lt;code>pipower-up&lt;/code> service will set the &lt;code>BOOT&lt;/code> line high late in the shutdown sequence to indicate that the Pi is shutting down. The microcontroller will wait an additional 30 seconds and will then turn off power to the Pi. If the &lt;code>BOOT&lt;/code> line goes low again during this time (e.g, if you rebooted the Pi instead of shutting it down), the microcontroller will cancel the shutdown.&lt;/p>
&lt;p>If while the Pi is running you press the power button on the board, this will set the &lt;code>SHUTDOWN&lt;/code> line high. The &lt;code>pipower-down&lt;/code> service will respond to this signal by starting a clean shut down. The controller will wait up to 30 seconds for the Pi to set the &lt;code>BOOT&lt;/code> line high, at which point it will wait another 30 seconds before removing power.&lt;/p>
&lt;p>If while the Pi is running external power is removed, the microcontroller will set the &lt;code>SHUTDOWN&lt;/code> line high, and will follow the same logic as if you had pressed the power button.&lt;/p>
&lt;p>If the microcontroller is in the idle state and external power is available, you can press the power button to boot the Pi. If external power is not available, then applying external power will cause the Pi to boot.&lt;/p>
&lt;p>At any point, a long press (two seconds ore more) of the power button will immediately remove power from the Pi and place the controller in the idle state.&lt;/p>
&lt;h2 id="notes-on-the-code">Notes on the code&lt;/h2>
&lt;p>I initially started writing the code using the Arduino IDE, but I decided to switch to &lt;code>avr-gcc&lt;/code> early on because I found that easier to work with. Since various aspects of the code require tracking the passage of time, the first thing I had to do was implement a version of the &lt;code>millis()&lt;/code> function. You can see my implementation in &lt;a href="https://github.com/larsks/pipower/blob/master/millis.c">millis.c&lt;/a>. This uses &lt;code>TIMER0&lt;/code> on the attiny85 with a divider of 64 (&lt;code>TCCR0B = 3&amp;lt;&amp;lt;CS00&lt;/code>), since that should allow the code to work with processor running at 16Mhz.&lt;/p>
&lt;p>I wrote debouncing code for the power button using the mechanism described by Elliot Williams in &amp;ldquo;&lt;a href="https://hackaday.com/2015/12/10/embed-with-elliot-debounce-your-noisy-buttons-part-ii/">Debounce your Noisy Buttons, Part II&lt;/a>&amp;rdquo;. I wrote an object-oriented implementation that you can find in &lt;a href="https://github.com/larsks/pipower/blob/master/button.c">button.c&lt;/a>.&lt;/p>
&lt;p>Most of the implementation logic can be found in the state machine implemented as a &lt;code>switch&lt;/code> statement in &lt;a href="https://github.com/larsks/pipower/blob/master/pipower.c#L125-L245">lines 125-254 of pipower.c&lt;/a>.&lt;/p>
&lt;p>I have documented the code using &lt;a href="http://www.doxygen.nl/">Doxygen&lt;/a>. If you have Doxygen installed, you can &lt;code>cd docs&lt;/code> and run &lt;code>make&lt;/code> to create the code documentation.&lt;/p>
&lt;h2 id="pins">pins&lt;/h2>
&lt;p>The attiny85 only has 5 available pins (6, if you&amp;rsquo;re either very confident or have a high voltage programmer available). I ended up setting things up like this:&lt;/p>
&lt;ul>
&lt;li>&lt;code>PB0&lt;/code> - connected to a momentary-contact switch&lt;/li>
&lt;li>&lt;code>PB1&lt;/code> - connected to &lt;code>USB&lt;/code> signal from the powerboost&lt;/li>
&lt;li>&lt;code>PB2&lt;/code> - connected to the &lt;code>EN&lt;/code> signal to the powerboost&lt;/li>
&lt;li>&lt;code>PB3&lt;/code> - connected to the &lt;code>SHUTDOWN&lt;/code> signal to the Pi&lt;/li>
&lt;li>&lt;code>PB4&lt;/code> - connected to the &lt;code>BOOT&lt;/code> signal from the Pi&lt;/li>
&lt;li>&lt;code>VCC&lt;/code> - connected to the &lt;code>Vs&lt;/code> output from the powerboost&lt;/li>
&lt;/ul>
&lt;p>I am intentionally not using the low battery (&lt;code>LBO&lt;/code>) signal, since I&amp;rsquo;m not trying to run the Pi off the battery for an extended period of time. If I build or acquire a high voltage programmer, I might wire &lt;code>LBO&lt;/code> to &lt;code>PB5&lt;/code>, or just connect the &lt;code>BAT&lt;/code> signal and use &lt;code>PB5&lt;/code> as an analog Pin, and trigger a shutdown on a low-battery condition as well.&lt;/p>
&lt;h2 id="systemd-units">systemd units&lt;/h2>
&lt;p>The only software required on the Raspberry Pi is &lt;a href="http://wiringpi.com/">wiringPi&lt;/a>, a library and toolset for manipulating GPIO on your Raspberry Pi, and the following &lt;a href="https://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a> &lt;a href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html">units&lt;/a>. If you are building this yourself and disagree with my pin selections, you can create the file &lt;code>/etc/default/pipower&lt;/code> and set one or both of &lt;code>PIN_SHUTDOWN&lt;/code> and &lt;code>PIN_BOOT&lt;/code> to BCM GPIO pins of your choice.&lt;/p>
&lt;h3 id="pipower-up">pipower-up&lt;/h3>
&lt;p>At boot, the &lt;code>pipower-up&lt;/code> service configures &lt;code>PIN_BOOT&lt;/code> (defaults to BCM GPIO 4) as an output and then brings it low. This notifies the code running on the attiny85 that the Pi has successfully booted. When the Pi shuts down, the unit sets &lt;code>PIN_BOOT&lt;/code> high, which notifies the controller that the Pi is about to shut down.&lt;/p>
&lt;p>This service is designed to run early in the boot process and late in the shutdown process.&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=[pipower] Assert BOOT signal
DefaultDependencies=no
After=final.target systemd-journald.service
[Service]
Type=oneshot
Environment=PIN_BOOT=4
EnvironmentFile=-/etc/default/pipower
RemainAfterExit=true
ExecStartPre=/usr/bin/gpio -g mode $PIN_BOOT output
ExecStart=/usr/bin/gpio -g write $PIN_BOOT 0
ExecStopPost=/bin/sh -c &amp;quot;test -f /run/pipower/inhibit || /usr/bin/gpio -g write $PIN_BOOT 1&amp;quot;
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;h3 id="pipower-down">pipower-down&lt;/h3>
&lt;p>At boot, the &lt;code>pipower-down&lt;/code> service configures &lt;code>PIN_SHUTDOWN&lt;/code> (defaults to BCM GPIO 17) as an input w/ the pulldown resistor enabled. It then uses the &lt;code>gpio&lt;/code> command to wait for a rising interrupt on &lt;code>PIN_SHUTDOWN&lt;/code>; when it receives one, it calls &lt;code>systemctl poweroff&lt;/code> to cleanly shut down the system.&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=[pipower] Monitor SHUTDOWN signal
[Service]
Type=simple
Environment=PIN_SHUTDOWN=17
EnvironmentFile=-/etc/default/pipower
ExecStartPre=/usr/bin/gpio -g mode $PIN_SHUTDOWN input
ExecStartPre=/usr/bin/gpio -g mode $PIN_SHUTDOWN down
ExecStart=/usr/bin/gpio -g wfi $PIN_SHUTDOWN rising
ExecStopPost=/bin/sh -c &amp;quot;test -f /run/pipower/inhibit || /bin/systemctl poweroff&amp;quot;
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>The PowerBoost 1000c does not provide adequate power for a Raspberry Pi 3B+. It seems to be just adequate for a Pi 2B.&lt;/p>
&lt;p>If you stop either of the systemd units, your Pi will either shutdown (if you &lt;code>systemctl stop pipower-down&lt;/code>) or will simply lose power (if you &lt;code>systemctl stop pipower-up&lt;/code>). You can inhibit the &lt;code>ExecStop*&lt;/code> actions of both units by creating the file &lt;code>/run/pipower/inhibit&lt;/code>.&lt;/p>
&lt;h2 id="todo">TODO&lt;/h2>
&lt;p>With a few more pins available &amp;ndash; maybe an &lt;a href="https://www.microchip.com/wwwproducts/en/ATtiny84">attiny84&lt;/a> &amp;ndash; it might be fun to provide battery voltage and current measurements to the Pi via an i2c interface. I would probably also add a status LED to show the current state of the controller code.&lt;/p>
&lt;figure class="left" >
&lt;img src="pipower-eth-left.jpg" />
&lt;/figure>
&lt;figure class="left" >
&lt;img src="pipower-eth-right.jpg" />
&lt;/figure></content></item><item><title>Integrating Bitwarden with Ansible</title><link>https://blog.oddbit.com/post/2018-10-19-integrating-bitwarden-with-ans/</link><pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-10-19-integrating-bitwarden-with-ans/</guid><description>Bitwarden is a password management service (like LastPass or 1Password). It&amp;rsquo;s unique in that it is built entirely on open source software. In addition to the the web UI and mobile apps that you would expect, Bitwarden also provides a command-line tool for interacting with the your password store.
At $WORK(-ish) we&amp;rsquo;re looking into Bitwarden because we want a password sharing and management solution that was better than dropping files into directories on remote hosts or sharing things over Slack.</description><content>&lt;p>&lt;a href="https://bitwarden.com">Bitwarden&lt;/a> is a password management service (like &lt;a href="https://www.lastpass.com/">LastPass&lt;/a> or
&lt;a href="https://1password.com/">1Password&lt;/a>). It&amp;rsquo;s unique in that it is built entirely on open
source software. In addition to the the web UI and mobile apps that
you would expect, Bitwarden also provides a &lt;a href="https://help.bitwarden.com/article/cli/">command-line tool&lt;/a> for
interacting with the your password store.&lt;/p>
&lt;p>At $WORK(-ish) we&amp;rsquo;re looking into Bitwarden because we want a password
sharing and management solution that was better than dropping files
into directories on remote hosts or sharing things over Slack. At
the same time, we are also thinking about bringing more automation to
our operational environment, possibly by making more extensive use of
&lt;a href="https://ansible.com">Ansible&lt;/a>. It looked like all the pieces were available to use
Bitwarden as a credential storage mechanism for Ansible playbooks, so
I set out to write a lookup plugin to implement the integration&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;only to find that I was not the first person to have this idea;
Matt Stofko &lt;a href="https://github.com/c0sco/ansible-modules-bitwarden/">beat me to it&lt;/a>. While it worked, the directory
structure of Matt&amp;rsquo;s repository made it difficult to integrate into an
existing Ansible project. It was also missing some convenience
features I wanted to see, so I have submitted
&lt;a href="https://github.com/c0sco/ansible-modules-bitwarden/pull/1" class="pull-request">#1&lt;/a>
that
makes several changes to the module.&lt;/p>
&lt;p>You can find my fork of the Bitwarden lookup plugin at
&lt;a href="https://github.com/larsks/ansible-modules-bitwarden">https://github.com/larsks/ansible-modules-bitwarden&lt;/a>.&lt;/p>
&lt;h2 id="make-it-installable">Make it installable&lt;/h2>
&lt;p>By re-arranging the repository to following the standard Ansible role
structure, it is now possible to install it either a submodule of your
own git repository, or to install it using the &lt;code>ansible-galaxy&lt;/code> tool:&lt;/p>
&lt;pre>&lt;code>ansible-galaxy install git+https://github.com/larsks/ansible-modules-bitwarden
&lt;/code>&lt;/pre>
&lt;p>This command would place the role in &lt;code>$HOME/.ansible/roles&lt;/code>, where it
will be available to any playbooks you run on your system.&lt;/p>
&lt;h2 id="add-explicit-support-for-custom-fields">Add explicit support for custom fields&lt;/h2>
&lt;p>While it was possible to access custom fields by fetching the complete
JSON representation of an item in Bitwarden and then querying the
resulting document, it wasn&amp;rsquo;t particularly graceful. I&amp;rsquo;ve added
explicit support for looking up custom fields. Whereas the normal
lookup will the specific keys that Bitwarden supports in the &lt;code>bw get&lt;/code>:&lt;/p>
&lt;pre>&lt;code>lookup('bitwarden', 'Google', field=username)
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;adding &lt;code>custom_field=True&lt;/code> causes the lookup to be performed against
the list of custom fields:&lt;/p>
&lt;pre>&lt;code>lookup('bitwarden', 'Google', field=mycustomfield, custom_field=true)
&lt;/code>&lt;/pre>
&lt;h2 id="add-support-for-the-sync-operation">Add support for the sync operation&lt;/h2>
&lt;p>The Bitwarden CLI operates by keeping a local cache of your
credentials. This means that if you have just modified an item through
the web ui (or mobile app), you may still be querying stale data when
querying Bitwarden through the CLI. The &lt;code>bw sync&lt;/code> command refreshes
the local cache.&lt;/p>
&lt;p>You can add &lt;code>sync=true&lt;/code> to the lookup to have Ansible run &lt;code>bw sync&lt;/code>
before querying Bitwarden for data:&lt;/p>
&lt;pre>&lt;code>lookup('bitwarden', 'Google', field=username, sync=true)
&lt;/code>&lt;/pre>
&lt;h2 id="using-the-lookup-module-in-practice">Using the lookup module in practice&lt;/h2>
&lt;p>We&amp;rsquo;re using &lt;a href="https://docs.openstack.org/tripleo-docs/latest/">TripleO&lt;/a> to deploy OpenStack. TripleO requires as input
to the deployment process a number of parameters, including various
credentials. For example, to set the password that will be assigned
to the Keystone admin user, one would pass in a file that looks
something like:&lt;/p>
&lt;pre>&lt;code>---
parameter_defaults:
AdminPassword: &amp;quot;secret.password.goes.here&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Because our deployment configuration is public, we don&amp;rsquo;t want to store
credentials there. We&amp;rsquo;ve been copying around a credentials file that
lives outside the repository, but that&amp;rsquo;s not a great solution.&lt;/p>
&lt;p>Using the Bitwarden lookup module, we can replace the above with:&lt;/p>
&lt;pre>&lt;code>---
parameter_defaults:
AdminPassword: &amp;quot;{{ lookup('bitwarden', 'keystone admin') }}&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>With this change, we can use Ansible to query Bitwarden to get the
Keystone admin password and generate as output a file with the
passwords included.&lt;/p>
&lt;p>Using the custom field support, we can include metadata associated
with a credential in the same place as the credential itself. To
configure access to a remote Ceph installation, we need to provide a
client key and cluster id. By putting the cluster id in a custom
field, we can do something like this:&lt;/p>
&lt;pre>&lt;code>CephClientKey: &amp;quot;{{ lookup('bitwarden', 'ceph client key') }}&amp;quot;
CephClusterFSID: &amp;quot;{{ ((lookup('bitwarden', 'ceph client key', field='clusterid', custom_field=true) }}&amp;quot;
&lt;/code>&lt;/pre>
&lt;h2 id="an-example-playbook">An example playbook&lt;/h2>
&lt;p>Before you can run a playbook making use of the Bitwarden lookup
module, you need to &lt;a href="https://help.bitwarden.com/article/cli/#download--install">install&lt;/a> the Bitwarden CLI. This is as simple
as grabbing an appropriate binary and dropping it somewhere in
your &lt;code>$PATH&lt;/code>. I&amp;rsquo;ve been doing this:&lt;/p>
&lt;pre>&lt;code>$ curl -L 'https://vault.bitwarden.com/download/?app=cli&amp;amp;platform=linux' |
funzip &amp;gt; $HOME/bin/bw
$ chmod 755 $HOME/bin/bw
&lt;/code>&lt;/pre>
&lt;p>For the following example, assume that we have a template named
&lt;code>no-passwords-here.yml&lt;/code> matching the earlier example:&lt;/p>
&lt;pre>&lt;code>---
parameter_defaults:
AdminPassword: &amp;quot;{{ lookup('bitwarden', 'keystone admin') }}&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>We can generate a version of the file named &lt;code>yes-passwords-here.yml&lt;/code>
that includes the actual passwords by running the following playbook:&lt;/p>
&lt;pre>&lt;code>---
- hosts: localhost
# we need to include the role in order to make the lookup plugin
# available.
roles:
- ansible-modules-bitwarden
tasks:
- name: inject passwords into a file
template:
src: ./no-passwords-here.yml
dest: ./yes-passwords-here.yml
&lt;/code>&lt;/pre>
&lt;p>To actually run the playbook, we need to be authenticated to Bitwarden
first. That means:&lt;/p>
&lt;ol>
&lt;li>Run &lt;code>bw login&lt;/code> (or &lt;code>bw unlock&lt;/code>) to log in and get a session key.&lt;/li>
&lt;li>Set the &lt;code>BW_SESSION&lt;/code> environment variable to this value.&lt;/li>
&lt;li>Run the playbook.&lt;/li>
&lt;/ol>
&lt;p>The above tasks would look something like this:&lt;/p>
&lt;pre>&lt;code>bash$ bw login
? Email address: lars@redhat.com
? Master password: [hidden]
You are logged in!
To unlock your vault, set your session key to the `BW_SESSION`
environment variable. ex:
$ export BW_SESSION=&amp;quot;...&amp;quot;
[...]
bash$ export BW_SESSION=&amp;quot;...&amp;quot;
bash$ ansible-playbook inject-passwords.yml
&lt;/code>&lt;/pre></content></item><item><title>Systemd unit for managing USB gadgets</title><link>https://blog.oddbit.com/post/2018-10-19-systemd-unit-for-managing-usb-/</link><pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-10-19-systemd-unit-for-managing-usb-/</guid><description>The Pi Zero (and Zero W) have support for acting as a USB gadget: that means that they can be configured to act as a USB device &amp;ndash; like a serial port, an ethernet interface, a mass storage device, etc.
There are two different ways of configuring this support. The first only allows you to configure a single type of gadget at a time, and boils down to:
Enable the dwc2 overlay in /boot/config.</description><content>&lt;p>The Pi Zero (and Zero W) have support for acting as a USB &lt;a href="http://www.linux-usb.org/gadget/">gadget&lt;/a>:
that means that they can be configured to act as a USB device &amp;ndash; like
a serial port, an ethernet interface, a mass storage device, etc.&lt;/p>
&lt;p>There are two different ways of configuring this support. The first
only allows you to configure a single type of gadget at a time, and
boils down to:&lt;/p>
&lt;ol>
&lt;li>Enable the dwc2 overlay in &lt;code>/boot/config.txt&lt;/code>&lt;/li>
&lt;li>Reboot.&lt;/li>
&lt;li>&lt;code>modprobe g_serial&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>This process is more fully documented &lt;a href="https://learn.adafruit.com/turning-your-raspberry-pi-zero-into-a-usb-gadget/overview">here&lt;/a>.&lt;/p>
&lt;p>The second mechanism makes use of the &lt;code>libcomposite&lt;/code> driver to create
multifunction gadgets. The manual procedure is documented in &lt;a href="https://www.kernel.org/doc/Documentation/usb/gadget_configfs.txt">the
kernel documentation&lt;/a>. While it&amp;rsquo;s a useful feature, the
configuration process requires several steps and if you only do it
infrequently it can be easy to forget.&lt;/p>
&lt;p>In order to make this easier for me to manage, I&amp;rsquo;ve wrapped the
process up in a &lt;a href="https://fedoramagazine.org/systemd-template-unit-files/">systemd template unit&lt;/a> that takes care of the
various steps necessary to both create and remove a multifunction USB
gadget.&lt;/p>
&lt;p>Once installed, creating a gadget that offers both a serial interface
and a network interface is as simple as:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create a file &lt;code>/etc/gadget/g0.conf&lt;/code> containing:&lt;/p>
&lt;pre>&lt;code> USB_FUNCTIONS=&amp;quot;rndis.usb0 acm.usb0&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Run &lt;code>systemctl start usb-gadget@g0&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>You can remove the gadget by running &lt;code>systemctl stop usb-gadget@g0&lt;/code>.
As with any systemd service, you can mark the unit to start
automatically when your system boots by running &lt;code>systemctl enable usb-gadget@g0&lt;/code>.&lt;/p>
&lt;p>The &lt;a href="https://github.com/larsks/systemd-usb-gadget">systemd-usb-gadget&lt;/a> project can be found at:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/systemd-usb-gadget">https://github.com/larsks/systemd-usb-gadget&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Configuring a static address for wlan0 on Raspbian Stretch</title><link>https://blog.oddbit.com/post/2018-06-14-configuring-a-static-address-f/</link><pubDate>Thu, 14 Jun 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-06-14-configuring-a-static-address-f/</guid><description>Recent releases of Raspbian have adopted the use of dhcpcd to manage both dynamic and static interface configuration. If you would prefer to use the traditional /etc/network/interfaces mechanism instead, follow these steps.
First, disable dhcpcd and wpa_supplicant.
systemctl disable --now dhdpcd wpa_supplicant You will need a wpa_supplicant configuration for wlan0 in /etc/wpa_supplicant/wpa_supplicant-wlan0.conf.
If you already have an appropriate configuration in /etc/wpa_supplicant/wpa_supplicant.conf, you can just symlink the file:
cd /etc/wpa_supplicant ln -s wpa_supplicant.</description><content>&lt;p>Recent releases of Raspbian have adopted the use of &lt;a href="http://manpages.ubuntu.com/manpages/trusty/man8/dhcpcd5.8.html">dhcpcd&lt;/a> to
manage both dynamic and static interface configuration. If you would
prefer to use the traditional &lt;code>/etc/network/interfaces&lt;/code> mechanism
instead, follow these steps.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>First, disable &lt;code>dhcpcd&lt;/code> and &lt;code>wpa_supplicant&lt;/code>.&lt;/p>
&lt;pre>&lt;code> systemctl disable --now dhdpcd wpa_supplicant
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>You will need a &lt;code>wpa_supplicant&lt;/code> configuration for &lt;code>wlan0&lt;/code> in
&lt;code>/etc/wpa_supplicant/wpa_supplicant-wlan0.conf&lt;/code>.&lt;/p>
&lt;p>If you already have an appropriate configuration in
&lt;code>/etc/wpa_supplicant/wpa_supplicant.conf&lt;/code>, you can just symlink the
file:&lt;/p>
&lt;pre>&lt;code> cd /etc/wpa_supplicant
ln -s wpa_supplicant.conf wpa_supplicant-wlan0.conf
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Enable the &lt;code>wpa_supplicant&lt;/code> service for &lt;code>wlan0&lt;/code>:&lt;/p>
&lt;pre>&lt;code> systemctl enable --now wpa_supplicant@wlan0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Create an appropriate configuration in
&lt;code>/etc/network/interfaces.d/wlan0&lt;/code>. For example:&lt;/p>
&lt;pre>&lt;code> allow-hotplug wlan0
iface wlan0 inet static
address 192.168.2.100
netmask 255.255.255.0
iface wlan0 inet6 static
address 2607:f0d0:2001:000a:0000:0000:0000:0010
netmask 64
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Reboot to make sure everything comes up as expected. With the
above configuration, after rebooting you should see:&lt;/p>
&lt;pre>&lt;code> root@raspberrypi:~# ip addr show wlan0
3: wlan0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc mq state UP group default qlen 1000
link/ether 00:e1:b0:67:98:67 brd ff:ff:ff:ff:ff:ff
inet 192.168.2.100/24 brd 192.168.2.255 scope global wlan0
valid_lft forever preferred_lft forever
inet6 2607:f0d0:2001:a::10/64 scope global
valid_lft forever preferred_lft forever
inet6 fe80::2e1:b0ff:fe67:9867/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol></content></item><item><title>Using a TM1637 LED module with CircuitPython</title><link>https://blog.oddbit.com/post/2018-05-03-using-a-tm-led-module-with-cir/</link><pubDate>Thu, 03 May 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-05-03-using-a-tm-led-module-with-cir/</guid><description>CircuitPython is &amp;ldquo;an education friendly open source derivative of MicroPython&amp;rdquo;. MicroPython is a port of Python to microcontroller environments; it can run on boards with very few resources such as the ESP8266. I&amp;rsquo;ve recently started experimenting with CircuitPython on a Wemos D1 mini, which is a small form-factor ESP8266 board.
I had previously been using Mike Causer&amp;rsquo;s micropython-tm1637 for MicroPython to drive a 4 digit LED display. I was hoping to get the same code working under CircuitPython, but when I tried to build an image that included the tm1637 module I ran into:</description><content>&lt;p>&lt;a href="https://learn.adafruit.com/welcome-to-circuitpython/overview">CircuitPython&lt;/a> is &amp;ldquo;an education friendly open source derivative of
&lt;a href="https://micropython.org/">MicroPython&lt;/a>&amp;rdquo;. MicroPython is a port of Python to microcontroller
environments; it can run on boards with very few resources such as the
&lt;a href="https://en.wikipedia.org/wiki/ESP8266">ESP8266&lt;/a>. I&amp;rsquo;ve recently started experimenting with CircuitPython
on a &lt;a href="https://wiki.wemos.cc/products:d1:d1_mini">Wemos D1 mini&lt;/a>, which is a small form-factor ESP8266 board.&lt;/p>
&lt;p>I had previously been using Mike Causer&amp;rsquo;s &lt;a href="https://github.com/mcauser/micropython-tm1637/">micropython-tm1637&lt;/a> for
MicroPython to drive a &lt;a href="http://a.co/gQVPtPr">4 digit LED display&lt;/a>. I was hoping to
get the same code working under CircuitPython, but when I tried to
build an image that included the &lt;code>tm1637&lt;/code> module I ran into:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#f92672">import&lt;/span> tm1637
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Traceback (most recent call last):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> File &lt;span style="color:#e6db74">&amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;&lt;/span>, line &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#f92672">&amp;lt;&lt;/span>module&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> File &lt;span style="color:#e6db74">&amp;#34;tm1637.py&amp;#34;&lt;/span>, line &lt;span style="color:#ae81ff">6&lt;/span>, &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#f92672">&amp;lt;&lt;/span>module&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">ImportError&lt;/span>: cannot &lt;span style="color:#f92672">import&lt;/span> name sleep_us
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>One of CircuitPython&amp;rsquo;s goals is to be as close to CPython as possible.
This means that in many cases the CircuitPython folks have
re-implemented MicroPython modules to have syntax that is more a
strict subset of the CPython equivalent, and the MicroPython &lt;code>time&lt;/code>
module is impacted by this change. With stock MicroPython, the &lt;code>time&lt;/code>
module has:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> print(&lt;span style="color:#e6db74">&amp;#39;&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>join(dir(time)))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__class__
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__name__
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>localtime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mktime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep_ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep_us
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ticks_add
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ticks_cpu
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ticks_diff
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ticks_ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ticks_us
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But the corresponding CircuitPython module has:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> print(&lt;span style="color:#e6db74">&amp;#39;&lt;/span>&lt;span style="color:#ae81ff">\n&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&lt;/span>&lt;span style="color:#f92672">.&lt;/span>join(dir(time)))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>__name__
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>monotonic
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sleep
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>struct_time
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>localtime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>mktime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It turns out that the necessary functions are defined in the &lt;code>utime&lt;/code>
module, which is implemented by &lt;code>ports/esp8266/modutime.c&lt;/code>, but this
module is not included in the CircuitPython build. How do we fix that?&lt;/p>
&lt;p>The most obvious change is to add &lt;code>modutime.c&lt;/code> to the &lt;code>SRC_C&lt;/code> variable
in &lt;code>ports/esp8266/Makefile&lt;/code>, which gets us:&lt;/p>
&lt;pre tabindex="0">&lt;code>SRC_C = \
[...]
modesp.c \
modnetwork.c \
modutime.c \
[...]
&lt;/code>&lt;/pre>&lt;p>After making this change and trying to build CircuitPython, I
hit 70 or so lines like:&lt;/p>
&lt;pre tabindex="0">&lt;code>Generating build/genhdr/mpversion.h
In file included from ../../py/mpstate.h:35:0,
from ../../py/runtime.h:29,
from modutime.c:32:
modutime.c:109:50: error: &amp;#39;MP_QSTR_utime&amp;#39; undeclared here (not in a function)
{ MP_ROM_QSTR(MP_QSTR___name__), MP_ROM_QSTR(MP_QSTR_utime) },
^
&lt;/code>&lt;/pre>&lt;p>The &lt;code>MP_QSTR_&lt;/code> macros are sort of magical: they are generated during
the build process by scanning for references of the form
&lt;code>MP_QSTR_utime&lt;/code> and creating definitions that look like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">QDEF&lt;/span>(MP_QSTR_utime, (&lt;span style="color:#66d9ef">const&lt;/span> byte&lt;span style="color:#f92672">*&lt;/span>)&lt;span style="color:#e6db74">&amp;#34;&lt;/span>&lt;span style="color:#ae81ff">\xe5\x9d\x05&lt;/span>&lt;span style="color:#e6db74">&amp;#34;&lt;/span> &lt;span style="color:#e6db74">&amp;#34;utime&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But&amp;hellip;and this is the immediate problem&amp;hellip;this generation only happens
with a clean build. Running &lt;code>make clean&lt;/code> and then re-running the
build yields:&lt;/p>
&lt;pre tabindex="0">&lt;code>build/shared-bindings/time/__init__.o:(.rodata.time_localtime_obj+0x0): multiple definition of `time_localtime_obj&amp;#39;
build/modutime.o:(.rodata.time_localtime_obj+0x0): first defined here
build/shared-bindings/time/__init__.o:(.rodata.time_mktime_obj+0x0): multiple definition of `time_mktime_obj&amp;#39;
build/modutime.o:(.rodata.time_mktime_obj+0x0): first defined here
build/shared-bindings/time/__init__.o:(.rodata.time_time_obj+0x0): multiple definition of `time_time_obj&amp;#39;
build/modutime.o:(.rodata.time_time_obj+0x0): first defined here
&lt;/code>&lt;/pre>&lt;p>The above errors show a conflict between the structures defined in
&lt;code>utime&lt;/code>, which have just activated, and the existing &lt;code>time&lt;/code>
module. A simple rename will take care of that problem; instead of:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">MP_DEFINE_CONST_FUN_OBJ_VAR_BETWEEN&lt;/span>(time_localtime_obj, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, time_localtime);
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We want:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">MP_DEFINE_CONST_FUN_OBJ_VAR_BETWEEN&lt;/span>(utime_localtime_obj, &lt;span style="color:#ae81ff">0&lt;/span>, &lt;span style="color:#ae81ff">1&lt;/span>, time_localtime);
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And so forth. At this point, everything builds correctly, but if we
deploy the image to our board and try to import the &lt;code>utime&lt;/code> module, we
see:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#f92672">import&lt;/span> utime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Traceback (most recent call last):
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> File &lt;span style="color:#e6db74">&amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;&lt;/span>, line &lt;span style="color:#ae81ff">1&lt;/span>, &lt;span style="color:#f92672">in&lt;/span> &lt;span style="color:#f92672">&amp;lt;&lt;/span>module&lt;span style="color:#f92672">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a6e22e">ImportError&lt;/span>: no module named &lt;span style="color:#e6db74">&amp;#39;utime&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The final piece of this puzzle is that there is a list of built-in
modules defined in &lt;code>mpconfigport.h&lt;/code>. We need to add our &lt;code>utime&lt;/code>
module to that list:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-c" data-lang="c">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">#define MICROPY_PORT_BUILTIN_MODULES \
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"> [...]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">&lt;/span> { &lt;span style="color:#a6e22e">MP_ROM_QSTR&lt;/span>(MP_QSTR_utime), &lt;span style="color:#a6e22e">MP_ROM_PTR&lt;/span>(&lt;span style="color:#f92672">&amp;amp;&lt;/span>utime_module) }, \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [...]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we build and deploy our image, we&amp;rsquo;re now able to use the methods
from the &lt;code>utime&lt;/code> module:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>Adafruit CircuitPython &lt;span style="color:#ae81ff">3.0.0&lt;/span>&lt;span style="color:#f92672">-&lt;/span>alpha&lt;span style="color:#ae81ff">.6&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">42&lt;/span>&lt;span style="color:#f92672">-&lt;/span>gb46567004 on &lt;span style="color:#ae81ff">2018&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">05&lt;/span>&lt;span style="color:#f92672">-&lt;/span>&lt;span style="color:#ae81ff">06&lt;/span>; ESP module &lt;span style="color:#66d9ef">with&lt;/span> ESP8266
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span style="color:#f92672">import&lt;/span> utime
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span> utime&lt;span style="color:#f92672">.&lt;/span>sleep_ms(&lt;span style="color:#ae81ff">1000&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">&amp;gt;&amp;gt;&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We need to make one final change to the &lt;code>tm1637&lt;/code> module, since as
written it imports methods from the &lt;code>time&lt;/code> module. Instead of:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">from&lt;/span> time &lt;span style="color:#f92672">import&lt;/span> sleep_us, sleep_ms
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We have to modify it to read:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">try&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">from&lt;/span> time &lt;span style="color:#f92672">import&lt;/span> sleep_us, sleep_ms
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">except&lt;/span> &lt;span style="color:#a6e22e">ImportError&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">from&lt;/span> utime &lt;span style="color:#f92672">import&lt;/span> sleep_us, sleep_ms
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With our working &lt;code>utime&lt;/code> module and the modified &lt;code>tm1637&lt;/code> module, we
are now able to drive our display:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/laH7HY-wlCk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></content></item><item><title>Multiple 1-Wire Buses on the Raspberry Pi</title><link>https://blog.oddbit.com/post/2018-03-27-multiple-1-wire-buses-on-the-/</link><pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-03-27-multiple-1-wire-buses-on-the-/</guid><description>The DS18B20 is a popular temperature sensor that uses the 1-Wire protocol for communication. Recent versions of the Linux kernel include a kernel driver for this protocol, making it relatively convenient to connect one or more of these devices to a Raspberry Pi or similar device. 1-Wire devices can be daisy chained, so it is possible to connect several devices to your Pi using only a single GPIO pin, and you&amp;rsquo;ll find many articles out there that describe how to do so.</description><content>&lt;p>The DS18B20 is a popular temperature sensor that uses the &lt;a href="https://en.wikipedia.org/wiki/1-Wire">1-Wire&lt;/a>
protocol for communication. Recent versions of the Linux kernel
include a kernel driver for this protocol, making it relatively
convenient to connect one or more of these devices to a Raspberry Pi
or similar device. 1-Wire devices can be daisy chained, so it is
possible to connect several devices to your Pi using only a single
GPIO pin, and you&amp;rsquo;ll find many articles out there that describe how to
do so.&lt;/p>
&lt;p>Occasionally, it may be necessary to have more than a single chain of
connected devices. For example, you may have reached the limits on
the size of your 1-Wire network, or you may simply need to route your
cables in a way that makes a single chain difficult. You can enable
&lt;em>multiple&lt;/em> 1-Wire buses on your Raspberry Pi to handle these
situations.&lt;/p>
&lt;p>For a single 1-Wire bus, you add the following to &lt;code>/boot/config.txt&lt;/code>:&lt;/p>
&lt;pre>&lt;code>dtoverlay=w1-gpio
&lt;/code>&lt;/pre>
&lt;p>This will enable the 1-Wire bus on GPIO 4. To enable &lt;em>multiple&lt;/em>
1-Wire buses, you will use multiple &lt;code>dtoverlay&lt;/code> statements and the
&lt;code>gpiopin&lt;/code> parameter to the &lt;code>w1-gpio&lt;/code> overlay. For example, to enable
1-Wire buses on GPIO 4 and GPIO 17, you would use:&lt;/p>
&lt;pre>&lt;code>dtoverlay=w1-gpio,gpiopin=4
dtoverlay=w1-gpio,gpiopin=17
&lt;/code>&lt;/pre>
&lt;p>In the picture at the top of this post, there are four DS18B20
sensors. Three are connected to the 1-Wire bus on GPIO 4, and one is
connected to the 1-Wire bus on GPIO 17. Looking in
&lt;code>/sys/bus/w1/devices&lt;/code>, I see two w1_bus_master devices (and the four
temperature sensors):&lt;/p>
&lt;pre>&lt;code>$ ls /sys/bus/w1/devices/
28-041722cbacff 28-0417231547ff w1_bus_master1
28-041722ce24ff 28-04172318c0ff w1_bus_master2
&lt;/code>&lt;/pre>
&lt;p>I can check the temperature on all four devices like this:&lt;/p>
&lt;pre>&lt;code>$ cat /sys/bus/w1/devices/28-*/w1_slave
50 01 4b 46 7f ff 0c 10 e8 : crc=e8 YES
50 01 4b 46 7f ff 0c 10 e8 t=21000
50 01 4b 46 7f ff 0c 10 e8 : crc=e8 YES
50 01 4b 46 7f ff 0c 10 e8 t=21000
57 01 4b 46 7f ff 0c 10 38 : crc=38 YES
57 01 4b 46 7f ff 0c 10 38 t=21437
57 01 4b 46 7f ff 0c 10 38 : crc=38 YES
57 01 4b 46 7f ff 0c 10 38 t=21437
&lt;/code>&lt;/pre>
&lt;p>You may have noted that there is also a DHT22 sensor in the picture.
Much like the 1-Wire overlay, the kernel driver for the DHT22 can be
associated with an arbitrary GPIO pin like this:&lt;/p>
&lt;pre>&lt;code>dtoverlay=dht11,gpiopin=27
&lt;/code>&lt;/pre></content></item><item><title>Using Docker macvlan networks</title><link>https://blog.oddbit.com/post/2018-03-12-using-docker-macvlan-networks/</link><pubDate>Mon, 12 Mar 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-03-12-using-docker-macvlan-networks/</guid><description>A question that crops up regularly on #docker is &amp;ldquo;How do I attach a container directly to my local network?&amp;rdquo; One possible answer to that question is the macvlan network type, which lets you create &amp;ldquo;clones&amp;rdquo; of a physical interface on your host and use that to attach containers directly to your local network. For the most part it works great, but it does come with some minor caveats and limitations.</description><content>&lt;p>A question that crops up regularly on &lt;a href="https://docs.docker.com/opensource/ways/#docker-users">#docker&lt;/a> is &amp;ldquo;How do I attach
a container directly to my local network?&amp;rdquo; One possible answer to that
question is the &lt;a href="https://docs.docker.com/network/macvlan/">macvlan&lt;/a> network type, which lets you create
&amp;ldquo;clones&amp;rdquo; of a physical interface on your host and use that to attach
containers directly to your local network. For the most part it works
great, but it does come with some minor caveats and limitations. I
would like to explore those here.&lt;/p>
&lt;p>For the purpose of this example, let&amp;rsquo;s say we have a host interface
&lt;code>eno1&lt;/code> that looks like this:&lt;/p>
&lt;pre>&lt;code>2: eno1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
link/ether 64:00:6a:7d:06:1a brd ff:ff:ff:ff:ff:ff
inet 192.168.1.24/24 brd 192.168.1.255 scope global dynamic eno1
valid_lft 73303sec preferred_lft 73303sec
inet6 fe80::b2c9:3793:303:2a55/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>To create a macvlan network named &lt;code>mynet&lt;/code> attached to that interface,
you might run something like this:&lt;/p>
&lt;pre>&lt;code>docker network create -d macvlan -o parent=eno1 \
--subnet 192.168.1.0/24 \
--gateway 192.168.1.1 \
mynet
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;but don&amp;rsquo;t do that.&lt;/p>
&lt;h2 id="address-assignment">Address assignment&lt;/h2>
&lt;p>When you create a container attached to your macvlan network, Docker
will select an address from the subnet range and assign it to your
container. This leads to the potential for conflicts: if Docker picks
an address that has already been assigned to another host on your
network, you have a problem!&lt;/p>
&lt;p>You can avoid this by reserving a portion of the subnet range for use
by Docker. There are two parts to this solution:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You must configure any DHCP service on your network such that it
will not assign addresses in a given range.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You must tell Docker about that reserved range of addresses.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>How you accomplish the former depends entirely on your local network
infrastructure and is beyond the scope of this document. The latter
task is accomplished with the &lt;code>--ip-range&lt;/code> option to &lt;code>docker network create&lt;/code>.&lt;/p>
&lt;p>On my local network, my DHCP server will not assign any addresses
above &lt;code>192.168.1.190&lt;/code>. I have decided to assign to Docker the subset
&lt;code>192.168.1.192/27&lt;/code>, which is a range of 32 address starting at
192.168.1.192 and ending at 192.168.1.223. The corresponding &lt;code>docker network create&lt;/code> command would be:&lt;/p>
&lt;pre>&lt;code>docker network create -d macvlan -o parent=eno1 \
--subnet 192.168.1.0/24 \
--gateway 192.168.1.1 \
--ip-range 192.168.1.192/27 \
mynet
&lt;/code>&lt;/pre>
&lt;p>Now it is possible to create containers attached to my local network
without worrying about the possibility of ip address conflicts.&lt;/p>
&lt;h2 id="host-access">Host access&lt;/h2>
&lt;p>With a container attached to a macvlan network, you will find that
while it can contact other systems on your local network without a
problem, the container will not be able to connect to your host (and
your host will not be able to connect to your container). This is a
limitation of macvlan interfaces: without special support from a
network switch, your host is unable to send packets to its own macvlan
interfaces.&lt;/p>
&lt;p>Fortunately, there is a workaround for this problem: you can create
another macvlan interface on your host, and use that to communicate
with containers on the macvlan network.&lt;/p>
&lt;p>First, I&amp;rsquo;m going to reserve an address from our network range for use
by the host interface by using the &lt;code>--aux-address&lt;/code> option to &lt;code>docker network create&lt;/code>. That makes our final command line look like:&lt;/p>
&lt;pre>&lt;code>docker network create -d macvlan -o parent=eno1 \
--subnet 192.168.1.0/24 \
--gateway 192.168.1.1 \
--ip-range 192.168.1.192/27 \
--aux-address 'host=192.168.1.223' \
mynet
&lt;/code>&lt;/pre>
&lt;p>This will prevent Docker from assigning that address to a container.&lt;/p>
&lt;p>Next, we create a new macvlan interface on the host. You can call it
whatever you want, but I&amp;rsquo;m calling this one &lt;code>mynet-shim&lt;/code>:&lt;/p>
&lt;pre>&lt;code>ip link add mynet-shim link eno1 type macvlan mode bridge
&lt;/code>&lt;/pre>
&lt;p>Now we need to configure the interface with the address we reserved
and bring it up:&lt;/p>
&lt;pre>&lt;code>ip addr add 192.168.1.223/32 dev mynet-shim
ip link set mynet-shim up
&lt;/code>&lt;/pre>
&lt;p>The last thing we need to do is to tell our host to use that interface
when communicating with the containers. This is relatively easy
because we have restricted our containers to a particular CIDR subset
of the local network; we just add a route to that range like this:&lt;/p>
&lt;pre>&lt;code>ip route add 192.168.1.192/27 dev mynet-shim
&lt;/code>&lt;/pre>
&lt;p>With that route in place, your host will automatically use ths
&lt;code>mynet-shim&lt;/code> interface when communicating with containers on the
&lt;code>mynet&lt;/code> network.&lt;/p>
&lt;p>Note that the interface and routing configuration presented here is
not persistent &amp;ndash; you will lose if if you were to reboot your host.
How to make it persistent is distribution dependent.&lt;/p></content></item><item><title>Listening for connections on all ports/any port</title><link>https://blog.oddbit.com/post/2018-02-27-listening-for-connections-on-a/</link><pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-02-27-listening-for-connections-on-a/</guid><description>On IRC &amp;ndash; and other online communities &amp;ndash; it is common to use a &amp;ldquo;pastebin&amp;rdquo; service to share snippets of code, logs, and other material, rather than pasting them directly into a conversation. These services will typically return a URL that you can share with others so that they can see the content in their browser.
One of my favorite pastebin services is termbin.com, because it works from the command line using tools you probably already have installed.</description><content>&lt;p>On &lt;a href="https://en.wikipedia.org/wiki/Internet_Relay_Chat">IRC&lt;/a> &amp;ndash; and other online communities &amp;ndash; it is common to use a
&amp;ldquo;pastebin&amp;rdquo; service to share snippets of code, logs, and other
material, rather than pasting them directly into a conversation.
These services will typically return a URL that you can share with
others so that they can see the content in their browser.&lt;/p>
&lt;p>One of my favorite pastebin services is &lt;a href="http://termbin.com">termbin.com&lt;/a>, because it
works from the command line using tools you probably already have
installed. Termbin runs the &lt;a href="https://github.com/solusipse/fiche">fiche&lt;/a> service, which listens for TCP
connections on port 9999, reads any content that you provide, and then
returns a URL. For example, if I wanted to share my &lt;code>iptables&lt;/code>
configuration with someone I could just run:&lt;/p>
&lt;pre>&lt;code>$ iptables-save | nc termbin.com 9999
http://termbin.com/gjfp
&lt;/code>&lt;/pre>
&lt;p>Visiting &lt;a href="http://termbin.com/gjfp">http://termbin.com/gjfp&lt;/a> would show the output of that
command.&lt;/p>
&lt;p>It&amp;rsquo;s very convenient, but I found myself wondering: would it be
possible to configure things such that a service like &lt;a href="https://github.com/solusipse/fiche">fiche&lt;/a>
could listen on &lt;em>any&lt;/em> port?&lt;/p>
&lt;p>I started by looking into &lt;a href="https://en.wikipedia.org/wiki/Network_socket#Raw_socket">raw sockets&lt;/a>, but that turned out to be a
terrible idea. The solution was actually much simpler: use an
iptables &lt;a href="http://ipset.netfilter.org/iptables-extensions.man.html#lbDM">REDIRECT&lt;/a> rule to take all traffic to a given ip address
and redirect it to the &lt;a href="https://github.com/solusipse/fiche">fiche&lt;/a> service. This requires that you have
a spare ip address to dedicate to this service, but it is otherwise
very easy.&lt;/p>
&lt;p>First, we start the &lt;a href="https://github.com/solusipse/fiche">fiche&lt;/a> service:&lt;/p>
&lt;pre>&lt;code>$ ./fiche
[Fiche][STATUS] Starting fiche on Tue Feb 27 11:53:01 2018...
[Fiche][STATUS] Domain set to: http://example.com.
[Fiche][STATUS] Server started listening on port: 9999.
============================================================
&lt;/code>&lt;/pre>
&lt;p>And we add an additional address to one of our network interfaces.
I&amp;rsquo;m adding &lt;code>192.168.1.250&lt;/code> to &lt;code>eth0&lt;/code> on my local system:&lt;/p>
&lt;pre>&lt;code>$ sudo ip addr add 192.168.1.250/32 dev eth0
&lt;/code>&lt;/pre>
&lt;p>Next, we create two firewall rules:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>One in the &lt;code>nat&lt;/code> &lt;code>PREROUTING&lt;/code> table, which will intercept traffic
from external systems:&lt;/p>
&lt;pre>&lt;code> $ sudo iptables -t nat -A PREROUTING -p tcp -d 192.168.1.250 -j REDIRECT --to-ports 9999
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>One in the &lt;code>nat&lt;/code> &lt;code>OUTPUT&lt;/code> table, which will intercept any locally
generated traffic:&lt;/p>
&lt;pre>&lt;code> $ sudo iptables -t nat -A OUTPUT -p tcp -d 192.168.1.250 -j REDIRECT --to-ports 9999
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>These two rules will intercept any traffic &amp;ndash; on &lt;em>any&lt;/em> port &amp;ndash; to
192.168.1.250 and redirect it to the &lt;a href="https://github.com/solusipse/fiche">fiche&lt;/a> service.&lt;/p>
&lt;p>For example, using no port (&lt;code>nc&lt;/code> on my system defaults to port &lt;code>0&lt;/code>):&lt;/p>
&lt;pre>&lt;code>$ echo hello | nc 192.168.1.250
http://example.com/tfka
&lt;/code>&lt;/pre>
&lt;p>And any other port works as well:&lt;/p>
&lt;pre>&lt;code>$ echo hello | nc 192.168.1.250 10
http://example.com/0j0c
$ echo hello | nc 192.168.1.250 80
http://example.com/u4fq
&lt;/code>&lt;/pre>
&lt;p>This solution will work with any TCP service. The service will need
to be listening on &lt;code>INADDR_ANY&lt;/code> (&lt;code>0.0.0.0&lt;/code>), because the &lt;code>REDIRECT&lt;/code>
rule rewrites the destination address to &amp;ldquo;the primary address of the
incoming interface&amp;rdquo;.&lt;/p></content></item><item><title>Grouping aggregation queries in Gnocchi 4.0.x</title><link>https://blog.oddbit.com/post/2018-02-26-grouping-aggregation-queries-i/</link><pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-02-26-grouping-aggregation-queries-i/</guid><description>In this article, we&amp;rsquo;re going to ask Gnocchi (the OpenStack telemetry storage service) how much memory was used, on average, over the course of each day by each project in an OpenStack environment.
Environment I&amp;rsquo;m working with an OpenStack &amp;ldquo;Pike&amp;rdquo; deployment, which means I have Gnocchi 4.0.x. More recent versions of Gnocchi (4.1.x and later) have a new aggregation API called dynamic aggregates, but that isn&amp;rsquo;t available in 4.0.x so in this article we&amp;rsquo;ll be using the legacy /v1/aggregations API.</description><content>&lt;p>In this article, we&amp;rsquo;re going to ask Gnocchi (the OpenStack telemetry
storage service) how much memory was used, on average, over the course
of each day by each project in an OpenStack environment.&lt;/p>
&lt;h2 id="environment">Environment&lt;/h2>
&lt;p>I&amp;rsquo;m working with an OpenStack &amp;ldquo;Pike&amp;rdquo; deployment, which means I have
Gnocchi 4.0.x. More recent versions of Gnocchi (4.1.x and later) have
a new aggregation API called &lt;a href="https://gnocchi.xyz/rest.html#dynamic-aggregates">dynamic aggregates&lt;/a>, but that isn&amp;rsquo;t
available in 4.0.x so in this article we&amp;rsquo;ll be using the legacy
&lt;code>/v1/aggregations&lt;/code> API.&lt;/p>
&lt;h2 id="the-gnocchi-data-model">The Gnocchi data model&lt;/h2>
&lt;p>In Gnocchi, named metrics are associated with &lt;em>resources&lt;/em>. A
&lt;em>resource&lt;/em> corresponds to an object in OpenStack, such as a Nova
instance, or a Neutron network, etc. Metrics on a resource are
created dynamically and depend entirely on which metrics you are
collecting with Ceilometer and delivering to Gnocchi, and two
different resources of the same resource type may have different sets
of metrics.&lt;/p>
&lt;p>The list of metrics available from OpenStack is available in the
&lt;a href="https://docs.openstack.org/ceilometer/latest/admin/telemetry-measurements.html">telemetry documentation&lt;/a>. The &lt;a href="https://docs.openstack.org/ceilometer/latest/admin/telemetry-measurements.html#openstack-compute">metrics available for Nova
servers&lt;/a> include statistics on cpu utilization,
memory utilization, disk read/write operations, network traffic, and
more.&lt;/p>
&lt;p>In this example, we&amp;rsquo;re going to look at the &lt;code>memory.usage&lt;/code> metric.&lt;/p>
&lt;h2 id="building-a-query">Building a query&lt;/h2>
&lt;p>The amount of memory used is available in the &lt;code>memory.usage&lt;/code> metric
associated with each &lt;code>instance&lt;/code> resource. We&amp;rsquo;re using the legacy
&lt;code>/v1/aggregations&lt;/code> API, so the request url will initially look like:&lt;/p>
&lt;pre>&lt;code>/v1/aggregation/resource/instance/metric/memory.usage
&lt;/code>&lt;/pre>
&lt;p>We need to specify which granularity we want to fetch. We&amp;rsquo;re using
the &lt;code>low&lt;/code> archive policy from the default Gnocchi configuration which
means we only have one option (metrics are aggregated into 5 minute
intervals). We use the &lt;code>granularity&lt;/code> parameter to tell Gnocchi which
granularity we want:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300
&lt;/code>&lt;/pre>
&lt;p>We want the average utilization, so that means:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;d like to limit this to the past 30 days of data, so we&amp;rsquo;ll add an
appropriate &lt;code>start&lt;/code> parameter. There are various ways of specifying
time in Gnocchi, documented in the &lt;a href="https://gnocchi.xyz/rest.html#timestamp-format">Timestamps&lt;/a> section of the docs.
Specifying &amp;ldquo;30 days ago&amp;rdquo; is as simple as: &lt;code>start=-30+days&lt;/code>. So our
request now looks like:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days
&lt;/code>&lt;/pre>
&lt;p>We have values at 5 minute intervals, but we would like daily
averages, so we&amp;rsquo;ll need to use the &lt;a href="https://gnocchi.xyz/rest.html#resample">resample&lt;/a> parameter to ask
Gnocchi to reshape the data for us. The argument to &lt;code>resample&lt;/code> is a
time period specified as a number of seconds; since we want daily
averages, that means &lt;code>resample=86400&lt;/code>:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days&amp;amp;resample=86400
&lt;/code>&lt;/pre>
&lt;p>We want to group the results by project, which means we&amp;rsquo;ll need the
&lt;code>groupby&lt;/code> parameter. Instances (and most other resources) store this
information in the &lt;code>project_id&lt;/code> attribute, so &lt;code>groupby=project_id&lt;/code>:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days&amp;amp;resample=86400&amp;amp;groupby=project_id
&lt;/code>&lt;/pre>
&lt;p>Lastly, not all metrics will have measurements covering the same
period. If we were to try the query as we have it right now, we would
probably see:&lt;/p>
&lt;pre>&lt;code>400 Bad Request
The server could not comply with the request since it is either
malformed or otherwise incorrect.
One of the metrics being aggregated doesn't have matching
granularity: Metrics &amp;lt;list of metrics here&amp;gt;...can't be aggregated:
No overlap
&lt;/code>&lt;/pre>
&lt;p>Gnocchi provides a &lt;code>fill&lt;/code> parameter to indicate what should happen
with missing data. In Gnocchi 4.0.x, the options are &lt;code>fill=0&lt;/code>
(replace missing data with &lt;code>0&lt;/code>) and &lt;code>fill=null&lt;/code> (compute the
aggregation using only available data points). Selecting &lt;code>fill=0&lt;/code> gets
us:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days&amp;amp;resample=86400&amp;amp;groupby=project_id&amp;amp;fill=0
&lt;/code>&lt;/pre>
&lt;p>That should give us the results we want. The return value from that
query looks something like this:&lt;/p>
&lt;pre>&lt;code>[
{
&amp;quot;group&amp;quot;: {
&amp;quot;project_id&amp;quot;: &amp;quot;00a8d5e942bb442b86733f0f92280fcc&amp;quot;
},
&amp;quot;measures&amp;quot;: [
[
&amp;quot;2018-02-14T00:00:00+00:00&amp;quot;,
86400.0,
2410.014338235294
],
[
&amp;quot;2018-02-15T00:00:00+00:00&amp;quot;,
86400.0,
2449.4921970791206
],
.
.
.
{
&amp;quot;group&amp;quot;: {
&amp;quot;project_id&amp;quot;: &amp;quot;03d2a1de5b2342d78d14e5294a81e0b0&amp;quot;
},
&amp;quot;measures&amp;quot;: [
[
&amp;quot;2018-02-14T00:00:00+00:00&amp;quot;,
86400.0,
381.0
],
[
&amp;quot;2018-02-15T00:00:00+00:00&amp;quot;,
86400.0,
380.99004975124376
],
[
&amp;quot;2018-02-16T00:00:00+00:00&amp;quot;,
86400.0,
380.99305555555554
],
.
.
.
&lt;/code>&lt;/pre>
&lt;h2 id="authenticating-to-gnocchi">Authenticating to Gnocchi&lt;/h2>
&lt;p>Since we&amp;rsquo;re using Gnocchi as part of an OpenStack deployment, we&amp;rsquo;ll
be authenticating to Gnocchi using a Keystone token. Let&amp;rsquo;s take a
look at how you could do that from the command line using &lt;code>curl&lt;/code>.&lt;/p>
&lt;h3 id="acquiring-a-token">Acquiring a token&lt;/h3>
&lt;p>You can acquire a token using the &lt;code>openstack token issue&lt;/code> command,
which will produce output like:&lt;/p>
&lt;pre>&lt;code>+------------+------------------------------------------------------------------------+
| Field | Value |
+------------+------------------------------------------------------------------------+
| expires | 2018-02-26T23:09:36+0000 |
| id | ... |
| project_id | c53c18b2d29641e0877bbbd8d87f8267 |
| user_id | 533ad9ab4fed403fb98f1ffc2f2b4436 |
+------------+------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>While it is possible to parse that with, say, &lt;code>awk&lt;/code>, it&amp;rsquo;s not
really designed to be machine readable. We can get just the token
value by instead calling:&lt;/p>
&lt;pre>&lt;code>openstack token issue -c id -f value
&lt;/code>&lt;/pre>
&lt;p>We should probably store that in a variable:&lt;/p>
&lt;pre>&lt;code>TOKEN=$(openstack token issue -c id -f value)
&lt;/code>&lt;/pre>
&lt;h3 id="querying-gnocchi">Querying Gnocchi&lt;/h3>
&lt;p>In order to make our command line shorter, let&amp;rsquo;s set &lt;code>ENDPOINT&lt;/code> to the
URL of our Gnocchi endpoint:&lt;/p>
&lt;pre>&lt;code>ENDPOINT=http://cloud.example.com:8041/v1
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ll pass the Keystone token in the &lt;code>X-Auth-Token&lt;/code> header. Gnocchi
is expecting a JSON document body as part of this request (you can use
that to specify a filter; see the &lt;a href="https://gnocchi.xyz/rest.html">API documentation&lt;/a> for details),
so we need to both set a &lt;code>Content-type&lt;/code> header and provide at least an
empty JSON object. That makes our final request look like:&lt;/p>
&lt;pre>&lt;code>curl \
-H &amp;quot;X-Auth-Token: $TOKEN&amp;quot; \
-H &amp;quot;Content-type: application/json&amp;quot; \
-d &amp;quot;{}&amp;quot; \
&amp;quot;$ENDPOINT/aggregation/resource/instance/metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days&amp;amp;resample=86400&amp;amp;groupby=project_id&amp;amp;fill=0&amp;quot;
&lt;/code>&lt;/pre></content></item><item><title>Listing iptables rules with line numbers</title><link>https://blog.oddbit.com/post/2018-02-08-listing-iptables-rules-with-li/</link><pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-02-08-listing-iptables-rules-with-li/</guid><description>You can list iptables rules with rule numbers using the --line-numbers option, but this only works in list (-L) mode. I find it much more convenient to view rules using the output from iptables -S or iptables-save.
You can augment the output from these commands with rule numbers with the following awk script:
#!/bin/awk -f state == 0 &amp;amp;&amp;amp; /^-A/ {state=1; chain=$2; counter=1; printf &amp;quot;\n&amp;quot;} state == 1 &amp;amp;&amp;amp; $2 !</description><content>&lt;p>You can list &lt;code>iptables&lt;/code> rules with rule numbers using the
&lt;code>--line-numbers&lt;/code> option, but this only works in list (&lt;code>-L&lt;/code>) mode. I
find it much more convenient to view rules using the output from
&lt;code>iptables -S&lt;/code> or &lt;code>iptables-save&lt;/code>.&lt;/p>
&lt;p>You can augment the output from these commands with rule numbers with
the following &lt;code>awk&lt;/code> script:&lt;/p>
&lt;pre>&lt;code>#!/bin/awk -f
state == 0 &amp;amp;&amp;amp; /^-A/ {state=1; chain=$2; counter=1; printf &amp;quot;\n&amp;quot;}
state == 1 &amp;amp;&amp;amp; $2 != chain {chain=$2; counter=1; printf &amp;quot;\n&amp;quot;}
!/^-A/ {state=0}
state == 1 {printf &amp;quot;[%03d] %s\n&amp;quot;, counter++, $0}
state == 0 {print}
&lt;/code>&lt;/pre>
&lt;p>This will produce output along the lines of:&lt;/p>
&lt;pre>&lt;code>-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
-N DOCKER
-N DOCKER-ISOLATION
-N LARS
[001] -A INPUT -i virbr1 -p udp -m udp --dport 53 -j ACCEPT
[002] -A INPUT -i virbr1 -p tcp -m tcp --dport 53 -j ACCEPT
[003] -A INPUT -i virbr1 -p udp -m udp --dport 67 -j ACCEPT
[004] -A INPUT -i virbr1 -p tcp -m tcp --dport 67 -j ACCEPT
[005] -A INPUT -i virbr0 -p udp -m udp --dport 53 -j ACCEPT
[006] -A INPUT -i virbr0 -p tcp -m tcp --dport 53 -j ACCEPT
[007] -A INPUT -i virbr0 -p udp -m udp --dport 67 -j ACCEPT
[008] -A INPUT -i virbr0 -p tcp -m tcp --dport 67 -j ACCEPT
[001] -A FORWARD -j DOCKER-ISOLATION
[002] -A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
[003] -A FORWARD -o docker0 -j DOCKER
[004] -A FORWARD -i docker0 ! -o docker0 -j ACCEPT
[005] -A FORWARD -i docker0 -o docker0 -j ACCEPT
[001] -A DOCKER-ISOLATION -i br-c9ab3aa72e98 -o docker0 -j DROP
[002] -A DOCKER-ISOLATION -i docker0 -o br-c9ab3aa72e98 -j DROP
[003] -A DOCKER-ISOLATION -i br-74ee392a7301 -o docker0 -j DROP
[004] -A DOCKER-ISOLATION -i docker0 -o br-74ee392a7301 -j DROP
[005] -A DOCKER-ISOLATION -i br-6b5fa040c423 -o docker0 -j DROP
[006] -A DOCKER-ISOLATION -i docker0 -o br-6b5fa040c423 -j DROP
[007] -A DOCKER-ISOLATION -i br-438e4f71d66d -o docker0 -j DROP
[008] -A DOCKER-ISOLATION -i docker0 -o br-438e4f71d66d -j DROP
&lt;/code>&lt;/pre>
&lt;p>That makes it much easier if you&amp;rsquo;re trying to insert or delete rules
by index (as in &lt;code>iptables -I INPUT 7 ...&lt;/code>). I keep the awk code itself
in a script named &lt;code>number-rules&lt;/code> so that running it locally usually
looks like:&lt;/p>
&lt;pre>&lt;code># iptables -S | number-rules | less
&lt;/code>&lt;/pre></content></item><item><title>Pelican and theme update</title><link>https://blog.oddbit.com/post/2018-01-26-pelican-theme-update/</link><pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-01-26-pelican-theme-update/</guid><description>I&amp;rsquo;ve just refreshed the version of Pelican used to generate this blog, along with the associated themes and plugins. It all seems to be working, but if you spot a problem feel free to drop me a line.</description><content>&lt;p>I&amp;rsquo;ve just refreshed the version of &lt;a href="https://github.com/getpelican/pelican">Pelican&lt;/a> used to generate this
blog, along with the associated themes and plugins. It all seems to be
working, but if you spot a problem feel free to &lt;a href="https://github.com/larsks/blog.oddbit.com/issues">drop me a line&lt;/a>.&lt;/p></content></item><item><title>Fun with devicemapper snapshots</title><link>https://blog.oddbit.com/post/2018-01-25-fun-with-devicemapper-snapshot/</link><pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-01-25-fun-with-devicemapper-snapshot/</guid><description>I find myself working with Raspbian disk images fairly often. A typical workflow is:
Download the disk image. Mount the filesystem somewhere to check something. Make some changes or install packages just to check something else. Crap I&amp;rsquo;ve made changes. &amp;hellip;at which point I need to fetch a new copy of the image next time I want to start fresh.
Sure, I could just make a copy of the image and work from there, but what fun is that?</description><content>&lt;p>I find myself working with &lt;a href="https://www.raspberrypi.org/downloads/raspbian/">Raspbian&lt;/a> disk images fairly often. A
typical workflow is:&lt;/p>
&lt;ul>
&lt;li>Download the disk image.&lt;/li>
&lt;li>Mount the filesystem somewhere to check something.&lt;/li>
&lt;li>Make some changes or install packages just to check something else.&lt;/li>
&lt;li>Crap I&amp;rsquo;ve made changes.&lt;/li>
&lt;/ul>
&lt;p>&amp;hellip;at which point I need to fetch a new copy of the image next time I
want to start fresh.&lt;/p>
&lt;p>Sure, I could just make a copy of the image and work from there, but
what fun is that? This seemed like a perfect opportunity to learn more
about the &lt;a href="https://www.kernel.org/doc/Documentation/device-mapper/">device mapper&lt;/a> and in particular how the &lt;a href="https://www.kernel.org/doc/Documentation/device-mapper/snapshot.txt">snapshot&lt;/a>
target works.&lt;/p>
&lt;h2 id="making-sure-we-have-a-block-device">Making sure we have a block device&lt;/h2>
&lt;p>The device mapper only operates on block devices, so the first thing
we need to do is to make the source image available as a block device.
We can do that with the &lt;a href="http://manpages.ubuntu.com/manpages/xenial/man8/losetup.8.html">losetup&lt;/a> command, which maps a file to a
virtual block device (or &lt;em>loop&lt;/em> device).&lt;/p>
&lt;p>I run something like this:&lt;/p>
&lt;pre>&lt;code>losetup --find --show --read-only 2017-11-29-raspbian-stretch-lite.img
&lt;/code>&lt;/pre>
&lt;p>This will find the first available block device, and then use it make
my disk image available in read-only mode. Those of you who are
familiar with &lt;code>losetup&lt;/code> may be thinking, &amp;ldquo;you know, &lt;code>losetup&lt;/code> knows
how to handle partitioned devices&amp;rdquo;, but I am ignoring that for the
purpose of using device mapper to solve things.&lt;/p>
&lt;h2 id="mapping-a-partition">Mapping a partition&lt;/h2>
&lt;p>We&amp;rsquo;ve just mapped the entire disk image to a block device. We can&amp;rsquo;t
use this directly because the image has multiple partitions:&lt;/p>
&lt;pre>&lt;code># sfdisk -l /dev/loop0
Disk /dev/loop0: 1.7 GiB, 1858076672 bytes, 3629056 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x37665771
Device Boot Start End Sectors Size Id Type
/dev/loop0p1 8192 93236 85045 41.5M c W95 FAT32 (LBA)
/dev/loop0p2 94208 3629055 3534848 1.7G 83 Linux
&lt;/code>&lt;/pre>
&lt;p>We want to expose partition 2, which contains the root filesystem. We
can see from the above output that partition 2 starts at sector
&lt;code>94208&lt;/code> and extends for &lt;code>3534848&lt;/code> sectors (where a &lt;em>sector&lt;/em> is for our
purposes 512 bytes). If you need to get at this information
programatically, &lt;code>sfdisk&lt;/code> has &lt;code>--json&lt;/code> option that can be useful; for
example:&lt;/p>
&lt;pre>&lt;code># p_start=$(sfdisk --json /dev/loop0 |
jq &amp;quot;.partitiontable.partitions[1].start&amp;quot;)
# echo $p_start
94208
&lt;/code>&lt;/pre>
&lt;p>We want to expose this partition as a distinct block device. We&amp;rsquo;re
going to do this by creating a device mapper &lt;code>linear&lt;/code> target. To
create device mapper devices, we use the &lt;code>dmsetup&lt;/code> command; the basic
syntax is:&lt;/p>
&lt;pre>&lt;code>dmsetup create &amp;lt;name&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>By default, this expects to read a table describing the device on
&lt;code>stdin&lt;/code>, although it is also possible to specify one on the command
line. A table consists of one or more lines of the format:&lt;/p>
&lt;pre>&lt;code>&amp;lt;base&amp;gt; &amp;lt;length&amp;gt; &amp;lt;target&amp;gt; [&amp;lt;options&amp;gt;]
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>&amp;lt;base&amp;gt;&lt;/code> is the starting offset in sectors for this particular
segment, &lt;code>&amp;lt;length&amp;gt;&lt;/code> is the length, &lt;code>&amp;lt;target&amp;gt;&lt;/code> is the target type
(linear, snapshot, zero, etc), and the option are specific to the
particular target in use.&lt;/p>
&lt;p>To create a device exposing partition 2 of our image, we run:&lt;/p>
&lt;pre>&lt;code># dmsetup create base &amp;lt;&amp;lt;EOF
0 3534848 linear /dev/loop0 94208
EOF
&lt;/code>&lt;/pre>
&lt;p>This creates a device named &lt;code>/dev/mapper/base&lt;/code>. Sectors &lt;code>0&lt;/code> through
&lt;code>3534848&lt;/code> of this device will be provided by &lt;code>/dev/loop0&lt;/code>, starting at
offset &lt;code>94208&lt;/code>. At this point, we can actually mount the filesystem:&lt;/p>
&lt;pre>&lt;code># mount -o ro /dev/mapper/base /mnt
# ls /mnt
bin dev home lost+found mnt proc run srv tmp var
boot etc lib media opt root sbin sys usr
# umount /mnt
&lt;/code>&lt;/pre>
&lt;p>But wait, there&amp;rsquo;s a problem! These disk images usually have very
little free space. We&amp;rsquo;re going to want to extend the length of our
base device by some amount so that we have room for new packages and
so forth. Fortunately, since our goal is that all writes are going to
a snapshot, we don&amp;rsquo;t need to use &lt;em>real&lt;/em> space. We can add another
segment to our table that uses the &lt;a href="https://www.kernel.org/doc/Documentation/device-mapper/zero.txt">zero&lt;/a> target.&lt;/p>
&lt;p>Let&amp;rsquo;s first get rid of the device we just created:&lt;/p>
&lt;pre>&lt;code># dmsetup remove base
&lt;/code>&lt;/pre>
&lt;p>And create a new one:&lt;/p>
&lt;pre>&lt;code># dmsetup create base &amp;lt;&amp;lt;EOF
0 3534848 linear /dev/loop0 94208
3534848 6950912 zero
EOF
&lt;/code>&lt;/pre>
&lt;p>This extends our &lt;code>base&lt;/code> device out to 5G (or a total of &lt;code>10485760&lt;/code>
sectors), although attempting to read from anything beyond sector
&lt;code>3534848&lt;/code> will return zeros, and writes will be discarded. But that&amp;rsquo;s
okay, because the space available for writes is going to come from a
COW (&amp;ldquo;copy-on-write&amp;rdquo;) device associated with our snapshot: in other
words, the capacity of our snapshot will be linked to size of our COW
device, rather than the size of the underlying base image.&lt;/p>
&lt;h2 id="creating-a-snapshot">Creating a snapshot&lt;/h2>
&lt;p>Now that we&amp;rsquo;ve sorted out our base image it&amp;rsquo;s time to create the
snapshot device. According to &lt;a href="https://www.kernel.org/doc/Documentation/device-mapper/snapshot.txt">the documentation&lt;/a>, the
table entry for a snapshot looks like:&lt;/p>
&lt;pre>&lt;code>snapshot &amp;lt;origin&amp;gt; &amp;lt;COW device&amp;gt; &amp;lt;persistent?&amp;gt; &amp;lt;chunksize&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We have our &lt;code>&amp;lt;origin&amp;gt;&lt;/code> (that&amp;rsquo;s the base image we created in the
previous step), but what are we going to use as our &lt;code>&amp;lt;COW device&amp;gt;&lt;/code>?
This is a chunk of storage that will receive any writes to the
snapshot device. This could be any block device (another loop device,
an LVM volume, a spare disk partition), but for my purposes it seemed
convenient to use a RAM disk, since I had no need for persistent
changes. We can use the &lt;a href="https://www.kernel.org/doc/Documentation/blockdev/zram.txt">zram&lt;/a> kernel module for that. Let&amp;rsquo;s start
by loading the module:&lt;/p>
&lt;pre>&lt;code># modprobe zram
&lt;/code>&lt;/pre>
&lt;p>Without any additional parameters this will create a single RAM disk,
&lt;code>/dev/zram0&lt;/code>. Initially, it&amp;rsquo;s not very big:&lt;/p>
&lt;pre>&lt;code># blockdev --getsz /dev/zram0
0
&lt;/code>&lt;/pre>
&lt;p>But we can change that using the &lt;code>sysfs&lt;/code> interface provided in
&lt;code>/sys/block/zram0/&lt;/code>. The &lt;code>disksize&lt;/code> option controls the size of the
disk. Let&amp;rsquo;s say we want to handle up to 512M of writes; that means we
need to write the value &lt;code>512M&lt;/code> to &lt;code>/sys/block/zram0/disksize&lt;/code>:&lt;/p>
&lt;pre>&lt;code>echo 512M &amp;gt; /sys/block/zram0/disksize
&lt;/code>&lt;/pre>
&lt;p>And now:&lt;/p>
&lt;pre>&lt;code># blockdev --getsz /dev/zram0
1048576
&lt;/code>&lt;/pre>
&lt;p>We now have all the requirements to create our snapshot device:&lt;/p>
&lt;pre>&lt;code>dmsetup create snap &amp;lt;&amp;lt;EOF
0 10485760 snapshot /dev/mapper/base /dev/zram0 N 16
EOF
&lt;/code>&lt;/pre>
&lt;p>This creates a device named &lt;code>/dev/mapper/snap&lt;/code>. It is a 5G block
device backed by &lt;code>/dev/mapper/base&lt;/code>, with changes written to
&lt;code>/dev/zram0&lt;/code>. We can mount it:&lt;/p>
&lt;pre>&lt;code># mount /dev/mapper/snap /mnt
# df -h /mnt
Filesystem Size Used Avail Use% Mounted on
/dev/mapper/snap 1.7G 943M 623M 61% /mnt
&lt;/code>&lt;/pre>
&lt;p>And we can resize it:&lt;/p>
&lt;pre>&lt;code># resize2fs !$
resize2fs /dev/mapper/snap
resize2fs 1.43.3 (04-Sep-2016)
Filesystem at /dev/mapper/snap is mounted on /mnt; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/mapper/snap is now 1310720 (4k) blocks long.
# df -h /mnt
Filesystem Size Used Avail Use% Mounted on
/dev/mapper/snap 4.9G 944M 3.8G 20% /mnt
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll note here that it looks like we have a 5G device, because
that&amp;rsquo;s the size of our base image. Because we&amp;rsquo;ve only allocated
&lt;code>512M&lt;/code> to our COW device, we can actually only handle up to 512M of
writes before we invalidate the snapshot.&lt;/p>
&lt;p>We can inspect the amount of our COW device that has been consumed by
changes by using &lt;code>dmsetup status&lt;/code>:&lt;/p>
&lt;pre>&lt;code># dmsetup status snap
0 10485760 snapshot 107392/1048576 0
&lt;/code>&lt;/pre>
&lt;p>This tells us that &lt;code>107392&lt;/code> sectors of &lt;code>1048576&lt;/code> total have been
consumed so far (in other words, about 54M out of 512M). We can get
similar information from the perspective of the &lt;code>zram&lt;/code> module using
&lt;code>zramctl&lt;/code>:&lt;/p>
&lt;pre>&lt;code># zramctl
NAME ALGORITHM DISKSIZE DATA COMPR TOTAL STREAMS MOUNTPOINT
/dev/zram0 lzo 512M 52.4M 34K 72K 4
&lt;/code>&lt;/pre>
&lt;p>This information is also available in &lt;code>/sys/block/zram0/mm_stat&lt;/code>, but
without any labels.&lt;/p></content></item><item><title>Safely restarting an OpenStack server with Ansible</title><link>https://blog.oddbit.com/post/2018-01-24-safely-restarting-an-openstack/</link><pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-01-24-safely-restarting-an-openstack/</guid><description>The other day on #ansible, someone was looking for a way to safely shut down a Nova server, wait for it to stop, and then start it up again using the openstack cli. The first part seemed easy:
- hosts: myserver tasks: - name: shut down the server command: poweroff become: true &amp;hellip;but that will actually fail with the following result:
TASK [shut down server] ************************************* fatal: [myserver]: UNREACHABLE! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;msg&amp;quot;: &amp;quot;Failed to connect to the host via ssh: Shared connection to 10.</description><content>&lt;p>The other day on &lt;a href="http://docs.ansible.com/ansible/latest/community.html#irc-channel">#ansible&lt;/a>, someone was looking for a way to safely
shut down a Nova server, wait for it to stop, and then start it up
again using the &lt;code>openstack&lt;/code> cli. The first part seemed easy:&lt;/p>
&lt;pre>&lt;code>- hosts: myserver
tasks:
- name: shut down the server
command: poweroff
become: true
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;but that will actually fail with the following result:&lt;/p>
&lt;pre>&lt;code>TASK [shut down server] *************************************
fatal: [myserver]: UNREACHABLE! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;msg&amp;quot;:
&amp;quot;Failed to connect to the host via ssh: Shared connection to
10.0.0.103 closed.\r\n&amp;quot;, &amp;quot;unreachable&amp;quot;: true}
&lt;/code>&lt;/pre>
&lt;p>This happens because running &lt;code>poweroff&lt;/code> immediately closes Ansible&amp;rsquo;s
ssh connection. The workaround here is to use a &amp;ldquo;fire-and-forget&amp;rdquo;
&lt;a href="http://docs.ansible.com/ansible/latest/playbooks_async.html">asynchronous task&lt;/a>:&lt;/p>
&lt;pre>&lt;code>- hosts: myserver
tasks:
- name: shut down the server
command: poweroff
become: true
async: 30
poll: 0
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>poll: 0&lt;/code> means that Ansible won&amp;rsquo;t wait around to check the result
of this task. Control will return to ansible immediately, so our
playbook can continue without errors resulting from the closed
connection.&lt;/p>
&lt;p>Now that we&amp;rsquo;ve started the shutdown process on the host, how do we
wait for it to complete? You&amp;rsquo;ll see people solve this with a
&lt;a href="http://docs.ansible.com/ansible/latest/pause_module.html">pause&lt;/a> task, but that&amp;rsquo;s fragile because the timing there can be
tricky to get right.&lt;/p>
&lt;p>Another option is to use something like Ansible&amp;rsquo;s &lt;a href="http://docs.ansible.com/ansible/latest/wait_for_module.html">wait_for&lt;/a> module.
For example, we could wait for &lt;code>sshd&lt;/code> to become unresponsive:&lt;/p>
&lt;pre>&lt;code>- name: wait for server to finishing shutting down
wait_for:
port: 22
state: stopped
&lt;/code>&lt;/pre>
&lt;p>Be this is really just checking whether or not &lt;code>sshd&lt;/code> is listening for
a connection, and &lt;code>sshd&lt;/code> may shut down long before the system shutdown
process completes.&lt;/p>
&lt;p>The best option is to ask Nova. We can query Nova for information
about a server using the Ansible&amp;rsquo;s &lt;a href="http://docs.ansible.com/ansible/latest/os_server_facts_module.html">os_server_facts&lt;/a> module. Like
the other OpenStack modules, this uses &lt;a href="https://docs.openstack.org/os-client-config/latest/">os-client-config&lt;/a> to find
authentication credentials for your OpenStack environment. If you&amp;rsquo;re
not explicitly providing authentication information in your playbook,
the module will use the &lt;code>OS_*&lt;/code> environment variables that are commonly
used with the OpenStack command line tools.&lt;/p>
&lt;p>The &lt;code>os_server_facts&lt;/code> module creates an &lt;code>openstack_servers&lt;/code> fact, the
value of which is a list of dictionaries which contains keys like
&lt;code>status&lt;/code>, which is the one in which we&amp;rsquo;re interested. A running
server has &lt;code>status == &amp;quot;ACTIVE&amp;quot;&lt;/code> and a server that has been powered off
has &lt;code>status == &amp;quot;SHUTOFF&lt;/code>.&lt;/p>
&lt;p>Given the above, the &amp;ldquo;wait for shutdown&amp;rdquo; task would look something
like the following:&lt;/p>
&lt;pre>&lt;code>- hosts: localhost
tasks:
- name: wait for server to stop
os_server_facts:
server: myserver
register: results
until: openstack_servers.0.status == &amp;quot;SHUTOFF&amp;quot;
retries: 120
delay: 5
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll note that I&amp;rsquo;m targeting &lt;code>localhost&lt;/code> right now, because my local
system has access to my OpenStack environment and I have the necessary
credentials in my environment. If you need to run these tasks
elsewhere, you&amp;rsquo;ll want to read up on how to provide credentials in
your playbook.&lt;/p>
&lt;p>This task will retry up to &lt;code>120&lt;/code> times, waiting &lt;code>5&lt;/code> seconds between
tries, until the server reaches the desired state.&lt;/p>
&lt;p>Next, we want to start the server back up. We can do this using
the &lt;a href="http://docs.ansible.com/ansible/latest/os_server_action_module.html">os_server_action&lt;/a> module, using the &lt;code>start&lt;/code> action. This task
also runs on &lt;code>localhost&lt;/code>:&lt;/p>
&lt;pre>&lt;code> - name: start the server
os_server_action:
server: larstest
action: start
&lt;/code>&lt;/pre>
&lt;p>Finally, we want to wait for the host to come back up before we run
any additional tasks on it. In this case, we can&amp;rsquo;t just look at the
status reported by Nova: the host will be &lt;code>ACTIVE&lt;/code> from Nova&amp;rsquo;s
perspective long before it is ready to accept &lt;code>ssh&lt;/code> connections. Nor
can we use the &lt;code>wait_for&lt;/code> module, since the &lt;code>ssh&lt;/code> port may be open
before we are actually able to log in. The solution to this is the
&lt;a href="http://docs.ansible.com/ansible/latest/wait_for_connection_module.html">wait_for_connection&lt;/a> module, which waits until Ansible is able to
successful execute an action on the target host:&lt;/p>
&lt;pre>&lt;code>- hosts: larstest
gather_facts: false
tasks:
- name: wait for server to start
wait_for_connection:
&lt;/code>&lt;/pre>
&lt;p>We need to set &lt;code>gather_facts: false&lt;/code> here because fact gathering
requires a functioning connection to the target host.&lt;/p>
&lt;p>And that&amp;rsquo;s it: a recipe for gently shutting down a remote host,
waiting for the shutdown to complete, then later on spinning it back
up and waiting until subsequent Ansible tasks will work correctly.&lt;/p></content></item><item><title>Some notes on PWM on the Raspberry Pi</title><link>https://blog.oddbit.com/post/2017-09-26-some-notes-on-pwm-on-the-raspb/</link><pubDate>Tue, 26 Sep 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-09-26-some-notes-on-pwm-on-the-raspb/</guid><description>I was recently working on a project in which I wanted to drive a simple piezo buzzer attached to a GPIO pin on a Raspberry Pi. I was already using the RPi.GPIO module in my project so that seemed like a logical place to start, but I ran into a few issues.
You drive a piezo buzzer by generating a PWM signal with the appropriate frequency. The RPi.GPIO module implements PWM via software, which is tricky on a non-realtime system.</description><content>&lt;p>I was recently working on a project in which I wanted to drive a
simple &lt;a href="https://www.adafruit.com/product/160">piezo buzzer&lt;/a> attached to a GPIO pin on a Raspberry Pi. I
was already using the &lt;a href="https://pypi.python.org/pypi/RPi.GPIO">RPi.GPIO&lt;/a> module in my project so that seemed
like a logical place to start, but I ran into a few issues.&lt;/p>
&lt;p>You drive a piezo buzzer by generating a &lt;a href="https://learn.sparkfun.com/tutorials/pulse-width-modulation">PWM&lt;/a> signal with the
appropriate frequency. The &lt;code>RPi.GPIO&lt;/code> module implements PWM via
software, which is tricky on a non-realtime system. It&amp;rsquo;s difficult to
get the timing completely accurate, which results in sounds that are a
little wobbly at best. Since I&amp;rsquo;m simply generating tones with a
buzzer (rather than, say, controlling a servo) this is mostly just an
annoyance.&lt;/p>
&lt;p>The second more significant problem is that the &lt;code>RPi.GPIO&lt;/code> seems to be
buggy. After driving the buzzer a few times, my application would
invariable crash with a segmentation fault:&lt;/p>
&lt;pre>&lt;code>Program terminated with signal SIGSEGV, Segmentation fault.
#0 0x764cbc54 in output_gpio () from /usr/lib/python3/dist-packages/RPi/_GPIO.cpython-35m-arm-linux-gnueabihf.so
(gdb) bt
#0 0x764dac54 in output_gpio () from /usr/lib/python3/dist-packages/RPi/_GPIO.cpython-35m-arm-linux-gnueabihf.so
#1 0x764dc9bc in pwm_thread () from /usr/lib/python3/dist-packages/RPi/_GPIO.cpython-35m-arm-linux-gnueabihf.so
#2 0x00001000 in ?? ()
Backtrace stopped: previous frame identical to this frame (corrupt stack?)
(gdb)
&lt;/code>&lt;/pre>
&lt;p>At this point, I started looking for alternatives. One option would
be to implement my own software PWM solution in my Python code, but
that would suffer from the same timing issues as the &lt;code>RPi.GPIO&lt;/code>
implementation. I knew that the Raspberry Pi has support for hardware
PWM, so I went looking for information on how to make use of that
feature.&lt;/p>
&lt;p>I found &lt;a href="http://www.jumpnowtek.com/rpi/Using-the-Raspberry-Pi-Hardware-PWM-timers.html">this article&lt;/a> which describes how to enable &lt;a href="https://www.kernel.org/doc/Documentation/pwm.txt">kernel
support for hardware PWM&lt;/a>. You can read the article for details, but if
you have a Raspberry Pi 3 running kernel 4.9 or later, the answer
boils down to:&lt;/p>
&lt;ul>
&lt;li>Edit &lt;code>/boot/config.txt&lt;/code>.&lt;/li>
&lt;li>Add the line &lt;code>dtoverlay=pwm-2chan&lt;/code>&lt;/li>
&lt;li>Save the file.&lt;/li>
&lt;li>Reboot.&lt;/li>
&lt;/ul>
&lt;p>After rebooting your Pi and you will have access to hardware PWM on (BCM) pins
18 and 19. You will find a new &lt;code>sysfs&lt;/code> directory
&lt;code>/sys/class/pwm/pwmchip0&lt;/code>, which operates much like the &lt;a href="https://www.kernel.org/doc/Documentation/gpio/sysfs.txt">sysfs support
for gpio&lt;/a>: there is a special file &lt;code>export&lt;/code> that you use to gain
access to PWN pins. To access pin 18:&lt;/p>
&lt;pre>&lt;code>echo 0 &amp;gt; export
&lt;/code>&lt;/pre>
&lt;p>To access pin 19:&lt;/p>
&lt;pre>&lt;code>echo 1 &amp;gt; export
&lt;/code>&lt;/pre>
&lt;p>Running the above will result in two new directories appearing,
&lt;code>/sys/class/pwm/pwmchip0/pwm0&lt;/code> and &lt;code>/sys/class/pwm/pwmchip0/pwm1&lt;/code>.
Each of these directories contains special files for controlling the
PWM output. Of interest in this case are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>duty_cycle&lt;/code> - set the duty cycle of the PWM signal.&lt;/li>
&lt;li>&lt;code>enable&lt;/code> - enable (write a &lt;code>1&lt;/code>) or disable (write a &lt;code>0&lt;/code>) PWM output.&lt;/li>
&lt;li>&lt;code>period&lt;/code> - set the period of the PWM signal.&lt;/li>
&lt;/ul>
&lt;p>Both &lt;code>duty_cycle&lt;/code> and &lt;code>period&lt;/code> expect values in nanoseconds. So, for
example, to emit a 440Hz tone, you first need to calculate the period
for that frequency:&lt;/p>
&lt;pre>&lt;code>period = 1 / frequency = 1 / 440 = (approx) .00227272 seconds
&lt;/code>&lt;/pre>
&lt;p>Then convert that into nanoseconds:&lt;/p>
&lt;pre>&lt;code>period = .00227272 * 1e+9 = 2272720
&lt;/code>&lt;/pre>
&lt;p>For a 50% duty cycle, just dive that number by 2:&lt;/p>
&lt;pre>&lt;code>duty_cycle = 2272720 / 2 = 1136360
&lt;/code>&lt;/pre>
&lt;p>Now, echo those values to the appropriate &lt;code>sysfs&lt;/code> files:&lt;/p>
&lt;pre>&lt;code>echo $period &amp;gt; /sys/class/pwm/pwmchip0/pwm1/period
echo $duty_cycle &amp;gt; /sys/class/pwm/pwmchip0/pwm1/duty_cycle
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll want to set &lt;code>period&lt;/code> first. The value of &lt;code>duty_cycle&lt;/code> must
always be lower than &lt;code>period&lt;/code>, so if you try setting &lt;code>duty_cycle&lt;/code>
first it&amp;rsquo;s possible you will get an error.&lt;/p>
&lt;p>To actually generate the tone, you need to enable the output:&lt;/p>
&lt;pre>&lt;code>echo 1 &amp;gt; /sys/class/pwm/pwmchip0/pwm1/enable
&lt;/code>&lt;/pre>
&lt;p>This all works great, but there is one problem: you need to be &lt;code>root&lt;/code>
to perform any of the above operations. This matches the default
behavior of the GPIO subsystem, but in that case there are standard
&lt;a href="https://www.freedesktop.org/software/systemd/man/udev.html">udev&lt;/a> rules that take care of granting permission to members of the
&lt;code>gpio&lt;/code> group. I was hoping to use the same solution for PWM. There
is a set of udev rules proposed at
&lt;a href="https://github.com/raspberrypi/linux/issues/1983">https://github.com/raspberrypi/linux/issues/1983&lt;/a>, but due to a
&lt;a href="https://www.spinics.net/lists/linux-pwm/msg06081.html">kernel issue&lt;/a>, no udev events are sent when exporting pins so the
rules have no impact on permissions in the &lt;code>pwm0&lt;/code> and &lt;code>pwm1&lt;/code>
directories.&lt;/p>
&lt;p>Until the necessary patch has merged, I&amp;rsquo;ve worked around this issue by
creating a systemd unit that takes care of exporting the pins and
setting permissions correctly. The unit is very simple:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=Configure PWM permissions
Before=myapp.service
[Service]
Type=oneshot
ExecStart=/usr/bin/rpi-configure-pwm
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;p>And the corresponding script is:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
PWM=/sys/class/pwm/pwmchip0
echo 0 &amp;gt; ${PWM}/export
echo 1 &amp;gt; ${PWM}/export
chown -R root:gpio $PWM/*
chmod -R g+rwX $PWM/*
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>Before=myapp.service&lt;/code> in the unit file ensures that this unit
will run before my application starts up. To use the above, drop the
unit file into &lt;code>/etc/systemd/system/rpi-configure-pwm.service&lt;/code>, and
drop the script into &lt;code>/usr/bin/rpi-configure-pwm&lt;/code>. Don&amp;rsquo;t forget to
&lt;code>systemctl enable rpi-configure-pwm&lt;/code>.&lt;/p></content></item><item><title>Ansible for Infrastructure Testing</title><link>https://blog.oddbit.com/post/2017-08-02-ansible-for-infrastructure-tes/</link><pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-08-02-ansible-for-infrastructure-tes/</guid><description>At $JOB we often find ourselves at customer sites where we see the same set of basic problems that we have previously encountered elsewhere (&amp;ldquo;your clocks aren&amp;rsquo;t in sync&amp;rdquo; or &amp;ldquo;your filesystem is full&amp;rdquo; or &amp;ldquo;you haven&amp;rsquo;t installed a critical update&amp;rdquo;, etc). We would like a simple tool that could be run either by the customer or by our own engineers to test for and report on these common issues. Fundamentally, we want something that acts like a typical code test suite, but for infrastructure.</description><content>&lt;p>At &lt;code>$JOB&lt;/code> we often find ourselves at customer sites where we see the
same set of basic problems that we have previously encountered
elsewhere (&amp;ldquo;your clocks aren&amp;rsquo;t in sync&amp;rdquo; or &amp;ldquo;your filesystem is full&amp;rdquo;
or &amp;ldquo;you haven&amp;rsquo;t installed a critical update&amp;rdquo;, etc). We would like a
simple tool that could be run either by the customer or by our own
engineers to test for and report on these common issues.
Fundamentally, we want something that acts like a typical code test
suite, but for infrastructure.&lt;/p>
&lt;p>It turns out that Ansible is &lt;em>almost&lt;/em> the right tool for the job:&lt;/p>
&lt;ul>
&lt;li>It&amp;rsquo;s easy to write simple tests.&lt;/li>
&lt;li>It works well in distributed environments.&lt;/li>
&lt;li>It&amp;rsquo;s easy to extend with custom modules and plugins.&lt;/li>
&lt;/ul>
&lt;p>The only real problem is that Ansible has, by default, &amp;ldquo;fail fast&amp;rdquo;
behavior: once a task fails on a host, no more tasks will run on that
host. That&amp;rsquo;s great if you&amp;rsquo;re actually making configuration changes,
but for our purposes we are running a set of read-only independent
checks, and we want to know the success or failure of all of those
checks in a single operation (and in many situations we may not have
the option of correcting the underlying problem ourselves).&lt;/p>
&lt;p>In this post, I would like to discuss a few Ansible extensions I&amp;rsquo;ve
put together to make it more useful as an infrastructure testing tool.&lt;/p>
&lt;h2 id="the-ansible-assertive-project">The ansible-assertive project&lt;/h2>
&lt;p>The &lt;a href="https://github.com/larsks/ansible-assertive/">ansible-assertive&lt;/a> project contains two extensions for Ansible:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The &lt;code>assert&lt;/code> action plugin replaces Ansible&amp;rsquo;s native &lt;code>assert&lt;/code>
behavior with something more appropriate for infrastructure testing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;code>assertive&lt;/code> callback plugin modifies the output of &lt;code>assert&lt;/code>
tasks and collects and reports results.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The idea is that you write all of your tests using the &lt;code>assert&lt;/code>
plugin, which means you can run your playbooks in a stock environment
and see the standard Ansible fail-fast behavior, or you can activate
the &lt;code>assert&lt;/code> plugin from the ansible-assertive project and get
behavior more useful for infrastructure testing.&lt;/p>
&lt;h2 id="a-simple-example">A simple example&lt;/h2>
&lt;p>Ansible&amp;rsquo;s native &lt;code>assert&lt;/code> plugin will trigger a task failure when an
assertion evaluates to &lt;code>false&lt;/code>. Consider the following example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">hosts&lt;/span>: &lt;span style="color:#ae81ff">localhost&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">vars&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fruits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">oranges&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">lemons&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tasks&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">assert&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">that&lt;/span>: &amp;gt;-&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#39;apples&amp;#39; in fruits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#ae81ff">you have no apples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">assert&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">that&lt;/span>: &amp;gt;-&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &amp;#39;lemons&amp;#39; in fruits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#ae81ff">you have no lemons&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we run this in a stock Ansible environment, we will see the
following:&lt;/p>
&lt;pre tabindex="0">&lt;code>PLAY [localhost] ***************************************************************
TASK [Gathering Facts] *********************************************************
ok: [localhost]
TASK [assert] ******************************************************************
fatal: [localhost]: FAILED! =&amp;gt; {
&amp;#34;assertion&amp;#34;: &amp;#34;&amp;#39;apples&amp;#39; in fruits&amp;#34;,
&amp;#34;changed&amp;#34;: false,
&amp;#34;evaluated_to&amp;#34;: false,
&amp;#34;failed&amp;#34;: true,
&amp;#34;msg&amp;#34;: &amp;#34;you have no apples&amp;#34;
}
to retry, use: --limit @/home/lars/projects/ansible-assertive/examples/ex-005/playbook1.retry
PLAY RECAP *********************************************************************
localhost : ok=1 changed=0 unreachable=0 failed=1
&lt;/code>&lt;/pre>&lt;h2 id="a-modified-assert-plugin">A modified assert plugin&lt;/h2>
&lt;p>Let&amp;rsquo;s activate the &lt;code>assert&lt;/code> plugin in &lt;a href="https://github.com/larsks/ansible-assertive/">ansible-assertive&lt;/a>. We&amp;rsquo;ll
start by cloning the project into our local directory:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/larsks/ansible-assertive
&lt;/code>&lt;/pre>
&lt;p>And we&amp;rsquo;ll activate the plugin by creating an &lt;code>ansible.cfg&lt;/code> file with
the following content:&lt;/p>
&lt;pre>&lt;code>[defaults]
action_plugins = ./ansible-assertive/action_plugins
&lt;/code>&lt;/pre>
&lt;p>Now when we re-run the playbook we see that a failed assertion now
registers as &lt;code>changed&lt;/code> rather than &lt;code>failed&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>PLAY [localhost] ***************************************************************
TASK [Gathering Facts] *********************************************************
ok: [localhost]
TASK [assert] ******************************************************************
changed: [localhost]
TASK [assert] ******************************************************************
ok: [localhost]
PLAY RECAP *********************************************************************
localhost : ok=3 changed=1 unreachable=0 failed=0
&lt;/code>&lt;/pre>&lt;p>While that doesn&amp;rsquo;t look like much of a change, there are two things of
interest going on here. The first is that the &lt;code>assert&lt;/code> plugin
provides detailed information about the assertions specified in the
task; if we were to &lt;code>register&lt;/code> the result of the failed assertion and
display it in a &lt;code>debug&lt;/code> task, it would look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>TASK [debug] *******************************************************************
ok: [localhost] =&amp;gt; {
&amp;#34;apples&amp;#34;: {
&amp;#34;ansible_stats&amp;#34;: {
&amp;#34;aggregate&amp;#34;: true,
&amp;#34;data&amp;#34;: {
&amp;#34;assertions&amp;#34;: 1,
&amp;#34;assertions_failed&amp;#34;: 1,
&amp;#34;assertions_passed&amp;#34;: 0
},
&amp;#34;per_host&amp;#34;: true
},
&amp;#34;assertions&amp;#34;: [
{
&amp;#34;assertion&amp;#34;: &amp;#34;&amp;#39;apples&amp;#39; in fruits&amp;#34;,
&amp;#34;evaluated_to&amp;#34;: false
}
],
&amp;#34;changed&amp;#34;: true,
&amp;#34;failed&amp;#34;: false,
&amp;#34;msg&amp;#34;: &amp;#34;you have no apples&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;p>The &lt;code>assertions&lt;/code> key in the result dictionary contains of a list of
tests and their results. The &lt;code>ansible_stats&lt;/code> key contains metadata
that will be consumed by the custom statistics support in recent
versions of Ansible. If you have Ansible 2.3.0.0 or later, add
the following to the &lt;code>defaults&lt;/code> section of your &lt;code>ansible.cfg&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>show_custom_stats = yes
&lt;/code>&lt;/pre>&lt;p>With this feature enabled, your playbook run will conclude with:&lt;/p>
&lt;pre tabindex="0">&lt;code>CUSTOM STATS: ******************************************************************
localhost: { &amp;#34;assertions&amp;#34;: 2, &amp;#34;assertions_failed&amp;#34;: 1, &amp;#34;assertions_passed&amp;#34;: 1}
&lt;/code>&lt;/pre>&lt;h2 id="a-callback-plugin-for-better-output">A callback plugin for better output&lt;/h2>
&lt;p>The &lt;code>assertive&lt;/code> callback plugin provided by the &lt;a href="https://github.com/larsks/ansible-assertive/">ansible-assertive&lt;/a>
project will provide more useful output concerning the result of
failed assertions. We activate it by adding the following to our
&lt;code>ansible.cfg&lt;/code>:&lt;/p>
&lt;pre>&lt;code>callback_plugins = ./ansible-assertive/callback_plugins
stdout_callback = assertive
&lt;/code>&lt;/pre>
&lt;p>Now when we run our playbook we see:&lt;/p>
&lt;pre tabindex="0">&lt;code>PLAY [localhost] ***************************************************************
TASK [Gathering Facts] *********************************************************
ok: [localhost]
TASK [assert] ******************************************************************
failed: [localhost] ASSERT(&amp;#39;apples&amp;#39; in fruits)
failed: you have no apples
TASK [assert] ******************************************************************
passed: [localhost] ASSERT(&amp;#39;lemons&amp;#39; in fruits)
PLAY RECAP *********************************************************************
localhost : ok=3 changed=1 unreachable=0 failed=0
&lt;/code>&lt;/pre>&lt;h2 id="machine-readable-statistics">Machine readable statistics&lt;/h2>
&lt;p>The above is nice but is still primarily human-consumable. What if we
want to collect test statistics for machine processing (maybe we want
to produce a nicely formatted report of some kind, or maybe we want to
aggregate information from multiple test runs, or maybe we want to
trigger some action in the event there are failed tests, or&amp;hellip;)? You
can ask the &lt;code>assertive&lt;/code> plugin to write a YAML format document with
this information by adding the following to your &lt;code>ansible.cfg&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[assertive]
results = testresult.yml
&lt;/code>&lt;/pre>
&lt;p>After running our playbook, this file would contain:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">groups&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">hosts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">localhost&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">stats&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_failed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_passed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_skipped&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">assertions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">test&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;apples&amp;#39;&amp;#39; in fruits&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testresult&lt;/span>: &lt;span style="color:#ae81ff">failed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#ae81ff">you have no apples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testresult&lt;/span>: &lt;span style="color:#ae81ff">failed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testtime&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;2017-08-04T21:20:58.624789&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">assertions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">test&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;lemons&amp;#39;&amp;#39; in fruits&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testresult&lt;/span>: &lt;span style="color:#ae81ff">passed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#ae81ff">All assertions passed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testresult&lt;/span>: &lt;span style="color:#ae81ff">passed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testtime&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;2017-08-04T21:20:58.669144&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">localhost&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">stats&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_failed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_passed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_skipped&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">stats&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_failed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_passed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_skipped&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">timing&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">test_finished_at&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;2017-08-04T21:20:58.670802&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">test_started_at&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;2017-08-04T21:20:57.918412&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With these tools it becomes much easier to design playbooks for
testing your infrastructure.&lt;/p></content></item><item><title>Better bulk filtering for Gmail</title><link>https://blog.oddbit.com/post/2017-07-07-better-bulk-filtering-for-gmai/</link><pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-07-07-better-bulk-filtering-for-gmai/</guid><description>I use Gmail extensively for my personal email, and recently my workplace has been migrated over to Gmail as well. I find that for my work email I rely much more extensively on filters and labels to organize things (like zillions of internal and upstream mailing lists), and that has posed some challenges. While Gmail is in general fairly snappy, attempting to apply an action to thousands of messages (for example, trying to mark 16000 messages as &amp;ldquo;read&amp;rdquo;, or applying a new filter to all your existing messages) results in a very poor experience: it is not possible to interact with Gmail (in the same tab) while the action is running, and frequently actions will timeout.</description><content>&lt;p>I use Gmail extensively for my personal email, and recently my
workplace has been migrated over to Gmail as well. I find that for my
work email I rely much more extensively on filters and labels to
organize things (like zillions of internal and upstream mailing
lists), and that has posed some challenges. While Gmail is in general
fairly snappy, attempting to apply an action to thousands of messages
(for example, trying to mark 16000 messages as &amp;ldquo;read&amp;rdquo;, or applying a
new filter to all your existing messages) results in a very poor
experience: it is not possible to interact with Gmail (in the same
tab) while the action is running, and frequently actions will timeout.&lt;/p>
&lt;p>Fortunately, we can take advantage of Gmail&amp;rsquo;s IMAP interface to
overcome most of these obstacles. The naive approach won&amp;rsquo;t work: If
you attempt to perform an IMAP action against thousands of messages
you will encounter the same timeouts you see with the browser. The
big advantage to IMAP is that it makes it easy, with a little bit of
code, to split a large operation up into smaller chunks. Gmail
provides a few [IMAP extensions][] that provide us with a mechanism
for accessing Gmail-specific features, such as the rich search syntax
and support for arbitrary labels.&lt;/p>
&lt;p>I&amp;rsquo;ve written a small tool to take care of this; you can find it in my
&lt;a href="https://github.com/larsks/gmailfilters">gmailfilters&lt;/a> repository on GitHub. The project provides two
commands, the &lt;code>gmf bulk-filter&lt;/code> command, which I will discuss here,
and the &lt;code>gmf manage-filters&lt;/code> command, which can translate between a
simpler YAML syntax and the XML syntax used by Gmail&amp;rsquo;s filter
import/export. I may write about that in a future post.&lt;/p>
&lt;h2 id="installing-gmf">Installing gmf&lt;/h2>
&lt;p>The &lt;code>gmailfilters&lt;/code> project is a standard Python package. You can
install it directly from GitHub like this:&lt;/p>
&lt;pre>&lt;code>pip install git+https://github.com/larsks/gmailfilters
&lt;/code>&lt;/pre>
&lt;p>This will install the &lt;code>gmf&lt;/code> command, which provides the following
subcommands:&lt;/p>
&lt;ul>
&lt;li>&lt;code>bulk-filter&lt;/code> &amp;ndash; a command line tool for applying bulk actions to
Gmail messages.&lt;/li>
&lt;li>&lt;code>manage-filters&lt;/code> &amp;ndash; translate between a YAML filter syntax and the
XML syntax used by Gmail for filter import/export.&lt;/li>
&lt;li>&lt;code>apply-filters&lt;/code> &amp;ndash; read the YAML filters file and apply (a supported
subset of) the actions to your mail.&lt;/li>
&lt;/ul>
&lt;p>This article is going to focus on the &lt;code>gmf bulk-filter&lt;/code> command.&lt;/p>
&lt;h2 id="configuring-gmf">Configuring gmf&lt;/h2>
&lt;p>Once installed, you will need to create a configuration file. By
default, &lt;code>gmf&lt;/code> will look for a file named &lt;code>gmailfilters.yml&lt;/code> located
in your current directory or in your &lt;code>$XDG_CONFIG_HOME&lt;/code> directory,
typically &lt;code>$HOME/.config&lt;/code>. The configuration file looks something
like this:&lt;/p>
&lt;pre>&lt;code>accounts:
default:
host: imap.gmail.com
ssl: true
username: username@example.com
password: secret-password
&lt;/code>&lt;/pre>
&lt;p>You can have multiple accounts in the file; &lt;code>gmf&lt;/code> will use the one
named &lt;code>default&lt;/code> by default.&lt;/p>
&lt;h2 id="using-the-bulk-filter-command">Using the bulk-filter command&lt;/h2>
&lt;p>I have a Gmail filter that applies the label &lt;code>topic/containers&lt;/code> to any
mail matching the search &lt;code>{docker container kubernetes lxc runc}&lt;/code>. I
want to apply this filter to all my existing messages, and I want to
mark all matching messages as read so that I can identity new matches.
I can use the &lt;code>bulk-filter&lt;/code> tool to accomplish this task using the
&lt;code>--label&lt;/code> and &lt;code>--flag&lt;/code> options:&lt;/p>
&lt;pre>&lt;code>gmf bulk-filter --query '{docker container kubernetes lxc runc}' \
--label topic/containers --flag seen
&lt;/code>&lt;/pre>
&lt;p>It turns out I had close to 15000 freecycle messages gathering dust in
my account. The messages were already labeled with the &lt;code>freecycle&lt;/code>
label. We can weed those out like this:&lt;/p>
&lt;p>gmf bulk-filter &amp;ndash;query &amp;rsquo;label:freecycle&amp;rsquo; &amp;ndash;trash&lt;/p>
&lt;p>The &lt;code>--trash&lt;/code> option acts like Gmail&amp;rsquo;s &amp;ldquo;move to trash&amp;rdquo; option. You
can also use &lt;code>--delete&lt;/code>, which will use an IMAP delete operation, but
the behavior of an IMAP delete is controlled by your Gmail
configuration (it may simply archive a message, or it may delete it
completely).&lt;/p>
&lt;p>The &lt;code>bulk-filter&lt;/code> tool can also be used to remove labels by preceding
them with a &lt;code>-&lt;/code>. For example, if I want to find all messages labeled
&lt;code>fedora-devel-list&lt;/code> and modify them so that they are labeled &lt;code>list&lt;/code>,
&lt;code>list/fedora&lt;/code>, and &lt;code>list/fedora/devel&lt;/code> I can run:&lt;/p>
&lt;pre>&lt;code>gmf bulk-filter --query 'label:fedora-devel-list' \
--label list --label list/fedora --label list/fedora-devel
--label='-fedora-devel-list'
&lt;/code>&lt;/pre>
&lt;p>This exposes a quirk of Python&amp;rsquo;s &lt;code>argparse&lt;/code> argument parser: if the
argument to an option starts with &lt;code>-&lt;/code>, argparse assumes that you&amp;rsquo;ve
made a mistake unless you explicitly attach it to the option with &lt;code>=&lt;/code>.&lt;/p>
&lt;p>By default, the &lt;code>bulk-filter&lt;/code> tool operates on the &lt;code>[Gmail]/All mail&lt;/code>
folder, which contains all of your messages. You can limit it to
specific folders instead by providing an optional list of folders
(that may contain wildcards). For example, if I want to perform the
above labelling operation only on internal company mailing lists, I
could limit it like this:&lt;/p>
&lt;pre>&lt;code>gmf bulk-filter --query '{docker container kubernetes lxc runc}' \
--label topic/containers list/internal/*
&lt;/code>&lt;/pre>
&lt;p>This assumes, of course, that you have filters in place that apply
labels nested under &lt;code>list/internal&lt;/code> to internal company mailing lists.&lt;/p>
&lt;p>The default behavior of &lt;code>bulk-filter&lt;/code> is to operate on 200 messages at
a time. You can change this using the &lt;code>--size&lt;/code> parameter, for
example:&lt;/p>
&lt;pre>&lt;code>gmf bulk-filter --size 50 ...&lt;/code>&lt;/pre></content></item><item><title>OpenStack, Containers, and Logging</title><link>https://blog.oddbit.com/post/2017-06-14-openstack-containers-and-loggi/</link><pubDate>Wed, 14 Jun 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-06-14-openstack-containers-and-loggi/</guid><description>I&amp;rsquo;ve been thinking about logging in the context of OpenStack and containerized service deployments. I&amp;rsquo;d like to lay out some of my thoughts on this topic and see if people think I am talking crazy or not.
There are effectively three different mechanisms that an application can use to emit log messages:
Via some logging-specific API, such as the legacy syslog API By writing a byte stream to stdout/stderr By writing a byte stream to a file A substantial advantage to the first mechanism (using a logging API) is that the application is logging messages rather than bytes.</description><content>&lt;p>I&amp;rsquo;ve been thinking about logging in the context of OpenStack and containerized service deployments. I&amp;rsquo;d like to lay out some of my thoughts on this topic and see if people think I am talking crazy or not.&lt;/p>
&lt;p>There are effectively three different mechanisms that an application can use to emit log messages:&lt;/p>
&lt;ul>
&lt;li>Via some logging-specific API, such as the legacy syslog API&lt;/li>
&lt;li>By writing a byte stream to stdout/stderr&lt;/li>
&lt;li>By writing a byte stream to a file&lt;/li>
&lt;/ul>
&lt;p>A substantial advantage to the first mechanism (using a logging API) is that the application is logging &lt;em>messages&lt;/em> rather than &lt;em>bytes&lt;/em>. This means that if you log a message containing embedded newlines (e.g., python or java tracebacks), you can collect that as a single message rather than having to impose some sort of structure on the byte stream after the fact in order to reconstruct those message.&lt;/p>
&lt;p>Another advantage to the use of a logging API is that whatever is receiving logs from your application may be able to annotate the message in various interesting ways.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>We&amp;rsquo;re probably going to need to support all three of the above mechanisms. Some applications (such as &lt;code>haproxy&lt;/code>) will only log to syslog. Others may only log to files (such as &lt;code>mariadb&lt;/code>), and still others may only log to stdout.&lt;/p>
&lt;h2 id="comparing-different-log-mechanisms">Comparing different log mechanisms&lt;/h2>
&lt;h3 id="logging-via-syslog">Logging via syslog&lt;/h3>
&lt;p>In RHEL, the &lt;code>journald&lt;/code> process is what listens to &lt;code>/dev/log&lt;/code>. If you bind mount journald&amp;rsquo;s &lt;code>/dev/log&lt;/code> inside a container and then run the following Python code inside that container&amp;hellip;&lt;/p>
&lt;pre>&lt;code>import logging
import logging.handlers
handler = logging.handlers.SysLogHandler(address='/dev/log')
log = logging.getLogger(__name__)
log.setLevel('DEBUG')
log.addHandler(handler)
log.warning('This is a test')
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;you will find that your simple log message has been annotated with
a variety of useful metadata (the output below is the result of
running &lt;code>journalctl -o verbose ...&lt;/code>):&lt;/p>
&lt;pre>&lt;code>Wed 2017-06-14 12:35:57.577061 EDT [s=dc1dd9d61cf045e991f265aa17c5af03;i=6eb6e;b=0d9dc78871c34f43a4a6c27f43cf4167;m=a171206ec6;t=551ee258c6492;x=4e3c71faa52ba9d8]
_BOOT_ID=0d9dc78871c34f43a4a6c27f43cf4167
_MACHINE_ID=229916fba5b54252ad4d08efbc581213
_HOSTNAME=lkellogg-pc0dzzve
_UID=0
_GID=0
_SYSTEMD_SLICE=-.slice
_TRANSPORT=syslog
PRIORITY=4
SYSLOG_FACILITY=1
_EXE=/usr/bin/python3.5
_CAP_EFFECTIVE=a80425fb
_SELINUX_CONTEXT=system_u:system_r:unconfined_service_t:s0
_COMM=python3
MESSAGE=This is a test
_PID=13849
_CMDLINE=python3 logtest.py
_SYSTEMD_CGROUP=/docker/7ed1e97d5bb4076caf99393ae3f88b07102a26b0ade2176ed07890bee9a84d24
_SOURCE_REALTIME_TIMESTAMP=1497458157577061
&lt;/code>&lt;/pre>
&lt;p>There are several items of interest there:&lt;/p>
&lt;ul>
&lt;li>A high resolution timestamp&lt;/li>
&lt;li>The kernel cgroup, which corresponds to the docker container id and thus uniquely identifies the container that originated the message&lt;/li>
&lt;li>The executable path inside the container that generated the message&lt;/li>
&lt;li>The machine id, which uniquely identifies the host&lt;/li>
&lt;/ul>
&lt;p>By logging via syslog you have removed the necessity of either (a) handling log rotation in your application or (b) handling log rotation in your container or (c) having to communicate log rotation configuration from the container to the host. Additionally, you can rely on journald to take care of rate limiting and log size management to prevent a broken application from performing a local DOS of the server.&lt;/p>
&lt;h3 id="logging-via-stdoutstderr">Logging via stdout/stderr&lt;/h3>
&lt;p>Applications that write a byte stream to stdout/stderr will have their output handled by the Docker log driver. If we run Docker with the &lt;code>journald&lt;/code> log driver (using the &lt;code>--log-driver=journald&lt;/code> option to the Docker server), then Docker will add metadata lines read from stdout/stderr. For example, if we run&amp;hellip;&lt;/p>
&lt;pre>&lt;code>docker run fedora echo This is a test.
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;then our journal will contain:&lt;/p>
&lt;pre>&lt;code>Wed 2017-06-14 12:46:45.511515 EDT [s=dc1dd9d61cf045e991f265aa17c5af03;i=6ee72;b=0d9dc78871c34f43a4a6c27f43cf4167;m=a197bf222b;t=551ee4c2b17f7;x=e7c1a220c93ef3cf]
_BOOT_ID=0d9dc78871c34f43a4a6c27f43cf4167
_MACHINE_ID=229916fba5b54252ad4d08efbc581213
_HOSTNAME=lkellogg-pc0dzzve
PRIORITY=6
_TRANSPORT=journal
_UID=0
_GID=0
_CAP_EFFECTIVE=3fffffffff
_SYSTEMD_SLICE=system.slice
_SELINUX_CONTEXT=system_u:system_r:unconfined_service_t:s0
_COMM=dockerd
_EXE=/usr/bin/dockerd
_SYSTEMD_CGROUP=/system.slice/docker.service
_SYSTEMD_UNIT=docker.service
_PID=14309
_CMDLINE=/usr/bin/dockerd -G docker --dns 172.23.254.1 --log-driver journald -s overlay2
MESSAGE=This is a test.
CONTAINER_NAME=happy_euclid
CONTAINER_TAG=82b87e8902e8
CONTAINER_ID=82b87e8902e8
CONTAINER_ID_FULL=82b87e8902e8ac36f3365012ef10c66444fbb8c00e8cec7d7c2a14c05b054127
&lt;/code>&lt;/pre>
&lt;p>Like the messages logged via syslog, this also containers information that identifies the source container. It does not identify the particular binary responsible for emitting the message.&lt;/p>
&lt;h3 id="logging-to-a-file">Logging to a file&lt;/h3>
&lt;p>When logging to a file, the system is unable to add any metadata for us automatically. We can derive similar information by logging to a container-specific location (&lt;code>/var/log/CONTAINERNAME/...&lt;/code>, for example), or by configuring our application to include specific information in the log messages, but ultimately this is the least information-rich mechanism available to us. Furthermore, it necessitates that we configure some sort of container-aware log rotation strategy to avoid eating up all the available disk space over time.&lt;/p>
&lt;h2 id="log-collection">Log collection&lt;/h2>
&lt;p>Our goal is not simply to make log messages available locally. In
general, we also want to aggregate log messages from several machines
into a single location where we can perform various sorts of queries,
analysis, and visualization. There are a number of solutions in place
for getting logs off a local server to a central collector, including both &lt;a href="http://www.fluentd.org/">fluentd&lt;/a> and &lt;a href="http://www.rsyslog.com/">rsyslog&lt;/a>.&lt;/p>
&lt;p>In the context of the above discussion, it turns out that &lt;code>rsyslog&lt;/code> has some very desirable features. In particular, the &lt;a href="http://www.rsyslog.com/doc/v8-stable/configuration/modules/imjournal.html">imjournal&lt;/a> input module has support for reading structured messages from journald and exporting those to a remote collector (such as &lt;a href="https://www.elastic.co/">ElasticSearch&lt;/a>) with their structure intact. Fluentd does not ship with journald support as a core plugin.&lt;/p>
&lt;p>Rsyslog version 8.x and later have a rich language for filtering, annotating, and otherwise modifying log messages that would allow us to do things such as add host-specific tags to messages, normalize log messages from applications with poorly designed log messages, and perform other transformations before sending them on to a remote collector.&lt;/p>
&lt;p>For example, we would ensure that messages from containerized services logged via syslog &lt;em>or&lt;/em> via stdout/stderr have a &lt;code>CONTAINER_ID_FULL&lt;/code> field with something like the following:&lt;/p>
&lt;pre>&lt;code>if re_match($!_SYSTEMD_CGROUP, &amp;quot;^/docker/&amp;quot;) then {
set $!CONTAINER_ID_FULL = re_extract($!_SYSTEMD_CGROUP, &amp;quot;^/docker/(.*)&amp;quot;, 0, 1, &amp;quot;unknown&amp;quot;);
}
&lt;/code>&lt;/pre>
&lt;p>This matches the &lt;code>_SYSTEMD_CGROUP&lt;/code> field of the message, extracts the container id, and uses that to set the &lt;code>CONTAINER_ID_FULL&lt;/code> property on the message.&lt;/p>
&lt;h2 id="recommendations">Recommendations&lt;/h2>
&lt;ol>
&lt;li>Provide a consistent logging environment to containerized services. Provide every container both with &lt;code>/dev/log&lt;/code> and a container-specific host directory mounted on &lt;code>/var/log&lt;/code>.&lt;/li>
&lt;li>For applications that support logging to syslog (such as all consumers of &lt;code>oslo.log&lt;/code>), configure them to log exclusively via syslog.&lt;/li>
&lt;li>For applications that are unable to log via syslog but are able to log to stdout/stderr, ensure that Docker is using the &lt;code>journald&lt;/code> log driver.&lt;/li>
&lt;li>For applications that can only log to files, configure rsyslog on the host to read those files using the &lt;a href="http://www.rsyslog.com/doc/v8-stable/configuration/modules/imfile.html">imfile&lt;/a> input plugin.&lt;/li>
&lt;li>Use rsyslog on the host to forward structured messages to a remote collector.&lt;/li>
&lt;/ol></content></item><item><title>FAA Cannot Require Drone Registration</title><link>https://blog.oddbit.com/post/2017-05-25-faa-cannot-require-drone-regis/</link><pubDate>Thu, 25 May 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-05-25-faa-cannot-require-drone-regis/</guid><description>This is now old news if you&amp;rsquo;re already following the drone industry, but if you&amp;rsquo;re not, I&amp;rsquo;d like to highlight a recent decision made by the US Court of Appeals regarding the FAA&amp;rsquo;s drone registration requirements.
To place this in context, back in 2015 the FAA established a new set of regulations (the &amp;ldquo;Registration Rule&amp;rdquo;) requiring anyone with a UAV (&amp;ldquo;unmanned aerial vehicle&amp;rdquo;, or &amp;ldquo;drone&amp;rdquo;) weighing between 0.5 and 55 lbs to register with the FAA.</description><content>&lt;p>This is now old news if you&amp;rsquo;re already following the drone industry,
but if you&amp;rsquo;re not, I&amp;rsquo;d like to highlight a recent decision made by the
US Court of Appeals regarding the FAA&amp;rsquo;s drone registration
requirements.&lt;/p>
&lt;p>To place this in context, back in 2015 the FAA established a new set
of regulations (the &amp;ldquo;Registration Rule&amp;rdquo;) requiring anyone with a UAV
(&amp;ldquo;unmanned aerial vehicle&amp;rdquo;, or &amp;ldquo;drone&amp;rdquo;) weighing between 0.5 and 55
lbs to register with the FAA. Unfortunately, the 2012 &lt;a href="https://www.congress.gov/bill/112th-congress/house-bill/658/text">FAA
Modernization and Reform Act says&lt;/a> says (in section 336(a)):&lt;/p>
&lt;blockquote>
&lt;p>In General&amp;hellip;the Administrator of the Federal Aviation
Administration may not promulgate any rule or regulation regarding
a model aircraft, or an aircraft being developed as a model
aircraft&amp;hellip;&lt;/p>
&lt;/blockquote>
&lt;p>John Taylor, a hobbiest in Washington, D.C, challenged the new
registration regulations in court and was handed a clear victory.&lt;/p>
&lt;p>&lt;a href="https://www.cadc.uscourts.gov/internet/opinions.nsf/FA6F27FFAA83E20585258125004FBC13/%24file/15-1495-1675918.pdf">The Court of Appeals decision&lt;/a> boils down to &amp;ldquo;when the law says
you &amp;lsquo;may not promulgate any rule regarding model aircraft&amp;rsquo;, that
means&amp;hellip;&amp;lsquo;you may not promulgate any rule regarding model aircraft&amp;rsquo;.
The decision is interesting in its simplicity. There is no arguing
about legal nuance here; it&amp;rsquo;s a clear case of the Court telling the
FAA &amp;ldquo;you&amp;rsquo;re wrong&amp;rdquo;.&lt;/p>
&lt;p>The following quote from the decision really sums that up (highlights
are mine):&lt;/p>
&lt;blockquote>
&lt;p>In short, the 2012 FAA Modernization and Reform Act provides that
the FAA “may not promulgate any rule or regulation regarding a model
aircraft,” yet the FAA’s 2015 Registration Rule is a “rule or
regulation regarding a model aircraft.” &lt;strong>Statutory interpretation
does not get much simpler.&lt;/strong> The Registration Rule is unlawful as
applied to model aircraft.&lt;/p>
&lt;/blockquote>
&lt;p>The FAA attempted to describe the registration requirements as simply
a &amp;ldquo;reinterpretation&amp;rdquo; of existing rules. That is, they claimed to be
using their discretion to apply an existing registration rule for
&amp;ldquo;aircraft&amp;rdquo; in general to model aircraft:&lt;/p>
&lt;blockquote>
&lt;p>Specifically, the FAA notes that, under longstanding statutes,
aircraft are statutorily required to register before operation.
&amp;hellip;But the FAA has never previously interpreted that registration
requirement to apply to model aircraft. The FAA responds that
nothing in the 2012 FAA Modernization and Reform Act prevents the
FAA from changing course and applying that registration requirement
to model aircraft now.&lt;/p>
&lt;/blockquote>
&lt;p>The Court disagreed with this characterization in no uncertain terms:&lt;/p>
&lt;blockquote>
&lt;p>We disagree. The Registration Rule does not merely announce an
intent to enforce a pre-existing statutory requirement. The
Registration Rule is a rule that creates a new regulatory regime for
model aircraft. The new regulatory regime includes a “new
registration process” for online registration of model
aircraft&amp;hellip;The new regulatory regime imposes new requirements – to
register, to pay fees, to provide information, and to display
identification on people who previously had no obligation to engage
with the FAA.&lt;/p>
&lt;p>In short, the Registration Rule is a rule regarding model aircraft.&lt;/p>
&lt;/blockquote>
&lt;p>And ultimately vacated the Registration Rule:&lt;/p>
&lt;blockquote>
&lt;p>The FAA’s Registration Rule violates Section 336 of the FAA
Modernization and Reform Act. We grant Taylor’s petition for review
of the Registration Rule, and we vacate the Registration Rule to the
extent it applies to model aircraft.&lt;/p>
&lt;/blockquote>
&lt;p>Taylor also challenged the FAA&amp;rsquo;s flight restrictions in the
Washington, D.C. area, but lost that challenge on a technicality (&amp;ldquo;A
person seeking to challenge an FAA order must file the challenge
within 60 days of the order’s issuance.&amp;rdquo;).&lt;/p></content></item><item><title>Making sure your Gerrit changes aren't broken</title><link>https://blog.oddbit.com/post/2017-01-22-making-sure-your-gerrit-change/</link><pubDate>Sun, 22 Jan 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-01-22-making-sure-your-gerrit-change/</guid><description>It&amp;rsquo;s a bit of an embarrassment when you submit a review to Gerrit only to have it fail CI checks immediately because of something as simple as a syntax error or pep8 failure that you should have caught yourself before submitting&amp;hellip;but you forgot to run your validations before submitting the change.
In many cases you can alleviate this through the use of the git pre-commit hook, which will run every time you commit changes locally.</description><content>&lt;p>It&amp;rsquo;s a bit of an embarrassment when you submit a review to Gerrit only
to have it fail CI checks immediately because of something as simple
as a syntax error or pep8 failure that you should have caught yourself
before submitting&amp;hellip;but you forgot to run your validations before
submitting the change.&lt;/p>
&lt;p>In many cases you can alleviate this through the use of the git
&lt;code>pre-commit&lt;/code> hook, which will run every time you commit changes
locally. You can have the hook run &lt;code>tox&lt;/code> or whatever tool your
project uses for validation on every commit. This works okay for
simple cases, but if the validation takes more than a couple of
seconds the delay can be disruptive to the flow of your work.&lt;/p>
&lt;p>What you want is something that stays out of the way while you are
working locally, but that will prevent you from submitting something
for review that&amp;rsquo;s going to fail CI immediately. If you are using the
&lt;a href="http://docs.openstack.org/infra/git-review/">git-review&lt;/a> tool to interact with Gerrit (and if you&amp;rsquo;re not, you
should be), you&amp;rsquo;re in luck! The &lt;code>git-review&lt;/code> tool supports a
&lt;code>pre-review&lt;/code> hook that does exactly what we want. &lt;code>git-review&lt;/code> looks
for hooks in a global location (&lt;code>~/.config/git-review/hooks&lt;/code>) and in a
per-project location (in &lt;code>.git/hooks/&lt;/code>). As with standard Git hooks,
the &lt;code>pre-review&lt;/code> hook must be executable (i.e., &lt;code>chmod u+x .git/hooks/pre-review&lt;/code>).&lt;/p>
&lt;p>The &lt;code>pre-review&lt;/code> script will be run before attempting to submit your
changes to Gerrit. If the script exits successfully, the output is
hidden and your changes will be submitted normally. If the hook
fails, you will see output along the lines of&amp;hellip;&lt;/p>
&lt;pre>&lt;code>Custom script execution failed.
The following command failed with exit code 1
&amp;quot;.git/hooks/pre-review&amp;quot;
-----------------------
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;followed by the output of the &lt;code>pre-review&lt;/code> script.&lt;/p>
&lt;p>For my work on the &lt;a href="https://github.com/openstack/tripleo-quickstart">tripleo-quickstart&lt;/a> project, the contents of my
&lt;code>.git/hooks/pre-review&lt;/code> script is as simple as:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
tox
&lt;/code>&lt;/pre></content></item><item><title>Exploring YAQL Expressions</title><link>https://blog.oddbit.com/post/2016-08-11-exploring-yaql-expressions/</link><pubDate>Thu, 11 Aug 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-08-11-exploring-yaql-expressions/</guid><description>The Newton release of Heat adds support for a yaql intrinsic function, which allows you to evaluate yaql expressions in your Heat templates. Unfortunately, the existing yaql documentation is somewhat limited, and does not offer examples of many of yaql&amp;rsquo;s more advanced features.
I am working on a Fluentd composable service for TripleO. I want to allow each service to specify a logging source configuration fragment, for example:
parameters: NovaAPILoggingSource: type: json description: Fluentd logging configuration for nova-api.</description><content>&lt;p>The Newton release of &lt;a href="https://wiki.openstack.org/wiki/Heat">Heat&lt;/a> adds support for a &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#yaql">yaql&lt;/a>
intrinsic function, which allows you to evaluate &lt;a href="https://yaql.readthedocs.io/en/latest/">yaql&lt;/a> expressions
in your Heat templates. Unfortunately, the existing yaql
documentation is somewhat limited, and does not offer examples of many
of yaql&amp;rsquo;s more advanced features.&lt;/p>
&lt;p>I am working on a &lt;a href="http://www.fluentd.org/">Fluentd&lt;/a> composable service for &lt;a href="https://wiki.openstack.org/wiki/TripleO">TripleO&lt;/a>. I
want to allow each service to specify a logging source configuration
fragment, for example:&lt;/p>
&lt;pre>&lt;code>parameters:
NovaAPILoggingSource:
type: json
description: Fluentd logging configuration for nova-api.
default:
tag: openstack.nova.api
type: tail
format: |
/(?&amp;lt;time&amp;gt;\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d+) (?&amp;lt;pid&amp;gt;\d+) (?&amp;lt;priority&amp;gt;\S+) (?&amp;lt;message&amp;gt;.*)/
path: /var/log/nova/nova-api.log
pos_file: /var/run/fluentd/openstack.nova.api.pos
&lt;/code>&lt;/pre>
&lt;p>This generally works, but several parts of this fragment are going to
be the same across all OpenStack services. I wanted to reduce the
above to just the unique attributes, which would look something like:&lt;/p>
&lt;pre>&lt;code>parameters:
NovaAPILoggingSource:
type: json
description: Fluentd logging configuration for nova-api.
default:
tag: openstack.nova.api
path: /var/log/nova/nova-api.log
&lt;/code>&lt;/pre>
&lt;p>This would ultimately give me a list of dictionaries of the form:&lt;/p>
&lt;pre>&lt;code>[
{
&amp;quot;tag&amp;quot;: &amp;quot;openstack.nova.api&amp;quot;,
&amp;quot;path&amp;quot;: &amp;quot;/var/log/nova/nova-api.log&amp;quot;
},
{
&amp;quot;tag&amp;quot;: &amp;quot;openstack.nova.scheduler&amp;quot;,
&amp;quot;path&amp;quot;: &amp;quot;/var/log/nova/nova-scheduler.log&amp;quot;
}
]
&lt;/code>&lt;/pre>
&lt;p>I want to iterate over this list, adding default values for attributes
that are not explicitly provided.&lt;/p>
&lt;p>The yaql language has a &lt;code>select&lt;/code> function, somewhat analagous to the
SQL &lt;code>select&lt;/code> statement, that can be used to construct a new data
structure from an existing one. For example, given the above data in
a parameter called &lt;code>sources&lt;/code>, I could write:&lt;/p>
&lt;pre>&lt;code>outputs:
sources:
yaql:
data:
sources: {get_param: sources}
expression: &amp;gt;
$.data.sources.select({
'path' =&amp;gt; $.path,
'tag' =&amp;gt; $.tag,
'type' =&amp;gt; $.get('type', 'tail')})
&lt;/code>&lt;/pre>
&lt;p>This makes use of the &lt;code>.get&lt;/code> method to insert a default value of
&lt;code>tail&lt;/code> for the &lt;code>type&lt;/code> attribute for items that don&amp;rsquo;t specify it
explicitly. This would produce a list that looks like:&lt;/p>
&lt;pre>&lt;code>[
{
&amp;quot;path&amp;quot;: &amp;quot;/var/log/nova/nova-api.log&amp;quot;,
&amp;quot;tag&amp;quot;: &amp;quot;openstack.nova.api&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;tail&amp;quot;
},
{
&amp;quot;path&amp;quot;: &amp;quot;/var/log/nova/nova-scheduler.log&amp;quot;,
&amp;quot;tag&amp;quot;: &amp;quot;openstack.nova.scheduler&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;tail&amp;quot;
}
]
&lt;/code>&lt;/pre>
&lt;p>That works fine, but what if I want to parameterize the default value
such that it can be provided as part of the template? I wanted to be
able to pass the yaql expression something like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>outputs:
sources:
yaql:
data:
sources: {get_param: sources}
default_type: tail
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then within the yaql expression, insert the value of
&lt;code>default_type&lt;/code> into items that don&amp;rsquo;t provide an explicit value for the
&lt;code>type&lt;/code> attribute.&lt;/p>
&lt;p>This is trickier than it might sound at first because within the
context of the &lt;code>select&lt;/code> method, &lt;code>$&lt;/code> is bound to the &lt;em>local&lt;/em> context,
which will be an individual item from the list. So while I can ask
for &lt;code>$.path&lt;/code>, there&amp;rsquo;s no way to refer to items from the top-level
context. Or is there?&lt;/p>
&lt;p>The &lt;a href="https://yaql.readthedocs.io/en/latest/getting_started.html#operators">operators&lt;/a> documentation for yaql mentions the &amp;ldquo;context pass&amp;rdquo;
operator, &lt;code>-&amp;gt;&lt;/code>, but doesn&amp;rsquo;t provide any examples of how it can be
used. It turns out that this operator will be the key to our solution.
But before we look at that in more detail, we need to introduce the
&lt;code>let&lt;/code> statement, which can be used to define variables. The &lt;code>let&lt;/code>
statement isn&amp;rsquo;t mentioned in the documentation at all, but it looks
like this:&lt;/p>
&lt;pre>&lt;code>let(var =&amp;gt; value, ...)
&lt;/code>&lt;/pre>
&lt;p>By itself, this isn&amp;rsquo;t particularly useful. In fact, if you were to
type a bare &lt;code>let&lt;/code> statement in a yaql evaluator, you would get an
error:&lt;/p>
&lt;pre>&lt;code>yaql&amp;gt; let(foo =&amp;gt; 10, bar =&amp;gt; 20)
Execution exception: &amp;lt;yaql.language.contexts.Context object at 0x7fbaf9772e50&amp;gt; is not JSON serializable
&lt;/code>&lt;/pre>
&lt;p>This is where the &lt;code>-&amp;gt;&lt;/code> operator comes into play. We use that to pass
the context created by the &lt;code>let&lt;/code> statement into a yaql expression. For
example:&lt;/p>
&lt;pre>&lt;code>yaql&amp;gt; let(foo =&amp;gt; 10, bar =&amp;gt; 20) -&amp;gt; $foo
10
yaql&amp;gt; let(foo =&amp;gt; 10, bar =&amp;gt; 20) -&amp;gt; $bar
20
&lt;/code>&lt;/pre>
&lt;p>With that in mind, we can return to our earlier task, and rewrite the
yaql expression like this:&lt;/p>
&lt;pre>&lt;code>outputs:
sources:
yaql:
data:
sources: {get_param: sources}
default_type: tail
expression: &amp;gt;
let(default_type =&amp;gt; $.data.default_type) -&amp;gt;
$.data.sources.select({
'path' =&amp;gt; $.path,
'tag' =&amp;gt; $.tag,
'type' =&amp;gt; $.get('type', $default_type)})
&lt;/code>&lt;/pre>
&lt;p>Which will give us exactly what we want. This can of course be
extended to support additional default values:&lt;/p>
&lt;pre>&lt;code>outputs:
sources:
yaql:
data:
sources: {get_param: sources}
default_type: tail
default_format: &amp;gt;
/some regular expression/
expression: &amp;gt;
let(
default_type =&amp;gt; $.data.default_type,
default_format =&amp;gt; $.data.default_format
) -&amp;gt;
$.data.sources.select({
'path' =&amp;gt; $.path,
'tag' =&amp;gt; $.tag,
'type' =&amp;gt; $.get('type', $default_type),
'format' =&amp;gt; $.get('format', $default_format)
})
&lt;/code>&lt;/pre>
&lt;p>Going out on a bit of a tangent, there is another statement not
mentioned in the documentation: the &lt;code>def&lt;/code> statement lets you defined a
yaql function. The general format is:&lt;/p>
&lt;pre>&lt;code>def(func_name, func_body)
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>func_body&lt;/code> is a yaql expresion. For example:&lt;/p>
&lt;pre>&lt;code>def(upperpath, $.path.toUpper()) -&amp;gt;
$.data.sources.select(upperpath($))
&lt;/code>&lt;/pre>
&lt;p>Which would generate:&lt;/p>
&lt;pre>&lt;code>[
&amp;quot;/VAR/LOG/NOVA/NOVA-API.LOG&amp;quot;,
&amp;quot;/VAR/LOG/NOVA/NOVA-SCHEDULER.LOG&amp;quot;
]
&lt;/code>&lt;/pre>
&lt;p>This obviously becomes more useful as you use user-defined functions
to encapsulate more complex yaql expressions for re-use.&lt;/p>
&lt;p>Thanks to &lt;a href="https://github.com/sergmelikyan">sergmelikyan&lt;/a> for his help figuring this out.&lt;/p></content></item><item><title>Connecting another vm to your tripleo-quickstart deployment</title><link>https://blog.oddbit.com/post/2016-05-19-connecting-another-vm-to-your-/</link><pubDate>Thu, 19 May 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-05-19-connecting-another-vm-to-your-/</guid><description>Let&amp;rsquo;s say that you have set up an environment using tripleo-quickstart and you would like to add another virtual machine to the mix that has both &amp;ldquo;external&amp;rdquo; connectivity (&amp;ldquo;external&amp;rdquo; in quotes because I am using it in the same way as the quickstart does w/r/t the undercloud) and connectivity to the overcloud nodes. How would you go about setting that up?
For a concrete example, let&amp;rsquo;s presume you have deployed an environment using the default tripleo-quickstart configuration, which looks like this:</description><content>&lt;p>Let&amp;rsquo;s say that you have set up an environment using
&lt;a href="https://github.com/openstack/tripleo-quickstart/">tripleo-quickstart&lt;/a> and you would like to add another virtual
machine to the mix that has both &amp;ldquo;external&amp;rdquo; connectivity (&amp;ldquo;external&amp;rdquo;
in quotes because I am using it in the same way as the quickstart does
w/r/t the undercloud) and connectivity to the overcloud nodes. How
would you go about setting that up?&lt;/p>
&lt;p>For a concrete example, let&amp;rsquo;s presume you have deployed an environment
using the default tripleo-quickstart configuration, which looks like
this:&lt;/p>
&lt;pre>&lt;code>overcloud_nodes:
- name: control_0
flavor: control
- name: compute_0
flavor: compute
extra_args: &amp;gt;-
--neutron-network-type vxlan
--neutron-tunnel-types vxlan
--ntp-server pool.ntp.org
network_isolation: true
&lt;/code>&lt;/pre>
&lt;p>That gets you one controller, one compute node, and enables network
isolation. When your deployment is complete, networking from the
perspective of the undercloud looks like this:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>eth0&lt;/code> is connected to the host&amp;rsquo;s &lt;code>brext&lt;/code> bridge and gives the
undercloud NAT access to the outside world. The interface will have
an address on the &lt;code>192.168.23.0/24&lt;/code> network.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>eth1&lt;/code> is connected to the host&amp;rsquo;s &lt;code>brovc&lt;/code> bridge, which is the
internal network for the overcloud. The interface is attached to
the OVS bridge &lt;code>br-ctlplane&lt;/code>.&lt;/p>
&lt;p>The &lt;code>br-ctlplane&lt;/code> bridge has the address &lt;code>192.0.2.1&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>And your overcloud environment probably looks something like this:&lt;/p>
&lt;pre>&lt;code>[stack@undercloud ~]$ nova list
+-------...+-------------------------+--------+...+--------------------+
| ID ...| Name | Status |...| Networks |
+-------...+-------------------------+--------+...+--------------------+
| 32f6ec...| overcloud-controller-0 | ACTIVE |...| ctlplane=192.0.2.7 |
| d98474...| overcloud-novacompute-0 | ACTIVE |...| ctlplane=192.0.2.8 |
+-------...+-------------------------+--------+...+--------------------+
&lt;/code>&lt;/pre>
&lt;p>We want to set up a new machine that has the same connectivity as the
undercloud.&lt;/p>
&lt;h2 id="upload-an-image">Upload an image&lt;/h2>
&lt;p>Before we can boot a new vm we&amp;rsquo;ll need an image; let&amp;rsquo;s start with the
standard CentOS 7 cloud image. First we&amp;rsquo;ll download it:&lt;/p>
&lt;pre>&lt;code>curl -O http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s add a root password to the image and disable &lt;a href="https://cloudinit.readthedocs.io/en/latest/">cloud-init&lt;/a>,
since we&amp;rsquo;re not booting in a cloud environment:&lt;/p>
&lt;pre>&lt;code>virt-customize -a CentOS-7-x86_64-GenericCloud.qcow2 \
--root-password password:changeme \
--run-command &amp;quot;yum -y remove cloud-init&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Now let&amp;rsquo;s upload it to libvirt:&lt;/p>
&lt;pre>&lt;code>virsh vol-create-as oooq_pool centos-7-cloud.qcow2 8G \
--format qcow2 \
--allocation 0
virsh vol-upload --pool oooq_pool centos-7-cloud.qcow2 \
CentOS-7-x86_64-GenericCloud.qcow2
&lt;/code>&lt;/pre>
&lt;h2 id="a-idboot-the-vmboot-the-vma">&lt;!-- raw HTML omitted -->Boot the vm&lt;!-- raw HTML omitted -->&lt;/h2>
&lt;p>I like to boot from a copy-on-write clone of the image, so that I can
use the base image multiple times or quickly revert to a pristine
state, so let&amp;rsquo;s first create that clone:&lt;/p>
&lt;pre>&lt;code>virsh vol-create-as oooq_pool myguest.qcow2 10G \
--allocation 0 --format qcow2 \
--backing-vol centos-7-cloud.qcow2 \
--backing-vol-format qcow2
&lt;/code>&lt;/pre>
&lt;p>And then boot our vm:
&lt;!-- raw HTML omitted -->&lt;!-- raw HTML omitted -->&lt;/p>
&lt;pre>&lt;code>virt-install --disk vol=oooq_pool/myguest.qcow2,bus=virtio \
--import \
-r 2048 -n myguest --cpu host \
--os-variant rhel7 \
-w bridge=brext,model=virtio \
-w bridge=brovc,model=virtio \
--serial pty \
--noautoconsole
&lt;/code>&lt;/pre>
&lt;p>The crucial parts of the above command are the two &lt;code>-w ...&lt;/code> arguments,
which create interfaces attached to the named bridges.&lt;/p>
&lt;p>We can now connect to the console and log in as &lt;code>root&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ virsh console myguest
.
.
.
localhost login: root
Password:
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ll see that the system already has an ip address on the &amp;ldquo;external&amp;rdquo;
network:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ip addr show eth0
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
link/ether 52:54:00:7f:5c:5a brd ff:ff:ff:ff:ff:ff
inet 192.168.23.27/24 brd 192.168.23.255 scope global dynamic eth0
valid_lft 3517sec preferred_lft 3517sec
inet6 fe80::5054:ff:fe7f:5c5a/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>And we have external connectivity:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ping -c1 google.com
PING google.com (216.58.219.206) 56(84) bytes of data.
64 bytes from lga25s40-in-f14.1e100.net (216.58.219.206): icmp_seq=1 ttl=56 time=20.6 ms
--- google.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 20.684/20.684/20.684/0.000 ms
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s give &lt;code>eth1&lt;/code> an address on the ctlplane network:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ip addr add 192.0.2.254/24 dev eth1
[root@localhost ~]# ip link set eth1 up
&lt;/code>&lt;/pre>
&lt;p>Now we can access the undercloud:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ping -c1 192.0.2.1
PING 192.0.2.1 (192.0.2.1) 56(84) bytes of data.
64 bytes from 192.0.2.1: icmp_seq=1 ttl=64 time=0.464 ms
--- 192.0.2.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.464/0.464/0.464/0.000 ms
&lt;/code>&lt;/pre>
&lt;p>As well as all of the overcloud hosts using their addresses on the
same network:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ping -c1 192.0.2.1
PING 192.0.2.1 (192.0.2.1) 56(84) bytes of data.
64 bytes from 192.0.2.1: icmp_seq=1 ttl=64 time=0.464 ms
--- 192.0.2.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.464/0.464/0.464/0.000 ms
&lt;/code>&lt;/pre>
&lt;h2 id="allocating-an-address-using-dhcp">Allocating an address using DHCP&lt;/h2>
&lt;p>In the above instructions we&amp;rsquo;ve manually assigned an ip address on the
ctlplane network. This works fine for testing, but it could
ultimately prove problematic if neutron were to allocate the same
address to another overcloud host. We can use neutron to configure a
static dhcp lease for our new host.&lt;/p>
&lt;p>First, we need the MAC address of our guest:&lt;/p>
&lt;pre>&lt;code>virthost$ virsh dumpxml myguest |
xmllint --xpath '//interface[source/@bridge=&amp;quot;brovc&amp;quot;]' -
&amp;lt;interface type=&amp;quot;bridge&amp;quot;&amp;gt;
&amp;lt;mac address=&amp;quot;52:54:00:42:d6:c2&amp;quot;/&amp;gt;
&amp;lt;source bridge=&amp;quot;brovc&amp;quot;/&amp;gt;
&amp;lt;target dev=&amp;quot;tap9&amp;quot;/&amp;gt;
&amp;lt;model type=&amp;quot;virtio&amp;quot;/&amp;gt;
&amp;lt;alias name=&amp;quot;net1&amp;quot;/&amp;gt;
&amp;lt;address type=&amp;quot;pci&amp;quot; domain=&amp;quot;0x0000&amp;quot; bus=&amp;quot;0x00&amp;quot; slot=&amp;quot;0x04&amp;quot; function=&amp;quot;0x0&amp;quot;/&amp;gt;
&amp;lt;/interface&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And then on the undercloud we run &lt;code>neutron port-create&lt;/code> to create a
port and associate it with our MAC address:&lt;/p>
&lt;pre>&lt;code>[stack@undercloud]$ neutron port-create --mac-address 52:54:00:42:d6:c2 ctlplane
&lt;/code>&lt;/pre>
&lt;p>Now if we run &lt;code>dhclient&lt;/code> on our guest, it will acquire a lease from
the neutron-managed DHCP server:&lt;/p>
&lt;pre>&lt;code>[root@localhost]# dhclient -d eth1
Internet Systems Consortium DHCP Client 4.2.5
Copyright 2004-2013 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/
Listening on LPF/eth1/52:54:00:42:d6:c2
Sending on LPF/eth1/52:54:00:42:d6:c2
Sending on Socket/fallback
DHCPREQUEST on eth1 to 255.255.255.255 port 67 (xid=0xc90c0ba)
DHCPACK from 192.0.2.5 (xid=0xc90c0ba)
bound to 192.0.2.9 -- renewal in 42069 seconds.
&lt;/code>&lt;/pre>
&lt;p>We can make this persistent by creating
&lt;code>/etc/sysconfig/network-scripts/ifcfg-eth1&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[root@localhost]# cd /etc/sysconfig/network-scripts
[root@localhost]# sed s/eth0/eth1/g ifcfg-eth0 &amp;gt; ifcfg-eth1
[root@localhost]# ifup eth1
Determining IP information for eth1... done.
&lt;/code>&lt;/pre></content></item><item><title>A collection of git tips</title><link>https://blog.oddbit.com/post/2016-02-19-a-collection-of-git-tips/</link><pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-19-a-collection-of-git-tips/</guid><description>This is a small collection of simple git tips and tricks I use to make my life easier.
Quickly amend an existing commit with new files I have this alias in place that will amend the current commit while automatically re-using the existing commit message:
alias.fix=commit --amend -C HEAD With this in place, fixing a review becomes:
$ vim some/file/somewhere $ git add -u $ git fix Which I find much more convenient than git commit --amend, following by saving the commit message.</description><content>&lt;p>This is a small collection of simple &lt;code>git&lt;/code> tips and tricks I use to
make my life easier.&lt;/p>
&lt;h2 id="quickly-amend-an-existing-commit-with-new-files">Quickly amend an existing commit with new files&lt;/h2>
&lt;p>I have this alias in place that will amend the current commit while
automatically re-using the existing commit message:&lt;/p>
&lt;pre>&lt;code>alias.fix=commit --amend -C HEAD
&lt;/code>&lt;/pre>
&lt;p>With this in place, fixing a review becomes:&lt;/p>
&lt;pre>&lt;code>$ vim some/file/somewhere
$ git add -u
$ git fix
&lt;/code>&lt;/pre>
&lt;p>Which I find much more convenient than &lt;code>git commit --amend&lt;/code>, following
by saving the commit message.&lt;/p>
&lt;h2 id="what-files-have-changed">What files have changed?&lt;/h2>
&lt;p>Sometimes, I just want to know what files were changed in a commit. I
have the following alias, because I have a hard time remembering
whether I want &lt;code>--name-only&lt;/code> or &lt;code>--names-only&lt;/code> or &lt;code>--only-names&lt;/code>&amp;hellip;&lt;/p>
&lt;pre>&lt;code>alias.changed=show --name-only
&lt;/code>&lt;/pre>
&lt;p>Which gets me the commit message and a list of changed files:&lt;/p>
&lt;pre>&lt;code>$ git changed
commit 8c2a00809817a047bf312b72f390b5cb50ef9819
Author: Lars Kellogg-Stedman &amp;lt;lars@redhat.com&amp;gt;
Date: Wed Feb 17 11:11:02 2016 -0500
yet another attempt at fixing image fetching
this uses curl rather than wget, because wget chokes on file:// urls
which makes it difficult to cache images locally. curl supports
resuming downloads, but explicitly rather than implicitly like wget,
so we need a do/until loop.
Change-Id: Ibd3c524ea6ddfd423aec439f9eb7fffa62dfe818
:100644 100644 342a002... 2b9f9cb... M playbooks/roles/libvirt/setup/undercloud/tasks/main.yml
:100644 000000 d22cb99... 0000000... D playbooks/roles/libvirt/setup/undercloud/templates/get-undercloud.sh.j2
&lt;/code>&lt;/pre>
&lt;h2 id="getting-the-name-of-the-current-branch">Getting the name of the current branch&lt;/h2>
&lt;p>For scripting purposes I often want the name of the current branch.
Rather than reading &lt;code>git rev-parse --help&lt;/code> every time, I have this
alias:&lt;/p>
&lt;pre>&lt;code>alias.branch-name=rev-parse --abbrev-ref --symbolic-full-name HEAD
&lt;/code>&lt;/pre>
&lt;p>Which gets me:&lt;/p>
&lt;pre>&lt;code>$ git branch-name
master
$ git checkout bug/missing-become
$ git branch-name
bug/missing-become
&lt;/code>&lt;/pre>
&lt;h2 id="prevent-accidental-commits-on-master">Prevent accidental commits on master&lt;/h2>
&lt;p>When working on upstream projects I always want to be working on a
feature branch. To prevent accidental commits on master I drop the
following script into &lt;code>.git/hooks/pre-commit&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
current_branch=$(git branch-name)
if [ &amp;quot;$current_branch&amp;quot; = &amp;quot;master&amp;quot; ]; then
echo &amp;quot;*** DO NOT COMMIT ON MASTER&amp;quot;
exit 1
fi
exit 0
&lt;/code>&lt;/pre>
&lt;p>If I try to commit to my local &lt;code>master&lt;/code> branch, I get:&lt;/p>
&lt;pre>&lt;code>$ git ci -m 'made a nifty change'
*** DO NOT COMMIT ON MASTER
&lt;/code>&lt;/pre></content></item><item><title>Deploying an HA OpenStack development environment with tripleo-quickstart</title><link>https://blog.oddbit.com/post/2016-02-19-deploy-an-ha-openstack-develop/</link><pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-19-deploy-an-ha-openstack-develop/</guid><description>In this article I would like to introduce tripleo-quickstart, a tool that will automatically provision a virtual environment and then use TripleO to deploy an HA OpenStack on top of it.
Introducing Tripleo-Quickstart The goal of the Tripleo-Quickstart project is to replace the instack-virt-setup tool for quickly setting up virtual TripleO environments, and to ultimately become the tool used by both developers and upstream CI for this purpose. The project is a set of Ansible playbooks that will take care of:</description><content>&lt;p>In this article I would like to introduce &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart">tripleo-quickstart&lt;/a>, a
tool that will automatically provision a virtual environment and then
use &lt;a href="http://docs.openstack.org/developer/tripleo-docs/">TripleO&lt;/a> to deploy an HA OpenStack on top of it.&lt;/p>
&lt;h2 id="introducing-tripleo-quickstart">Introducing Tripleo-Quickstart&lt;/h2>
&lt;p>The goal of the &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart">Tripleo-Quickstart&lt;/a> project is to replace the
&lt;code>instack-virt-setup&lt;/code> tool for quickly setting up virtual TripleO
environments, and to ultimately become the tool used by both
developers and upstream CI for this purpose. The project is a set of
&lt;a href="http://ansible.com/">Ansible&lt;/a> playbooks that will take care of:&lt;/p>
&lt;ul>
&lt;li>Creating virtual undercloud node&lt;/li>
&lt;li>Creating virtual overcloud nodes&lt;/li>
&lt;li>Deploying the undercloud&lt;/li>
&lt;li>Deploying the overcloud&lt;/li>
&lt;li>Validating the overcloud&lt;/li>
&lt;/ul>
&lt;p>In this article, I will be using &lt;code>tripleo-quickstart&lt;/code> to set up a
development environment on a 32GB desktop. This is probably the
minimum sized system if your goal is to create an HA install (a
single controller/single compute environment could be deployed on
something smaller).&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>Before we get started, you will need:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>A target system with at least 32GB of RAM.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ansible 2.0.x. This is what you get if you &lt;code>pip install ansible&lt;/code>;
it is also available in the Fedora &lt;code>updates-testing&lt;/code> repository and
in the EPEL &lt;code>epel-testing&lt;/code> repository.&lt;/p>
&lt;p>Do &lt;strong>not&lt;/strong> use Ansible from the HEAD of the git repository; the
development version is not necessarily backwards compatible with
2.0.x and may break in unexpected ways.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A user account on the target system with which you can (a) log in
via ssh without a password and (b) use &lt;code>sudo&lt;/code> without a password to
gain root privileges. In other words, this should work:&lt;/p>
&lt;pre>&lt;code> ssh -tt targetuser@targethost sudo echo it worked
&lt;/code>&lt;/pre>
&lt;p>Your &lt;em>targetuser&lt;/em> could be &lt;code>root&lt;/code>, in which case the &lt;code>sudo&lt;/code> is
superfluous and you should be all set.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A copy of the tripleo-quickstart repository:&lt;/p>
&lt;pre>&lt;code> git clone https://github.com/redhat-openstack/tripleo-quickstart/
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>The remainder of this document assumes that you are running things
from inside the &lt;code>tripleo-quickstart&lt;/code> directory.&lt;/p>
&lt;h2 id="the-quick-way">The quick way&lt;/h2>
&lt;p>If you just want to take things out for a spin using the defaults
&lt;em>and&lt;/em> you can ssh to your target host as &lt;code>root&lt;/code>, you can skip the
remainder of this article and just run:&lt;/p>
&lt;pre>&lt;code>ansible-playbook playbooks/centosci/minimal.yml \
-e virthost=my.target.host
&lt;/code>&lt;/pre>
&lt;p>Or for an HA deployment:&lt;/p>
&lt;pre>&lt;code>ansible-playbook playbooks/centosci/ha.yml \
-e virthost=my.target.host
&lt;/code>&lt;/pre>
&lt;p>(Where you replace &lt;code>my.target.host&lt;/code> with the hostname of the host on
which you want to install your virtual environment.)&lt;/p>
&lt;p>In the remainder of this article I will discuss ways in which you can
customize this process (and make subsequent deployments faster).&lt;/p>
&lt;h2 id="create-an-inventory-file">Create an inventory file&lt;/h2>
&lt;p>An inventory file tells Ansible to which hosts it should connect and
provides information about how it should connect. For the quickstart,
your inventory needs to have your target host listed in the &lt;code>virthost&lt;/code>
group. For example:&lt;/p>
&lt;pre>&lt;code>[virthost]
my.target.host ansible_user=targetuser
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m going to assume you put this into a file named &lt;code>inventory&lt;/code>.&lt;/p>
&lt;h2 id="creating-a-playbook">Creating a playbook&lt;/h2>
&lt;p>A playbook tells Ansible what do to do.&lt;/p>
&lt;p>First, we want to tear down any existing virtual environment, and then
spin up a new undercloud node and create guests that will be used as
overcloud nodes. We do this with the &lt;code>libvirt/teardown&lt;/code> and
&lt;code>libvirt/setup&lt;/code> roles:&lt;/p>
&lt;pre>&lt;code>- hosts: virthost
roles:
- libvirt/teardown
- libvirt/setup
&lt;/code>&lt;/pre>
&lt;p>The next play will generate an Ansible inventory file (by default
&lt;code>$HOME/.quickstart/hosts&lt;/code>) that we can use in the future to refer to
our deployment:&lt;/p>
&lt;pre>&lt;code>- hosts: localhost
roles:
- rebuild-inventory
&lt;/code>&lt;/pre>
&lt;p>Lastly, we install the undercloud host and deploy the overcloud:&lt;/p>
&lt;pre>&lt;code>- hosts: undercloud
roles:
- overcloud
&lt;/code>&lt;/pre>
&lt;p>Put this content in a file named &lt;code>ha.yml&lt;/code> (the actual name doesn&amp;rsquo;t
matter, but this gives us something to refer to later on in this
article).&lt;/p>
&lt;h2 id="configuring-the-deployment">Configuring the deployment&lt;/h2>
&lt;p>Before we run tripleo-quickstart, we need to make a few configuration
changes. We&amp;rsquo;ll do this by creating a &lt;a href="http://yaml.org/">YAML&lt;/a> file that describes our
configuration, and we&amp;rsquo;ll feed this to ansible using the &lt;a href="http://docs.ansible.com/ansible/playbooks_variables.html#passing-variables-on-the-command-line">-e
@filename.yml&lt;/a> syntax.&lt;/p>
&lt;h3 id="describing-your-virtual-servers">Describing your virtual servers&lt;/h3>
&lt;p>By default, tripleo-quickstart will deploy an environment consisting
of four overcloud nodes:&lt;/p>
&lt;ul>
&lt;li>3 controller nodes&lt;/li>
&lt;li>1 compute node&lt;/li>
&lt;/ul>
&lt;p>All of these will have 4GB of memory, which when added to the default
overcloud node size of 12GB comes to a total memory footprint of 24GB.
These defaults are defined in
&lt;code>playbooks/roles/nodes/defaults/main.yml&lt;/code>. There are a number of ways
we can override this default configuration.&lt;/p>
&lt;p>To simply change the amount of memory assigned to each class of
server, we can set the &lt;code>undercloud_memory&lt;/code>, &lt;code>control_memory&lt;/code>, and
&lt;code>compute_memory&lt;/code> keys. For example:&lt;/p>
&lt;pre>&lt;code>control_memory: 6000
compute_memory: 2048
&lt;/code>&lt;/pre>
&lt;p>To change the number of CPUs assigned to a server, we can change the
corresponding &lt;code>_vcpu&lt;/code> key. Your deployments will generally run faster
if your undercloud host has more CPUs available:&lt;/p>
&lt;pre>&lt;code>undercloud_vcpu: 4
&lt;/code>&lt;/pre>
&lt;p>To change the number and type of nodes, you can provide an
&lt;code>overcloud_nodes&lt;/code> key with entries for each virtual system. The
default looks like this:&lt;/p>
&lt;pre>&lt;code>overcloud_nodes:
- name: control_0
flavor: control
- name: control_1
flavor: control
- name: control_2
flavor: control
- name: compute_0
flavor: compute
&lt;/code>&lt;/pre>
&lt;p>To create a minimal environment with a single controller and a single
compute node, we could instead put the following into our configuration
file:&lt;/p>
&lt;pre>&lt;code>overcloud_nodes:
- name: control_0
flavor: control
- name: compute_0
flavor: compute
&lt;/code>&lt;/pre>
&lt;p>You may intuit from the above examples that you can actually describe
custom flavors. This is true, but is beyond the scope of this post;
take a look at &lt;code>playbooks/roles/nodes/defaults/main.yml&lt;/code> for an
example.&lt;/p>
&lt;h3 id="configuring-ha">Configuring HA&lt;/h3>
&lt;p>To actually deploy an HA OpenStack environment, we need to pass a few
additional options to the &lt;code>openstack overcloud deploy&lt;/code> command. Based
on &lt;a href="http://docs.openstack.org/developer/tripleo-docs/basic_deployment/basic_deployment_cli.html#deploy-the-overcloud">the docs&lt;/a> I need:&lt;/p>
&lt;pre>&lt;code>--control-scale 3 \
-e /usr/share/openstack-tripleo-heat-templates/environments/puppet-pacemaker.yaml \
--ntp-server pool.ntp.org
&lt;/code>&lt;/pre>
&lt;p>We configure deploy arguments in the &lt;code>extra_args&lt;/code> variable, so for the
above configuration we would add:&lt;/p>
&lt;pre>&lt;code>extra_args: &amp;gt;
--control-scale 3
-e /usr/share/openstack-tripleo-heat-templates/environments/puppet-pacemaker.yaml
--ntp-server pool.ntp.org
&lt;/code>&lt;/pre>
&lt;h3 id="configuring-nested-kvm">Configuring nested KVM&lt;/h3>
&lt;p>I want &lt;a href="https://www.kernel.org/doc/Documentation/virtual/kvm/nested-vmx.txt">nested KVM&lt;/a> on my compute hosts,
which requires changes both to the libvirt XML used to deploy the
&amp;ldquo;baremetal&amp;rdquo; hosts and the nova.conf configuration. I was able to
accomplish this by adding the following to the configuration:&lt;/p>
&lt;pre>&lt;code>baremetal_vm_xml: |
&amp;lt;cpu mode='host-passthrough'/&amp;gt;
libvirt_args: --libvirt-type kvm
&lt;/code>&lt;/pre>
&lt;p>For this to work, you will need to have your target host correctly
configured to support nested KVM, which generally means adding the
following to &lt;code>/etc/modprobe.d/kvm.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code>options kvm_intel nested=1
&lt;/code>&lt;/pre>
&lt;p>(And possibly unloading/reloading the &lt;code>kvm_intel&lt;/code> module if it was
already loaded.)&lt;/p>
&lt;h3 id="disable-some-steps">Disable some steps&lt;/h3>
&lt;p>The default behavior is to:&lt;/p>
&lt;ul>
&lt;li>Install the undercloud&lt;/li>
&lt;li>Deploy the overcloud&lt;/li>
&lt;li>Validate the overcloud&lt;/li>
&lt;/ul>
&lt;p>You can enable or disable individual steps with the following
variables:&lt;/p>
&lt;ul>
&lt;li>&lt;code>step_install_undercloud&lt;/code>&lt;/li>
&lt;li>&lt;code>step_deploy_overcloud&lt;/code>&lt;/li>
&lt;li>&lt;code>step_validate_overcloud&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>These all default to &lt;code>true&lt;/code>. If, for example, overcloud validation is
failing because of a known issue, we could add the following to
&lt;code>nodes.yml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>step_validate_overcloud: false
&lt;/code>&lt;/pre>
&lt;h2 id="pre-caching-the-undercloud-image">Pre-caching the undercloud image&lt;/h2>
&lt;p>Fetching the undercloud image from the CentOS CI environment can take
a really long time. If you&amp;rsquo;re going to be deploying often, you can
speed up this step by manually saving the image and the corresponding
&lt;code>.md5&lt;/code> file to a file on your target host:&lt;/p>
&lt;pre>&lt;code>mkdir -p /usr/share/quickstart_images/mitaka/
cd /usr/share/quickstart_images/mitaka/
wget https://ci.centos.org/artifacts/rdo/images/mitaka/delorean/stable/undercloud.qcow2.md5 \
https://ci.centos.org/artifacts/rdo/images/mitaka/delorean/stable/undercloud.qcow2
&lt;/code>&lt;/pre>
&lt;p>And then providing the path to that file in the &lt;code>url&lt;/code> variable when
you run the playbook. I&amp;rsquo;ve added the following to my &lt;code>nodes.yml&lt;/code>
file, but you could also do this on the command line:&lt;/p>
&lt;pre>&lt;code>url: file:///usr/share/quickstart_images/mitaka/undercloud.qcow2
&lt;/code>&lt;/pre>
&lt;h2 id="intermission">Intermission&lt;/h2>
&lt;p>I&amp;rsquo;ve made the examples presented in this article available for
download at the following URLs:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="ha.yml">ha.yml&lt;/a> playbook&lt;/li>
&lt;li>&lt;a href="nodes.yml">nodes.yml&lt;/a> example configuration file&lt;/li>
&lt;li>&lt;a href="nodes-minimal.yml">nodes-minimal.yml&lt;/a> example configuration file for a minimal environment&lt;/li>
&lt;/ul>
&lt;h2 id="running-tripleo-quickstart">Running tripleo-quickstart&lt;/h2>
&lt;p>With all of the above in place, we can run:&lt;/p>
&lt;pre>&lt;code>ansible-playbook ha.yml -i inventory -e @nodes.yml
&lt;/code>&lt;/pre>
&lt;p>Which will proceed through the following phases:&lt;/p>
&lt;h3 id="tear-down-existing-environment">Tear down existing environment&lt;/h3>
&lt;p>This step deletes any libvirt guests matching the ones we are about to
deploy, removes the &lt;code>stack&lt;/code> user from the target host, and otherwise
ensures a clean slate from which to start.&lt;/p>
&lt;h3 id="create-overcloud-vms">Create overcloud vms&lt;/h3>
&lt;p>This uses the node definitions in &lt;code>vm.overcloud.nodes&lt;/code> to create a set
of libvirt guests. They will &lt;em>not&lt;/em> be booted at this stage; that
happens later during the ironic discovery process.&lt;/p>
&lt;h3 id="fetch-the-undercloud-image">Fetch the undercloud image&lt;/h3>
&lt;p>This will fetch the undercloud appliance image either from the CentOS
CI environment or from wherever you point the &lt;code>url&lt;/code> variable.&lt;/p>
&lt;h3 id="configure-the-undercloud-image">Configure the undercloud image&lt;/h3>
&lt;p>This performs some initial configuration steps such as injecting ssh
keys into the image.&lt;/p>
&lt;h3 id="create-undercloud-vm">Create undercloud vm&lt;/h3>
&lt;p>In this step, tripleo-quickstart uses the configured appliance image
to create a new &lt;code>undercloud&lt;/code> libvirt guest.&lt;/p>
&lt;h3 id="install-undercloud">Install undercloud&lt;/h3>
&lt;p>This runs &lt;code>openstack undercloud install&lt;/code>.&lt;/p>
&lt;h3 id="deploy-overcloud">Deploy overcloud&lt;/h3>
&lt;p>This does everything else:&lt;/p>
&lt;ul>
&lt;li>Discover the available nodes via the Ironic discovery process&lt;/li>
&lt;li>Use &lt;code>openstack overcloud deploy&lt;/code> to kick off the provisioning
process. This feeds &lt;a href="https://wiki.openstack.org/wiki/Heat">Heat&lt;/a> a collection of templates that will be
used to configure the overcloud nodes.&lt;/li>
&lt;/ul>
&lt;h2 id="accessing-the-undercloud">Accessing the undercloud&lt;/h2>
&lt;p>You can ssh directly into the undercloud host by taking advantage of
the ssh configuration that tripleo-quickstart generated for you. By
default this will be &lt;code>$HOME/.quickstart/ssh.config.ansible&lt;/code>, but you
can override that directory by specifying a value for the
&lt;code>local_working_dir&lt;/code> variable when you run Ansible. You use the &lt;code>-F&lt;/code>
option to ssh to point it at that file:&lt;/p>
&lt;pre>&lt;code>ssh -F $HOME/.quickstart/ssh.config.ansible undercloud
&lt;/code>&lt;/pre>
&lt;p>The configuration uses an ssh &lt;code>ProxyConnection&lt;/code> configuration to
automatically proxy your connection to the undercloud vm through your
physical host.&lt;/p>
&lt;h2 id="accessing-the-overcloud-hosts">Accessing the overcloud hosts&lt;/h2>
&lt;p>Once you have logged into the undercloud, you&amp;rsquo;ll need to source in
some credentials. The file &lt;code>stackrc&lt;/code> contains credentials for the
undercloud:&lt;/p>
&lt;pre>&lt;code>. stackrc
&lt;/code>&lt;/pre>
&lt;p>Now you can run &lt;code>nova list&lt;/code> to get a list of your overcloud nodes,
investigate the &lt;code>overcloud&lt;/code> heat stack, and so forth:&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
+----------...+------------+-----------------+--------------...+--------------+
| id ...| stack_name | stack_status | creation_time...| updated_time |
+----------...+------------+-----------------+--------------...+--------------+
| b6cfd621-...| overcloud | CREATE_COMPLETE | 2016-02-19T20...| None |
+----------...+------------+-----------------+--------------...+--------------+
&lt;/code>&lt;/pre>
&lt;p>You can find the ip addresses of your overcloud nodes by running &lt;code>nova list&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova list
+----------...+-------------------------+--------+...+---------------------+
| ID ...| Name | Status |...| Networks |
+----------...+-------------------------+--------+...+---------------------+
| 1fc5d5e8-...| overcloud-controller-0 | ACTIVE |...| ctlplane=192.0.2.9 |
| ab6439e8-...| overcloud-controller-1 | ACTIVE |...| ctlplane=192.0.2.10 |
| 82e12f81-...| overcloud-controller-2 | ACTIVE |...| ctlplane=192.0.2.11 |
| 53402a35-...| overcloud-novacompute-0 | ACTIVE |...| ctlplane=192.0.2.8 |
+----------...+-------------------------+--------+...+---------------------+
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll use the &lt;code>ctlplane&lt;/code> address to log into each host as the
&lt;code>heat-admin&lt;/code> user. For example, to log into my compute host:&lt;/p>
&lt;pre>&lt;code>$ ssh heat-admin@192.0.2.8
&lt;/code>&lt;/pre>
&lt;h2 id="accessing-the-overcloud-openstack-environment">Accessing the overcloud OpenStack environment&lt;/h2>
&lt;p>The file &lt;code>overcloudrc&lt;/code> on the undercloud host has administrative
credentials for the overcloud environment:&lt;/p>
&lt;pre>&lt;code>. overcloudrc
&lt;/code>&lt;/pre>
&lt;p>After sourcing in the overcloud credentials you can use OpenStack
clients to interact with your deployed cloud environment.&lt;/p>
&lt;h2 id="if-you-find-bugs">If you find bugs&lt;/h2>
&lt;p>If anything in the above process doesn&amp;rsquo;t work as described or
expected, feel free to visit the &lt;code>#rdo&lt;/code> channel on &lt;a href="https://freenode.net/">freenode&lt;/a>, or
open a bug report on the &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart/issues">issue tracker&lt;/a>.&lt;/p></content></item><item><title>Gruf gets superpowers</title><link>https://blog.oddbit.com/post/2016-02-19-gruf-gets-superpowers/</link><pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-19-gruf-gets-superpowers/</guid><description>In my last article article I introduced Gruf, a command line tool for interacting with Gerrit. Since then, Gruf has gained a few important new features.
Caching Gruf will now by default cache results for five minutes. This avoids repeatedly querying the server for the same information when you&amp;rsquo;re just displaying it with different templates (for example, if you run a gruf query open here followed by a gruf -t patches query open here).</description><content>&lt;p>In my &lt;a href="https://blog.oddbit.com/post/2016-02-16-gruf-a-gerrit-command-line-uti/">last article&lt;/a> article I introduced &lt;a href="http://github.com/larsks/gruf">Gruf&lt;/a>, a command line
tool for interacting with &lt;a href="https://www.gerritcodereview.com/">Gerrit&lt;/a>. Since then, Gruf has gained a
few important new features.&lt;/p>
&lt;h2 id="caching">Caching&lt;/h2>
&lt;p>Gruf will now by default cache results for five minutes. This avoids
repeatedly querying the server for the same information when you&amp;rsquo;re
just displaying it with different templates (for example, if you run a
&lt;code>gruf query open here&lt;/code> followed by a &lt;code>gruf -t patches query open here&lt;/code>).&lt;/p>
&lt;p>The cache lifetime can be tuned on the command line (with the
&lt;code>--cache-lifetime&lt;/code> option) or in the &lt;code>gruf.yml&lt;/code> configuration file (as
the &lt;code>cache_lifetime&lt;/code> parameter). Gruf has also learned the
&lt;code>invalidate-cache&lt;/code> command if you want to clear out the cache.&lt;/p>
&lt;h2 id="better-streaming">Better streaming&lt;/h2>
&lt;p>I have substantially enhanced the support for the Gerrit
&lt;a href="https://gerrit.googlecode.com/svn/documentation/2.1.2/cmd-stream-events.html">stream-events&lt;/a> command.&lt;/p>
&lt;h3 id="automatic-reconnection">Automatic reconnection&lt;/h3>
&lt;p>Gruf will now automatically reconnect to the Gerrit server if the
connection is lost while streaming events.&lt;/p>
&lt;h3 id="better-default-templates">Better default templates&lt;/h3>
&lt;p>The default &lt;code>stream-events&lt;/code> template now produces colorized output,
and there is also a &lt;code>short&lt;/code> template that produces one or two line
output for each event that can be useful if you just want to see
what&amp;rsquo;s going on.&lt;/p>
&lt;p>The default output looks like this:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>The short output looks something like this:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted --></content></item><item><title>Gruf, a Gerrit command line utility</title><link>https://blog.oddbit.com/post/2016-02-16-gruf-a-gerrit-command-line-uti/</link><pubDate>Tue, 16 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-16-gruf-a-gerrit-command-line-uti/</guid><description>(See also the followup to this article.)
I&amp;rsquo;ve recently started spending more time interacting with Gerrit, the code review tool used both by OpenStack, at review.openstack.org, and by a variety of other open source projects at GerritForge&amp;rsquo;s GitHub-linked review.gerrithub.io. I went looking for command line tools and was largely disappointed with what I found. Many of the solutions out there assume that you&amp;rsquo;re regularly interacting with a single Gerrit instance, and that&amp;rsquo;s seldom the case: more often, the Gerrit server in use varies from project to project.</description><content>&lt;p>(See also &lt;a href="https://blog.oddbit.com/post/2016-02-19-gruf-gets-superpowers/">the followup&lt;/a> to this article.)&lt;/p>
&lt;p>I&amp;rsquo;ve recently started spending more time interacting with &lt;a href="https://www.gerritcodereview.com/">Gerrit&lt;/a>,
the code review tool used both by &lt;a href="http://openstack.org/">OpenStack&lt;/a>, at
&lt;a href="http://review.openstack.org/">review.openstack.org&lt;/a>, and by a variety of other open source projects
at GerritForge&amp;rsquo;s GitHub-linked &lt;a href="http://review.gerrithub.io/">review.gerrithub.io&lt;/a>. I went
looking for command line tools and was largely disappointed with what
I found. Many of the solutions out there assume that you&amp;rsquo;re regularly
interacting with a single Gerrit instance, and that&amp;rsquo;s seldom the case:
more often, the Gerrit server in use varies from project to project.&lt;br>
I also found that many of the tools were opinionated in what sort of
output they would produce.&lt;/p>
&lt;p>For these reasons, I ended up rolling my own tool called &lt;a href="https://github.com/larsks/gruf">Gruf&lt;/a>.
This is a wrapper for the Gerrit &lt;a href="https://review.openstack.org/Documentation/cmd-index.html">command line API&lt;/a> that will let
you query and review Gerrit change requests from the comfort of your
command line. It is meant to supplement, not replace, the
&lt;a href="https://github.com/openstack-infra/git-review">git-review&lt;/a> tool that can be used to submit code for review and
download patchsets for reviewing changes locally.&lt;/p>
&lt;p>Gruf produces output by passing the result of Gerrit commands through
&lt;a href="http://jinja.pocoo.org/">Jinja&lt;/a> templates, which means you can produce just about any sort
of output you want without needing to modify the code.&lt;/p>
&lt;h2 id="basic-usage">Basic usage&lt;/h2>
&lt;p>You can use pretty much any of the Gerrit &lt;a href="https://review.openstack.org/Documentation/cmd-index.html#user_commands">user
commands&lt;/a> as they are presented in the documentation. For example,
to get a list of review requests that are owned by you:&lt;/p>
&lt;pre>&lt;code>$ gruf query status:open owner:self
262882 7 larsks introduce global &amp;quot;nodes&amp;quot; configuration role
&lt;/code>&lt;/pre>
&lt;p>Or to review an existing review request:&lt;/p>
&lt;pre>&lt;code>$ gruf review --code-review +2 262882
&lt;/code>&lt;/pre>
&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;p>Gruf will attempt to read configuration information from
&lt;code>$HOME/.config/gruf/gruf.yml&lt;/code> (unless &lt;code>$XDG_CONFIG_DIR&lt;/code> in your
environment points somewhere other than &lt;code>$HOME/.config&lt;/code>). This is a
&lt;a href="http://yaml.org/">YAML&lt;/a> format file that can contain two keys:&lt;/p>
&lt;ul>
&lt;li>&lt;code>querymap&lt;/code> &amp;ndash; contains query term aliases&lt;/li>
&lt;li>&lt;code>cmdalias&lt;/code> &amp;ndash; contains command aliases&lt;/li>
&lt;/ul>
&lt;p>These are both discussed in more detail below.&lt;/p>
&lt;h2 id="templates">Templates&lt;/h2>
&lt;p>Gruf produces output by passing the response from Gerrit through a
template. You can provide an explicit template name on the command
line using the &lt;code>-t&lt;/code> flag. The previous &lt;code>gruf query&lt;/code> example is
exactly equivalent to:&lt;/p>
&lt;pre>&lt;code>$ gruf -t default query status:open owner:self
&lt;/code>&lt;/pre>
&lt;p>But you can instead ask to see all of the comments for the results:&lt;/p>
&lt;pre>&lt;code>$ gruf -t comments query status:open owner:self
&lt;/code>&lt;/pre>
&lt;p>Gruf looks for templates in two places:&lt;/p>
&lt;ul>
&lt;li>In the gruf module directory.&lt;/li>
&lt;li>In a &lt;code>templates&lt;/code> directory co-located with your &lt;code>gruf.yml&lt;/code>
configuration file.&lt;/li>
&lt;/ul>
&lt;p>Within each directory, Gruf first looks for templates in a
subdirectory named after the Python class used to process the response
from Gerrit. For the &lt;code>query&lt;/code> command, this is &lt;code>QueryResponse&lt;/code>; which
means that to override the default template for the &lt;code>query&lt;/code> command,
you would create &lt;code>$HOME/.config/gruf/templates/QueryResponse/default&lt;/code>.&lt;/p>
&lt;p>You can also provide Gruf with inline templates using the
&lt;code>--inline-template&lt;/code> (aka &lt;code>-T&lt;/code>) command line option:&lt;/p>
&lt;pre>&lt;code>$ gruf -T '{{url}}' query here limit:5
https://review.gerrithub.io/263378
https://review.gerrithub.io/263342
https://review.gerrithub.io/263341
https://review.gerrithub.io/263340
https://review.gerrithub.io/263268
&lt;/code>&lt;/pre>
&lt;p>If you want to see what attributes are available in the response from
Gerrit, use the &lt;code>yaml&lt;/code> template:&lt;/p>
&lt;pre>&lt;code>$ gruf -t yaml query here limit:1
&lt;/code>&lt;/pre>
&lt;p>This will dump the results from Gerrit as a YAML document.&lt;/p>
&lt;h2 id="referring-to-changes">Referring to changes&lt;/h2>
&lt;p>Gerrit itself allows you to refer to reviews using change numbers
(&amp;ldquo;262882&amp;rdquo;), change IDs (&amp;ldquo;Id55e1baa0adf10f704dec2516e98a112be381d14&amp;rdquo;),
and git commit IDs (&amp;ldquo;80ce4ea09ab7c16aeb5b356ad17e8fb740f3d22b&amp;rdquo;).&lt;/p>
&lt;p>Gruf adds the option of using git reference names (e.g., branches and
tags) by prefixing a term with &lt;code>git:&lt;/code>. So if you want to get an
overview of a review associated with your current commit, you can ask
for &lt;code>git:HEAD&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ gruf -t patches query git:HEAD
262882 larsks introduce global &amp;quot;nodes&amp;quot; configuration role
https://review.gerrithub.io/262882
[007] refs/changes/82/262882/7
rdo-ci-centos Verified -1
[006] refs/changes/82/262882/6
rdo-ci-centos Verified -1
[005] refs/changes/82/262882/5
rdo-ci-centos Verified -1
[004] refs/changes/82/262882/4
rdo-ci-centos Verified -1
[003] refs/changes/82/262882/3
rdo-ci-centos Verified -1
[002] refs/changes/82/262882/2
larsks Code-Review -1
[001] refs/changes/82/262882/1
&lt;/code>&lt;/pre>
&lt;p>This works for any other valid &lt;code>git&lt;/code> reference (a relative reference
like &lt;code>git:HEAD^&lt;/code>, a branch or tag name like &lt;code>stable/liberty&lt;/code>, or whatever).&lt;/p>
&lt;h2 id="query-aliases">Query aliases&lt;/h2>
&lt;p>The Gerrit query language supports a variety of query operators. For
example, you can search for reviews that you own with &lt;code>owner:self&lt;/code> and
you can limit results to a particular project with something like
&lt;code>project:redhat-openstack/triple-quickstart&lt;/code>. While that&amp;rsquo;s very
useful, it can be annoying if you find yourself typing the same
operators over and over.&lt;/p>
&lt;p>Gruf supports a simple form of query aliasing. There are three
built-in aliases:&lt;/p>
&lt;ul>
&lt;li>&lt;code>mine&lt;/code> expands to &lt;code>owner:self&lt;/code>&lt;/li>
&lt;li>&lt;code>open&lt;/code> expands to &lt;code>status:open&lt;/code>&lt;/li>
&lt;li>&lt;code>here&lt;/code> expands to &lt;code>project:{project}&lt;/code>, where &lt;code>{project}&lt;/code> is replaced
by the name of the current project.&lt;/li>
&lt;/ul>
&lt;p>This allows you to simplify this:&lt;/p>
&lt;pre>&lt;code>gruf query status:open project:redhat-openstack/tripleo-quickstart
&lt;/code>&lt;/pre>
&lt;p>Into:&lt;/p>
&lt;pre>&lt;code>gruf query open here
&lt;/code>&lt;/pre>
&lt;p>You can define additional aliases in the &lt;code>querymap&lt;/code> section of your
&lt;code>gruf.yml&lt;/code> file. For example, given the following:&lt;/p>
&lt;pre>&lt;code>querymap:
needsreview: status:open -is:reviewed
&lt;/code>&lt;/pre>
&lt;p>You can now find changes in the project that need review by running:&lt;/p>
&lt;pre>&lt;code>gruf query needsreview here
&lt;/code>&lt;/pre>
&lt;h2 id="command-aliases">Command aliases&lt;/h2>
&lt;p>You may get tired of typing:&lt;/p>
&lt;pre>&lt;code>gruf -t patches query ...
&lt;/code>&lt;/pre>
&lt;p>If you create the following entry in your &lt;code>gruf.yml&lt;/code> configuration
file:&lt;/p>
&lt;pre>&lt;code>cmdalias:
patches:
cmd: query
template: patches
&lt;/code>&lt;/pre>
&lt;p>You can now type something like:&lt;/p>
&lt;pre>&lt;code>gruf patches git:HEAD
&lt;/code>&lt;/pre>
&lt;p>And gruf will behave as if you typed:&lt;/p>
&lt;pre>&lt;code>gruf -t patches query git:HEAD
&lt;/code>&lt;/pre>
&lt;p>This is especially useful for simple inline templates. For example,
given the following entry:&lt;/p>
&lt;pre>&lt;code>refs:
cmd: query
inline_template: &amp;gt;-
{{number}}
{{currentPatchSet.ref}}
{{currentPatchSet.revision}}
&lt;/code>&lt;/pre>
&lt;p>You can type:&lt;/p>
&lt;pre>&lt;code>gruf refs open here
&lt;/code>&lt;/pre>
&lt;p>And get output like:&lt;/p>
&lt;pre>&lt;code>262882 refs/changes/82/262882/7 80ce4ea09ab7c16aeb5b356ad17e8fb740f3d22b
263336 refs/changes/36/263336/2 5600951b5ce6e18b3d3fff75599518a00ea25384
263241 refs/changes/41/263241/1 27dbb595d58372b396cafe6dfacf97e58f43bc26
260561 refs/changes/61/260561/4 ee7f35d0528894990b736b8cece338d1c57ab0ac
262397 refs/changes/97/262397/1 6181c9b7361e4a804ab7069491a0780d119144f6
261934 refs/changes/34/261934/1 cf84db84bb67f201df5b59bbdf831dcf3d83056d
261218 refs/changes/18/261218/1 0d383494c579932e1edddfed23755da7fb2c9aae
&lt;/code>&lt;/pre>
&lt;h2 id="example">Example&lt;/h2>
&lt;p>The following example session assumes the following configuration:&lt;/p>
&lt;pre>&lt;code>cmdalias:
comments:
cmd: query
template: comments
patches:
cmd: query
template: patches
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>Get a list of open reviews:&lt;/p>
&lt;pre>&lt;code> $ gruf query open here
262882 7 larsks introduce global &amp;quot;nodes&amp;quot; configuration role
263336 2 trown WIP refactor and simplify image build
263241 1 trown Move mention of pre-downloaded image to lower section of README
260561 4 trown Make release rpm location configurable
262397 1 sshnaidm Split mitaka installation playbook
261934 1 trown WIP Use IPA ramdisk for liberty deploy
261218 1 trown WIP self writing docs
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>See an overview of patch sets for a particular change:&lt;/p>
&lt;pre>&lt;code> $ gruf patches 262882
262882 larsks introduce global &amp;quot;nodes&amp;quot; configuration role
https://review.gerrithub.io/262882
[007] refs/changes/82/262882/7
rdo-ci-centos Verified -1
[006] refs/changes/82/262882/6
rdo-ci-centos Verified -1
[005] refs/changes/82/262882/5
rdo-ci-centos Verified -1
...
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>See comments for a particular change:&lt;/p>
&lt;pre>&lt;code> $ gruf comments 262882
...
From: Lars Kellogg-Stedman (larsks) &amp;lt;lars@redhat.com&amp;gt;
Uploaded patch set 7.
From: (rdo-ci-centos) &amp;lt;whayutin+ci_centos@redhat.com&amp;gt;
Patch Set 7: Verified-1
Build Failed
https://ci.centos.org/job/tripleo-quickstart-gate-liberty-delorean-ha/86/ : Test failed
https://ci.centos.org/job/tripleo-quickstart-gate-mitaka-delorean-ha/94/ : Test failed
https://ci.centos.org/job/tripleo-quickstart-gate-mitaka-delorean-minimal/97/ : Test failed
https://ci.centos.org/job/tripleo-quickstart-gate-liberty-delorean-minimal/99/ : Test failed
https://ci.centos.org/job/trown-poc-tripleo-quickstart-gate-quickstart/22/ : Test ran successfully
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Abandon a change (with a comment):&lt;/p>
&lt;pre>&lt;code> $ gruf review -m 'this was a terrible idea' --abandon 262882,7
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;h2 id="future-plans">Future plans&lt;/h2>
&lt;p>I&amp;rsquo;ve already started using this regularly myself, but I&amp;rsquo;m sure that as
I work with Gerrit I will develop a better understanding of what I
want in a command-line tool. At the very least I need to implement
some form of caching to avoid hammering the Gerrit servers with
repeated requests for the same information.&lt;/p>
&lt;p>Beyond that, I&amp;rsquo;m curious if anyone else finds this useful and if there
are features you would like to see.&lt;/p>
&lt;p>Happy hacking!&lt;/p></content></item><item><title>A systemd-nspawn connection driver for Ansible</title><link>https://blog.oddbit.com/post/2016-02-08-a-systemd-nspawn-connection-dr/</link><pubDate>Mon, 08 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-08-a-systemd-nspawn-connection-dr/</guid><description>I wrote earlier about systemd-nspawn, and how it can take much of the fiddly work out of setting up functional chroot environments. I&amp;rsquo;m a regular Ansible user, and I wanted to be able to apply some of those techniques to my playbooks.
Ansible already has a chroot module, of course, but for some situations &amp;ndash; such as targeting an emulated chroot environment &amp;ndash; that just means a lot of extra work.</description><content>&lt;p>I wrote &lt;a href="https://blog.oddbit.com/post/2016-02-07-systemd-nspawn-for-fun-and-wel/">earlier&lt;/a> about &lt;a href="https://www.freedesktop.org/software/systemd/man/systemd-nspawn.html">systemd-nspawn&lt;/a>, and how it can take much
of the fiddly work out of setting up functional &lt;code>chroot&lt;/code> environments.
I&amp;rsquo;m a regular &lt;a href="http://ansible.com/">Ansible&lt;/a> user, and I wanted to be able to apply some
of those techniques to my playbooks.&lt;/p>
&lt;p>Ansible already has a &lt;code>chroot&lt;/code> module, of course, but for some
situations &amp;ndash; such as targeting an emulated &lt;code>chroot&lt;/code> environment &amp;ndash;
that just means a lot of extra work. Using &lt;code>systemd-nspawn&lt;/code> makes
this trivial.&lt;/p>
&lt;p>I&amp;rsquo;ve submitted
&lt;a href="https://github.com/ansible/ansible/pull/14334" class="pull-request">#14334&lt;/a>
to the Ansible project,
which introduces a new connection driver named &lt;code>nspawn&lt;/code>. It acts very
much like the &lt;code>chroot&lt;/code> driver, but it adds a few new configuration
options:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>ansible_nspawn_args&lt;/code> &amp;ndash; analagous to &lt;code>ansible_ssh_args&lt;/code>, setting
this will override the arguments that are passed to &lt;code>systemd-nspawn&lt;/code>
by default.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ansible_nspawn_extra_args&lt;/code> &amp;ndash; analgous to &lt;code>ansible_ssh_extra_args&lt;/code>,
setting this will &lt;em>append&lt;/em> the values to the default
&lt;code>systemd-nspawn&lt;/code> command line.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="advantages-over-chroot">Advantages over chroot&lt;/h2>
&lt;p>Let&amp;rsquo;s say we had a Fedora filesystem mounted on &lt;code>/fedora&lt;/code> and we want
to run the following playbook:&lt;/p>
&lt;pre>&lt;code>- hosts: /fedora
tasks:
- raw: dnf -y install python libselinux-python python2-dnf
- dnf:
name: git
state: installed
&lt;/code>&lt;/pre>
&lt;p>Using the &lt;code>chroot&lt;/code> driver, we get:&lt;/p>
&lt;pre>&lt;code>$ sudo ansible-playbook -i /fedora, -c chroot playbook.yml
PLAY ***************************************************************************
TASK [raw] *********************************************************************
fatal: [/fedora]: FAILED! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;failed&amp;quot;: true, &amp;quot;rc&amp;quot;: -6, &amp;quot;stderr&amp;quot;: &amp;quot;Fatal Python error: Failed to open /dev/urandom\n&amp;quot;, &amp;quot;stdout&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;stdout_lines&amp;quot;: []}
&lt;/code>&lt;/pre>
&lt;p>Adding the necessary tasks to our playbook to set up the chroot
environment properly will add a lot of additional complexity and will
make the playbook substantially less generic. Now compare that to the
result of running the same playbook using the &lt;code>nspawn&lt;/code> driver:&lt;/p>
&lt;pre>&lt;code>$ sudo ansible-playbook -i /fedora, -c nspawn playbook.yml
PLAY ***************************************************************************
TASK [raw] *********************************************************************
ok: [/fedora]
TASK [dnf] *********************************************************************
changed: [/fedora]
PLAY RECAP *********************************************************************
/fedora : ok=2 changed=1 unreachable=0 failed=0
&lt;/code>&lt;/pre>
&lt;h2 id="ansible-in-emulation">Ansible in emulation&lt;/h2>
&lt;p>By taking advantage of &lt;code>ansible_nspawn_extra_args&lt;/code> you can create
more complex containers. For example, in my &lt;a href="https://blog.oddbit.com/post/2016-02-07-systemd-nspawn-for-fun-and-wel/">last post&lt;/a> on
&lt;code>systemd-nspawn&lt;/code> I showed how to start a container for a different
architecture through the use of QEMU user-mode emulation. We can
apply the same idea to Ansible with an inventory entry like this:&lt;/p>
&lt;pre>&lt;code>target
ansible_host=/fedora
ansible_connection=nspawn
ansible_nspawn_extra_args=&amp;quot;--bind /usr/bin/qemu-arm&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The above will allow you to run a playbook against a filesystem
containing ARM architecture binaries, even though you&amp;rsquo;re running on an
x86_64 host.&lt;/p></content></item><item><title>Folding long lines in Ansible inventory files</title><link>https://blog.oddbit.com/post/2016-02-07-folding-long-lines-in-ansible-/</link><pubDate>Sun, 07 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-07-folding-long-lines-in-ansible-/</guid><description>If you have an Ansible inventory file that includes lots of per host variables, it&amp;rsquo;s not unusual for lines to get long enough that they become unwieldly, particularly if you want to discuss them in an email or write about them in some context (e.g., a blog post).
I&amp;rsquo;ve just submitted pull request #14359 to Ansible which implements support for folding long lines using the INI-format convention of using indent to mark extended logical lines.</description><content>&lt;p>If you have an Ansible inventory file that includes lots of per host
variables, it&amp;rsquo;s not unusual for lines to get long enough that they
become unwieldly, particularly if you want to discuss them in an email
or write about them in some context (e.g., a blog post).&lt;/p>
&lt;p>I&amp;rsquo;ve just submitted pull request &lt;a href="https://github.com/ansible/ansible/pull/14359">#14359&lt;/a> to Ansible which
implements support for folding long lines using the INI-format
convention of using indent to mark extended logical lines.&lt;/p>
&lt;p>With this patch in place, you can turn this:&lt;/p>
&lt;pre>&lt;code>myhost ansible_host=a.b.c.d ansible_user=alice ansible_become=true ansible_ssh_extra_args=&amp;quot;-F ~/.ssh/specialconfig&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Into this:&lt;/p>
&lt;pre>&lt;code>myhost
ansible_host=a.b.c.d
ansible_user=alice
ansible_become=true
ansible_ssh_extra_args=&amp;quot;-F ~/.ssh/specialconfig&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>I think that&amp;rsquo;s a lot easier to read.&lt;/p>
&lt;p>If you think this is a good idea (or not!), feel free to comment on
the
&lt;a href="https://github.com/ansible/ansible/pull/14359" class="pull-request">#14359&lt;/a>
. I considered (and implemented, then discarded)
using a backslash-based model instead&amp;hellip;&lt;/p>
&lt;pre>&lt;code>myhost \
ansible_host=a.b.c.d \
...
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;but I was swayed by the fact that the indent-style model is at
least documented &lt;a href="https://docs.python.org/3/library/configparser.html#supported-ini-file-structure">somewhere&lt;/a>, and with the backslash
model it&amp;rsquo;s easy to end up with something like this:&lt;/p>
&lt;pre>&lt;code>myhost \
ansible_host=a.b.c.d # &amp;lt;--- OOOPS NO BACKSLASH
ansible_user=alice \
ansible_become=true
&lt;/code>&lt;/pre></content></item><item><title>Systemd-nspawn for fun and...well, mostly for fun</title><link>https://blog.oddbit.com/post/2016-02-07-systemd-nspawn-for-fun-and-wel/</link><pubDate>Sun, 07 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-07-systemd-nspawn-for-fun-and-wel/</guid><description>systemd-nspawn has been called &amp;ldquo;chroot on steroids&amp;rdquo;, but if you think of it as Docker with a slightly different target you wouldn&amp;rsquo;t be far wrong, either. It can be used to spawn containers on your host, and has a variety of options for configuring the containerized environment through the use of private networking, bind mounts, capability controls, and a variety of other facilities that give you flexible container management.
There are many different ways in which it can be used.</description><content>&lt;p>&lt;code>systemd-nspawn&lt;/code> has been called &lt;a href="https://wiki.archlinux.org/index.php/Systemd-nspawn">&amp;ldquo;chroot on steroids&amp;rdquo;&lt;/a>,
but if you think of it as &lt;a href="http://docker.com">Docker&lt;/a> with a slightly different target
you wouldn&amp;rsquo;t be far wrong, either. It can be used to spawn containers
on your host, and has a variety of options for configuring the
containerized environment through the use of private networking, bind
mounts, capability controls, and a variety of other facilities that
give you flexible container management.&lt;/p>
&lt;p>There are many different ways in which it can be used. I&amp;rsquo;m going to
focus on one that&amp;rsquo;s a bit of a corner use case that I find
particularly interesting. In this article we&amp;rsquo;re going to explore how
we can use &lt;a href="https://www.freedesktop.org/software/systemd/man/systemd-nspawn.html">systemd-nspawn&lt;/a> to spawn lightweight containers for
architectures other than that of our host system.&lt;/p>
&lt;h2 id="why-systemd-nspawn">Why systemd-nspawn?&lt;/h2>
&lt;p>While everything described in this article could be accomplished
through the use of &lt;code>chroot&lt;/code> and a chunk of additional configuration,
using &lt;code>systemd-nspawn&lt;/code> makes it much easier. For example,
&lt;code>systemd-nspawn&lt;/code> takes care of making virtual filesystems like
&lt;code>/proc&lt;/code>, &lt;code>/sys&lt;/code>, and a minimal &lt;code>/dev&lt;/code> available inside the container
(without which some programs simply won&amp;rsquo;t work). And of course
&lt;code>systemd-nspawn&lt;/code> takes care of cleaning up these mounts when the
container exits. For a simple container, this:&lt;/p>
&lt;pre>&lt;code># systemd-nspawn -D /mnt /some/command
&lt;/code>&lt;/pre>
&lt;p>Is roughly equivalent to:&lt;/p>
&lt;pre>&lt;code># mount -o bind /proc /mnt/proc
# mount -o bind /sys /mnt/sys
# mount -t tmpfs tmpfs /mnt/run
# mount -t tmpfs tmpfs /mnt/dev
# ...populate /mnt/dev here...
# chroot /mnt /some/command
# umount /mnt/dev
# umount /mnt/run
# umount /mnt/sys
# umount /mnt/proc
&lt;/code>&lt;/pre>
&lt;p>&lt;code>systemd-nspawn&lt;/code> does all of this for us, and does much of it via
private mount namespaces so that the temporary filesystems aren&amp;rsquo;t
visible from the host.&lt;/p>
&lt;h2 id="in-which-we-perform-magic">In which we perform magic&lt;/h2>
&lt;p>Linux allows you to run binaries intended for other architectures
via the &lt;a href="https://www.kernel.org/doc/Documentation/binfmt_misc.txt">binfmt-misc&lt;/a> subsystem, which allows you to use bits at the
beginning of a file to match against an appropriate interpreter. This
can be used, for example, to make Java binaries directly executable.
We&amp;rsquo;re going to use this technique to accomplish the following:&lt;/p>
&lt;ul>
&lt;li>Teach our system how to run Raspberry Pi ARM binaries, and&lt;/li>
&lt;li>Allow us to spawn a &lt;code>systemd-nspawn&lt;/code> container into a Raspberry Pi
filesystem.&lt;/li>
&lt;/ul>
&lt;p>When a &lt;code>systemd&lt;/code>-based system boots, the &lt;a href="https://www.freedesktop.org/software/systemd/man/binfmt.d.html">systemd-binfmt&lt;/a> service
(if it&amp;rsquo;s enabled) will automatically register configurations found in
&lt;code>/etc/binfmt.d&lt;/code> or &lt;code>/usr/lib/binfmt.d&lt;/code>. You can set these up by hand,
of course, but we&amp;rsquo;re going to take the easy route and install the
&lt;code>qemu-user-static&lt;/code> package, which includes both the necessary &lt;code>binfmt.d&lt;/code>
configuration files as well as the associated emulators:&lt;/p>
&lt;pre>&lt;code># dnf -y install qemu-user-static
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>qemu-user-static&lt;/code> package on my system has installed, among other files,
&lt;code>/usr/lib/binfmt.d/qemu-arm.conf&lt;/code>, which looks like this:&lt;/p>
&lt;pre>&lt;code>:qemu-arm:M::\x7fELF\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x28\x00:\xff\xff\xff\xff\xff\xff\xff\x00\xff\xff\xff\xff\xff\xff\xff\xff\xfe\xff\xff\xff:/usr/bin/qemu-arm-static:
&lt;/code>&lt;/pre>
&lt;p>This gets registered with &lt;code>/proc/sys/fs/binfmt_misc/register&lt;/code> and
informs the kernel that there is a new binfmt called &lt;code>qemu-arm&lt;/code>, and
that files that contain the specified byte pattern in the header
should be handled with &lt;code>/usr/bin/qemu-arm-static&lt;/code>.&lt;/p>
&lt;p>With all this set up, we can mount a Raspberry Pi filesystem (I&amp;rsquo;m
starting with &lt;a href="https://minibianpi.wordpress.com/">minibian&lt;/a>)&amp;hellip;&lt;/p>
&lt;pre>&lt;code># tar xf 2015-11-12-jessie-minibian.tar.gz
# losetup -fP --show 2015-11-12-jessie-minibian.img
/dev/loop1
# mount /dev/loop1p2 /mnt
# mount /dev/loop1p1 /mnt/boot
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then start up a process in it:&lt;/p>
&lt;pre>&lt;code># systemd-nspawn -D /mnt
Spawning container mnt on /mnt.
Press ^] three times within 1s to kill container.
root@mnt:~#
&lt;/code>&lt;/pre>
&lt;p>And there we are! We&amp;rsquo;re now running a shell inside a container
running an ARM userspace. We can modify the image by installing or
udpating packages or making any other necessary configuration changes:&lt;/p>
&lt;pre>&lt;code>root@mnt:/# apt-get install raspberrypi-bootloader
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following extra packages will be installed:
libraspberrypi-bin libraspberrypi0
The following packages will be upgraded:
libraspberrypi-bin libraspberrypi0 raspberrypi-bootloader
3 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.
Need to get 32.5 MB of archives.
After this operation, 827 kB of additional disk space will be used.
Do you want to continue? [Y/n]
&lt;/code>&lt;/pre>
&lt;p>When we&amp;rsquo;re done, we exit our container:&lt;/p>
&lt;pre>&lt;code>root#mnt:/# exit
&lt;/code>&lt;/pre>
&lt;p>Unmount the directory:o&lt;/p>
&lt;pre>&lt;code># umount /mnt
&lt;/code>&lt;/pre>
&lt;p>And finally clean up the loopback device:&lt;/p>
&lt;pre>&lt;code># losetup -d /dev/loop1
&lt;/code>&lt;/pre>
&lt;p>Now we have an updated image file that we can write to an SD card and
use to boot our Raspberry Pi.&lt;/p>
&lt;p>&lt;strong>NB&lt;/strong> You&amp;rsquo;ll note that in this document I&amp;rsquo;m mounting &lt;code>loop1p2&lt;/code> on &lt;code>/&lt;/code>
and &lt;code>loop1p1&lt;/code> on &lt;code>/boot&lt;/code>. You obviously don&amp;rsquo;t need &lt;code>/boot&lt;/code> in your
container in order for things to run, but you will regret not mounting
it if you happen to install an updated kernel package, which needs to
populate &lt;code>/boot&lt;/code> with the new kernel image.&lt;/p>
&lt;h2 id="bonus-growing-the-image">Bonus: growing the image&lt;/h2>
&lt;p>The stock &lt;a href="https://minibianpi.wordpress.com/">minibian&lt;/a> image doesn&amp;rsquo;t have much free space on it; this
is intentional, and in general you&amp;rsquo;re expected to grow the root
partition and resize the filesystem after booting on your Pi.
However, if we&amp;rsquo;re going to use the above process to pre-configure our
image, there&amp;rsquo;s a good chance we&amp;rsquo;ll need more space immediately. We
start by growing the size of the image file itself; you can that with
&lt;code>qemu-img&lt;/code>, like this:&lt;/p>
&lt;pre>&lt;code># qemu-img resize 2015-11-12-jessie-minibian.img 2G
&lt;/code>&lt;/pre>
&lt;p>Or by using &lt;code>truncate&lt;/code>:&lt;/p>
&lt;pre>&lt;code># truncate -s 2G 2015-11-12-jessie-minibian.img
&lt;/code>&lt;/pre>
&lt;p>Or by using &lt;code>dd&lt;/code>:&lt;/p>
&lt;pre>&lt;code># dd of=2015-11-12-jessie-minibian.img if=/dev/zero \
count=0 bs=1G seek=2
&lt;/code>&lt;/pre>
&lt;p>Once the file has been extended, we need to grow the corresponding
partition. Assuming that you have a recent version of &lt;code>util-linux&lt;/code>
(where &amp;ldquo;recent&amp;rdquo; means &amp;ldquo;at least &lt;a href="http://karelzak.blogspot.com/2015/05/resize-by-sfdisk.html">v2.26.2&lt;/a>) installed, this is easy:&lt;/p>
&lt;pre>&lt;code># echo &amp;quot;, +&amp;quot; | sfdisk -N 2 2015-11-12-jessie-minibian.img
&lt;/code>&lt;/pre>
&lt;p>And lastly, we need to grow the filesystem. This requires
attaching the image to a loop device:&lt;/p>
&lt;pre>&lt;code># losetup -fP --show 2015-11-12-jessie-minibian.img
/dev/loop1
&lt;/code>&lt;/pre>
&lt;p>And then:&lt;/p>
&lt;pre>&lt;code># e2fsck -f /dev/loop1p2
# resize2fs /dev/loop1p2
&lt;/code>&lt;/pre>
&lt;p>Now when we mount the filesystem:&lt;/p>
&lt;pre>&lt;code># mount /dev/loop1p2 /mnt
# mount /dev/loop1p1 /mnt/boot
&lt;/code>&lt;/pre>
&lt;p>We see that there is more space available:&lt;/p>
&lt;pre>&lt;code># df -h /mnt
Filesystem Size Used Avail Use% Mounted on
/dev/loop1p2 1.9G 432M 1.4G 24% /mnt
&lt;/code>&lt;/pre>
&lt;h2 id="bonus-static-qemu-arm-binaries">Bonus: Static qemu-arm binaries&lt;/h2>
&lt;p>Earlier we saw that it was necessary to mount &lt;code>/lib64&lt;/code> into my
Raspberry Pi container because the &lt;code>qemu-arm&lt;/code> binary was dynamically
linked. You can acquire statically built versions of the QEMU
binaries from the Debian project, e.g., &lt;a href="https://packages.debian.org/sid/amd64/qemu-user-static/download">here&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://packages.debian.org/sid/amd64/qemu-user-static/download">https://packages.debian.org/sid/amd64/qemu-user-static/download&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Then unpack the &lt;code>.deb&lt;/code> file and extract the &lt;code>qemu-arm-static&lt;/code> binary:&lt;/p>
&lt;pre>&lt;code>$ ar xv qemu-user-static_2.5+dfsg-5_amd64.deb
x - debian-binary
x - control.tar.gz
x - data.tar.xz
$ tar xf data.tar.xz ./usr/bin/qemu-arm-static
&lt;/code>&lt;/pre>
&lt;p>And copy it into place:&lt;/p>
&lt;pre>&lt;code># cp qemu-arm-static /usr/bin/qemu-arm
&lt;/code>&lt;/pre>
&lt;p>And now our earlier command will work without further modification:&lt;/p>
&lt;pre>&lt;code># systemd-nspawn -q --bind /usr/bin/qemu-arm -D /mnt /bin/bash
root@mnt:/#
&lt;/code>&lt;/pre></content></item><item><title>Installing pyspatialite on Fedora</title><link>https://blog.oddbit.com/post/2015-11-17-installing-pyspatialite-on-fed/</link><pubDate>Tue, 17 Nov 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-11-17-installing-pyspatialite-on-fed/</guid><description>If you should find yourself wanting to install pyspatialite on Fedora &amp;ndash; perhaps because you want to use the Processing plugin for QGIS &amp;ndash; you will first need to install the following dependencies:
gcc python-devel sqlite-devel geos-devel proj-devel python-pip redhat-rpm-config After which you can install pyspatialite using pip by running:
CFLAGS=-I/usr/include pip install pyspatialite At this point, you should be able to use the &amp;ldquo;Processing&amp;rdquo; plugin.</description><content>&lt;p>If you should find yourself wanting to install &lt;a href="https://github.com/lokkju/pyspatialite">pyspatialite&lt;/a> on
Fedora &amp;ndash; perhaps because you want to use the &lt;a href="https://plugins.qgis.org/plugins/processing/">Processing plugin&lt;/a>
for &lt;a href="http://www.qgis.org/">QGIS&lt;/a> &amp;ndash; you will first need to install the following
dependencies:&lt;/p>
&lt;ul>
&lt;li>&lt;code>gcc&lt;/code>&lt;/li>
&lt;li>&lt;code>python-devel&lt;/code>&lt;/li>
&lt;li>&lt;code>sqlite-devel&lt;/code>&lt;/li>
&lt;li>&lt;code>geos-devel&lt;/code>&lt;/li>
&lt;li>&lt;code>proj-devel&lt;/code>&lt;/li>
&lt;li>&lt;code>python-pip&lt;/code>&lt;/li>
&lt;li>&lt;code>redhat-rpm-config&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>After which you can install &lt;code>pyspatialite&lt;/code> using &lt;code>pip&lt;/code> by running:&lt;/p>
&lt;pre>&lt;code>CFLAGS=-I/usr/include pip install pyspatialite
&lt;/code>&lt;/pre>
&lt;p>At this point, you should be able to use the &amp;ldquo;Processing&amp;rdquo; plugin.&lt;/p></content></item><item><title>Ansible 2.0: New OpenStack modules</title><link>https://blog.oddbit.com/post/2015-10-26-ansible-20-new-openstack-modul/</link><pubDate>Mon, 26 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-26-ansible-20-new-openstack-modul/</guid><description>This is the second in a loose sequence of articles looking at new features in Ansible 2.0. In the previous article I looked at the Docker connection driver. In this article, I would like to provide an overview of the new-and-much-improved suite of modules for interacting with an OpenStack environment, and provide a few examples of their use.
In versions of Ansible prior to 2.0, there was a small collection of OpenStack modules.</description><content>&lt;p>This is the second in a loose sequence of articles looking at new
features in Ansible 2.0. In the previous article I looked at the
&lt;a href="https://blog.oddbit.com/post/2015-10-13-ansible-20-the-docker-connecti/">Docker connection driver&lt;/a>. In this article, I would like to
provide an overview of the new-and-much-improved suite of modules for
interacting with an &lt;a href="http://www.openstack.org/">OpenStack&lt;/a> environment, and provide a few
examples of their use.&lt;/p>
&lt;p>In versions of Ansible prior to 2.0, there was a small collection of
OpenStack modules. There was the minimum necessary to boot a Nova
instance:&lt;/p>
&lt;ul>
&lt;li>&lt;code>glance_image.py&lt;/code>&lt;/li>
&lt;li>&lt;code>keystone_user.py&lt;/code>&lt;/li>
&lt;li>&lt;code>nova_compute.py&lt;/code>&lt;/li>
&lt;li>&lt;code>nova_keypair.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>And a collection of modules for interacting with &lt;a href="https://wiki.openstack.org/wiki/Neutron">Neutron&lt;/a> (previously
Quantum):&lt;/p>
&lt;ul>
&lt;li>&lt;code>quantum_floating_ip_associate.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_floating_ip.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_network.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router_gateway.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router_interface.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_subnet.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>While functional, these modules did not provide very comprehensive
coverage of even basic OpenStack services, and they suffered from
having a great deal of duplicated code (which made ensuring
consistent behavior across all the modules more difficult). The
behavior of these modules was not always what you would expect (e.g.,
the &lt;code>nova_compute&lt;/code> module would return information in different forms
depending on whether it had to create an instance or not).&lt;/p>
&lt;h2 id="throwing-shade">Throwing Shade&lt;/h2>
&lt;p>The situation is much improved in Ansible 2.0, which introduces a new
suite of OpenStack modules in which the common code has been factored
out into the &lt;a href="https://pypi.python.org/pypi/shade">Shade&lt;/a> project, a Python package that provides a
simpler interface to OpenStack than is available using the native
clients. Collecting this code in one place will help ensure both that
these Ansible modules share consistent behavior and that they are
easier to maintain.&lt;/p>
&lt;p>There are modules for managing Keystone:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_auth.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_user_group.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_user.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Glance:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_image_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_image.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Cinder:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_server_volume.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_volume.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Nova:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_keypair.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_nova_flavor.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server_actions.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Ironic:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_ironic_node.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_ironic.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Neutron and Nova Networking:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_floating_ip.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_network.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_networks_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_port.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_router.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_security_group.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_security_group_rule.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_subnet.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_subnets_facts.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>and Swift:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_object.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="authentication">Authentication&lt;/h2>
&lt;p>Shade uses the &lt;a href="https://pypi.python.org/pypi/os-client-config/">os-client-config&lt;/a> library to configure
authentication credentials for your OpenStack environment.&lt;/p>
&lt;p>In the absence of any authentication information provided in your
Ansible playbook, these modules will attempt to use the standard suite
of &lt;code>OS_*&lt;/code> variables (&lt;code>OS_USERNAME&lt;/code>, &lt;code>OS_PASSWORD&lt;/code>, etc). This is fine
for testing, but you usually want to provide some sort of
authentication configuration in your Ansible environment.&lt;/p>
&lt;p>You can provide credentials directly in your plays by providing an
&lt;code>auth&lt;/code> argument to any of the modules. For example:&lt;/p>
&lt;pre>&lt;code>- os_image:
auth:
auth_url: http://openstack.local:5000/v2.0
username: admin
password: secret
project_name: admin
[...]
&lt;/code>&lt;/pre>
&lt;p>But that can get complicated, especially if you are maintaining
multiple sets of credentials. The &lt;code>shade&lt;/code> library allows you to
manage credentials in a file named (by default) &lt;code>clouds.yml&lt;/code>, which
&lt;code>shade&lt;/code> searches for in:&lt;/p>
&lt;ul>
&lt;li>The current directory&lt;/li>
&lt;li>&lt;code>$HOME/.config/openstack/&lt;/code>&lt;/li>
&lt;li>&lt;code>/etc/xdg/openstack/&lt;/code>&lt;/li>
&lt;li>&lt;code>/etc/openstack&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>This file may contain credentials for one or more cloud environments,
for example:&lt;/p>
&lt;pre>&lt;code>clouds:
testing:
auth:
auth_url: http://openstack.local:5000/v2.0
username: admin
password: secret
project_name: admin
&lt;/code>&lt;/pre>
&lt;p>If you have the above in &lt;code>clouds.yml&lt;/code> along with your playbook, the
above &lt;code>os_image&lt;/code> example can be rewritten as:&lt;/p>
&lt;pre>&lt;code>- os_image:
cloud: testing
[...]
&lt;/code>&lt;/pre>
&lt;h2 id="return-values">Return values&lt;/h2>
&lt;p>The new modules all return useful information about the objects they
have created. For example, if you create a network using
&lt;a href="http://docs.ansible.com/ansible/os_network_module.html">os_network&lt;/a> and register that result:&lt;/p>
&lt;pre>&lt;code>- os_network:
cloud: testing
name: mynetwork
register: mynetwork
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll get back a dictionary containing a top-level &lt;code>id&lt;/code> attribute,
which is the UUID of the created network, along with a &lt;code>network&lt;/code>
attribute containing a dictionary of information about the created
object. The &lt;a href="http://docs.ansible.com/ansible/debug_module.html">debug&lt;/a> module is an excellent tool for exploring these
return values. If we put the following in our playbook immediately
after the above task:&lt;/p>
&lt;pre>&lt;code>- debug:
var: mynetwork
&lt;/code>&lt;/pre>
&lt;p>We would get output that looks something like:&lt;/p>
&lt;pre>&lt;code>ok: [localhost] =&amp;gt; {
&amp;quot;changed&amp;quot;: false,
&amp;quot;mynetwork&amp;quot;: {
&amp;quot;changed&amp;quot;: true,
&amp;quot;id&amp;quot;: &amp;quot;02b77e32-794a-4102-ab1b-1b90e6d4d92f&amp;quot;,
&amp;quot;invocation&amp;quot;: {
&amp;quot;module_args&amp;quot;: {
&amp;quot;cloud&amp;quot;: &amp;quot;testing&amp;quot;,
&amp;quot;name&amp;quot;: &amp;quot;mynetwork&amp;quot;
},
&amp;quot;module_name&amp;quot;: &amp;quot;os_network&amp;quot;
},
&amp;quot;network&amp;quot;: {
&amp;quot;admin_state_up&amp;quot;: true,
&amp;quot;id&amp;quot;: &amp;quot;02b77e32-794a-4102-ab1b-1b90e6d4d92f&amp;quot;,
&amp;quot;mtu&amp;quot;: 0,
&amp;quot;name&amp;quot;: &amp;quot;mynetwork&amp;quot;,
&amp;quot;provider:network_type&amp;quot;: &amp;quot;vxlan&amp;quot;,
&amp;quot;provider:physical_network&amp;quot;: null,
&amp;quot;provider:segmentation_id&amp;quot;: 79,
&amp;quot;router:external&amp;quot;: false,
&amp;quot;shared&amp;quot;: false,
&amp;quot;status&amp;quot;: &amp;quot;ACTIVE&amp;quot;,
&amp;quot;subnets&amp;quot;: [],
&amp;quot;tenant_id&amp;quot;: &amp;quot;349a8b95c5ad4a3383149f65f8c44cff&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>I have written a set of basic &lt;a href="https://github.com/ansible/ansible/pull/12875">integration tests&lt;/a> for these modules.
I hope the pull request is merged, but even if not it provides an
example of how to make use of many of these new modules.&lt;/p>
&lt;p>I&amp;rsquo;d like to present a few brief examples here to give you a sense of
what working with the new modules is like.&lt;/p>
&lt;h3 id="uploading-an-image-to-glance">Uploading an image to Glance&lt;/h3>
&lt;p>The &lt;a href="http://docs.ansible.com/ansible/os_image_module.html">os_image&lt;/a> module is used to upload an image to Glance. Assuming
that you have file named &lt;code>cirros.qcow2&lt;/code> available locally, this will
create an image named &lt;code>cirros&lt;/code> in Glance:&lt;/p>
&lt;pre>&lt;code>- os_image:
cloud: testing
name: cirros
state: present
disk_format: qcow2
container_format: bare
filename: cirros.qcow2
&lt;/code>&lt;/pre>
&lt;h3 id="booting-a-nova-server">Booting a Nova server&lt;/h3>
&lt;p>The &lt;a href="http://docs.ansible.com/ansible/os_server_module.html">os_server&lt;/a> module, which is used for booting virtual servers
(&amp;ldquo;instances&amp;rdquo;) in Nova, replaces the &lt;a href="http://docs.ansible.com/ansible/nova_compute_module.html">nova_compute&lt;/a> module available
in Ansible versions before 2.0:&lt;/p>
&lt;pre>&lt;code>- name: create a nova server
os_server:
cloud: testing
name: myserver
state: present
nics:
- net-name: private
image: cirros
flavor: m1.small
key_name: my_ssh_key
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>nics&lt;/code> parameter can accept net names, net ids, port names, and
port ids. So you could also do this (assuming you were attaching your
server to two different tenant networks):&lt;/p>
&lt;pre>&lt;code>nics:
- net-id: c875770c-a20b-45b5-a9da-5aca97153053
- net-name: private
&lt;/code>&lt;/pre>
&lt;p>The above examples are using a YAML list of dictionaries to provide
the information. You can also pass in a comma-delimited key=value
string, like this:&lt;/p>
&lt;pre>&lt;code>nics: net-name=private,net-name=database
&lt;/code>&lt;/pre>
&lt;p>This syntax is particular useful if you are running ad-hoc commands on
the command line:&lt;/p>
&lt;pre>&lt;code>ansible localhost -m os_server -a '
cloud=testing name=myserver nics=net-name=private
image=cirros flavor=m1.small key_name=my_ssh_key'
&lt;/code>&lt;/pre>
&lt;h3 id="adding-a-nova-server-to-your-ansible-inventory">Adding a Nova server to your Ansible inventory&lt;/h3>
&lt;p>I&amp;rsquo;d like to conclude this post with a longer example, that
demonstrates how you can use the &lt;a href="http://docs.ansible.com/ansible/add_host_module.html">add_host&lt;/a> module to add a freshly
created server to your inventory, and then target that new server in
your playbook. I&amp;rsquo;ve split up this playbook with commentary; in
practice, the pre-formatted text in this section would all be in a
single playbook (like &lt;a href="playbook.yml">this&lt;/a>).&lt;/p>
&lt;pre>&lt;code>- hosts: localhost
tasks:
&lt;/code>&lt;/pre>
&lt;p>This first task boots the server. The values for &lt;code>image&lt;/code>, &lt;code>nics&lt;/code>, and
`key_name will need to be adjusted for your environment.&lt;/p>
&lt;pre>&lt;code> - os_server:
cloud: testing
name: myserver
image: centos-7-atomic
nics:
- net-name: private
flavor: m1.small
key_name: lars
auto_ip: true
register: myserver
&lt;/code>&lt;/pre>
&lt;p>This &lt;code>debug&lt;/code> entry simply shows us what values were returned in the
&lt;code>myserver&lt;/code> variable.&lt;/p>
&lt;pre>&lt;code> - debug:
var: myserver
&lt;/code>&lt;/pre>
&lt;p>Now we add the new host to our Ansible inventory. For this to work,
you need to have assigned a floating ip to the server (either using
&lt;code>auto_ip&lt;/code>, as in this example, or by assigning one explicitly), and
you need to be running this playbook somewhere that has a route to the
floating ip address.&lt;/p>
&lt;pre>&lt;code> - add_host:
name: myserver
groups: openstack
ansible_host: &amp;quot;{{myserver.server.public_v4}}&amp;quot;
ansible_user: centos
ansible_become: true
&lt;/code>&lt;/pre>
&lt;p>Note that in the above play you can&amp;rsquo;t use information from the
inventory because that new host won&amp;rsquo;t exist in the inventory until
&lt;em>after&lt;/em> this play completes.&lt;/p>
&lt;p>We&amp;rsquo;ll need to wait for the server to finish booting and provisioning
before we are able to target it with ansible. A typical cloud image
is configured to run &lt;a href="https://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> when it boots, which will take
care of a number of initial configuration tasks, including
provisioning the ssh key we configured using &lt;code>os_server&lt;/code>. Until this
process is complete, we won&amp;rsquo;t have remote access to the server.&lt;/p>
&lt;p>We can&amp;rsquo;t use the &lt;code>wait_for&lt;/code> module because that will only
check for an open port. Instead, we use a &lt;a href="http://docs.ansible.com/ansible/playbooks_loops.html#do-until-loops">do-until loop&lt;/a> to
wait until we are able to successfully run a command on the server via
ssh.&lt;/p>
&lt;pre>&lt;code> - command: &amp;gt;
ssh -o BatchMode=yes
centos@{{myserver.server.public_v4}} true
register: result
until: result|success
retries: 300
delay: 5
&lt;/code>&lt;/pre>
&lt;p>Now that we have added the new server to our inventory
we can target it in subsequent plays (such as this one):&lt;/p>
&lt;pre>&lt;code>- hosts: myserver
tasks:
- service:
name: docker
state: running
enabled: true
&lt;/code>&lt;/pre></content></item><item><title>Automatic git cache</title><link>https://blog.oddbit.com/post/2015-10-19-automatic-git-cache/</link><pubDate>Mon, 19 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-19-automatic-git-cache/</guid><description>This post is in response to a comment someone made on irc earlier today:
[I] would really like a git lookaside cache which operated on an upstream repo, but pulled objects locally when they&amp;rsquo;re available
In this post I present a proof-of-concept solution to this request. Please note that thisand isn&amp;rsquo;t something that has actually been used or tested anywhere!
If you access a git repository via ssh, it&amp;rsquo;s easy to provide a wrapper for git operations via the command= option in an authorized_keys file.</description><content>&lt;p>This post is in response to a comment someone made on irc earlier
today:&lt;/p>
&lt;blockquote>
&lt;p>[I] would really like a git lookaside cache which operated on an upstream
repo, but pulled objects locally when they&amp;rsquo;re available&lt;/p>
&lt;/blockquote>
&lt;p>In this post I present a proof-of-concept solution to this request.
Please note that thisand isn&amp;rsquo;t something that has actually been used
or tested anywhere!&lt;/p>
&lt;p>If you access a git repository via &lt;code>ssh&lt;/code>, it&amp;rsquo;s easy to provide a
wrapper for git operations via the &lt;code>command=&lt;/code> option in an
&lt;code>authorized_keys&lt;/code> file. We can take advantage of this to update a a
local &amp;ldquo;cache&amp;rdquo; repository prior to responding to a &lt;code>clone&lt;/code>/&lt;code>pull&lt;/code>/etc.
operation.&lt;/p>
&lt;p>A simple wrapper might look like this:&lt;/p>
&lt;pre>&lt;code>#!/bin/bash
[ &amp;quot;$SSH_ORIGINAL_COMMAND&amp;quot; ] || exit 1
eval set -- $SSH_ORIGINAL_COMMAND
cd repos
case $1 in
(git-receive-pack|git-upload-pack)
:;;
(*) echo &amp;quot;*** Unrecognized command.&amp;quot; &amp;gt;&amp;amp;2
exit 1
;;
esac
if [ &amp;quot;$1&amp;quot; = &amp;quot;git-upload-pack&amp;quot; ]; then
(
# Update the local repository cache if the file
# 'git-auto-update' exists.
cd &amp;quot;$2&amp;quot;
[ -f git-auto-update ] &amp;amp;&amp;amp;
git remote update &amp;gt;&amp;amp;2
)
fi
exec &amp;quot;$@&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>If we have a &lt;code>git&lt;/code> user locally, we can place the above script into
&lt;code>/home/git/bin/gitwrapper&lt;/code>, and then set up a &lt;code>.ssh/authorized_keys&lt;/code>
file that looks something like this:&lt;/p>
&lt;pre>&lt;code>command=&amp;quot;/home/git/bin/gitwrapper&amp;quot; ssh-rsa AAAAB3NzaC...
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s set up a local repository mirror:&lt;/p>
&lt;pre>&lt;code># su - git
git$ mkdir repos
git$ cd repos
git$ git clone --mirror http://github.com/openstack-dev/devstack.git
&lt;/code>&lt;/pre>
&lt;p>In order to tell the wrapper script that it should perform the
automatic update logic on this repository, we need to touch the
appropriate flag file:&lt;/p>
&lt;pre>&lt;code>git$ touch devstack.git/git-auto-update
&lt;/code>&lt;/pre>
&lt;p>If we then attempt to clone that repository:&lt;/p>
&lt;pre>&lt;code>lars$ git clone git@localhost:devstack.git
&lt;/code>&lt;/pre>
&lt;p>The wrapper script will check for the presence of that flag file, and
if it exists it will first perform a &lt;code>git remote update&lt;/code> before
responding to the &lt;code>clone&lt;/code> operation. This ensures that any new
objects are fetched, while ensuring fast transfer of objects that are
already available locally. The output we see after running the above
&lt;code>git clone&lt;/code> looks something like:&lt;/p>
&lt;pre>&lt;code>Fetching origin
From http://github.com/openstack-dev/devstack
8ce00ac..0ba1848 master -&amp;gt; master
First, rewinding head to replay your work on top of it...
Fast-forwarded master to 0ba18481672964808bbbc4160643387dc931c654.
&lt;/code>&lt;/pre>
&lt;p>The first three lines are the result &lt;code>git remote update&lt;/code> operation in
our cache repository.&lt;/p></content></item><item><title>Stupid Ansible Tricks: Running a role from the command line</title><link>https://blog.oddbit.com/post/2015-10-19-stupid-ansible-tricks-running-/</link><pubDate>Mon, 19 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-19-stupid-ansible-tricks-running-/</guid><description>When writing Ansible roles I occasionally want a way to just run a role from the command line, without having to muck about with a playbook. I&amp;rsquo;ve seen similar requests on the mailing lists and on irc.
I&amp;rsquo;ve thrown together a quick wrapper that will allow you (and me!) to do exactly that, called ansible-role. The --help output looks like this:
usage: ansible-role [-h] [--verbose] [--gather] [--no-gather] [--extra-vars EXTRA_VARS] [-i INVENTORY] [--hosts HOSTS] [--sudo] [--become] [--user USER] role positional arguments: role optional arguments: -h, --help show this help message and exit --verbose, -v --gather, -g --no-gather, -G --extra-vars EXTRA_VARS, -e EXTRA_VARS Inventory: -i INVENTORY, --inventory INVENTORY --hosts HOSTS, -H HOSTS Identity: --sudo, -s --become, -b --user USER, -u USER Example If you have a role roles/testrole that contains the following in tasks/main.</description><content>&lt;p>When writing &lt;a href="http://www.ansible.com/">Ansible&lt;/a> roles I occasionally want a way to just run a
role from the command line, without having to muck about with a
playbook. I&amp;rsquo;ve seen &lt;a href="https://groups.google.com/forum/#!topic/ansible-project/h-SGLuPDRrs">similar&lt;/a> &lt;a href="https://groups.google.com/forum/#!topic/ansible-devel/GqzZ6zsn6eY">requests&lt;/a> on the mailing lists
and on irc.&lt;/p>
&lt;p>I&amp;rsquo;ve thrown together a quick wrapper that will allow you (and me!) to
do exactly that, called &lt;a href="http://github.com/larsks/ansible-role">ansible-role&lt;/a>. The &lt;code>--help&lt;/code> output looks
like this:&lt;/p>
&lt;pre>&lt;code>usage: ansible-role [-h] [--verbose] [--gather] [--no-gather]
[--extra-vars EXTRA_VARS] [-i INVENTORY] [--hosts HOSTS]
[--sudo] [--become] [--user USER]
role
positional arguments:
role
optional arguments:
-h, --help show this help message and exit
--verbose, -v
--gather, -g
--no-gather, -G
--extra-vars EXTRA_VARS, -e EXTRA_VARS
Inventory:
-i INVENTORY, --inventory INVENTORY
--hosts HOSTS, -H HOSTS
Identity:
--sudo, -s
--become, -b
--user USER, -u USER
&lt;/code>&lt;/pre>
&lt;h2 id="example">Example&lt;/h2>
&lt;p>If you have a role &lt;code>roles/testrole&lt;/code> that contains the following in
&lt;code>tasks/main.yml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>- name: figure out who I am
command: whoami
register: whoami
- name: show who I am
debug:
msg: &amp;quot;I am {{whoami.stdout}}&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>You could run it like this:&lt;/p>
&lt;pre>&lt;code>$ ansible-role testrole -i localhost,
&lt;/code>&lt;/pre>
&lt;p>Which would get you:&lt;/p>
&lt;pre>&lt;code>PLAY ***************************************************************************
TASK [setup] *******************************************************************
ok: [localhost -&amp;gt; localhost]
TASK [testrole : figure out who I am] ******************************************
changed: [localhost -&amp;gt; localhost]
TASK [testrole : show who I am] ************************************************
ok: [localhost -&amp;gt; localhost] =&amp;gt; {
&amp;quot;changed&amp;quot;: false,
&amp;quot;msg&amp;quot;: &amp;quot;I am lars&amp;quot;
}
PLAY RECAP *********************************************************************
localhost : ok=3 changed=1 unreachable=0 failed=0
&lt;/code>&lt;/pre>
&lt;p>You can use the &lt;code>-b&lt;/code> (formerly &lt;code>-s&lt;/code>) flag to enable privilege
escalation via &lt;code>sudo&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ ansible-role testrole -i localhost, -s
&lt;/code>&lt;/pre>
&lt;p>Which will work as expected:&lt;/p>
&lt;pre>&lt;code>TASK [testrole : show who I am] ************************************************
ok: [localhost -&amp;gt; localhost] =&amp;gt; {
&amp;quot;changed&amp;quot;: false,
&amp;quot;msg&amp;quot;: &amp;quot;I am root&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>ansible-role&lt;/code> command does not currently provide proxy arguments
for &lt;em>all&lt;/em> of the options supported by &lt;code>ansible-playbook&lt;/code>, but
hopefully it supports enough to be useful. If you have bug reports or
pull requests, feel free to leave them &lt;a href="http://github.com/larsks/ansible-role/issues">on the GitHub
repository&lt;/a>.&lt;/p></content></item><item><title>Bootstrapping Ansible on Fedora 23</title><link>https://blog.oddbit.com/post/2015-10-15-bootstrapping-ansible-on-fedor/</link><pubDate>Thu, 15 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-15-bootstrapping-ansible-on-fedor/</guid><description>If you&amp;rsquo;ve tried running Ansible against a Fedora 23 system, you may have run into the following problem:
fatal: [myserver]: FAILED! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;failed&amp;quot;: true, &amp;quot;msg&amp;quot;: &amp;quot;/bin/sh: /usr/bin/python: No such file or directory\r\n&amp;quot;, &amp;quot;parsed&amp;quot;: false} Fedora has recently made the switch to only including Python 3 on the base system (at least for the cloud variant), while Ansible still requires Python 2. With Fedora 23, Python 3 is available as /usr/bin/python3, and /usr/bin/python is only available if you have installed the Python 2 interpreter.</description><content>&lt;p>If you&amp;rsquo;ve tried running &lt;a href="http://ansible.com/">Ansible&lt;/a> against a &lt;a href="http://fedoraproject.org/">Fedora&lt;/a> 23 system,
you may have run into the following problem:&lt;/p>
&lt;pre>&lt;code>fatal: [myserver]: FAILED! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;failed&amp;quot;: true,
&amp;quot;msg&amp;quot;: &amp;quot;/bin/sh: /usr/bin/python: No such file or directory\r\n&amp;quot;,
&amp;quot;parsed&amp;quot;: false}
&lt;/code>&lt;/pre>
&lt;p>Fedora has recently made the switch to only including Python 3 on the
base system (at least for the &lt;a href="https://getfedora.org/en/cloud/prerelease/">cloud&lt;/a> variant), while Ansible still
requires Python 2. With Fedora 23, Python 3 is available as
&lt;code>/usr/bin/python3&lt;/code>, and &lt;code>/usr/bin/python&lt;/code> is only available if you
have installed the Python 2 interpreter.&lt;/p>
&lt;p>This is not an insurmountable problem; Ansible&amp;rsquo;s &lt;a href="http://docs.ansible.com/ansible/raw_module.html">raw&lt;/a> module can be
used to run arbitrary commands on a remote host without requiring an
installed Python interpreter. This gives us everything we need to
bootstrap the remote environment.&lt;/p>
&lt;p>The simplest playbook might look something like:&lt;/p>
&lt;pre>&lt;code>- hosts: all
tasks:
- name: install packages for ansible support
raw: dnf -y -e0 -d0 install python python-dnf
&lt;/code>&lt;/pre>
&lt;p>(The &lt;code>python-dnf&lt;/code> package is required if you want to install packages
using the &lt;code>dnf&lt;/code> module.)&lt;/p>
&lt;p>So you drop this into a playbook and run it and&amp;hellip;it still fails, with
the same error. This is because Ansible will, by default, attempt to
gather facts from the remote host by running the &lt;code>setup&lt;/code> module, which
requires Python. So we modify our playbook to look like this:&lt;/p>
&lt;pre>&lt;code>- hosts: all
gather_facts: false
tasks:
- name: install packages for ansible support
raw: dnf -y -e0 -d0 install python python-dnf
&lt;/code>&lt;/pre>
&lt;p>Setting &lt;code>gather_facts: false&lt;/code> inhibits this initial fact collection;
with this change, the playbook should run successfully:&lt;/p>
&lt;pre>&lt;code>$ ansible-playbook playbook.yml
PLAY ***************************************************************************
TASK [install packages for ansible support] ************************************
ok: [myserver -&amp;gt; localhost]
PLAY RECAP *********************************************************************
myserver : ok=1 changed=0 unreachable=0 failed=0
&lt;/code>&lt;/pre>
&lt;p>Having installed the basics, you can now use many of the standard
Ansible modules:&lt;/p>
&lt;pre>&lt;code>- hosts: all
gather_facts: true
tasks:
- lineinefile:
dest: /etc/hosts
line: &amp;quot;{{ansible_eth0.ipv4.address}} {{inventory_hostname}}&amp;quot;
regexp: &amp;quot;{{inventory_hostname}}&amp;quot;
- package:
name: git
state: present
&lt;/code>&lt;/pre>
&lt;p>As the above example demonstrates, now that the necessary Python stack
is installed on the remote Fedora 23 host, Ansible is is able to
gather &lt;a href="http://docs.ansible.com/ansible/playbooks_variables.html#information-discovered-from-systems-facts">facts&lt;/a> about the host that can be used in tasks, templates,
etc.&lt;/p>
&lt;p>Note that with the &lt;code>raw&lt;/code> module I had to use the &lt;code>dnf&lt;/code> command
explicitly, while in the above playbook I can use the &lt;code>package&lt;/code> module
for package installation, which relies on available facts to determine
the correct package module.&lt;/p></content></item><item><title>Ansible 2.0: The Docker connection driver</title><link>https://blog.oddbit.com/post/2015-10-13-ansible-20-the-docker-connecti/</link><pubDate>Tue, 13 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-13-ansible-20-the-docker-connecti/</guid><description>As the release of Ansible 2.0 draws closer, I&amp;rsquo;d like to take a look at some of the new features that are coming down the pipe. In this post, we&amp;rsquo;ll look at the docker connection driver.
A &amp;ldquo;connection driver&amp;rdquo; is the mechanism by which Ansible connects to your target hosts. These days it uses ssh by default (which relies on the OpenSSH command line client for connectivity), and it also offers the Paramiko library as an alternative ssh implementation (this was in fact the default driver in earlier versions of Ansible).</description><content>&lt;p>As the release of &lt;a href="http://ansible.com/">Ansible&lt;/a> 2.0 draws closer, I&amp;rsquo;d like to take a
look at some of the new features that are coming down the pipe. In
this post, we&amp;rsquo;ll look at the &lt;code>docker&lt;/code> connection driver.&lt;/p>
&lt;p>A &amp;ldquo;connection driver&amp;rdquo; is the mechanism by which Ansible connects to
your target hosts. These days it uses &lt;code>ssh&lt;/code> by default (which relies
on the OpenSSH command line client for connectivity), and it also
offers the &lt;a href="http://www.paramiko.org/">Paramiko&lt;/a> library as an alternative ssh implementation
(this was in fact the default driver in earlier versions of Ansible).
Alternative drivers offered by recent versions of ansible included the
&lt;code>winrm&lt;/code> driver, for accessing Windows hosts, the &lt;code>fireball&lt;/code> driver, a
(deprecated) driver that used &lt;a href="http://zeromq.org/">0mq&lt;/a> for communication, and &lt;code>jail&lt;/code>, a
driver for connecting to FreeBSD jails.&lt;/p>
&lt;p>Ansible 2.0 will offer a &lt;code>docker&lt;/code> connection driver, which can be used
to connect to Docker containers via the &lt;code>docker exec&lt;/code> command.
Assuming you have a running container named &lt;code>target&lt;/code>, you can run an
ad-hoc command like this:&lt;/p>
&lt;pre>&lt;code>$ ansible all -i target, -c docker -m command -a 'uptime'
target | SUCCESS | rc=0 &amp;gt;&amp;gt;
03:54:21 up 7 days, 15:00, 0 users, load average: 0.81, 0.60, 0.46
&lt;/code>&lt;/pre>
&lt;p>You can specify the connection driver as part of a play in your
playbook:&lt;/p>
&lt;pre>&lt;code>- hosts: target
connection: docker
tasks:
- package:
name: git
state: latest
&lt;/code>&lt;/pre>
&lt;p>Or as a variable in your inventory. Here&amp;rsquo;s an example that has both a
docker container and an ssh-accessible host:&lt;/p>
&lt;pre>&lt;code>target ansible_connection=docker
server ansible_host=192.168.1.20 ansible_user=root
&lt;/code>&lt;/pre>
&lt;p>Given the following playbook:&lt;/p>
&lt;pre>&lt;code>- hosts: all
tasks:
- ping:
&lt;/code>&lt;/pre>
&lt;p>If we run it like this, assuming the above inventory is in the file
&lt;code>inventory&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ ansible-playbook -i inventory playbook.yml
&lt;/code>&lt;/pre>
&lt;p>The output will look something like:&lt;/p>
&lt;pre>&lt;code>TASK [ping] ********************************************************************
&amp;lt;192.168.1.20&amp;gt; ESTABLISH SSH CONNECTION FOR USER: root
&amp;lt;192.168.1.20&amp;gt; SSH: EXEC ssh -C -q -o ControlMaster=auto -o ControlPersist=60s ... 192.168.1.20 ...
&amp;lt;192.168.1.20&amp;gt; PUT /tmp/tmpbtrmo5 TO /root/.ansible/tmp/ansible-tmp-1444795190.49-64658551273604/ping
&amp;lt;192.168.1.20&amp;gt; SSH: EXEC sftp -b - -C -o ControlMaster=auto -o ControlPersist=60s ... 192.168.1.20 ...
ESTABLISH DOCKER CONNECTION FOR USER: lars
&amp;lt;target&amp;gt; EXEC ['/usr/bin/docker', 'exec', '-i', u'target', '/bin/sh', '-c', ...
&amp;lt;target&amp;gt; PUT /tmp/tmpNmcPTf TO /root/.ansible/tmp/ansible-tmp-1444795190.53-251446545325875/ping
&amp;lt;192.168.1.20&amp;gt; ESTABLISH SSH CONNECTION FOR USER: root
&amp;lt;192.168.1.20&amp;gt; SSH: EXEC ssh -C -q -o ControlMaster=auto -o ControlPersist=60s ... 192.168.1.20 ...
ok: [server -&amp;gt; localhost] =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;}
&amp;lt;target&amp;gt; EXEC ['/usr/bin/docker', 'exec', '-i', u'target', '/bin/sh', '-c', ...
ok: [target -&amp;gt; localhost] =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;}
PLAY RECAP *********************************************************************
server : ok=2 changed=0 unreachable=0 failed=0
target : ok=2 changed=0 unreachable=0 failed=0
&lt;/code>&lt;/pre>
&lt;p>Now you have a unified mechanism for managing configuration changes in
traditional hosts as well as in Docker containers.&lt;/p></content></item><item><title>Running NTP in a Container</title><link>https://blog.oddbit.com/post/2015-10-09-running-ntp-in-a-container/</link><pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-09-running-ntp-in-a-container/</guid><description>Someone asked on IRC about running ntpd in a container on Atomic, so I&amp;rsquo;ve put together a small example. We&amp;rsquo;ll start with a very simple Dockerfile:
FROM alpine RUN apk update RUN apk add openntpd ENTRYPOINT [&amp;quot;ntpd&amp;quot;] I&amp;rsquo;m using the alpine image as my starting point because it&amp;rsquo;s very small, which makes this whole process go a little faster. I&amp;rsquo;m installing the openntpd package, which provides the ntpd binary.
By setting an ENTRYPOINT here, the ntpd binary will be started by default, and any arguments passed to docker run after the image name will be passed to ntpd.</description><content>&lt;p>Someone asked on IRC about running ntpd in a container on &lt;a href="http://www.projectatomic.io/">Atomic&lt;/a>,
so I&amp;rsquo;ve put together a small example. We&amp;rsquo;ll start with a very simple
&lt;code>Dockerfile&lt;/code>:&lt;/p>
&lt;pre>&lt;code>FROM alpine
RUN apk update
RUN apk add openntpd
ENTRYPOINT [&amp;quot;ntpd&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m using the &lt;code>alpine&lt;/code> image as my starting point because it&amp;rsquo;s very
small, which makes this whole process go a little faster. I&amp;rsquo;m
installing the &lt;a href="http://www.openntpd.org/">openntpd&lt;/a> package, which provides the &lt;code>ntpd&lt;/code> binary.&lt;/p>
&lt;p>By setting an &lt;code>ENTRYPOINT&lt;/code> here, the &lt;code>ntpd&lt;/code> binary will be started by
default, and any arguments passed to &lt;code>docker run&lt;/code> after the image name
will be passed to &lt;code>ntpd&lt;/code>.&lt;/p>
&lt;p>We need to first build the image:&lt;/p>
&lt;pre>&lt;code># docker build -t larsks/ntpd .
&lt;/code>&lt;/pre>
&lt;p>And then we can try to run it:&lt;/p>
&lt;pre>&lt;code># docker run larsks/ntpd -h
ntpd: unrecognized option: h
usage: ntpd [-dnSsv] [-f file] [-p file]
&lt;/code>&lt;/pre>
&lt;p>That confirms that we can run the command. Now we need to provide it
with a configuration file. I looked briefly at &lt;a href="http://www.openbsd.org/cgi-bin/man.cgi/OpenBSD-current/man5/ntpd.conf.5?query=ntpd.conf">the ntpd.conf man
page&lt;/a>, and I think we can get away with just providing the
name of an ntp server. I created &lt;code>/etc/ntpd.conf&lt;/code> on my atomic host
with the following content:&lt;/p>
&lt;pre>&lt;code>servers pool.ntp.org
&lt;/code>&lt;/pre>
&lt;p>And then I tried running the container like this:&lt;/p>
&lt;pre>&lt;code>docker run -v /etc/ntpd.conf:/etc/ntpd.conf larsks/ntpd -d -f /etc/ntpd.conf
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-v&lt;/code> in the above command line makes &lt;code>/etc/ntpd.conf&lt;/code> on the host
available as &lt;code>/etc/ntpd.conf&lt;/code> inside the container. This gets me:&lt;/p>
&lt;pre>&lt;code>ntpd: can't set priority: Permission denied
reset adjtime failed: Operation not permitted
adjtimex (2) failed: Operation not permitted
adjtimex adjusted frequency by 0.000000ppm
fatal: privsep dir /var/empty could not be opened: No such file or directory
Lost child: child exited
dispatch_imsg in main: pipe closed
Terminating
&lt;/code>&lt;/pre>
&lt;p>The first few errors (&amp;ldquo;Permission denied&amp;rdquo;) mean that we need to pass
&lt;code>--privileged&lt;/code> on the &lt;code>docker run&lt;/code> command line. Normally, Docker
runs containers with limited capabilities, but because an ntp service
needs to be able to set the time in the kernel it won&amp;rsquo;t run in that
limited environment.&lt;/p>
&lt;p>The &amp;ldquo;fatal: privsep dir /var/empty could not be opened&amp;rdquo; suggests we
need an empty directory at &lt;code>/var/empty&lt;/code>. With the above two facts in
mind, I tried:&lt;/p>
&lt;pre>&lt;code>docker run --privileged -v /var/empty \
-v /etc/ntpd.conf:/etc/ntpd.conf larsks/ntpd -d -f /etc/ntpd.conf -s
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-s&lt;/code> on the end means &amp;ldquo;Try to set the time immediately at
startup.&amp;rdquo; This results in:&lt;/p>
&lt;pre>&lt;code>adjtimex adjusted frequency by 0.000000ppm
ntp engine ready
reply from 104.131.53.252: offset -3.543963 delay 0.018517, next query 5s
set local clock to Fri Oct 9 18:03:41 UTC 2015 (offset -3.543963s)
reply from 198.23.200.19: negative delay -7.039390s, next query 3190s
reply from 107.170.224.8: negative delay -6.983865s, next query 3139s
reply from 209.118.204.201: negative delay -6.982698s, next query 3216s
reply from 104.131.53.252: offset 3.523820 delay 0.018231, next query 8s
&lt;/code>&lt;/pre>
&lt;p>So that seems to work correctly. To make this service persistent, I
can add &lt;code>-d&lt;/code> to start the container in the background, and
&lt;code>--restart=always&lt;/code> to make Docker responsible for restarting it if it
fails:&lt;/p>
&lt;pre>&lt;code>docker run --privileged -v /var/empty \
--restart=always -d \
-v /etc/ntpd.conf:/etc/ntpd.conf larsks/ntpd -d -f /etc/ntpd.conf -s
&lt;/code>&lt;/pre>
&lt;p>And my Atomic host has an ntp service keeping the time in sync.&lt;/p></content></item><item><title>Migrating Cinder volumes between OpenStack environments using shared NFS storage</title><link>https://blog.oddbit.com/post/2015-09-29-migrating-cinder-volumes-betwe/</link><pubDate>Tue, 29 Sep 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-09-29-migrating-cinder-volumes-betwe/</guid><description>Many of the upgrade guides for OpenStack focus on in-place upgrades to your OpenStack environment. Some organizations may opt for a less risky (but more hardware intensive) option of setting up a parallel environment, and then migrating data into the new environment. In this article, we look at how to use Cinder backups with a shared NFS volume to facilitate the migration of Cinder volumes between two different OpenStack environments.</description><content>&lt;p>Many of the upgrade guides for OpenStack focus on in-place upgrades to
your OpenStack environment. Some organizations may opt for a less
risky (but more hardware intensive) option of setting up a parallel
environment, and then migrating data into the new environment. In
this article, we look at how to use Cinder backups with a shared NFS
volume to facilitate the migration of Cinder volumes between two
different OpenStack environments.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>This is how we&amp;rsquo;re going to proceed:&lt;/p>
&lt;p>In the source environment:&lt;/p>
&lt;ol>
&lt;li>Configure Cinder for NFS backups&lt;/li>
&lt;li>Create a backup&lt;/li>
&lt;li>Export the backup metadata&lt;/li>
&lt;/ol>
&lt;p>In the target environment:&lt;/p>
&lt;ol>
&lt;li>Configure Cinder for NFS backups&lt;/li>
&lt;li>Import the backup metadata&lt;/li>
&lt;li>Create a new volume matching the size of the source volume&lt;/li>
&lt;li>Restore the backup to the new volume&lt;/li>
&lt;/ol>
&lt;h2 id="cinder-configuration">Cinder configuration&lt;/h2>
&lt;p>We&amp;rsquo;ll be using the NFS backup driver for cinder, which means
&lt;code>cinder.conf&lt;/code> must contain:&lt;/p>
&lt;pre>&lt;code>backup_driver=cinder.backup.drivers.nfs
&lt;/code>&lt;/pre>
&lt;p>And you need to configure an NFS share to use for backups:&lt;/p>
&lt;pre>&lt;code>backup_share=fileserver:/vol/backups
&lt;/code>&lt;/pre>
&lt;p>Cinder in both environments should be pointing at the same
&lt;code>backup_share&lt;/code>. This is how we make backups made in the source
environment available in the target environment &amp;ndash; they will both have
access to the same storage, so that we only need to copy the metadata
into the target environment.&lt;/p>
&lt;p>After making changes to your Cinder configuration
you will need to restart Cinder. If you are using RDO or RHEL-OSP,
this is:&lt;/p>
&lt;pre>&lt;code>openstack-service restart cinder
&lt;/code>&lt;/pre>
&lt;h2 id="creating-a-backup">Creating a backup&lt;/h2>
&lt;p>Assume we have a volume named &lt;code>testvol&lt;/code> that is currently attached to
a running Nova server. The output of &lt;code>cinder list&lt;/code> looks like:&lt;/p>
&lt;pre>&lt;code>$ cinder list
+----------...+--------+--------------+------+...+----------+-------------...+
| ID ...| Status | Display Name | Size |...| Bootable | Attached to...|
+----------...+--------+--------------+------+...+----------+-------------...+
| bec9b02f-...| in-use | testvol | 1 |...| false | d97e9193-cf2...|
+----------...+--------+--------------+------+...+----------+-------------...+
&lt;/code>&lt;/pre>
&lt;p>We can try to create a backup of this using &lt;code>cinder backup-create&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-create testvol
&lt;/code>&lt;/pre>
&lt;p>But this will fail because the volume is currently attached to a Nova
server:&lt;/p>
&lt;pre>&lt;code>ERROR: Invalid volume: Volume to be backed up must be available
(HTTP 400) (Request-ID: req-...)
&lt;/code>&lt;/pre>
&lt;p>There are two ways we can deal with this:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>We can pass the &lt;code>--force&lt;/code> flag to &lt;code>cinder backup-create&lt;/code>, which
will allow the backup to continue even if the source volume is
attached. This should be done with care, because the on-disk
filesystem may not be in consistent state.&lt;/p>
&lt;p>The &lt;code>--force&lt;/code> flag was introduced in OpenStack Liberty. If you
are using an earlier OpenStack release you will need to use the
following procedure.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We can make the volume available by detaching it from the server.
In this case, you probably want to shut down the server first:&lt;/p>
&lt;pre>&lt;code> $ nova stop d97e9193-cf2c-41c4-afa2-fdd201b575d9
Request to stop server d97e9193-cf2c-41c4-afa2-fdd201b575d9 has
been accepted.
&lt;/code>&lt;/pre>
&lt;p>And then detach the volume:&lt;/p>
&lt;pre>&lt;code> $ nova volume-detach \
d97e9193-cf2c-41c4-afa2-fdd201b575d9 \
bec9b02f-f66f-4f15-a254-9acebd9c7c34
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Now the backup will start successfully:&lt;/p>
&lt;pre>&lt;code> # cinder backup-create testvol
+-----------+--------------------------------------+
| Property | Value |
+-----------+--------------------------------------+
| id | 96128a75-e143-4d8a-9b93-e246af8e6a7d |
| name | None |
| volume_id | bec9b02f-f66f-4f15-a254-9acebd9c7c34 |
+-----------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;p>At this point, you can use the &lt;code>cinder backup-list&lt;/code> command to see the
status of the backup. Initially it will be in state &lt;code>creating&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-list
+----------...+------------...+----------+------+------+...+----------------...+
| ID ...| Volume ID ...| Status | Name | Size |...| Container ...|
+----------...+------------...+----------+------+------+...+----------------...+
| 96128a75-...| bec9b02f-f6...| creating | - | 1 |...| 96/12/96128a75-...|
+----------...+------------...+----------+------+------+...+----------------...+
&lt;/code>&lt;/pre>
&lt;p>When the backup is finished, the status will read &lt;code>available&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-list
+----------...+------------...+-----------+------+------+...+----------------...+
| ID ...| Volume ID ...| Status | Name | Size |...| Container ...|
+----------...+------------...+-----------+------+------+...+----------------...+
| 96128a75-...| bec9b02f-f6...| available | - | 1 |...| 96/12/96128a75-...|
+----------...+------------...+-----------+------+------+...+----------------...+
&lt;/code>&lt;/pre>
&lt;p>At this point, you may want to re-attach the volume to your Nova server and
restart the server:&lt;/p>
&lt;pre>&lt;code> $ nova volume-attach \
d97e9193-cf2c-41c4-afa2-fdd201b575d9 \
bec9b02f-f66f-4f15-a254-9acebd9c7c34
$ nova start d97e9193-cf2c-41c4-afa2-fdd201b575d9
&lt;/code>&lt;/pre>
&lt;h2 id="exporting-the-backup">Exporting the backup&lt;/h2>
&lt;p>Now that we have successfully created the backup, we need to export
the Cinder metadata regarding the backup using the &lt;code>cinder backup-export&lt;/code> command (which can only be run by a user with &lt;code>admin&lt;/code>
privileges):&lt;/p>
&lt;pre>&lt;code>$ cinder backup-export 96128a75-e143-4d8a-9b93-e246af8e6a7d
&lt;/code>&lt;/pre>
&lt;p>This will return something like the following:&lt;/p>
&lt;pre>&lt;code>+----------------+------------------------------------------------------------------------------+
| Property | Value |
+----------------+------------------------------------------------------------------------------+
| backup_service | cinder.backup.drivers.nfs |
| backup_url | eyJzdGF0dXMiOiAiYXZhaWxhYmxlIiwgIm9iamVjdF9jb3VudCI6IDIsICJkZWxldGVkX2F0Ijog |
| | bnVsbCwgInNlcnZpY2VfbWV0YWRhdGEiOiAiYmFja3VwIiwgInVzZXJfaWQiOiAiYTY1MzQ5NzU5 |
| | YjZmNGVjNWEwYmIwY2MzZmViMWU5ZmEiLCAic2VydmljZSI6ICJjaW5kZXIuYmFja3VwLmRyaXZl |
| | cnMubmZzIiwgImF2YWlsYWJpbGl0eV96b25lIjogIm5vdmEiLCAiZGVsZXRlZCI6IGZhbHNlLCAi |
| | Y3JlYXRlZF9hdCI6ICIyMDE1LTA5LTI5VDE4OjU5OjEwLjAwMDAwMCIsICJ1cGRhdGVkX2F0Ijog |
| | IjIwMTUtMDktMjlUMTg6NTk6MzEuMDAwMDAwIiwgImRpc3BsYXlfZGVzY3JpcHRpb24iOiBudWxs |
| | LCAicGFyZW50X2lkIjogbnVsbCwgImhvc3QiOiAiaWJtLWhzMjItMDMucmh0cy5lbmcuYnJxLnJl |
| | ZGhhdC5jb20iLCAiY29udGFpbmVyIjogIjk2LzEyLzk2MTI4YTc1LWUxNDMtNGQ4YS05YjkzLWUy |
| | NDZhZjhlNmE3ZCIsICJ2b2x1bWVfaWQiOiAiYmVjOWIwMmYtZjY2Zi00ZjE1LWEyNTQtOWFjZWJk |
| | OWM3YzM0IiwgImRpc3BsYXlfbmFtZSI6IG51bGwsICJmYWlsX3JlYXNvbiI6IG51bGwsICJwcm9q |
| | ZWN0X2lkIjogImJjYWUzM2JkZjViODRkYzlhYjljYTY1MThhNDM4NTYxIiwgImlkIjogIjk2MTI4 |
| | YTc1LWUxNDMtNGQ4YS05YjkzLWUyNDZhZjhlNmE3ZCIsICJzaXplIjogMX0= |
| | |
+----------------+------------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>That giant block of text labeled &lt;code>backup_url&lt;/code> is not, in fact, a URL.
In this case, the actual content is a base64 encoded JSON string. You
will need to copy the base64 data to your target OpenStack
environment. You can extract just the base64 data like this:&lt;/p>
&lt;pre>&lt;code>cinder backup-export 96128a75-e143-4d8a-9b93-e246af8e6a7d |
sed -n '/backup_url/,$ s/|.*| *\(.*\) |/\1/p'
&lt;/code>&lt;/pre>
&lt;p>Which will give you:&lt;/p>
&lt;pre>&lt;code>eyJzdGF0dXMiOiAiYXZhaWxhYmxlIiwgIm9iamVjdF9jb3VudCI6IDIsICJkZWxldGVkX2F0Ijog
bnVsbCwgInNlcnZpY2VfbWV0YWRhdGEiOiAiYmFja3VwIiwgInVzZXJfaWQiOiAiYTY1MzQ5NzU5
YjZmNGVjNWEwYmIwY2MzZmViMWU5ZmEiLCAic2VydmljZSI6ICJjaW5kZXIuYmFja3VwLmRyaXZl
cnMubmZzIiwgImF2YWlsYWJpbGl0eV96b25lIjogIm5vdmEiLCAiZGVsZXRlZCI6IGZhbHNlLCAi
Y3JlYXRlZF9hdCI6ICIyMDE1LTA5LTI5VDE4OjU5OjEwLjAwMDAwMCIsICJ1cGRhdGVkX2F0Ijog
IjIwMTUtMDktMjlUMTg6NTk6MzEuMDAwMDAwIiwgImRpc3BsYXlfZGVzY3JpcHRpb24iOiBudWxs
LCAicGFyZW50X2lkIjogbnVsbCwgImhvc3QiOiAiaWJtLWhzMjItMDMucmh0cy5lbmcuYnJxLnJl
ZGhhdC5jb20iLCAiY29udGFpbmVyIjogIjk2LzEyLzk2MTI4YTc1LWUxNDMtNGQ4YS05YjkzLWUy
NDZhZjhlNmE3ZCIsICJ2b2x1bWVfaWQiOiAiYmVjOWIwMmYtZjY2Zi00ZjE1LWEyNTQtOWFjZWJk
OWM3YzM0IiwgImRpc3BsYXlfbmFtZSI6IG51bGwsICJmYWlsX3JlYXNvbiI6IG51bGwsICJwcm9q
ZWN0X2lkIjogImJjYWUzM2JkZjViODRkYzlhYjljYTY1MThhNDM4NTYxIiwgImlkIjogIjk2MTI4
YTc1LWUxNDMtNGQ4YS05YjkzLWUyNDZhZjhlNmE3ZCIsICJzaXplIjogMX0=
&lt;/code>&lt;/pre>
&lt;p>While not critical to this process, it may be interesting to see that
this string actually decodes to:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;availability_zone&amp;quot;: &amp;quot;nova&amp;quot;,
&amp;quot;container&amp;quot;: &amp;quot;96/12/96128a75-e143-4d8a-9b93-e246af8e6a7d&amp;quot;,
&amp;quot;created_at&amp;quot;: &amp;quot;2015-09-29T18:59:10.000000&amp;quot;,
&amp;quot;deleted&amp;quot;: false,
&amp;quot;deleted_at&amp;quot;: null,
&amp;quot;display_description&amp;quot;: null,
&amp;quot;display_name&amp;quot;: null,
&amp;quot;fail_reason&amp;quot;: null,
&amp;quot;host&amp;quot;: &amp;quot;ibm-hs22-03.rhts.eng.brq.redhat.com&amp;quot;,
&amp;quot;id&amp;quot;: &amp;quot;96128a75-e143-4d8a-9b93-e246af8e6a7d&amp;quot;,
&amp;quot;object_count&amp;quot;: 2,
&amp;quot;parent_id&amp;quot;: null,
&amp;quot;project_id&amp;quot;: &amp;quot;bcae33bdf5b84dc9ab9ca6518a438561&amp;quot;,
&amp;quot;service&amp;quot;: &amp;quot;cinder.backup.drivers.nfs&amp;quot;,
&amp;quot;service_metadata&amp;quot;: &amp;quot;backup&amp;quot;,
&amp;quot;size&amp;quot;: 1,
&amp;quot;status&amp;quot;: &amp;quot;available&amp;quot;,
&amp;quot;updated_at&amp;quot;: &amp;quot;2015-09-29T18:59:31.000000&amp;quot;,
&amp;quot;user_id&amp;quot;: &amp;quot;a65349759b6f4ec5a0bb0cc3feb1e9fa&amp;quot;,
&amp;quot;volume_id&amp;quot;: &amp;quot;bec9b02f-f66f-4f15-a254-9acebd9c7c34&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;h2 id="importing-the-backup">Importing the backup&lt;/h2>
&lt;p>In the target OpenStack environment, you need to import the backup
metadata to make Cinder aware of the backup. You do this with the
&lt;code>cinder backup-import&lt;/code> command, which requires both a &lt;code>backup_service&lt;/code>
parameter and a &lt;code>backup_url&lt;/code>. These are the values produces by the
&lt;code>cinder backup-export&lt;/code> command in the previous step.&lt;/p>
&lt;p>Assuming that we have dumped the base64 data into a file named
&lt;code>metadata.txt&lt;/code>, we can import the metadata using the following
command:&lt;/p>
&lt;pre>&lt;code># cinder backup-import cinder.backup.drivers.nfs $(tr -d '\n' &amp;lt; metadata.txt)
+----------+--------------------------------------+
| Property | Value |
+----------+--------------------------------------+
| id | a23891b2-e757-4d8f-9623-3d982e5616cb |
| name | None |
+----------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>And now if we run &lt;code>cinder backup-list&lt;/code> we should see a new backup
available:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-list
+----------...+----------...+-----------+------+------+...+-----------...+
| ID ...| Volume ID...| Status | Name | Size |...| Container ...|
+----------...+----------...+-----------+------+------+...+-----------...+
| a23891b2-...| 0000-0000...| available | - | 1 |...| 96/12/9612...|
+----------...+----------...+-----------+------+------+...+-----------...+
&lt;/code>&lt;/pre>
&lt;h2 id="creating-a-new-volume">Creating a new volume&lt;/h2>
&lt;p>At this point, we could simply run &lt;code>cinder backup-restore&lt;/code> on the
target system, and Cinder would restore the data onto a new volume
owned by the &lt;code>admin&lt;/code> user. If you want to restore to a volume owned
by another user, it is easiest to first create the volume as that
user. You will want to make sure that the size is at least as large
as the source volume:&lt;/p>
&lt;pre>&lt;code>$ cinder create --display_name mydata 1
+---------------------------------------+--------------------------------------+
| Property | Value |
+---------------------------------------+--------------------------------------+
[...]
| id | 145277e1-4733-4374-9b9c-677cb5334379 |
[...]
+---------------------------------------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>Note that is is possible to transfer a volume between tenants using
the &lt;code>cinder transfer-create&lt;/code> and &lt;code>cinder transfer-accept&lt;/code> commands,
but I will not be covering that in this article.&lt;/p>
&lt;h2 id="restoring-the-backup">Restoring the backup&lt;/h2>
&lt;p>Now that we have created a target volume we can restore the data from
our backup:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-restore --volume 145277e1-4733-4374-9b9c-677cb5334379 \
a23891b2-e757-4d8f-9623-3d982e5616cb
+-----------+--------------------------------------+
| Property | Value |
+-----------+--------------------------------------+
| backup_id | a23891b2-e757-4d8f-9623-3d982e5616cb |
| volume_id | 145277e1-4733-4374-9b9c-677cb5334379 |
+-----------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>While the backup is running the backup status will be &lt;code>restoring&lt;/code>:&lt;/p>
&lt;pre>&lt;code>+----------...+------------...+-----------+------+------+...+----------------...+
| ID ...| Volume ID ...| Status | Name | Size |...| Container ...|
+----------...+------------...+-----------+------+------+...+----------------...+
| a23891b2-...| 0000-0000-0...| restoring | - | 1 |...| 96/12/96128a75-...|
+----------...+------------...+-----------+------+------+...+----------------...+
&lt;/code>&lt;/pre>
&lt;p>When the backup is complete that status will be &lt;code>available&lt;/code>:&lt;/p>
&lt;pre>&lt;code>+----------...+------------...+-----------+------+------+...+----------------...+
| ID ...| Volume ID ...| Status | Name | Size |...| Container ...|
+----------...+------------...+-----------+------+------+...+----------------...+
| a23891b2-...| 0000-0000-0...| available | - | 1 |...| 96/12/96128a75-...|
+----------...+------------...+-----------+------+------+...+----------------...+
&lt;/code>&lt;/pre>
&lt;h2 id="verification">Verification&lt;/h2>
&lt;p>If you spawn a Nova server in your target environment and attach the
volume we just created, you should find that it contains the same data
as the source volume contained at the time of the backup.&lt;/p>
&lt;h2 id="for-more-information">For more information&lt;/h2>
&lt;p>The &lt;a href="http://docs.openstack.org/admin-guide-cloud/">Cloud Adminstrator Guide&lt;/a> has more information about
&lt;a href="http://docs.openstack.org/admin-guide-cloud/blockstorage_volume_backups.html">volume backups and restores&lt;/a> and &lt;a href="http://docs.openstack.org/admin-guide-cloud/blockstorage_volume_backups_export_import.html">managing backup
metadata&lt;/a>.&lt;/p></content></item><item><title>Provider external networks (in an appropriate amount of detail)</title><link>https://blog.oddbit.com/post/2015-08-13-provider-external-networks-det/</link><pubDate>Thu, 13 Aug 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-08-13-provider-external-networks-det/</guid><description>In Quantum in Too Much Detail, I discussed the architecture of a Neutron deployment in detail. Since that article was published, Neutron gained the ability to handle multiple external networks with a single L3 agent. While I wrote about that back in 2014, I covered the configuration side of it in much more detail than I discussed the underlying network architecture. This post addresses the architecture side.
The players This document describes the architecture that results from a particular OpenStack configuration, specifically:</description><content>&lt;p>In &lt;a href="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/">Quantum in Too Much Detail&lt;/a>, I discussed the architecture of a
Neutron deployment in detail. Since that article was published,
Neutron gained the ability to handle multiple external networks with a
single L3 agent. While I &lt;a href="https://blog.oddbit.com/post/2014-05-28-multiple-external-networks-wit/">wrote about that&lt;/a> back in 2014, I
covered the configuration side of it in much more detail than I
discussed the underlying network architecture. This post addresses
the architecture side.&lt;/p>
&lt;h2 id="the-players">The players&lt;/h2>
&lt;p>This document describes the architecture that results from a
particular OpenStack configuration, specifically:&lt;/p>
&lt;ul>
&lt;li>Neutron networking using VXLAN or GRE tunnels;&lt;/li>
&lt;li>A dedicated network controller;&lt;/li>
&lt;li>Two external networks&lt;/li>
&lt;/ul>
&lt;h2 id="the-lay-of-the-land">The lay of the land&lt;/h2>
&lt;p>This is a simplified architecture diagram of the network connectivity
in this scenario:&lt;/p>
&lt;p>Everything on the compute hosts is identical to &lt;a href="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/">my previous
article&lt;/a>, so I will only be discussing the network host here.&lt;/p>
&lt;p>For the purposes of this article, we have two external networks and
two internal networks defined:&lt;/p>
&lt;pre>&lt;code>$ neutron net-list
+--------------------------------------+-----------+----------...------------------+
| id | name | subnets ... |
+--------------------------------------+-----------+----------...------------------+
| 6f0a5622-4d2b-4e4d-b34a-09b70cacf3f1 | net1 | beb767f8-... 192.168.101.0/24 |
| 972f2853-2ba6-474d-a4be-a400d4e3dc97 | net2 | f6d0ca0f-... 192.168.102.0/24 |
| 12136507-9bbe-406f-b68b-151d2a78582b | external2 | 106db3d6-... 172.24.5.224/28 |
| 973a6eb3-eaf8-4697-b90b-b30315b0e05d | external1 | fe8e8193-... 172.24.4.224/28 |
+--------------------------------------+-----------+----------...------------------+
&lt;/code>&lt;/pre>
&lt;p>And two routers:&lt;/p>
&lt;pre>&lt;code>$ neutron router-list
+--------------------------------------+---------+-----------------------...-------------------+...
| id | name | external_gateway_info ... |...
+--------------------------------------+---------+-----------------------...-------------------+...
| 1b19e179-5d67-4d80-8449-bab42119a4c5 | router2 | {&amp;quot;network_id&amp;quot;: &amp;quot;121365... &amp;quot;172.24.5.226&amp;quot;}]} |...
| e2117de3-58ca-420d-9ac6-c4eccf5e7a53 | router1 | {&amp;quot;network_id&amp;quot;: &amp;quot;973a6e... &amp;quot;172.24.4.227&amp;quot;}]} |...
+--------------------------------------+---------+-----------------------...-------------------+...
&lt;/code>&lt;/pre>
&lt;p>And our logical connectivity is:&lt;/p>
&lt;pre>&lt;code>+---------+ +----------+ +-------------+
| | | | | |
| net1 +----&amp;gt; router1 +----&amp;gt; external1 |
| | | | | |
+---------+ +----------+ +-------------+
+---------+ +----------+ +-------------+
| | | | | |
| net2 +----&amp;gt; router2 +----&amp;gt; external2 |
| | | | | |
+---------+ +----------+ +-------------+
&lt;/code>&lt;/pre>
&lt;h2 id="router-attachments-to-integration-bridge">Router attachments to integration bridge&lt;/h2>
&lt;p>In the &lt;a href="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/">legacy model&lt;/a>, in which an L3 agent supported a single
external network, the &lt;code>qrouter-...&lt;/code> namespaces that implement Neutron
routers were attached to both the integration bridge &lt;code>br-int&lt;/code> and the
external network bridge (the &lt;code>external_network_bridge&lt;/code> configuration
option from your &lt;code>l3_agent.ini&lt;/code>, often named &lt;code>br-ex&lt;/code>).&lt;/p>
&lt;p>In the provider network model, &lt;em>both&lt;/em> interfaces in a &lt;code>qrouter&lt;/code>
namespace are attached to the integration bridge. For the
configuration we&amp;rsquo;ve described above, the configuration of the
integration bridge ends up looking something like:&lt;/p>
&lt;pre>&lt;code>Bridge br-int
fail_mode: secure
Port &amp;quot;qvoc532d46c-33&amp;quot;
tag: 3
Interface &amp;quot;qvoc532d46c-33&amp;quot;
Port br-int
Interface br-int
type: internal
Port &amp;quot;qg-09e9da38-fb&amp;quot;
tag: 4
Interface &amp;quot;qg-09e9da38-fb&amp;quot;
type: internal
Port &amp;quot;qvo3ccea690-c2&amp;quot;
tag: 2
Interface &amp;quot;qvo3ccea690-c2&amp;quot;
Port &amp;quot;int-br-ex2&amp;quot;
Interface &amp;quot;int-br-ex2&amp;quot;
type: patch
options: {peer=&amp;quot;phy-br-ex2&amp;quot;}
Port &amp;quot;tapd2ff89e7-16&amp;quot;
tag: 2
Interface &amp;quot;tapd2ff89e7-16&amp;quot;
type: internal
Port patch-tun
Interface patch-tun
type: patch
options: {peer=patch-int}
Port &amp;quot;int-br-ex1&amp;quot;
Interface &amp;quot;int-br-ex1&amp;quot;
type: patch
options: {peer=&amp;quot;phy-br-ex1&amp;quot;}
Port &amp;quot;qr-affdbcee-5c&amp;quot;
tag: 3
Interface &amp;quot;qr-affdbcee-5c&amp;quot;
type: internal
Port &amp;quot;qr-b37877cd-42&amp;quot;
tag: 2
Interface &amp;quot;qr-b37877cd-42&amp;quot;
type: internal
Port &amp;quot;qg-19250d3f-5c&amp;quot;
tag: 1
Interface &amp;quot;qg-19250d3f-5c&amp;quot;
type: internal
Port &amp;quot;tap0881edf5-e5&amp;quot;
tag: 3
Interface &amp;quot;tap0881edf5-e5&amp;quot;
type: internal
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>qr-...&lt;/code> interface on each router is attached to an internal
network. The VLAN tag associated with this interface is whatever VLAN
Neutron has selected internally for the private network. In the above
output, these ports are on the network named &lt;code>net1&lt;/code>:&lt;/p>
&lt;pre>&lt;code>Port &amp;quot;qr-affdbcee-5c&amp;quot;
tag: 3
Interface &amp;quot;qr-affdbcee-5c&amp;quot;
type: internal
Port &amp;quot;tap0881edf5-e5&amp;quot;
tag: 3
Interface &amp;quot;tap0881edf5-e5&amp;quot;
type: internal
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>qr-affdbcee-5c&lt;/code> is &lt;code>router1&lt;/code>&amp;rsquo;s interface on that network, and
&lt;code>tap0881edf5-e5&lt;/code> is the port attached to a &lt;code>dhcp-...&lt;/code> namespace. The
same router is attached to the &lt;code>external1&lt;/code> network; this attachment is
represented by:&lt;/p>
&lt;pre>&lt;code>Port &amp;quot;qg-09e9da38-fb&amp;quot;
tag: 4
Interface &amp;quot;qg-09e9da38-fb&amp;quot;
type: internal
&lt;/code>&lt;/pre>
&lt;p>The external bridges are connected to the integration bridge using OVS
&amp;ldquo;patch&amp;rdquo; interfaces (the &lt;code>int-br-ex1&lt;/code> on the integration bridge and the
&lt;code>phy-br-ex1&lt;/code> interface on the &lt;code>br-ex1&lt;/code>).&lt;/p>
&lt;h2 id="from-here-to-there">From here to there&lt;/h2>
&lt;p>Connectivity between the &lt;code>qg-...&lt;/code> interface and the appropriate
external bridge (&lt;code>br-ex1&lt;/code> in this case) happens due to the VLAN tag
assigned on egress by the &lt;code>qg-...&lt;/code> interface and the following
OpenFlow rules associated with &lt;code>br-ex1&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-ex1
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=794.876s, table=0, n_packets=0, n_bytes=0, idle_age=794, priority=1 actions=NORMAL
cookie=0x0, duration=785.833s, table=0, n_packets=0, n_bytes=0, idle_age=785, priority=4,in_port=3,dl_vlan=4 actions=strip_vlan,NORMAL
cookie=0x0, duration=792.945s, table=0, n_packets=24, n_bytes=1896, idle_age=698, priority=2,in_port=3 actions=drop
&lt;/code>&lt;/pre>
&lt;p>Each of these rules contains some state information (like the
packet/byte counts), some conditions (like
&lt;code>priority=4,in_port=3,dl_vlan=4&lt;/code>) and one or more actions (like
&lt;code>actions=strip_vlan,NORMAL&lt;/code>). So, the second rule there matches
packets associated with VLAN tag 4 and strips the VLAN tag (after
which the packet is delivered to any physical interfaces that are
attached to this OVS bridge).&lt;/p>
&lt;p>Putting this all together:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>An outbound packet from a Nova server running on a compute node
enters via &lt;code>br-tun&lt;/code> (&lt;strong>H&lt;/strong>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Flow rules on &lt;code>br-tun&lt;/code> translate the tunnel id into an internal
VLAN tag.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The packet gets delivered to the &lt;code>qr-...&lt;/code> interface of the
appropriate router. (&lt;strong>O&lt;/strong>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The packet exits the &lt;code>qg-...&lt;/code> interface of the router (where it
is assigned the VLAN tag associated with the external network).
(&lt;strong>N&lt;/strong>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The packet is delivered to the external bridge, where a flow rule
strip the VLAN tag. (&lt;strong>P&lt;/strong>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The packet is sent out the physical interface associated with the
bridge.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="for-the-sake-of-completeness">For the sake of completeness&lt;/h2>
&lt;p>The second private network, &lt;code>net2&lt;/code>, is attached to &lt;code>router2&lt;/code> on the
&lt;code>qr-b37877cd-42&lt;/code> interface. It exits on the &lt;code>qg-19250d3f-5c&lt;/code>
interface, where packets will be assigned to VLAN 1:&lt;/p>
&lt;pre>&lt;code>Port &amp;quot;qr-b37877cd-42&amp;quot;
tag: 2
Interface &amp;quot;qr-b37877cd-42&amp;quot;
type: internal
Port &amp;quot;qg-19250d3f-5c&amp;quot;
tag: 1
Interface &amp;quot;qg-19250d3f-5c&amp;quot;
type: internal
&lt;/code>&lt;/pre>
&lt;p>The network interface configuration in the associated router namespace
looks like this:&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-1b19e179-5d67-4d80-8449-bab42119a4c5 ip a
30: qg-19250d3f-5c: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:01:e9:e3 brd ff:ff:ff:ff:ff:ff
inet 172.24.5.226/28 brd 172.24.5.239 scope global qg-19250d3f-5c
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe01:e9e3/64 scope link
valid_lft forever preferred_lft forever
37: qr-b37877cd-42: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:4c:6c:f2 brd ff:ff:ff:ff:ff:ff
inet 192.168.102.1/24 brd 192.168.102.255 scope global qr-b37877cd-42
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe4c:6cf2/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>OpenFlow rules attached to &lt;code>br-ex2&lt;/code> will match these packets:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-ex2
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=3841.678s, table=0, n_packets=0, n_bytes=0, idle_age=3841, priority=1 actions=NORMAL
cookie=0x0, duration=3831.396s, table=0, n_packets=0, n_bytes=0, idle_age=3831, priority=4,in_port=3,dl_vlan=1 actions=strip_vlan,NORMAL
cookie=0x0, duration=3840.085s, table=0, n_packets=26, n_bytes=1980, idle_age=3742, priority=2,in_port=3 actions=drop
&lt;/code>&lt;/pre>
&lt;p>We can see that the second rule here will patch traffic on VLAN 1
(&lt;code>priority=4,in_port=3,dl_vlan=1&lt;/code>) and strip the VLAN tag, after which
the packet will be delivered to any other interfaces attached to this
bridge.&lt;/p></content></item><item><title>In which we are amazed it doesn't all fall apart</title><link>https://blog.oddbit.com/post/2015-07-26-in-which-we-are-amazed-it-does/</link><pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-07-26-in-which-we-are-amazed-it-does/</guid><description>So, the Kilo release notes say:
nova-manage migrate-flavor-data But nova-manage says:
nova-manage db migrate_flavor_data But that says:
Missing arguments: max_number And the help says:
usage: nova-manage db migrate_flavor_data [-h] [--max-number &amp;lt;number&amp;gt;] Which indicates that &amp;ndash;max-number is optional, but whatever, so you try:
nova-manage db migrate_flavor_data --max-number 100 And that says:
Missing arguments: max_number So just for kicks you try:
nova-manage db migrate_flavor_data --max_number 100 And that says:
nova-manage: error: unrecognized arguments: --max_number So finally you try:</description><content>&lt;p>So, the Kilo release notes say:&lt;/p>
&lt;pre>&lt;code>nova-manage migrate-flavor-data
&lt;/code>&lt;/pre>
&lt;p>But nova-manage says:&lt;/p>
&lt;pre>&lt;code>nova-manage db migrate_flavor_data
&lt;/code>&lt;/pre>
&lt;p>But that says:&lt;/p>
&lt;pre>&lt;code>Missing arguments: max_number
&lt;/code>&lt;/pre>
&lt;p>And the help says:&lt;/p>
&lt;pre>&lt;code>usage: nova-manage db migrate_flavor_data [-h]
[--max-number &amp;lt;number&amp;gt;]
&lt;/code>&lt;/pre>
&lt;p>Which indicates that &amp;ndash;max-number is optional, but whatever, so you
try:&lt;/p>
&lt;pre>&lt;code>nova-manage db migrate_flavor_data --max-number 100
&lt;/code>&lt;/pre>
&lt;p>And that says:&lt;/p>
&lt;pre>&lt;code>Missing arguments: max_number
&lt;/code>&lt;/pre>
&lt;p>So just for kicks you try:&lt;/p>
&lt;pre>&lt;code>nova-manage db migrate_flavor_data --max_number 100
&lt;/code>&lt;/pre>
&lt;p>And that says:&lt;/p>
&lt;pre>&lt;code>nova-manage: error: unrecognized arguments: --max_number
&lt;/code>&lt;/pre>
&lt;p>So finally you try:&lt;/p>
&lt;pre>&lt;code>nova-manage db migrate_flavor_data 100
&lt;/code>&lt;/pre>
&lt;p>And holy poorly implement client, Batman, it works.&lt;/p></content></item><item><title>Mapping local users to Kerberos principals with SSSD</title><link>https://blog.oddbit.com/post/2015-07-16-mapping-local-users-to-kerbero/</link><pubDate>Thu, 16 Jul 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-07-16-mapping-local-users-to-kerbero/</guid><description>I work for an organization that follows the common model of assigning people systematically generated user ids. Like most technically inclined employees of this organization, I have local accounts on my workstation that don&amp;rsquo;t bear any relation to the generated account ids. For the most part this isn&amp;rsquo;t a problem, except that our organization uses Kerberos to authenticate access to a variety of resources (such as the mailserver and a variety of web applications).</description><content>&lt;p>I work for an organization that follows the common model of assigning
people systematically generated user ids. Like most technically
inclined employees of this organization, I have local accounts on my
workstation that don&amp;rsquo;t bear any relation to the generated account ids.
For the most part this isn&amp;rsquo;t a problem, except that our organization
uses Kerberos to authenticate access to a variety of resources (such
as the mailserver and a variety of web applications).&lt;/p>
&lt;p>In the past, I&amp;rsquo;ve gotten along by running an explicit &lt;code>kinit lkellogg@EXAMPLE.COM&lt;/code> on the command line once in a while, and that
works, but it&amp;rsquo;s not particularly graceful.&lt;/p>
&lt;p>I&amp;rsquo;m running Fedora, which of course ships with &lt;a href="https://fedorahosted.org/sssd/">SSSD&lt;/a>. Two of the
neat features available through SSSD are (a) you can have it acquire a
token for you automatically when you authenticate and (b) renew that
token periodically, assuming that you have a renewable token.&lt;/p>
&lt;p>There are two problems that were preventing me from taking advantage
of this service.&lt;/p>
&lt;h2 id="combining-kerberos-with-local-accounts">Combining Kerberos with local accounts&lt;/h2>
&lt;p>The first problem is that there is a general assumption that if you&amp;rsquo;re
using Kerberos for authentication, you are also using some sort of
enterprise-wide identity service like LDAP. The practical evidence of
this in SSSD is that you can&amp;rsquo;t use Kerberos as an &lt;code>auth_provider&lt;/code> if
you are using the &lt;code>local&lt;/code> &lt;code>id_provider&lt;/code>. If you attempt a naive
configuration that includes the following:&lt;/p>
&lt;pre>&lt;code>[domain/local]
id_provider = local
auth_provider = krb5
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll get:&lt;/p>
&lt;pre>&lt;code>(Thu Jul 16 22:19:44:802460 2015) [sssd] [confdb_get_domain_internal]
(0x0010): Local ID provider does not support [krb5] as an AUTH provider.
&lt;/code>&lt;/pre>
&lt;p>It turns out that you can work around this limitation with a &amp;ldquo;proxy&amp;rdquo;
identity provider. With this method, SSSD &lt;em>proxies&lt;/em> identity requests
to an existing NSS library. This can, for example, be used to get
SSSD to interoperate with a legacy NIS environment, as in &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/15/html/Deployment_Guide/sect-SSSD_User_Guide-Domain_Configuration_Options-Configuring_a_Proxy_Domain.html#sect-SSSD-proxy-krb5">this
example&lt;/a>:&lt;/p>
&lt;pre>&lt;code>[domain/PROXY_KRB5]
auth_provider = krb5
krb5_server = 192.168.1.1
krb5_realm = EXAMPLE.COM
id_provider = proxy
proxy_lib_name = nis
enumerate = true
cache_credentials = true
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>proxy_lib_name&lt;/code> setting identifies the particular NSS provider to
use for identity information. This would make use of the &lt;code>nis&lt;/code> NSS
module (&lt;code>libnss_nis.so.2&lt;/code>) for identity information while using
Kerberos for authentication.&lt;/p>
&lt;p>For my own use case I want to use my local accounts for identity
information, which means I need to use the &lt;code>files&lt;/code> NSS provider:&lt;/p>
&lt;pre>&lt;code>[domain/example.com]
id_provider = proxy
proxy_lib_name = files
auth_provider = krb5
&lt;/code>&lt;/pre>
&lt;h2 id="mapping-a-local-username-to-a-kerberos-principal">Mapping a local username to a Kerberos principal&lt;/h2>
&lt;p>The second problem I had been struggling with was how to map my local
username (&lt;code>lars&lt;/code>) to the organizational Kerberos principal
(&lt;code>lkellogg@EXAMPLE.COM&lt;/code>). I had originally been looking at solutions
involving &lt;code>kinit&lt;/code>, but despite promising verbage in the
&lt;a href="http://web.mit.edu/kerberos/krb5-1.12/doc/user/user_config/k5identity.html">k5identity(5&lt;/a> man page, I wasn&amp;rsquo;t meeting with much success.&lt;/p>
&lt;p>It turns out that SSSD has the &lt;code>krb5_map_user&lt;/code> option for exactly this
purpose; the syntax looks like:&lt;/p>
&lt;pre>&lt;code>krb5_map_user = &amp;lt;local name&amp;gt;:&amp;lt;principal name&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>So, for me:&lt;/p>
&lt;pre>&lt;code>krb5_map_user = lars:lkellogg
&lt;/code>&lt;/pre>
&lt;h2 id="automatic-ticket-renewal">Automatic ticket renewal&lt;/h2>
&lt;p>SSSD is able to automatically renew your Kerberos tickets for you,
provided that you&amp;rsquo;re able to acquire a renewable ticket. You can
check for this by running &lt;code>klist&lt;/code> and seeing if your ticket has a
&lt;code>renew until&lt;/code> date in the future, as in the following example:&lt;/p>
&lt;pre>&lt;code>Ticket cache: KEYRING:persistent:1000:krb_ccache_rOS6mR8
Default principal: lkellogg@REDHAT.COM
Valid starting Expires Service principal
07/17/2015 11:02:31 07/17/2015 21:02:31 krbtgt/REDHAT.COM@REDHAT.COM
renew until 07/24/2015 11:02:31
&lt;/code>&lt;/pre>
&lt;p>If you meet this criteria, then you can add the following
configuration options to your domain configuration:&lt;/p>
&lt;pre>&lt;code>krb5_renewable_lifetime = 7d
krb5_renew_interval = 30m
&lt;/code>&lt;/pre>
&lt;p>The first (&lt;code>krb5_renewable_lifetime&lt;/code>) specifies the renewable lifetime
to request when requesting a ticket, and the second (&lt;code>krb5_renew_interval&lt;/code>) indicates how often SSSD should check to see if the ticket should be renewed.&lt;/p>
&lt;h2 id="an-example-configuration">An example configuration&lt;/h2>
&lt;p>This is approximately (names of been changed to protect the innocent)
configuration that I am currently using with SSSD:&lt;/p>
&lt;pre>&lt;code>[domain/default]
cache_credentials = True
[sssd]
config_file_version = 2
reconnection_retries = 3
sbus_timeout = 30
services = nss, pam
domains = example.com
[nss]
filter_groups = root
filter_users = root
reconnection_retries = 3
[pam]
reconnection_retries = 3
[domain/example.com]
id_provider = proxy
proxy_lib_name = files
enumerate = True
auth_provider = krb5
krb5_server = kerberos.example.com
krb5_realm = EXAMPLE.COM
cache_credentials = True
krb5_store_password_if_offline = True
krb5_map_user = lars:lkellogg
chpass_provider = krb5
krb5_kpasswd = kerberos.example.com
offline_credentials_expiration = 0
krb5_renewable_lifetime = 7d
krb5_renew_interval = 30m
&lt;/code>&lt;/pre>
&lt;h2 id="configuring-pam">Configuring PAM&lt;/h2>
&lt;p>You&amp;rsquo;re not done yet! Once you have SSSD configured correctly, you
need to configure your system to make use of it for authentication.
First, you&amp;rsquo;ll want to ensure that your &lt;code>/etc/nsswitch.conf&lt;/code> file is
configured to use SSSD. You&amp;rsquo;ll want at least the &lt;code>passwd&lt;/code>, &lt;code>shadow&lt;/code>,
and &lt;code>group&lt;/code> databases configured to use SSSD:&lt;/p>
&lt;pre>&lt;code>passwd: files sss
shadow: files sss
group: files sss
&lt;/code>&lt;/pre>
&lt;p>Next, you&amp;rsquo;ll want configure PAM. On my system, I need to change two
configuration files:&lt;/p>
&lt;ul>
&lt;li>&lt;code>/etc/pam.d/system-auth&lt;/code>, which is the default for many services,
and&lt;/li>
&lt;li>&lt;code>/etc/pam.d/password-auth&lt;/code>, which provides defaults for other
services, including &lt;code>sshd&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>In my case, both files actually end up having identical content, which
looks like this (largely cribbed from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/15/html/Deployment_Guide/chap-SSSD_User_Guide-Setting_Up_SSSD.html">the Fedora documentation&lt;/a>):&lt;/p>
&lt;pre>&lt;code>#%PAM-1.0
# This file is auto-generated.
# User changes will be destroyed the next time authconfig is run.
auth required pam_env.so
auth sufficient pam_unix.so nullok try_first_pass
auth requisite pam_succeed_if.so uid &amp;gt;= 1000 quiet_success
auth sufficient pam_sss.so use_first_pass
auth required pam_deny.so
account required pam_unix.so
account sufficient pam_localuser.so
account sufficient pam_succeed_if.so uid &amp;lt; 1000 quiet
account [default=bad success=ok user_unknown=ignore] pam_sss.so
account required pam_permit.so
password requisite pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type=
password sufficient pam_unix.so sha512 shadow nullok try_first_pass use_authtok
password sufficient pam_sss.so use_authtok
password required pam_deny.so
session optional pam_keyinit.so revoke
session required pam_limits.so
-session optional pam_systemd.so
session [success=1 default=ignore] pam_succeed_if.so service in crond quiet use_uid
session optional pam_sss.so
session required pam_unix.so
&lt;/code>&lt;/pre>
&lt;p>Note the entries for &lt;code>pam_sss.so&lt;/code> in each stanza.&lt;/p>
&lt;h2 id="the-proof-in-the-pudding">The proof in the pudding&lt;/h2>
&lt;p>I start on my local system with no Kerberos tickets:&lt;/p>
&lt;pre>&lt;code>$ klist
klist: Credentials cache keyring 'persistent:1000:krb_ccache_Pzo4C6u' not found
&lt;/code>&lt;/pre>
&lt;p>Then I lock my screen and unlock it using my Kerberos password, and
now:&lt;/p>
&lt;pre>&lt;code>$ klist
Ticket cache: KEYRING:persistent:1000:krb_ccache_rOS6mR8
Default principal: lkellogg@EXAMPLE.COM
Valid starting Expires Service principal
07/16/2015 22:45:43 07/17/2015 08:45:43 krbtgt/EXAMPLE.COM@EXAMPLE.COM
renew until 07/23/2015 22:45:43
&lt;/code>&lt;/pre>
&lt;h2 id="troubleshooting">Troubleshooting&lt;/h2>
&lt;p>I found the easiest way to troubleshoot SSSD was to stop the service:&lt;/p>
&lt;pre>&lt;code># systemctl stop sssd
&lt;/code>&lt;/pre>
&lt;p>And then run &lt;code>sssd&lt;/code> on the command line in debug mode:&lt;/p>
&lt;pre>&lt;code># sssd -d 5 -i
&lt;/code>&lt;/pre>
&lt;p>This generates logs on &lt;code>stderr&lt;/code> and helped me identity problems in my
configuration.&lt;/p>
&lt;h2 id="kudos">Kudos&lt;/h2>
&lt;p>Thanks to Jakub Hrozek for suggesting the use of the a proxy identity
provider to overcome the limitation on combining Kerberos with the
&lt;code>local&lt;/code> provider.&lt;/p></content></item><item><title>OpenStack Networking without DHCP</title><link>https://blog.oddbit.com/post/2015-06-26-openstack-networking-without-d/</link><pubDate>Fri, 26 Jun 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-06-26-openstack-networking-without-d/</guid><description>In an OpenStack environment, cloud-init generally fetches information from the metadata service provided by Nova. It also has support for reading this information from a configuration drive, which under OpenStack means a virtual CD-ROM device attached to your instance containing the same information that would normally be available via the metadata service.
It is possible to generate your network configuration from this configuration drive, rather than relying on the DHCP server provided by your OpenStack environment.</description><content>&lt;p>In an OpenStack environment, &lt;a href="https://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> generally fetches
information from the metadata service provided by Nova. It also has
support for reading this information from a &lt;em>configuration drive&lt;/em>,
which under OpenStack means a virtual CD-ROM device attached to your
instance containing the same information that would normally be
available via the metadata service.&lt;/p>
&lt;p>It is possible to generate your network configuration from this
configuration drive, rather than relying on the DHCP server provided
by your OpenStack environment. In order to do this you will need to
make the following changes to your Nova configuration:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>You must be using a subnet that does have a DHCP server. This
means that you have created it using &lt;code>neutron subnet-create --disable-dhcp ...&lt;/code>, or that you disabled DHCP on an existing
network using &lt;code>neutron net-update --disable-dhcp ...&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You must set &lt;code>flat_inject&lt;/code> to &lt;code>true&lt;/code> in &lt;code>/etc/nova/nova.conf&lt;/code>.
This causes Nova to embed network configuration information in the
meta-data embedded on the configuration drive.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You must ensure that &lt;code>injected_network_template&lt;/code> in
&lt;code>/etc/nova/nova.conf&lt;/code> points to an appropriately formatted
template.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Cloud-init expects the network configuration information to be
presented in the format of a Debian &lt;code>/etc/network/interfaces&lt;/code> file,
even if you&amp;rsquo;re using it on RHEL (or a derivative). The template is
rendered using the &lt;a href="http://jinja.pocoo.org/docs/dev/">Jinja2&lt;/a> template engine, and receives a
top-level key called &lt;code>interfaces&lt;/code> that contains a list of
dictionaries, one for each interface.&lt;/p>
&lt;p>A template similar to the following ought to be sufficient:&lt;/p>
&lt;pre>&lt;code>{% for interface in interfaces %}
auto {{ interface.name }}
iface {{ interface.name }} inet static
address {{ interface.address }}
netmask {{ interface.netmask }}
broadcast {{ interface.broadcast }}
gateway {{ interface.gateway }}
dns-nameservers {{ interface.dns }}
{% endfor %}
&lt;/code>&lt;/pre>
&lt;p>This will directly populate &lt;code>/etc/network/interfaces&lt;/code> on an Ubuntu
system, or will get translated into
&lt;code>/etc/sysconfig/network-scripts/ifcfg-eth0&lt;/code> on a RHEL system (a RHEL
environment can only configure a single network interface using this
mechanism).&lt;/p></content></item><item><title>Heat-kubernetes Demo with Autoscaling</title><link>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</link><pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</guid><description>Next week is the Red Hat Summit in Boston, and I&amp;rsquo;ll be taking part in a Project Atomic presentation in which I will discuss various (well, two) options for deploying Atomic into an OpenStack environment, focusing on my heat-kubernetes templates.
As part of that presentation, I&amp;rsquo;ve put together a short demonstration video:
This shows off the autoscaling behavior available with recent versions of these templates (and also serves as a very brief introduction to working with Kubernetes).</description><content>&lt;p>Next week is the &lt;a href="http://www.redhat.com/summit/">Red Hat Summit&lt;/a> in Boston, and I&amp;rsquo;ll be taking part
in a &lt;a href="http://www.projectatomic.io/">Project Atomic&lt;/a> presentation in which I will discuss various
(well, two) options for deploying Atomic into an OpenStack
environment, focusing on my &lt;a href="https://github.com/projectatomic/heat-kubernetes/">heat-kubernetes&lt;/a> templates.&lt;/p>
&lt;p>As part of that presentation, I&amp;rsquo;ve put together a short demonstration video:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/tS5X0qi04ZU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>This shows off the autoscaling behavior available with recent versions
of these templates (and also serves as a very brief introduction to
working with Kubernetes).&lt;/p></content></item><item><title>Teach git about GIT_SSL_CIPHER_LIST</title><link>https://blog.oddbit.com/post/2015-05-08-git-ssl-cipher-list/</link><pubDate>Fri, 08 May 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-05-08-git-ssl-cipher-list/</guid><description>Someone named hithard on StackOverflow was trying to clone a git repository via https, and was running into an odd error: &amp;ldquo;Cannot communicate securely with peer: no common encryption algorithm(s).&amp;rdquo;. This was due to the fact that the server (openhatch.org) was configured to use a cipher suite that was not supported by default in the underlying SSL library (which could be either OpenSSL or NSS, depending on how git was built).</description><content>&lt;p>Someone named &lt;a href="https://stackoverflow.com/users/4713895/hithard">hithard&lt;/a> on &lt;a href="https://stackoverflow.com/">StackOverflow&lt;/a> was trying to clone a git repository via https, and was &lt;a href="https://stackoverflow.com/a/30090725/147356">running into an odd error&lt;/a>: &amp;ldquo;Cannot communicate securely with peer: no common encryption algorithm(s).&amp;rdquo;. This was due to the fact that the server (&lt;code>openhatch.org&lt;/code>) was configured to use a cipher suite that was not supported by default in the underlying SSL library (which could be either &lt;a href="https://www.openssl.org/">OpenSSL&lt;/a> or &lt;a href="https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS">NSS&lt;/a>, depending on how git was built).&lt;/p>
&lt;p>Many applications allow the user to configure an explicit list of ciphers to consider when negotiating a secure connection. For example, &lt;a href="https://curl.haxx.se/">curl&lt;/a> has the &lt;a href="https://curl.haxx.se/libcurl/c/CURLOPT_SSL_CIPHER_LIST.html">CURLOPT_SSL_CIPHER_LIST&lt;/a> option. This turns out to be especially relevant because git relies on &lt;a href="https://curl.haxx.se/libcurl/">libcurl&lt;/a> for all of its http operations, which means all we need to do is (a) create a new configuration option for git, and then (b) pass that value through to libcurl.&lt;/p>
&lt;p>I took a look at the code and it turned out to be surprisingly easy. The functional part of the patch ends up being less than 10 lines total:&lt;/p>
&lt;pre tabindex="0">&lt;code>diff --git a/http.c b/http.c
index 679862006..c5e947965 100644
--- a/http.c
+++ b/http.c
@@ -35,6 +35,7 @@ char curl_errorstr[CURL_ERROR_SIZE];
static int curl_ssl_verify = -1;
static int curl_ssl_try;
static const char *ssl_cert;
+static const char *ssl_cipherlist;
#if LIBCURL_VERSION_NUM &amp;gt;= 0x070903
static const char *ssl_key;
#endif
@@ -153,6 +154,8 @@ static int http_options(const char *var, const char *value, void *cb)
curl_ssl_verify = git_config_bool(var, value);
return 0;
}
+ if (!strcmp(&amp;#34;http.sslcipherlist&amp;#34;, var))
+ return git_config_string(&amp;amp;ssl_cipherlist, var, value);
if (!strcmp(&amp;#34;http.sslcert&amp;#34;, var))
return git_config_string(&amp;amp;ssl_cert, var, value);
#if LIBCURL_VERSION_NUM &amp;gt;= 0x070903
@@ -327,6 +330,13 @@ static CURL *get_curl_handle(void)
if (http_proactive_auth)
init_curl_http_auth(result);
+ if (getenv(&amp;#34;GIT_SSL_CIPHER_LIST&amp;#34;))
+ ssl_cipherlist = getenv(&amp;#34;GIT_SSL_CIPHER_LIST&amp;#34;);
+
+ if (ssl_cipherlist != NULL &amp;amp;&amp;amp; *ssl_cipherlist)
+ curl_easy_setopt(result, CURLOPT_SSL_CIPHER_LIST,
+ ssl_cipherlist);
+
if (ssl_cert != NULL)
curl_easy_setopt(result, CURLOPT_SSLCERT, ssl_cert);
if (has_cert_password())
&lt;/code>&lt;/pre>&lt;p>I &lt;a href="https://marc.info/?l=git&amp;amp;m=143100824118409&amp;amp;w=2">submitted this patch&lt;/a> to the git mailing list, and after some discussion and a few revisions it was accepted. This changed was &lt;a href="https://github.com/git/git/commit/f6f2a9e42d14e61429af418d8038aa67049b3821">committed to git&lt;/a> on May 8, 2015.&lt;/p></content></item><item><title>Suggestions for the Docker MAINTAINER directive</title><link>https://blog.oddbit.com/post/2015-04-27-suggestions-for-the-docker-mai/</link><pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-04-27-suggestions-for-the-docker-mai/</guid><description>Because nobody asked for it, this is my opinion on the use of the MAINTAINER directive in your Dockerfiles.
The documentation says simply:
The MAINTAINER instruction allows you to set the Author field of the generated images. Many people end up putting the name and email address of an actual person here. I think this is ultimately a bad idea, and does a disservice both to members of a project that produce Docker images and to people consuming those images.</description><content>&lt;p>Because nobody asked for it, this is my opinion on the use of the
&lt;code>MAINTAINER&lt;/code> directive in your Dockerfiles.&lt;/p>
&lt;p>The &lt;a href="https://docs.docker.com/reference/builder/#maintainer">documentation&lt;/a> says simply:&lt;/p>
&lt;pre>&lt;code>The MAINTAINER instruction allows you to set the Author field of the generated images.
&lt;/code>&lt;/pre>
&lt;p>Many people end up putting the name and email address of an actual
person here. I think this is ultimately a bad idea, and does a
disservice both to members of a project that produce Docker images and
to people consuming those images.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Your image probably has (or will have) more than one &amp;ldquo;maintainer&amp;rdquo;&lt;/p>
&lt;p>Any non-trivial project is going to have more than one person
contributing to things. If you are the original creator of a
Dockerfile, but later on someone else starts making some changes,
which of you is the &amp;ldquo;maintainer&amp;rdquo;?&lt;/p>
&lt;p>Furthermore, asserting individual ownership over something that is
better off being maintained collectively tends to discourage
people from making changes (oh, this belongs to Bob, I&amp;rsquo;d better not
touch it&amp;hellip;).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Individual contributors may feel overwhelmed&lt;/p>
&lt;p>The individual responsible for creating a Docker image may or may be
great at communicating with consumers. If all questions about an
image are going into a well-intentioned by ultimately unresponsive
black hole, nobody is going to be happy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Individual contributors come and go&lt;/p>
&lt;p>Most open source projects have fluid membership. Someone who is
around now and actively maintaining things may not be around several
months down the road. Having an absentee member listed as the
&amp;ldquo;maintainer&amp;rdquo; of your images means that email about those images will
probably not reach anybody in a position to respond.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Your project probably has a bug tracker&lt;/p>
&lt;p>Most projects have some sort of bug tracking mechanism available.
These are generally in place both to keep track of the bug reports
and support requests coming in as well as acting as a mechanism to
distribute the work involved in responding to them to all members of
a project.&lt;/p>
&lt;p>Ideally, you want questions about any images you maintain to go
through the same tracking mechanism.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>For all of these reasons, the &lt;code>MAINTAINER&lt;/code> field of your Dockerfiles
should point to either a web site URL or to a common project email
address that goes into a bug tracker or is at least distributed to
more than one person.&lt;/p></content></item><item><title>Using tools badly: time shifting git commits with Workinghours</title><link>https://blog.oddbit.com/post/2015-04-10-workinghours-time-shifting-git/</link><pubDate>Fri, 10 Apr 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-04-10-workinghours-time-shifting-git/</guid><description>This is a terrible hack. If you are easily offended by bad ideas implemented poorly, move along!
You are working on a wonderful open source project&amp;hellip;but you are not supposed to be working on that project! You&amp;rsquo;re supposed to be doing your real work! Unfortunately, your extra-curricular activity is well documented in the git history of your project for all to see:
And now your boss knows why the TPS reports are late.</description><content>&lt;p>This is a terrible hack. If you are easily offended by bad ideas
implemented poorly, move along!&lt;/p>
&lt;p>You are working on a wonderful open source project&amp;hellip;but you are not
&lt;em>supposed&lt;/em> to be working on that project! You&amp;rsquo;re supposed to be doing
your &lt;em>real&lt;/em> work! Unfortunately, your extra-curricular activity is
well documented in the git history of your project for all to see:&lt;/p>
&lt;p>&lt;img src="repo-before.png" alt="Heatmap of original commit history">&lt;/p>
&lt;p>And now your boss knows why the TPS reports are late. You need
&lt;a href="https://github.com/larsks/workinghours.git">workinghours&lt;/a>, a terrible utility for doing awful things to your
repository history. &lt;a href="https://github.com/larsks/workinghours.git">Workinghours&lt;/a> will programatically time shift
your git commits so that they appear to have happened within specified
time intervals (for example, &amp;ldquo;between 7PM and midnight&amp;rdquo;).&lt;/p>
&lt;p>Running &lt;code>workinghours&lt;/code> on your repository makes things better:&lt;/p>
&lt;pre>&lt;code>workinghours --afterhours | workinghours-apply
&lt;/code>&lt;/pre>
&lt;p>And now you have:&lt;/p>
&lt;p>&lt;img src="repo-after.png" alt="Heatmap of modified commit history">&lt;/p>
&lt;p>But that looks suspicious. What are you, some kind of machine?
Fortunately, &lt;code>workinghours&lt;/code> has a &lt;code>--drift&lt;/code> option that will introduce
some variety into your start and end times. The syntax is &lt;code>--drift P before after&lt;/code>, where for each commit &lt;code>workinghours&lt;/code> will with
probability &lt;em>P&lt;/em> extend the beginning of the time interval by a random
amount between 0 and &lt;em>before&lt;/em>
hours, and the end of the time interval by a random amount between 0
and &lt;em>after&lt;/em> hours.&lt;/p>
&lt;p>Introducing a low probability drift to the beginning of the interval:&lt;/p>
&lt;pre>&lt;code>workinghours --afterhours -d 0.2 8 2 | workinghours-apply
&lt;/code>&lt;/pre>
&lt;p>Gives us:&lt;/p>
&lt;p>&lt;img src="repo-drifted.png" alt="Heatmap of modified commit history">&lt;/p>
&lt;p>Congratulations, you are a model employee.&lt;/p></content></item><item><title>Booting cloud images with libvirt</title><link>https://blog.oddbit.com/post/2015-03-10-booting-cloud-images-with-libv/</link><pubDate>Tue, 10 Mar 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-03-10-booting-cloud-images-with-libv/</guid><description>Most major distributions now provide &amp;ldquo;cloud-enabled&amp;rdquo; images designed for use in cloud environments like OpenStack and AWS. These images are usually differentiated by (a) being relatively small, and (b) running cloud-init at boot to perform initial system configuration tasks using metadata provided by the cloud environment.
Because of their small size and support for automatic configuration (including such useful tasks as provisioning ssh keys), these images are attractive for use outside of a cloud environment.</description><content>&lt;p>Most major distributions now provide &amp;ldquo;cloud-enabled&amp;rdquo; images designed
for use in cloud environments like OpenStack and AWS. These images
are usually differentiated by (a) being relatively small, and (b) running
&lt;a href="http://cloudinit.readthedocs.org/">cloud-init&lt;/a> at boot to perform initial system configuration tasks
using metadata provided by the cloud environment.&lt;/p>
&lt;p>Because of their small size and support for automatic configuration
(including such useful tasks as provisioning ssh keys), these images
are attractive for use &lt;em>outside&lt;/em> of a cloud environment.
Unfortunately, when people first try to boot them they are met with
frustration as first the image takes forever to boot as it tries to
contact a non-existent metadata service, and then when it finally does
boot they are unable to log in because the images typically only
support key-based login.&lt;/p>
&lt;p>Fortunately, there are ways to work around these issues. In addition
to working with various network-accessible metadata services,
&lt;a href="http://cloudinit.readthedocs.org/">cloud-init&lt;/a> is also able to read configuration information from an
attached [virtual] CD-ROM device. This is known as a &amp;ldquo;configuration
drive&amp;rdquo;, and it is relatively easy to create.&lt;/p>
&lt;p>For this purpose, the simplest solution is use &lt;a href="http://cloudinit.readthedocs.org/">cloud-init&lt;/a>&amp;rsquo;s &amp;ldquo;no
cloud&amp;rdquo; data source. For this, we need to create an ISO filesystem
creating two files, &lt;code>meta-data&lt;/code> and (optionally) &lt;code>user-data&lt;/code>.&lt;/p>
&lt;h2 id="the-meta-data-file">The meta-data file&lt;/h2>
&lt;p>The &lt;code>meta-data&lt;/code> file is effectively a YAML version of the data
typically available in the EC2 metadata service, and will look
something like this:&lt;/p>
&lt;pre>&lt;code>instance-id: my-instance-id
local-hostname: my-host-name
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>instance-id&lt;/code> key is required. You can also include SSH public
keys in this file, like this:&lt;/p>
&lt;pre>&lt;code>instance-id: my-instance-id
local-hostname: my-host-name
public-keys:
- ssh-rsa AAAAB3NzaC1...
&lt;/code>&lt;/pre>
&lt;p>You will see examples that place ssh keys in the &lt;code>user-data&lt;/code> file
instead, but I believe this is the wrong solution, since it forces you
to use a &amp;ldquo;cloud-config&amp;rdquo; format &lt;code>user-data&lt;/code> file. Putting ssh keys
into the &lt;code>meta-data&lt;/code> provides you more flexibility with your
&lt;code>user-data&lt;/code> content.&lt;/p>
&lt;h2 id="the-user-data-file">The user-data file&lt;/h2>
&lt;p>The &lt;code>user-data&lt;/code> can be any of the various formats &lt;a href="http://cloudinit.readthedocs.org/en/latest/topics/format.html">supported by
cloud-init&lt;/a>. For example, it could simply be a shell script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
yum -y install some-critical-package
&lt;/code>&lt;/pre>
&lt;p>Or it could be a &lt;a href="http://cloudinit.readthedocs.org/en/latest/topics/examples.html#yaml-examples">cloud-config&lt;/a> YAML document:&lt;/p>
&lt;pre>&lt;code>#cloud-config
write-files:
- path: /etc/profile.d/gitaliases.sh
content: |
alias gc=&amp;quot;git commit&amp;quot;
alias gcv=&amp;quot;git commit --no-verify&amp;quot;
runcmd:
- setenforce 1
&lt;/code>&lt;/pre>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>Once you have created your &lt;code>meta-data&lt;/code> and &lt;code>user-data&lt;/code> files, you can
create the configuration drive like this:&lt;/p>
&lt;pre>&lt;code>genisoimage -o config.iso -V cidata -r -J meta-data user-data
&lt;/code>&lt;/pre>
&lt;p>To boot an instance using this configuration drive, you could do
something like this:&lt;/p>
&lt;pre>&lt;code>virt-install -n example -r 512 -w network=default \
--disk vol=default/fedora-21-cloud.qcow2 --import \
--disk path=config.iso,device=cdrom
&lt;/code>&lt;/pre>
&lt;p>(This assumes, obviously, that you have an image named
&lt;code>fedora-21-cloud.qcow2&lt;/code> available in libvirt&amp;rsquo;s &lt;code>default&lt;/code> storage
pool.)&lt;/p>
&lt;h2 id="a-little-automation">A little automation&lt;/h2>
&lt;p>I have written a &lt;a href="https://github.com/larsks/virt-utils/blob/master/create-config-drive">create-config-drive&lt;/a> script that will automate
this process. With this script available, the above process is
simply:&lt;/p>
&lt;pre>&lt;code>create-config-drive -k ~/.ssh/id_rsa.pub -u user-data config.iso
adding pubkey from /home/lars/.ssh/id_rsa.pub
adding user data from userdata
generating configuration image at config.iso
&lt;/code>&lt;/pre></content></item><item><title>Diagnosing problems with an OpenStack deployment</title><link>https://blog.oddbit.com/post/2015-03-09-diagnosing-problems-with-an-op/</link><pubDate>Mon, 09 Mar 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-03-09-diagnosing-problems-with-an-op/</guid><description>I recently had the chance to help a colleague debug some problems in his OpenStack installation. The environment was unique because it was booting virtualized aarch64 instances, which at the time did not have any PCI bus support&amp;hellip;which in turn precluded things like graphic consoles (i.e., VNC or SPICE consoles) for the Nova instances.
This post began life as an email summarizing the various configuration changes we made on the systems to get things up and running.</description><content>&lt;p>I recently had the chance to help a colleague debug some problems in
his OpenStack installation. The environment was unique because it was
booting virtualized &lt;a href="https://fedoraproject.org/wiki/Architectures/AArch64">aarch64&lt;/a> instances, which at the time did not
have any PCI bus support&amp;hellip;which in turn precluded things like graphic
consoles (i.e., VNC or SPICE consoles) for the Nova instances.&lt;/p>
&lt;p>This post began life as an email summarizing the various configuration
changes we made on the systems to get things up and running. After
writing it, I decided it presented an interesting summary of some
common (and maybe not-so-common) issues, so I am posting it here in
the hopes that other folks will find it interesting.&lt;/p>
&lt;h2 id="serial-console-configuration">Serial console configuration&lt;/h2>
&lt;h3 id="the-problem">The problem&lt;/h3>
&lt;p>We needed console access to the Nova instances in order to diagnose
some networking issues, but there was no VGA console support in the
virtual machines. Recent versions of Nova provide serial console
support, but do not provide any client-side tool for &lt;em>accessing&lt;/em> the
serial console.&lt;/p>
&lt;p>We wanted to:&lt;/p>
&lt;ul>
&lt;li>Correctly configure Nova to provide serial console support, and&lt;/li>
&lt;li>Get the &lt;a href="https://github.com/larsks/novaconsole">novaconsole&lt;/a> tool installed in order to access the serial
consoles.&lt;/li>
&lt;/ul>
&lt;h3 id="making-novaconsole-work">Making novaconsole work&lt;/h3>
&lt;p>In order to get &lt;code>novaconsole&lt;/code> installed we needed the
&lt;code>websocket-client&lt;/code> library, which is listed in &lt;code>requirements.txt&lt;/code> at
the top level of the &lt;code>novaconsole&lt;/code> source. Normally one would just
&lt;code>pip install .&lt;/code> from the source directory, but &lt;code>python-pip&lt;/code> was not
available on our platform.&lt;/p>
&lt;p>That wasn&amp;rsquo;t a big issue because we &lt;em>did&lt;/em> have &lt;code>python-setuptools&lt;/code>
available, so I was able to simply run (inside the &lt;code>novaconsole&lt;/code>
source directory):&lt;/p>
&lt;pre>&lt;code>python setup.py install
&lt;/code>&lt;/pre>
&lt;p>And now we had a &lt;code>/usr/bin/novaconsole&lt;/code> script, and we were able to
use it like this to connect to the console of a nova instance named
&amp;ldquo;test0&amp;rdquo;:&lt;/p>
&lt;pre>&lt;code>novaconsole test0
&lt;/code>&lt;/pre>
&lt;p>(For this to work you need appropriate Keystone credentials loaded in
your environment. You can also provide a websocket URL in lieu of an
instance name.)&lt;/p>
&lt;h3 id="configuration-changes-on-the-controller">Configuration changes on the controller&lt;/h3>
&lt;p>The controller did not have the &lt;code>openstack-nova-serialproxy&lt;/code> package
installed, which provides the &lt;code>nova-serialproxy&lt;/code> service. This service
provides the websocket endpoint used by clients, so without this
service you you won&amp;rsquo;t be able to connect to serial consoles.&lt;/p>
&lt;p>Installing the service was a simple matter of:&lt;/p>
&lt;pre>&lt;code>yum -y install openstack-nova-serialproxy
systemctl enable openstack-nova-serialproxy
systemctl start openstack-nova-serialproxy
&lt;/code>&lt;/pre>
&lt;h3 id="configuration-changes-on-the-compute-nodes">Configuration changes on the compute nodes&lt;/h3>
&lt;p>We also need to enable the serial console support on our compute nodes,
and we need to change the following configuration options in
&lt;code>nova.conf&lt;/code> in the &lt;code>serial_console&lt;/code> section:&lt;/p>
&lt;pre>&lt;code># Set this to 'true' to enable serial console support.
enabled=true
# Enabling serial console support means that spawning an instance
# causes qemu to open up a listening TCP socket for the serial
# console. This socket binds to the `listen` address. It
# defaults to 127.0.0.1, which will not permit a remote host --
# such as your controller -- from connecting to the port. Setting
# this to 0.0.0.0 means &amp;quot;listen on all available addresses&amp;quot;, which
# is *usually* what you want.
listen=0.0.0.0
# `proxyclient_address` is the address to which the
# nova-serialproxy service will connect to access serial consoles
# of instances located on this physical host. That means it needs
# to be an address of a local interface (and so this value will be
# unique to each compute host).
proxyclient_address=10.16.184.118
&lt;/code>&lt;/pre>
&lt;p>In a production deployment, we would also need to modify the
&lt;code>base_url&lt;/code> option in this section, which is used to generate the URLs
provided via the &lt;code>nova get-serial-console&lt;/code> command. With the default
configuration, the URLs will point to 127.0.0.1, which is fine as long
as we are running &lt;code>novaconsole&lt;/code> on the same host as
&lt;code>nova-serialproxy&lt;/code>.&lt;/p>
&lt;p>After making these changes, we need to restart nova-compute on all the
compute hosts:&lt;/p>
&lt;pre>&lt;code># openstack-service restart nova
&lt;/code>&lt;/pre>
&lt;p>&lt;em>And&lt;/em> we will need to re-deploy any running instances, because they
will still have sockets listening on 127.0.0.1.&lt;/p>
&lt;p>The network ports opened for the serial console service are controlled
by the &lt;code>port_range&lt;/code> setting in the &lt;code>serial_console&lt;/code> section. We must
permit connections to these ports from our controller. I added the
following rule with iptables:&lt;/p>
&lt;pre>&lt;code># iptables -I INPUT 1 -p tcp --dport 10000:20000 -j ACCEPT
&lt;/code>&lt;/pre>
&lt;p>In practice, we would probably want to limit this specifically to our
controller(s).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="networking-on-the-controller">Networking on the controller&lt;/h2>
&lt;h3 id="the-problem-1">The problem&lt;/h3>
&lt;p>Nova instances were not successfully obtaining ip addresses from the
Nova-managed DHCP service.&lt;/p>
&lt;h3 id="selinux-and-the-case-of-the-missing-interfaces">Selinux and the case of the missing interfaces&lt;/h3>
&lt;p>When I first looked at the system, it was obvious that something
fundamental was broken because the Neutron routers were missing
interfaces.&lt;/p>
&lt;p>Each neutron router is realized as a network namespace on the
network host. We can see these namespaces with the &lt;code>ip netns&lt;/code>
command:&lt;/p>
&lt;pre>&lt;code># ip netns
qrouter-42389195-c8c1-4d68-a16c-3937453f149d
qdhcp-d2719d67-fd00-4620-be00-ea8525dc6524
&lt;/code>&lt;/pre>
&lt;p>We can use the &lt;code>ip netns exec&lt;/code> command to run commands inside the
router namespace. For instance, we can run the following to see a
list of network interfaces inside the namespace:&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-42389195-c8c1-4d68-a16c-3937453f149d \
ip addr show
&lt;/code>&lt;/pre>
&lt;p>For a router we would expect to see something like this:&lt;/p>
&lt;pre>&lt;code>1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
18: qr-b3cd13d6-94: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:61:89:49 brd ff:ff:ff:ff:ff:ff
inet 10.0.0.1/24 brd 10.0.0.255 scope global qr-b3cd13d6-94
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe61:8949/64 scope link
valid_lft forever preferred_lft forever
19: qg-89591203-47: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:30:b5:05 brd ff:ff:ff:ff:ff:ff
inet 172.24.4.231/28 brd 172.24.4.239 scope global qg-89591203-47
valid_lft forever preferred_lft forever
inet 172.24.4.232/32 brd 172.24.4.232 scope global qg-89591203-47
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe30:b505/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>But all I found was the loopback interface:&lt;/p>
&lt;pre>&lt;code>1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>After a few attempts to restore the router to health through API commands (such
as by clearing and re-setting the network gateway), I looked in the
logs for the &lt;code>neutron-l3-agent&lt;/code> service, which is the service
responsible for configuring the routers. There I found:&lt;/p>
&lt;pre>&lt;code>2015-02-20 17:16:52.324 22758 TRACE neutron.agent.l3_agent Stderr:
'Error: argument &amp;quot;qrouter-42389195-c8c1-4d68-a16c-3937453f149d&amp;quot; is
wrong: Invalid &amp;quot;netns&amp;quot; value\n\n'
&lt;/code>&lt;/pre>
&lt;p>This is weird because clearly a network namespace with a matching name
was available. When weird inexplicable errors happen, we often look
first to selinux, and indeed, running &lt;code>audit2allow -a&lt;/code> showed us that
neutron was apparently missing a privilege:&lt;/p>
&lt;pre>&lt;code>#============= neutron_t ==============
allow neutron_t unlabeled_t:file { read open };
&lt;/code>&lt;/pre>
&lt;p>After putting selinux in permissive mode&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> and restarting neutron
services, things looked a lot better. To completely restart neutron,
I usually do:&lt;/p>
&lt;pre>&lt;code>openstack-service stop neutron
neutron-ovs-cleanup
neutron-netns-cleanup
openstack-service start neutron
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>openstack-service&lt;/code> command is a wrapper over &lt;code>systemctl&lt;/code> or
&lt;code>chkconfig&lt;/code> and &lt;code>service&lt;/code> that operates on whatever openstack services
you have enabled on your host. Providing a additional arguments
limits the action to services that match that name, so in addition to
&lt;code>openstack-service stop neutron&lt;/code> you can do something like
&lt;code>openstack-service stop nova glance&lt;/code> to stop all Nova and Glance
services, etc.&lt;/p>
&lt;h3 id="iptables-and-the-case-of-the-missing-packets">Iptables and the case of the missing packets&lt;/h3>
&lt;p>After diagnosing the selinux issue noted above, virtual networking
layer looked fine, but we still weren&amp;rsquo;t able to get traffic between
the test instance and the router/dhcp server on the controller.&lt;/p>
&lt;p>Traffic was clearly traversing the VXLAN tunnels, as revealed by
running &lt;code>tcpdump&lt;/code> on both ends of the tunnel (where 4789 is the vxlan
port):&lt;/p>
&lt;pre>&lt;code>tcpdump -i eth0 -n port 4789
&lt;/code>&lt;/pre>
&lt;p>But that traffic was never reaching, e.g., the dhcp namespace.
Investigating the Open vSwitch (OVS) configuration on our host showed
that everything look correct; commands I use to look at things were:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>ovs-vsctl show&lt;/code> to look at the basic layout of switches and
interfaces,&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ovs-ofctl dump-flows &amp;lt;bridge&amp;gt;&lt;/code> to look at the openflow rules
associated with a particular OVS switch, and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ovs-dpctl-top&lt;/code>, which provides a top-like view of flow activity on
the OVS bridges.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Ultimately, it turns out that there were some iptables rule missing
from our configuration. On the host, looking for rules that match
vxlan traffic I found a single rule for vxlan traffic:&lt;/p>
&lt;pre>&lt;code># iptables -S | grep 4789
-A INPUT -s 10.16.184.117/32 -p udp -m multiport --dports 4789 ...
&lt;/code>&lt;/pre>
&lt;p>The compute node we were operating with was 10.16.184.118 (which is
not the address listed in the above rule), so vxlan traffic from this
host was being rejected by the kernel. I added a new rule to match
vxlan traffic from the compute host:&lt;/p>
&lt;pre>&lt;code># iptables -I INPUT 18 -s 10.16.184.118/32 -p udp -m multiport --dports 4789 ...
&lt;/code>&lt;/pre>
&lt;p>This seemed to take care of things, but it&amp;rsquo;s a bit of a mystery why
this wasn&amp;rsquo;t configured for us in the first place by the installer.
This may have been a bug in &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a>; we would need to do clean
re-deploy to verify this behavior.&lt;/p>
&lt;h3 id="access-to-floating-ip-addresses">Access to floating ip addresses&lt;/h3>
&lt;p>In order to access our instances using their floating ip addresses
from our host, we need a route to the floating ip network. The
easiest way to do this in a test environment, if you are happy with
host-only networking, is to assign interface &lt;code>br-ex&lt;/code> the address of
the default gateway for your floating ip network. The default
floating ip network configured by &lt;code>packstack&lt;/code> is 172.24.4.224/28, and
the gateway for that network is &lt;code>172.24.24.225&lt;/code>. We can assign this
address to &lt;code>br-ex&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code># ip addr add 172.24.4.225/28 dev br-ex
&lt;/code>&lt;/pre>
&lt;p>With this in place, connections to floating ips will route via br-ex,
which in turn is bridged to the external interface of your neutron
router.&lt;/p>
&lt;p>Setting the address by hand like this means it will be lost next time
we reboot. We can make this configuration persistent by modifying (or
creating)
&lt;code>/etc/sysconfig/network-scripts/ifcfg-br-ex&lt;/code> so that it looks like this:&lt;/p>
&lt;pre>&lt;code>DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROT=static
IPADDR=172.24.4.225
NETMASK=255.255.255.240
ONBOOT=yes
&lt;/code>&lt;/pre>
&lt;p>If you&amp;rsquo;re not able to map CIDR prefixes to dotted-quad netmasks in
your head, the &lt;code>ipcalc&lt;/code> tool is useful:&lt;/p>
&lt;pre>&lt;code>$ ipcalc -m 172.24.4.224/28
NETMASK=255.255.255.240
&lt;/code>&lt;/pre>
&lt;h2 id="the-state-of-things">The state of things&lt;/h2>
&lt;p>With all the above changes in place, we had a functioning OpenStack
environment.&lt;/p>
&lt;p>We could spawn an instance as the &amp;ldquo;demo&amp;rdquo; user:&lt;/p>
&lt;pre>&lt;code># . keystonerc_demo
# nova boot --image &amp;quot;rhelsa&amp;quot; --flavor m1.small example
&lt;/code>&lt;/pre>
&lt;p>Create a floating ip address:&lt;/p>
&lt;pre>&lt;code># nova floating-ip-create public
+--------------+-----------+----------+--------+
| Ip | Server Id | Fixed Ip | Pool |
+--------------+-----------+----------+--------+
| 172.24.4.233 | - | - | public |
+--------------+-----------+----------+--------+
&lt;/code>&lt;/pre>
&lt;p>Assign that address to our instance:&lt;/p>
&lt;pre>&lt;code># nova floating-ip-associate example 172.4.4.233
&lt;/code>&lt;/pre>
&lt;p>And finally we were able to access services on that instance (provided
that our security groups (and local iptables configuration on the
instance) permit access to that service).&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&amp;hellip;as a temporary measure, pending opening a bug report to get
things corrected so that this step would no longer be necessary.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></content></item><item><title>Converting hexadecimal ip addresses to dotted quads with Bash</title><link>https://blog.oddbit.com/post/2015-03-08-converting-hexadecimal-ip-addr/</link><pubDate>Sun, 08 Mar 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-03-08-converting-hexadecimal-ip-addr/</guid><description>This is another post that is primarily for my own benefit for the next time I forget how to do this.
I wanted to read routing information directly from /proc/net/route using bash, because you never know what may or may not be available in the minimal environment of a Docker container (for example, the iproute package is not installed by default in the Fedora Docker images). The contents of /proc/net/route looks something like:</description><content>&lt;p>This is another post that is primarily for my own benefit for the next
time I forget how to do this.&lt;/p>
&lt;p>I wanted to read routing information directly from &lt;code>/proc/net/route&lt;/code>
using &lt;code>bash&lt;/code>, because you never know what may or may not be available
in the minimal environment of a Docker container (for example, the
&lt;code>iproute&lt;/code> package is not installed by default in the Fedora Docker
images). The contents of &lt;code>/proc/net/route&lt;/code> looks something like:&lt;/p>
&lt;pre>&lt;code>Iface Destination Gateway Flags RefCnt Use Metric Mask MTU Window IRTT
eth0 00000000 0101A8C0 0003 0 0 1024 00000000 0 0 0
eth0 37E9BB42 0101A8C0 0007 0 0 20 FFFFFFFF 0 0 0
&lt;/code>&lt;/pre>
&lt;p>If I want the address of the default gateway, I can trivially get the
hexadecimal form like this:&lt;/p>
&lt;pre>&lt;code>awk '$2 == &amp;quot;00000000&amp;quot; {print $3}' /proc/net/route
&lt;/code>&lt;/pre>
&lt;p>Which gives me:&lt;/p>
&lt;pre>&lt;code>0101A8C0
&lt;/code>&lt;/pre>
&lt;p>This is in little-endian order; that is, the above bytes represent &lt;code>1 1 168 192&lt;/code>, which you may recognize better as &lt;code>192.168.1.1&lt;/code>. So, we
need to convert this into a sequence of individual octets, reverse the
order, and produce the decimal equivalent of each octet.&lt;/p>
&lt;p>The following gives us the octets in the correct order, prefixed by
&lt;code>0x&lt;/code> (which we&amp;rsquo;re going to want in the next step):&lt;/p>
&lt;pre>&lt;code>awk '$2 == &amp;quot;00000000&amp;quot; {print $3}' /proc/net/route |
sed 's/../0x&amp;amp; /g' | tr ' ' '\n' | tac
&lt;/code>&lt;/pre>
&lt;p>We can put this into a bash array like this:&lt;/p>
&lt;pre>&lt;code>octets=($(
awk '$2 == &amp;quot;00000000&amp;quot; {print $3}' /proc/net/route |
sed 's/../0x&amp;amp; /g' | tr ' ' '\n' | tac
))
&lt;/code>&lt;/pre>
&lt;p>And we convert those hexadecimal octets into decimal like this:&lt;/p>
&lt;pre>&lt;code>printf &amp;quot;%d.&amp;quot; ${octets[@]} | sed 's/\.$/\n/'
&lt;/code>&lt;/pre>
&lt;p>An interesting feature of the Bash &lt;code>printf&lt;/code> command &amp;ndash; and one that
may be surprising to people who are coming from a C background &amp;ndash; is
that:&lt;/p>
&lt;blockquote>
&lt;p>The format is re-used as necessary to consume all of the arguments.&lt;/p>
&lt;/blockquote>
&lt;p>That means, that a command like this:&lt;/p>
&lt;pre>&lt;code>printf &amp;quot;%d.&amp;quot; 1 2 3 4
&lt;/code>&lt;/pre>
&lt;p>Will yield:&lt;/p>
&lt;pre>&lt;code>1.2.3.4.
&lt;/code>&lt;/pre>
&lt;p>If we put this all together, we might end up with something like:&lt;/p>
&lt;pre>&lt;code>hexaddr=$(awk '$2 == &amp;quot;00000000&amp;quot; {print $3}' /proc/net/route)
ipaddr=$(printf &amp;quot;%d.&amp;quot; $(
echo $hexaddr | sed 's/../0x&amp;amp; /g' | tr ' ' '\n' | tac
) | sed 's/\.$/\n/')&lt;/code>&lt;/pre></content></item><item><title>Visualizing Pacemaker resource constraints</title><link>https://blog.oddbit.com/post/2015-02-24-visualizing-pacemaker-constrai/</link><pubDate>Tue, 24 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-24-visualizing-pacemaker-constrai/</guid><description>If a picture is worth a thousand words, then code that generates pictures from words is worth&amp;hellip;uh, anyway, I wrote a script that produces dot output from Pacemaker start and colocation constraints:
https://github.com/larsks/pacemaker-tools/
You can pass this output to graphviz to create visualizations of your Pacemaker resource constraints.
The graph-constraints.py script in that repository consumes the output of cibadmin -Q and can produce output for either start constraints (-S, the default) or colocation constraints (-C).</description><content>&lt;p>If a picture is worth a thousand words, then code that generates
pictures from words is worth&amp;hellip;uh, anyway, I wrote a script that
produces &lt;a href="http://en.wikipedia.org/wiki/DOT_%28graph_description_language%29">dot&lt;/a> output from Pacemaker start and colocation
constraints:&lt;/p>
&lt;p>&lt;a href="https://github.com/larsks/pacemaker-tools/">https://github.com/larsks/pacemaker-tools/&lt;/a>&lt;/p>
&lt;p>You can pass this output to &lt;a href="http://www.graphviz.org/">graphviz&lt;/a> to create visualizations of
your Pacemaker resource constraints.&lt;/p>
&lt;p>The &lt;code>graph-constraints.py&lt;/code> script in that repository consumes the
output of &lt;code>cibadmin -Q&lt;/code> and can produce output for either start
constraints (&lt;code>-S&lt;/code>, the default) or colocation constraints (&lt;code>-C&lt;/code>).&lt;/p>
&lt;p>Given a document like &lt;a href="cib.xml">this&lt;/a>, if you run:&lt;/p>
&lt;pre>&lt;code>cibadmin -Q |
python graph-constraints.py -o cib.svg
&lt;/code>&lt;/pre>
&lt;p>You get a graph like &lt;a href="cib.svg">this&lt;/a>:&lt;/p>
&lt;figure class="left" >
&lt;img src="cib.svg" />
&lt;/figure>
&lt;p>Nodes are colored by their tag (so, &lt;code>primitive&lt;/code>, &lt;code>clone&lt;/code>, etc).&lt;/p></content></item><item><title>Stupid Pacemaker XML tricks</title><link>https://blog.oddbit.com/post/2015-02-19-stupid-pacemaker-xml-tricks/</link><pubDate>Thu, 19 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-19-stupid-pacemaker-xml-tricks/</guid><description>I&amp;rsquo;ve recently spent some time working with Pacemaker, and ended up with an interesting collection of XPath snippets that I am publishing here for your use and/or amusement.
Check if there are any inactive resources pcs status xml | xmllint --xpath '//resource[@active=&amp;quot;false&amp;quot;]' - &amp;gt;&amp;amp;/dev/null &amp;amp;&amp;amp; echo &amp;quot;There are inactive resources&amp;quot; This selects any resource (//resource) in the output of pcs status xml that has the attribute active set to false. If there are no matches to this query, xmllint exits with an error code.</description><content>&lt;p>I&amp;rsquo;ve recently spent some time working with &lt;a href="http://clusterlabs.org/">Pacemaker&lt;/a>, and ended up
with an interesting collection of &lt;a href="http://www.w3.org/TR/xpath/">XPath&lt;/a> snippets that I am publishing
here for your use and/or amusement.&lt;/p>
&lt;h2 id="check-if-there-are-any-inactive-resources">Check if there are any inactive resources&lt;/h2>
&lt;pre>&lt;code>pcs status xml |
xmllint --xpath '//resource[@active=&amp;quot;false&amp;quot;]' - &amp;gt;&amp;amp;/dev/null &amp;amp;&amp;amp;
echo &amp;quot;There are inactive resources&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>This selects &lt;em>any&lt;/em> resource (&lt;code>//resource&lt;/code>) in the output of &lt;code>pcs status xml&lt;/code> that has the attribute &lt;code>active&lt;/code> set to &lt;code>false&lt;/code>. If there
are no matches to this query, &lt;code>xmllint&lt;/code> exits with an error code.&lt;/p>
&lt;h2 id="get-a-list-of-inactive-resources">Get a list of inactive resources&lt;/h2>
&lt;pre>&lt;code>pcs status xml |
xmllint --xpath '//resource[@active=&amp;quot;false&amp;quot;]/@id' - |
tr ' ' '\n' |
cut -f2 -d'&amp;quot;'
&lt;/code>&lt;/pre>
&lt;p>This uses the same xpath query as the previous snippet, but here we
then extract the &lt;code>id&lt;/code> attribute of the matches and then print out all
the resulting ids, one per line.&lt;/p>
&lt;h2 id="check-if-there-are-no-inactive-resources">Check if there are &lt;em>no&lt;/em> inactive resources&lt;/h2>
&lt;pre>&lt;code>! pcs status xml |
xmllint --xpath '//resource[@active=&amp;quot;false&amp;quot;]' - &amp;amp;&amp;amp;
echo &amp;quot;There are no inactive resources&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>This is the opposite of our earlier snippet, and demonstrates the use
of &lt;code>!&lt;/code> in a shell script to negate the success/failure of a shell
pipeline.&lt;/p>
&lt;h2 id="check-top-level-resources">Check top-level resources&lt;/h2>
&lt;pre>&lt;code>tmpfile=$(mktemp xmlXXXXXX)
trap &amp;quot;rm -f $tmpfile&amp;quot; EXIT
pcs status xml &amp;gt; $tmpfile
xmllint --xpath '/crm_mon/resources/*/@id' $tmpfile |
tr ' ' '\n'| cut -f2 -d'&amp;quot;' |
while read id; do
[ &amp;quot;$id&amp;quot; ] || continue
if ! xmllint --xpath &amp;quot;
/crm_mon/resources/*[@id='$id' and @active='true']|
/crm_mon/resources/*[@id='$id']/*[@active='true']&amp;quot; \
$tmpfile &amp;gt; /dev/null 2&amp;gt;&amp;amp;1; then
echo &amp;quot;$id: no active resources&amp;quot; &amp;gt;&amp;amp;2
exit 1
fi
done
&lt;/code>&lt;/pre>
&lt;p>This snippet checks that each top-level resource or resource container
(clone, resource group, etc.) has at least one active resources.
First we extract the &lt;code>id&lt;/code> attribute from the just the top-level
contents of &lt;code>/cr_mon/resources&lt;/code>:&lt;/p>
&lt;pre>&lt;code>/crm_mon/resources/*/@id
&lt;/code>&lt;/pre>
&lt;p>And then we iterate over the extracted ids, and for each one, we check
if either (a) a resource with that id is active, or (b) if any child
of a resource with that id is active:&lt;/p>
&lt;pre>&lt;code>/crm_mon/resources/*[@id='$id' and @active='true']|
/crm_mon/resources/*[@id='$id']/*[@active='true']
&lt;/code>&lt;/pre>
&lt;h1 id="wait-for-all-resources-to-become-inactive">Wait for all resources to become inactive&lt;/h1>
&lt;pre>&lt;code>pcs set property stop-all-resources=true
while pcs status xml |
xmllint --xpath '//resource[@active=&amp;quot;true&amp;quot;]' -; do
sleep 1
done
&lt;/code>&lt;/pre>
&lt;p>This is a good way to programatically wait for Pacemaker to finish
responding to setting &lt;code>stop-all-resources=true&lt;/code>.&lt;/p>
&lt;h1 id="get-a-list-of-all-top-level-resources">Get a list of all top-level resources&lt;/h1>
&lt;pre>&lt;code>cibadmin -Q |
xmllint --xpath '/cib/configuration/resources/*/@id' - |
tr ' ' '\n' |
cut -f2 -d'&amp;quot;'
&lt;/code>&lt;/pre>
&lt;p>This generates a list of the ids of &amp;ldquo;top-level&amp;rdquo; resources (either
standalone resources, or resource containers such as groups or
clones).&lt;/p>
&lt;h1 id="wait-for-all-members-of-a-resource-container-to-become-active">Wait for all members of a resource container to become active&lt;/h1>
&lt;pre>&lt;code>id='neutron-scale-clone'
while pcs status xml |
xmllint --xpath &amp;quot;//clone[@id='$id']/resource[@active='false']&amp;quot; -; do
sleep 1
done
&lt;/code>&lt;/pre>
&lt;p>This waits until all children of the specified resource id become
active.&lt;/p></content></item><item><title>Unpacking Docker images with Undocker</title><link>https://blog.oddbit.com/post/2015-02-13-unpacking-docker-images/</link><pubDate>Fri, 13 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-13-unpacking-docker-images/</guid><description>In some ways, the most exciting thing about Docker isn&amp;rsquo;t the ability to start containers. That&amp;rsquo;s been around for a long time in various forms, such as LXC or OpenVZ. What Docker brought to the party was a convenient method of building and distributing the filesystems necessary for running containers. Suddenly, it was easy to build a containerized service and to share it with other people.
I was taking a closer at the systemd-nspawn command, which it seems has been developing it&amp;rsquo;s own set of container-related superpowers recently, including a number of options for setting up the network environment of a container.</description><content>&lt;p>In some ways, the most exciting thing about &lt;a href="http://docker.com/">Docker&lt;/a> isn&amp;rsquo;t the ability
to start containers. That&amp;rsquo;s been around for a long time in various
forms, such as &lt;a href="https://linuxcontainers.org/">LXC&lt;/a> or &lt;a href="http://openvz.org/Main_Page">OpenVZ&lt;/a>. What Docker brought to the
party was a convenient method of building and distributing the
filesystems necessary for running containers. Suddenly, it was easy
to build a containerized service &lt;em>and&lt;/em> to share it with other people.&lt;/p>
&lt;p>I was taking a closer at the &lt;a href="http://www.freedesktop.org/software/systemd/man/systemd-nspawn.html">systemd-nspawn&lt;/a> command, which it
seems has been developing it&amp;rsquo;s own set of container-related
superpowers recently, including a number of options for setting up the
network environment of a container. Like Docker, &lt;code>systemd-nspawn&lt;/code>
needs a filesystem on which to operate, but &lt;em>unlike&lt;/em> Docker, there is
no convenient distribution mechanism and no ecosystem of existing
images. In fact, the official documentation seems to assume that
you&amp;rsquo;ll be building your own from scratch. Ain&amp;rsquo;t nobody got time for
that&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;but with that attracting Docker image ecosystem sitting right next
door, surely there was something we can do?&lt;/p>
&lt;h2 id="the-format-of-a-docker-image">The format of a Docker image&lt;/h2>
&lt;p>A Docker image is a tar archive that contains a top level
&lt;code>repositories&lt;/code> files, and then a number of layers stored as
directories containing a &lt;code>json&lt;/code> file with some metadata about the
layer and a tar file named &lt;code>layer.tar&lt;/code> with the layer content. For
example, if you &lt;code>docker save busybox&lt;/code>, you get:&lt;/p>
&lt;pre>&lt;code>4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/
4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/VERSION
4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/json
4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/layer.tar
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/VERSION
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer.tar
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/VERSION
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/json
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/layer.tar
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/VERSION
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/json
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/layer.tar
repositories
&lt;/code>&lt;/pre>
&lt;p>In order to re-create the filesystem that would result from starting a
Docker container with this image, you need to unpack the &lt;code>layer.tar&lt;/code>
files from the bottom up. You can find the topmost layer in the
&lt;code>repositories&lt;/code> file, which looks like this:&lt;/p>
&lt;p>{
&amp;ldquo;busybox&amp;rdquo;: {
&amp;ldquo;latest&amp;rdquo;: &amp;ldquo;4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125&amp;rdquo;
}
}&lt;/p>
&lt;p>From there, you can investigate the &lt;code>json&lt;/code> file for each layer looking
for the &lt;code>parent&lt;/code> tag.&lt;/p>
&lt;h2 id="introducing-undocker">Introducing undocker&lt;/h2>
&lt;p>I wrote the &lt;a href="http://github.com/larsks/undocker/">undocker&lt;/a> command to extract all or part of the layers
of a Docker image onto the local filesystem. In other words, if you
want to use the &lt;code>busybox&lt;/code> Docker image, you can fetch and unpack the
image:&lt;/p>
&lt;pre>&lt;code># docker pull busybox
# docker save busybox | undocker -o busybox
&lt;/code>&lt;/pre>
&lt;p>This will first look in the &lt;code>repositories&lt;/code> file for the &lt;code>busybox&lt;/code>
entry with the &lt;code>latest&lt;/code> tag, then build the necessary chain of layers
and unpack them in the correct order.&lt;/p>
&lt;p>Once you have the filesystem extracted, you can boot it with
&lt;code>systemd-nspawn&lt;/code>:&lt;/p>
&lt;pre>&lt;code># systemd-nspawn -D busybox /bin/sh
Spawning container busybox on /root/busybox.
Press ^] three times within 1s to kill container.
Timezone America/New_York does not exist in container, not updating container timezone.
Failed to copy /etc/resolv.conf to /root/busybox/etc/resolv.conf: Too many levels of symbolic links
/bin/sh: can't access tty; job control turned off
/ #
&lt;/code>&lt;/pre>
&lt;p>Undocker is able to extract specific layers from the image as well.
We can get a list of layers with the &lt;code>--layers&lt;/code> option:&lt;/p>
&lt;pre>&lt;code>$ docker save busybox | undocker --layers
511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158
df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b
ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2
4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125
&lt;/code>&lt;/pre>
&lt;p>And we can extract one or more specific layers with the &lt;code>--layer&lt;/code>
(&lt;code>-l&lt;/code>) option:&lt;/p>
&lt;pre>&lt;code>$ docker save busybox |
undocker -vi -o busybox -l ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2
INFO:undocker:extracting image busybox (4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125)
INFO:undocker:extracting layer ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m using the &lt;code>-i&lt;/code> (&lt;code>--ignore-errors&lt;/code>) option here because this layer
contains a device node (&lt;code>/dev/console&lt;/code>), and I am running this as an
unprivileged user. Without the &lt;code>-i&lt;/code> option, we would see:&lt;/p>
&lt;pre>&lt;code>OSError: [Errno 1] Operation not permitted
&lt;/code>&lt;/pre>
&lt;p>A Docker image archive can actually contain multiple images, each with
multiple tags. For a single image, &lt;code>undocker&lt;/code> will default to
extracting the &lt;code>latest&lt;/code> tag. If the &lt;code>latest&lt;/code> tag doesn&amp;rsquo;t exist,
you&amp;rsquo;ll see:&lt;/p>
&lt;pre>&lt;code># docker pull fedora:20
# docker save fedora:20 | undocker -o fedora
ERROR:undocker:failed to find image fedora with tag latest
&lt;/code>&lt;/pre>
&lt;p>You can specify an explicit tag in the same way you provide one to
Docker:&lt;/p>
&lt;pre>&lt;code># docker save fedora:20 | undocker -o fedora fedora:20
&lt;/code>&lt;/pre>
&lt;p>If an archive contains multiple images, you&amp;rsquo;ll get a different error:&lt;/p>
&lt;pre>&lt;code># docker save busybox larsks/thttpd | undocker -o busybox
ERROR:undocker:No image name specified and multiple images contained in archive
&lt;/code>&lt;/pre>
&lt;p>You can get a list of available images and tags with the &lt;code>--list&lt;/code>
option:&lt;/p>
&lt;pre>&lt;code># docker save busybox larsks/thttpd | undocker --list
larsks/thttpd: latest
busybox: latest
# docker save fedora | undocker --list
fedora: heisenbug 20 21 rawhide latest
&lt;/code>&lt;/pre>
&lt;p>You can specify the image (and tag) to extract on the command line:&lt;/p>
&lt;pre>&lt;code># docker save busybox larsks/thttpd | undocker -o busybox busybox
&lt;/code>&lt;/pre></content></item><item><title>Installing nova-docker with devstack</title><link>https://blog.oddbit.com/post/2015-02-11-installing-novadocker-with-dev/</link><pubDate>Wed, 11 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-11-installing-novadocker-with-dev/</guid><description>This is a long-form response to this question, and describes how to get the nova-docker driver up running with devstack under Ubuntu 14.04 (Trusty). I wrote a similar post for Fedora 21, although that one was using the RDO Juno packages, while this one is using devstack and the upstream sources.
Getting started We&amp;rsquo;ll be using the Ubuntu 14.04 cloud image (because my test environment runs on OpenStack).
First, let&amp;rsquo;s install a few prerequisites:</description><content>&lt;p>This is a long-form response to &lt;a href="https://ask.openstack.org/en/question/60679/installing-docker-on-openstack-with-ubuntu/">this question&lt;/a>, and describes
how to get the &lt;a href="http://github.com/stackforge/nova-docker/">nova-docker&lt;/a> driver up running with &lt;a href="http://devstack.org/">devstack&lt;/a>
under Ubuntu 14.04 (Trusty). I wrote a &lt;a href="https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/">similar post&lt;/a> for Fedora
21, although that one was using the &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> Juno packages, while this
one is using &lt;a href="http://devstack.org/">devstack&lt;/a> and the upstream sources.&lt;/p>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>We&amp;rsquo;ll be using the &lt;a href="https://cloud-images.ubuntu.com/trusty/current/">Ubuntu 14.04 cloud image&lt;/a> (because my test
environment runs on &lt;a href="http://www.openstack.org/">OpenStack&lt;/a>).&lt;/p>
&lt;p>First, let&amp;rsquo;s install a few prerequisites:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get update
$ sudo apt-get -y install git git-review python-pip python-dev
&lt;/code>&lt;/pre>
&lt;p>And generally make sure things are up-to-date:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get -y upgrade
&lt;/code>&lt;/pre>
&lt;h2 id="installing-docker">Installing Docker&lt;/h2>
&lt;p>We need to install Docker if we&amp;rsquo;re going to use &lt;code>nova-docker&lt;/code>.&lt;/p>
&lt;p>Ubuntu 14.04 includes a fairly dated version of Docker, so I followed
&lt;a href="https://docs.docker.com/installation/ubuntulinux/#docker-maintained-package-installation">the instructions&lt;/a> on the Docker website for installing the current
version of Docker on Ubuntu; this ultimately got me:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get -y install lxc-docker
$ sudo docker version
Client version: 1.5.0
Client API version: 1.17
Go version (client): go1.4.1
Git commit (client): a8a31ef
OS/Arch (client): linux/amd64
Server version: 1.5.0
Server API version: 1.17
Go version (server): go1.4.1
Git commit (server): a8a31ef
&lt;/code>&lt;/pre>
&lt;p>Docker by default creates its socket (&lt;code>/var/run/docker.socket&lt;/code>) with
&lt;code>root:root&lt;/code> ownership. We&amp;rsquo;re going to be running devstack as the
&lt;code>ubuntu&lt;/code> user, so let&amp;rsquo;s change that by editing &lt;code>/etc/default/docker&lt;/code>
and setting:&lt;/p>
&lt;pre>&lt;code>DOCKER_OPTS='-G ubuntu'
&lt;/code>&lt;/pre>
&lt;p>And restart &lt;code>docker&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ sudo restart docker
&lt;/code>&lt;/pre>
&lt;p>And verify that we can access Docker as the &lt;code>ubuntu&lt;/code> user:&lt;/p>
&lt;pre>&lt;code>$ docker version
Client version: 1.5.0
Client API version: 1.17
[...]
&lt;/code>&lt;/pre>
&lt;h2 id="installing-nova-docker">Installing nova-docker&lt;/h2>
&lt;p>As the &lt;code>ubuntu&lt;/code> user, let&amp;rsquo;s get the &lt;code>nova-docker&lt;/code> source code:&lt;/p>
&lt;pre>&lt;code>$ git clone http://github.com/stackforge/nova-docker.git
$ cd nova-docker
&lt;/code>&lt;/pre>
&lt;p>As of this writing (&lt;code>HEAD&lt;/code> is &amp;ldquo;984900a Give some time for docker.stop
to work&amp;rdquo;), you need to apply &lt;a href="https://review.openstack.org/#/c/154750/">a patch&lt;/a> to &lt;code>nova-docker&lt;/code> to get it to
work with the current Nova &lt;code>master&lt;/code> branch:&lt;/p>
&lt;pre>&lt;code>$ git fetch https://review.openstack.org/stackforge/nova-docker \
refs/changes/50/154750/3 \
&amp;amp;&amp;amp; git checkout FETCH_HEAD
&lt;/code>&lt;/pre>
&lt;p>Once &lt;a href="https://review.openstack.org/#/c/154750/">that change&lt;/a> has merged (&lt;strong>update&lt;/strong>, 2015-02-12: the
patch has merged), this step should no longer be
necessary. With the patch we applied, we can install the
&lt;code>nova-docker&lt;/code> driver:&lt;/p>
&lt;pre>&lt;code>$ sudo pip install .
&lt;/code>&lt;/pre>
&lt;h2 id="configuring-devstack">Configuring devstack&lt;/h2>
&lt;p>Now we&amp;rsquo;re ready to get devstack up and running. Start by cloning the
repository:&lt;/p>
&lt;pre>&lt;code>$ git clone https://git.openstack.org/openstack-dev/devstack
$ cd devstack
&lt;/code>&lt;/pre>
&lt;p>Then create a &lt;code>local.conf&lt;/code> file with the following content:&lt;/p>
&lt;pre>&lt;code>[[local|localrc]]
ADMIN_PASSWORD=secret
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=super-secret-admin-token
VIRT_DRIVER=novadocker.virt.docker.DockerDriver
DEST=$HOME/stack
SERVICE_DIR=$DEST/status
DATA_DIR=$DEST/data
LOGFILE=$DEST/logs/stack.sh.log
LOGDIR=$DEST/logs
# The default fixed range (10.0.0.0/24) conflicted with an address
# range I was using locally.
FIXED_RANGE=10.254.1.0/24
NETWORK_GATEWAY=10.254.1.1
# This enables Neutron, because that's how I roll.
disable_service n-net
enable_service q-svc
enable_service q-agt
enable_service q-dhcp
enable_service q-l3
enable_service q-meta
# I am disabling horizon (because I rarely use the web ui)
# and tempest in order to make the installer complete a
# little faster.
disable_service horizon
disable_service tempest
# Introduce glance to docker images
[[post-config|$GLANCE_API_CONF]]
[DEFAULT]
container_formats=ami,ari,aki,bare,ovf,ova,docker
# Configure nova to use the nova-docker driver
[[post-config|$NOVA_CONF]]
[DEFAULT]
compute_driver=novadocker.virt.docker.DockerDriver
&lt;/code>&lt;/pre>
&lt;p>This will result in things getting installed in subdirectories of
&lt;code>$HOME/stack&lt;/code>. We enable Neutron and leave pretty much everything
else set to default values.&lt;/p>
&lt;h2 id="start-the-installation">Start the installation&lt;/h2>
&lt;p>So, now we&amp;rsquo;re all ready to roll!&lt;/p>
&lt;pre>&lt;code>$ ./stack.sh
[Call Trace]
./stack.sh:151:source
/home/ubuntu/devstack/stackrc:665:die
[ERROR] /home/ubuntu/devstack/stackrc:665 Could not determine host ip address. See local.conf for suggestions on setting HOST_IP.
/home/ubuntu/devstack/functions-common: line 322: /home/ubuntu/stack/logs/error.log: No such file or directory
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;or not. This error happens if devstack is unable to turn your
hostname into an IP address. We can set &lt;code>HOST_IP&lt;/code> in our
environment:&lt;/p>
&lt;pre>&lt;code>$ HOST_IP=10.0.0.232 ./stack.sh
&lt;/code>&lt;/pre>
&lt;p>And then go grab a cup of coffee or something.&lt;/p>
&lt;h2 id="install-nova-docker-rootwrap-filters">Install nova-docker rootwrap filters&lt;/h2>
&lt;p>Once &lt;code>stack.sh&lt;/code> is finished running, we need to install a &lt;code>rootwrap&lt;/code>
configuration file for &lt;code>nova-docker&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ sudo cp nova-docker/etc/nova/rootwrap.d/docker.filters \
/etc/nova/rootwrap.d/
&lt;/code>&lt;/pre>
&lt;h2 id="starting-a-docker-container">Starting a Docker container&lt;/h2>
&lt;p>Now that our environment is up and running, we should be able to start
a container. We&amp;rsquo;ll start by grabbing some admin credentials for our
OpenStack environment:&lt;/p>
&lt;pre>&lt;code>$ . openrc admin
&lt;/code>&lt;/pre>
&lt;p>Next, we need an appropriate image; my &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">larsks/thttpd&lt;/a> image
is small (so it&amp;rsquo;s quick to download) and does not require any
interactive terminal (so it&amp;rsquo;s appropriate for nova-docker), so let&amp;rsquo;s
start with that:&lt;/p>
&lt;pre>&lt;code>$ docker pull larsks/thttpd
$ docker save larsks/thttpd |
glance image-create --name larsks/thttpd \
--is-public true --container-format docker \
--disk-format raw
&lt;/code>&lt;/pre>
&lt;p>And now we&amp;rsquo;ll boot it up. I like to do this as a non-admin user:&lt;/p>
&lt;pre>&lt;code>$ . openrc demo
$ nova boot --image larsks/thttpd --flavor m1.small test0
&lt;/code>&lt;/pre>
&lt;p>After a bit, we should see:&lt;/p>
&lt;pre>&lt;code>$ nova list
+----...+-------+--------+...+-------------+--------------------+
| ID ...| Name | Status |...| Power State | Networks |
+----...+-------+--------+...+-------------+--------------------+
| 0c3...| test0 | ACTIVE |...| Running | private=10.254.1.4 |
+----...+-------+--------+...+-------------+--------------------+
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s create a floating ip address:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-create
+------------+-----------+----------+--------+
| Ip | Server Id | Fixed Ip | Pool |
+------------+-----------+----------+--------+
| 172.24.4.3 | - | - | public |
+------------+-----------+----------+--------+
&lt;/code>&lt;/pre>
&lt;p>And assign it to our container:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-associate test0 172.24.4.3
&lt;/code>&lt;/pre>
&lt;p>And now access our service:&lt;/p>
&lt;pre>&lt;code>$ curl http://172.24.4.3
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Your web server is working&amp;lt;/title&amp;gt;
[...]
&lt;/code>&lt;/pre></content></item><item><title>External networking for Kubernetes services</title><link>https://blog.oddbit.com/post/2015-02-10-external-networking-for-kubern/</link><pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-10-external-networking-for-kubern/</guid><description>I have recently started running some &amp;ldquo;real&amp;rdquo; services (that is, &amp;ldquo;services being consumed by someone other than myself&amp;rdquo;) on top of Kubernetes (running on bare metal), which means I suddenly had to confront the question of how to provide external access to Kubernetes hosted services. Kubernetes provides two solutions to this problem, neither of which is particularly attractive out of the box:
There is a field createExternalLoadBalancer that can be set in a service description.</description><content>&lt;p>I have recently started running some &amp;ldquo;real&amp;rdquo; services (that is,
&amp;ldquo;services being consumed by someone other than myself&amp;rdquo;) on top of
Kubernetes (running on bare metal), which means I suddenly had to
confront the question of how to provide external access to Kubernetes
hosted services. Kubernetes provides two solutions to this problem,
neither of which is particularly attractive out of the box:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>There is a field &lt;code>createExternalLoadBalancer&lt;/code> that can be set in a
service description. This is meant to integrate with load
balancers provided by your local cloud environment, but at the
moment there is only support for this when running under &lt;a href="https://cloud.google.com/compute/">GCE&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A service description can have a list of public IP addresses
associated with it in the &lt;code>publicIPS&lt;/code> field. This will cause
&lt;code>kube-proxy&lt;/code> to create rules in the &lt;code>KUBE-PROXY&lt;/code> chain of your
&lt;code>nat&lt;/code> table to direct traffic inbound to those addresses to the
appropriate local &lt;code>kube-proxy&lt;/code> port.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The second option is a good starting point, since if you were to
simply list the public IP addresses of your Kubernetes minions in the
&lt;code>publicIPs&lt;/code> field, everything would Just Work. That is, inbound
traffic to the appropriate port on your minions would get directed to
&lt;code>kube-proxy&lt;/code> by the &lt;code>nat&lt;/code> rules. That&amp;rsquo;s great for simple cases, but
in practice it means that you cannot have more that &lt;em>N&lt;/em> services
exposed on a given port where &lt;em>N&lt;/em> is the number of minions in your
cluster. That limit is difficult if you &amp;ndash; like I do &amp;ndash; have an
all-in-one (e.g., on a single host) Kubernetes deployment on which you
wish to host multiple web services exposed on port 80 (and even in a
larger environment, you really don&amp;rsquo;t want &amp;ldquo;number of things on port
XX&amp;rdquo; tightly coupled to &amp;ldquo;number of minions&amp;rdquo;).&lt;/p>
&lt;h2 id="introducing-kiwi">Introducing Kiwi&lt;/h2>
&lt;p>To overcome this problem, I wrote &lt;a href="http://github.com/larsks/kiwi/">Kiwi&lt;/a>, a service that listens to
Kubernetes for events concerning new/modified/deleted services, and in
response to those events manages (a) the assignment of IP addresses to
network interfaces on your minions and (b) creating additional
firewall rules to permit traffic inbound to your services to pass a
default-deny firewall configuration.&lt;/p>
&lt;p>Kiwi uses &lt;a href="https://github.com/coreos/etcd">etcd&lt;/a> to coordinate ownership of IP addresses between
minions in your Kubernetes cluster.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>Kiwi listens to event streams from both Kubernetes and Etcd.&lt;/p>
&lt;p>On the Kubernetes side, Kiwi listens to &lt;code>/api/v1beta/watch/services&lt;/code>,
which produces events in response to new, modified, or deleted
services. The Kubernetes API uses a server-push model, in which a
client makes a single HTTP request and then receives a series of
events over the same connection. A event looks something like:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;type&amp;quot;: &amp;quot;ADDED&amp;quot;,
&amp;quot;object&amp;quot;: {
&amp;quot;portalIP&amp;quot;: &amp;quot;10.254.93.176&amp;quot;,
&amp;quot;containerPort&amp;quot;: 80,
&amp;quot;publicIPs&amp;quot;: [
&amp;quot;192.168.1.100&amp;quot;
],
&amp;quot;selector&amp;quot;: {
&amp;quot;name&amp;quot;: &amp;quot;test-web&amp;quot;
},
&amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;,
&amp;quot;port&amp;quot;: 8080,
&amp;quot;kind&amp;quot;: &amp;quot;Service&amp;quot;,
&amp;quot;id&amp;quot;: &amp;quot;test-web&amp;quot;,
&amp;quot;uid&amp;quot;: &amp;quot;72bc1286-a440-11e4-b83e-20cf30467e62&amp;quot;,
&amp;quot;creationTimestamp&amp;quot;: &amp;quot;2015-01-24T22:15:43-05:00&amp;quot;,
&amp;quot;selfLink&amp;quot;: &amp;quot;/api/v1beta1/services/test-web&amp;quot;,
&amp;quot;resourceVersion&amp;quot;: 245,
&amp;quot;apiVersion&amp;quot;: &amp;quot;v1beta1&amp;quot;,
&amp;quot;namespace&amp;quot;: &amp;quot;default&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>I am using the Python &lt;a href="http://docs.python-requests.org/en/latest/">requests&lt;/a> library, which it turns out &lt;a href="https://github.com/kennethreitz/requests/issues/2433">has a
bug&lt;/a> in its handling of streaming server responses, but I was
able to work around that issue once I realized what was going on.&lt;/p>
&lt;p>On the Etcd side, Kiwi uses keys under the &lt;code>/kiwi/publicips&lt;/code> prefix to
coordinate address ownership among Kiwi instances. It listens to
events from Etcd regarding key create/delete/set/etc operations in
this prefix by calling
&lt;code>/v2/keys/kiwi/publicips?watch=true&amp;amp;recursive=true&lt;/code>. This is a
long-poll request, rather than a streaming request: that means that a
request will only ever receive a single event, but it may need to wait
for a while before it receives that response. This model worked well
with the &lt;code>requests&lt;/code> library out of the box.&lt;/p>
&lt;p>After receiving an event from Kubernetes, Kiwi iterates over the
public IP addresses in the &lt;code>publicIPs&lt;/code> key, and for any address that
is not already being manged by the local instance it makes a claim on
that address by attempting to atomically create a key in etcd under
&lt;code>/kiwi/publicips/&lt;/code> (such as &lt;code>/kiwi/publicips/192.168.1.100&lt;/code>). If this
attempt succeeds, Kiwi on the local minion has claimed that address
and proceeds to assign it to the local interface. If the attempt to
set that key does not succeed, it means the address is already being
managed by Kiwi on another minion.&lt;/p>
&lt;p>The address keys are set with a TTL of 20 seconds, after which they
will be expired. If an address expires, other Kiwi instances will
receive notification from Etcd and ownership of that address will
transfer to another Kiwi instance.&lt;/p>
&lt;h2 id="getting-started-with-kiwi">Getting started with Kiwi&lt;/h2>
&lt;p>The easiest way to get started with Kiwi is to use the &lt;a href="https://registry.hub.docker.com/u/larsks/kiwi/">larsks/kiwi&lt;/a>
Docker image that is automatically built from the &lt;a href="http://github.com/larsks/kiwi/">Git
repository&lt;/a>. For example, if you want to host public ip
addresses on &lt;code>eth0&lt;/code> in the range &lt;code>192.168.1.32/28&lt;/code>, you would start it
like this:&lt;/p>
&lt;pre>&lt;code>docker run --privileged --net=host larsks/kiwi \
--interface eth0 \
--range 192.168.1.32/28
&lt;/code>&lt;/pre>
&lt;p>You need both &lt;code>--privileged&lt;/code> and &lt;code>--net=host&lt;/code> in order for Kiwi to
assign addresses to your host interfaces and to manage the iptables
configuration.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>Start Kiwi as described above. Next, plae the following content in a
file called &lt;code>service.yaml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kind: Service
apiVersion: v1beta1
id: test-web
port: 8888
selector:
name: test-web
containerPort: 80
publicIPs:
- 192.168.1.100
&lt;/code>&lt;/pre>
&lt;p>Create the service using &lt;code>kubectl&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kubectl create -f service.yaml
&lt;/code>&lt;/pre>
&lt;p>After a short pause, you should see the address show up on interface
&lt;code>eth0&lt;/code>; the entry will look something like:&lt;/p>
&lt;pre>&lt;code>inet 192.168.1.100/32 scope global dynamic eth0:kube
valid_lft 17sec preferred_lft 17sec
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>eth0:kube&lt;/code> is a label applied to the address; this allows Kiwi to
clean up these addresses at startup (by getting a list of
Kiwi-configured addresses with &lt;code>ip addr show label eth0:kube&lt;/code>).&lt;/p>
&lt;p>The &lt;code>valid_lft&lt;/code> and &lt;code>preferred_lft&lt;/code> fields control the lifetime of the
interface. When these counters reach 0, the addresses are removed by
the kernel. This ensure that if Kiwi dies, the addresses can
successfully be re-assigned on another node.&lt;/p></content></item><item><title>Installing nova-docker on Fedora 21/RDO Juno</title><link>https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/</link><pubDate>Fri, 06 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/</guid><description>This post comes about indirectly by a request on IRC in #rdo for help getting nova-docker installed on Fedora 21. I ran through the process from start to finish and decided to write everything down for posterity.
Getting started I started with the Fedora 21 Cloud Image, because I&amp;rsquo;m installing onto OpenStack and the cloud images include some features that are useful in this environment.
We&amp;rsquo;ll be using OpenStack packages from the RDO Juno repository.</description><content>&lt;p>This post comes about indirectly by a request on IRC in &lt;code>#rdo&lt;/code> for help getting &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> installed on Fedora 21. I ran through the process from start to finish and decided to write everything down for posterity.&lt;/p>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>I started with the &lt;a href="https://getfedora.org/en/cloud/download/">Fedora 21 Cloud Image&lt;/a>, because I&amp;rsquo;m
installing onto OpenStack and the cloud images include
some features that are useful in this environment.&lt;/p>
&lt;p>We&amp;rsquo;ll be using OpenStack packages from the &lt;a href="https://repos.fedorapeople.org/repos/openstack/openstack-juno/">RDO Juno&lt;/a> repository.
Because there is often some skew between the RDO packages and the
current Fedora selinux policy, we&amp;rsquo;re going to start by putting SELinux
into permissive mode (sorry, Dan):&lt;/p>
&lt;pre>&lt;code># setenforce 0
# sed -i '/^SELINUX=/ s/=.*/=permissive/' /etc/selinux/config
&lt;/code>&lt;/pre>
&lt;p>Next, install the RDO Juno repository:&lt;/p>
&lt;pre>&lt;code># yum -y install \
https://repos.fedorapeople.org/repos/openstack/openstack-juno/rdo-release-juno-1.noarch.rpm
&lt;/code>&lt;/pre>
&lt;p>And upgrade all our existing packages:&lt;/p>
&lt;pre>&lt;code># yum -y upgrade
&lt;/code>&lt;/pre>
&lt;h2 id="install-openstack">Install OpenStack&lt;/h2>
&lt;p>We&amp;rsquo;ll be using &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a> to install OpenStack onto this host.
Start by installing the package:&lt;/p>
&lt;pre>&lt;code># yum -y install openstack-packstack
&lt;/code>&lt;/pre>
&lt;p>And then run a &lt;code>--allinone&lt;/code> install, which sets up all OpenStack
services on a single host:&lt;/p>
&lt;pre>&lt;code># packstack --allinone
&lt;/code>&lt;/pre>
&lt;h2 id="install-nova-docker-prequisites">Install nova-docker prequisites&lt;/h2>
&lt;p>Once &lt;code>packstack&lt;/code> has completed successfully, we need to install some
prerequisites for &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a>.&lt;/p>
&lt;pre>&lt;code># yum -y install git python-pip python-pbr \
docker-io fedora-repos-rawhide
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>fedora-repos-rawhide&lt;/code> package provides a yum configuration for the
&lt;code>rawhide&lt;/code> repository (disabled by default). We&amp;rsquo;re going to need that
to pick up more recent versions of &lt;code>systemd&lt;/code> (because of &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1187882">this
bug&lt;/a>) and
&lt;code>python-six&lt;/code> (because &lt;code>nova-docker&lt;/code> needs the &lt;code>six.add_metaclass&lt;/code>
method):&lt;/p>
&lt;pre>&lt;code># yum --enablerepo=rawhide install python-six systemd
&lt;/code>&lt;/pre>
&lt;p>At this point, having upgraded &lt;code>systemd&lt;/code>, you should probably reboot:&lt;/p>
&lt;pre>&lt;code># reboot
&lt;/code>&lt;/pre>
&lt;h2 id="configure-docker">Configure Docker&lt;/h2>
&lt;p>Once things are up and running, we will expect the &lt;code>nova-compute&lt;/code>
service to launch Docker containers. In order for this to work, the
&lt;code>nova&lt;/code> user will need access to the Docker socket,
&lt;code>/var/run/docker.sock&lt;/code>. By default, this is owned by &lt;code>root:root&lt;/code> and
has mode &lt;code>660&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ls -l /var/run/docker.sock
srw-rw----. 1 root root 0 Feb 1 12:43 /var/run/docker.sock
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>nova-compute&lt;/code> service runs as the &lt;code>nova&lt;/code> user and will not have
access to that socket. There are a few ways of resolving this; an
expedient method is simply to make this socket owned by the &lt;code>nova&lt;/code>
group, which we can do with &lt;code>docker&lt;/code>&amp;rsquo;s &lt;code>-G&lt;/code> option.&lt;/p>
&lt;p>Edit &lt;code>/etc/sysconfig/docker&lt;/code>, and modify the &lt;code>OPTIONS=&lt;/code> line to look
like:&lt;/p>
&lt;pre>&lt;code>OPTIONS='--selinux-enabled -G nova'
&lt;/code>&lt;/pre>
&lt;p>Then enable and start the &lt;code>docker&lt;/code> service:&lt;/p>
&lt;pre>&lt;code># systemctl enable docker
# systemctl start docker
&lt;/code>&lt;/pre>
&lt;h2 id="install-nova-docker">Install nova-docker&lt;/h2>
&lt;p>Clone the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> repository:&lt;/p>
&lt;pre>&lt;code># git clone http://github.com/stackforge/nova-docker.git
# cd nova-docker
&lt;/code>&lt;/pre>
&lt;p>And check out the &lt;code>stable/juno&lt;/code> branch, since we&amp;rsquo;re operating with an
OpenStack Juno environment:&lt;/p>
&lt;pre>&lt;code># git checkout stable/juno
&lt;/code>&lt;/pre>
&lt;p>Now install the driver:&lt;/p>
&lt;pre>&lt;code># python setup.py install
&lt;/code>&lt;/pre>
&lt;h2 id="configure-nova">Configure Nova&lt;/h2>
&lt;p>Following the &lt;a href="https://github.com/stackforge/nova-docker/blob/master/README.rst">README&lt;/a> from &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a>, we need to modify
the Nova configuration to use the &lt;code>nova-docker&lt;/code> driver. Edit
&lt;code>/etc/nova/nova.conf&lt;/code> and add the following line to the &lt;code>DEFAULT&lt;/code>
section:&lt;/p>
&lt;pre>&lt;code>compute_driver=novadocker.virt.docker.DockerDriver
&lt;/code>&lt;/pre>
&lt;p>If there is already a line setting &lt;code>compute_driver&lt;/code>, then comment it
out or delete before adding the new one.&lt;/p>
&lt;p>Modify the Glance configuration to permit storage of Docker images.
Edit &lt;code>/etc/glance/glance-api.conf&lt;/code>, and add the following line to the
&lt;code>DEFAULT&lt;/code> section:&lt;/p>
&lt;pre>&lt;code>container_formats=ami,ari,aki,bare,ovf,ova,docker
&lt;/code>&lt;/pre>
&lt;p>Next, we need to augment the &lt;code>rootwrap&lt;/code> configuration such that
&lt;code>nova-docker&lt;/code> is able run the &lt;code>ln&lt;/code> command with &lt;code>root&lt;/code> privileges.
We&amp;rsquo;ll install the &lt;a href="https://github.com/stackforge/nova-docker/blob/master/etc/nova/rootwrap.d/docker.filters">docker.filters&lt;/a> file from the &lt;code>nova-docker&lt;/code>
source:&lt;/p>
&lt;pre>&lt;code># mkdir -p /etc/nova/rootwrap.d
# cp etc/nova/rootwrap.d/docker.filters \
/etc/nova/rootwrap.d/docker.filters
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ve changed a number of configuration files, so we should restart
the affected services:&lt;/p>
&lt;pre>&lt;code># openstack-service restart nova glance
&lt;/code>&lt;/pre>
&lt;h2 id="testing-things-out">Testing things out&lt;/h2>
&lt;p>Let&amp;rsquo;s try starting a container! We need to select one that will run
in the &lt;code>nova-docker&lt;/code> environment. Generally, that means one that does
not expect to have an interactive terminal and that will automatically
start some sort of web-accessible service. I have a &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">minimal thttpd
container&lt;/a> that fits the bill nicely:&lt;/p>
&lt;pre>&lt;code># docker pull larsks/thttpd
&lt;/code>&lt;/pre>
&lt;p>We need to store this image into Glance using the same name:&lt;/p>
&lt;pre>&lt;code># docker save larsks/thttpd |
glance image-create --name larsks/thttpd \
--container-format docker --disk-format raw --is-public true
&lt;/code>&lt;/pre>
&lt;p>And now we should be able to start a container:&lt;/p>
&lt;pre>&lt;code># nova boot --image larsks/thttpd --flavor m1.tiny test0
&lt;/code>&lt;/pre>
&lt;p>After a bit, &lt;code>nova list&lt;/code> should show:&lt;/p>
&lt;pre>&lt;code>+------...+-------+--------+...+------------------+
| ID ...| Name | Status |...| Networks |
+------...+-------+--------+...+------------------+
| 430a1...| test0 | ACTIVE |...| private=10.0.0.6 |
+------...+-------+--------+...+------------------+
&lt;/code>&lt;/pre>
&lt;p>And we should also see the container if we run &lt;code>docker ps&lt;/code>:&lt;/p>
&lt;pre>&lt;code># docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
ee864da30cf1 larsks/thttpd:latest &amp;quot;/thttpd -D -l /dev/ 7 hours ago Up 7 hours nova-430a197e-a0ca-4e72-a7db-1969d0773cf7
&lt;/code>&lt;/pre>
&lt;h2 id="getting-connected">Getting connected&lt;/h2>
&lt;p>At this point, the container will &lt;em>not&lt;/em> be network accessible; it&amp;rsquo;s
attached to a private tenant network. Let&amp;rsquo;s assign it a floating ip
address:&lt;/p>
&lt;pre>&lt;code># nova floating-ip-create public
+--------------+-----------+----------+--------+
| Ip | Server Id | Fixed Ip | Pool |
+--------------+-----------+----------+--------+
| 172.24.4.229 | - | - | public |
+--------------+-----------+----------+--------+
# nova floating-ip-associate test0 172.24.4.229
&lt;/code>&lt;/pre>
&lt;p>This isn&amp;rsquo;t going to be immediately accessible because Packstack left
us without a route to the floating ip network. We can fix that
temporarily like this:&lt;/p>
&lt;pre>&lt;code># ip addr add 172.24.4.225/28 dev br-ex
&lt;/code>&lt;/pre>
&lt;p>And now we can ping our Docker container:&lt;/p>
&lt;pre>&lt;code># ping -c2 172.24.4.229
PING 172.24.4.229 (172.24.4.229) 56(84) bytes of data.
64 bytes from 172.24.4.229: icmp_seq=1 ttl=63 time=0.291 ms
64 bytes from 172.24.4.229: icmp_seq=2 ttl=63 time=0.074 ms
--- 172.24.4.229 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.074/0.182/0.291/0.109 ms
&lt;/code>&lt;/pre>
&lt;p>And access the webserver:&lt;/p>
&lt;pre>&lt;code># curl http://172.24.4.229
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Your web server is working&amp;lt;/title&amp;gt;
.
.
.&lt;/code>&lt;/pre></content></item><item><title>Creating minimal Docker images from dynamically linked ELF binaries</title><link>https://blog.oddbit.com/post/2015-02-05-creating-minimal-docker-images/</link><pubDate>Thu, 05 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-05-creating-minimal-docker-images/</guid><description>In this post, we&amp;rsquo;ll look at a method for building minimal Docker images for dynamically linked ELF binaries, and then at a tool for automating this process.
It is tempting, when creating a simple Docker image, to start with one of the images provided by the major distributions. For example, if you need an image that provides tcpdump for use on your Atomic host, you might do something like:
FROM fedora RUN yum -y install tcpdump And while this will work, you end up consuming 250MB for tcpdump.</description><content>&lt;p>In this post, we&amp;rsquo;ll look at a method for building minimal Docker
images for dynamically linked ELF binaries, and then at &lt;a href="https://github.com/larsks/dockerize">a
tool&lt;/a> for automating this process.&lt;/p>
&lt;p>It is tempting, when creating a simple Docker image, to start with one
of the images provided by the major distributions. For example, if
you need an image that provides &lt;code>tcpdump&lt;/code> for use on your &lt;a href="http://www.projectatomic.io/">Atomic&lt;/a>
host, you might do something like:&lt;/p>
&lt;pre>&lt;code>FROM fedora
RUN yum -y install tcpdump
&lt;/code>&lt;/pre>
&lt;p>And while this will work, you end up consuming 250MB for &lt;code>tcpdump&lt;/code>.
In theory, the layering mechanism that Docker uses to build images
will reduce the practical impact of this (because other images based on
the &lt;code>fedora&lt;/code> image will share the common layers), but in practice the
size is noticeable, especially if you often find yourself pulling this
image into a fresh environment with no established cache.&lt;/p>
&lt;p>You can substantially reduce the space requirements for a Docker image
by including only those things that are absolutely necessary. For
statically linked files, that may only be the binary itself, but the
situation is a little more complex for dynamically linked executables.
You might naively start with this (assuming that you had the &lt;code>tcpdump&lt;/code>
binary in your local directory):&lt;/p>
&lt;pre>&lt;code>FROM scratch
COPY tcpdump /usr/sbin/tcpdump
&lt;/code>&lt;/pre>
&lt;p>If you were to build an image with this and tag it &lt;code>tcpdump&lt;/code>&amp;hellip;&lt;/p>
&lt;pre>&lt;code>docker build -t tcpdump .
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then try running it:&lt;/p>
&lt;pre>&lt;code>docker run tcpdump
&lt;/code>&lt;/pre>
&lt;p>You would immediately see:&lt;/p>
&lt;pre>&lt;code>no such file or directory
FATA[0003] Error response from daemon: Cannot start container ...:
no such file or directory
&lt;/code>&lt;/pre>
&lt;p>And this is because the image is missing two things:&lt;/p>
&lt;ul>
&lt;li>The Linux dynamic runtime loader, and&lt;/li>
&lt;li>The shared libraries required by the &lt;code>tcpdump&lt;/code> binary&lt;/li>
&lt;/ul>
&lt;p>The path to the appropriate loader is stored in the ELF binary in the
&lt;code>.interp&lt;/code> section, which we can inspect using the &lt;code>objdump&lt;/code> tool:&lt;/p>
&lt;pre>&lt;code>$ objdump -s -j .interp tcpdump
tcpdump: file format elf64-x86-64
Contents of section .interp:
400238 2f6c6962 36342f6c 642d6c69 6e75782d /lib64/ld-linux-
400248 7838362d 36342e73 6f2e3200 x86-64.so.2.
&lt;/code>&lt;/pre>
&lt;p>Which tells us we need &lt;code>/lib64/ld-linux-x86-64.so.2&lt;/code>.&lt;/p>
&lt;p>We can use the &lt;code>ldd&lt;/code> tool to get a list of shared libraries required
by the binary:&lt;/p>
&lt;pre>&lt;code>$ ldd tcpdump
linux-vdso.so.1 =&amp;gt; (0x00007fffed1fe000)
libcrypto.so.10 =&amp;gt; /lib64/libcrypto.so.10 (0x00007fb2c05a3000)
libpcap.so.1 =&amp;gt; /lib64/libpcap.so.1 (0x00007fb2c0361000)
libc.so.6 =&amp;gt; /lib64/libc.so.6 (0x00007fb2bffa3000)
libdl.so.2 =&amp;gt; /lib64/libdl.so.2 (0x00007fb2bfd9f000)
libz.so.1 =&amp;gt; /lib64/libz.so.1 (0x00007fb2bfb89000)
/lib64/ld-linux-x86-64.so.2 (0x00007fb2c09b7000)
&lt;/code>&lt;/pre>
&lt;p>If we copy all of the dependencies into a local directory, along with
the &lt;code>tcpdump&lt;/code> binary itself, and use the following layout:&lt;/p>
&lt;pre>&lt;code>Dockerfile
usr/sbin/tcpdump
lib64/libcrypto.so.10
lib64/libpcap.so.1
lib64/libc.so.6
lib64/libdl.so.2
lib64/libz.so.1
lib64/ld-linux-x86-64.so.2
&lt;/code>&lt;/pre>
&lt;p>And the following Dockerfile content:&lt;/p>
&lt;pre>&lt;code>FROM scratch
COPY . /
ENTRYPOINT [&amp;quot;/usr/sbin/tcpdump&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>And then we turn this into a Docker image and run it, we get:&lt;/p>
&lt;pre>&lt;code>$ docker build -t tcpdump .
[...]
$ docker run tcpdump tcpdump -i eth0 -n
tcpdump: Couldn't find user 'tcpdump'
&lt;/code>&lt;/pre>
&lt;p>Well, so let&amp;rsquo;s create an &lt;code>/etc/passwd&lt;/code> file with the &lt;code>tcpdump&lt;/code> user
and add that to our collection:&lt;/p>
&lt;pre>&lt;code>$ mkdir etc
$ grep tcpdump /etc/passwd &amp;gt; etc/passwd
$ grep tcpdump /etc/group &amp;gt; etc/group
$ docker build -t tcpdump .
$ docker run tcpdump tcpdump -i eth0 -n
tcpdump: Couldn't find user 'tcpdump'
&lt;/code>&lt;/pre>
&lt;p>And &lt;em>this&lt;/em> is because most programs don&amp;rsquo;t reference files like
&lt;code>/etc/passwd&lt;/code> directly, but instead delegate this task to the C
library, which relies on the &lt;a href="http://www.gnu.org/software/libc/manual/html_node/Name-Service-Switch.html">name service switch&lt;/a> (nss)
mechanism to support multiple sources of information. Let&amp;rsquo;s add the
nss libraries necessary for supporting legacy files (&lt;code>/etc/passwd&lt;/code>,
etc) and DNS for hostname lookups:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>lib64/libnss_files.so.2&lt;/code> &amp;ndash; this includes support the traditional
files in &lt;code>/etc&lt;/code>, such as &lt;code>/etc/passwd&lt;/code>, &lt;code>/etc/group&lt;/code>, and
&lt;code>/etc/hosts&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>lib64/libnss_dns.so.2&lt;/code> &amp;ndash; this supports hostname resolution via
dns.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>And we&amp;rsquo;ll also need &lt;code>/etc/nsswitch.conf&lt;/code> to go along with that:&lt;/p>
&lt;pre>&lt;code>passwd: files
shadow: files
group: files
hosts: files dns
&lt;/code>&lt;/pre>
&lt;p>After all this, we have:&lt;/p>
&lt;pre>&lt;code>Dockerfile
usr/sbin/tcpdump
lib64/libcrypto.so.10
lib64/libpcap.so.1
lib64/libc.so.6
lib64/libdl.so.2
lib64/libz.so.1
lib64/ld-linux-x86-64.so.2
lib64/libnss_files.so.2
lib64/libnss_dns.so.2
etc/passwd
etc/group
etc/nsswitch.conf
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s rebuild the image and run it one more time:&lt;/p>
&lt;pre>&lt;code>$ docker build -t tcpdump .
$ docker run tcpdump -i eth0 -n
&lt;/code>&lt;/pre>
&lt;p>And now, finally it runs. Wouldn&amp;rsquo;t it be nice if that process were
easier?&lt;/p>
&lt;h2 id="introducing-dockerize">Introducing Dockerize&lt;/h2>
&lt;p>&lt;a href="https://github.com/larsks/dockerize">Dockerize&lt;/a> is a tool that largely automates the above process. To
build a minimal &lt;code>tcpdump&lt;/code> image, for example, you would run:&lt;/p>
&lt;pre>&lt;code>$ dockerize -u tcpdump -t tcpdump /usr/sbin/tcpdump
&lt;/code>&lt;/pre>
&lt;p>This would include the &lt;code>tcpdump&lt;/code> user (&lt;code>-u tcpdump&lt;/code>) from your local
system, as well as &lt;code>/usr/sbin/tcpdump&lt;/code>, all it&amp;rsquo;s dependencies, and
file-based nss support, and build an image tagged &lt;code>tcpdump&lt;/code> (&lt;code>-t tcpdump&lt;/code>). When you build an image from a single command, like this,
Dockerize will up that command as the Docker &lt;code>ENTRYPOINT&lt;/code>, so you can
run it like this:&lt;/p>
&lt;pre>&lt;code>$ docker run tcpdump -i eth0 -n
&lt;/code>&lt;/pre>
&lt;p>You can also build images containing multiple binaries. For example:&lt;/p>
&lt;pre>&lt;code>$ dockerize -t dockerizeme/xmltools \
/usr/bin/xmllint \
/usr/bin/xml2 \
/usr/bin/2xml \
/usr/bin/tidyp
&lt;/code>&lt;/pre>
&lt;p>In this case, you need to provide the command name when running the
image:&lt;/p>
&lt;pre>&lt;code>$ docker run dockerizeme/xmltools tidyp -h
&lt;/code>&lt;/pre>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>You can find some scripts that generate marginally useful example
images in the &lt;a href="https://github.com/larsks/dockerize/tree/master/examples">examples&lt;/a> folder of the repository.&lt;/p>
&lt;p>These are pushed into the &lt;a href="https://hub.docker.com/u/dockerizeme/">dockerizeme&lt;/a> namespace on the Docker hub,
so you can, for example, get yourself a minimal webserver by running:&lt;/p>
&lt;pre>&lt;code>$ docker run -v $PWD:/content -p 8888:80 dockerizeme/thttpd -d /content
&lt;/code>&lt;/pre>
&lt;p>And then browse to &lt;a href="http://localhost:8888">http://localhost:8888&lt;/a> and see your current
directory.&lt;/p></content></item><item><title>Filtering libvirt XML in Nova</title><link>https://blog.oddbit.com/post/2015-02-05-filtering-libvirt-xml-in-nova/</link><pubDate>Thu, 05 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-05-filtering-libvirt-xml-in-nova/</guid><description>I saw a request from a customer float by the other day regarding the ability to filter the XML used to create Nova instances in libvirt. The customer effectively wanted to blacklist a variety of devices (and device types). The consensus seems to be &amp;ldquo;you can&amp;rsquo;t do this right now and upstream is unlikely to accept patches that implement this behavior&amp;rdquo;, but it sounded like an interesting problem, so&amp;hellip;
https://github.com/larsks/nova/tree/feature/xmlfilter This is a fork of Nova (Juno) that includes support for an extensible filtering mechanism that is applied to the generated XML before it gets passed to libvirt.</description><content>&lt;p>I saw a request from a customer float by the other day regarding the
ability to filter the XML used to create Nova instances in libvirt.
The customer effectively wanted to blacklist a variety of devices (and
device types). The consensus seems to be &amp;ldquo;you can&amp;rsquo;t do this right now
and upstream is unlikely to accept patches that implement this
behavior&amp;rdquo;, but it sounded like an interesting problem, so&amp;hellip;&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/nova/tree/feature/xmlfilter">https://github.com/larsks/nova/tree/feature/xmlfilter&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a fork of Nova (Juno) that includes support for an extensible
filtering mechanism that is applied to the generated XML before it
gets passed to libvirt.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>The code uses the &lt;a href="https://github.com/dreamhost/stevedore">stevedore&lt;/a> module to handle locating and loading
filters. A filter is a Python class that implements a &lt;code>filter&lt;/code>
method with the following signature:&lt;/p>
&lt;pre>&lt;code>def filter(self, xml, instance=None, context=None)
&lt;/code>&lt;/pre>
&lt;p>The code in &lt;code>nova.virt.libvirt.domxmlfilters&lt;/code> collects all filters
registered in the &lt;code>nova.filters.domxml&lt;/code> namespace, and then runs them
in sequence, passing the output of one filter as the output to the
next:&lt;/p>
&lt;pre>&lt;code>filters = stevedore.extension.ExtensionManager(
'nova.filters.domxml',
invoke_on_load=True,
)
def filter_domain_xml(xml,
instance=None,
context=None):
'''Filter the XML content in 'xml' through any filters registered in
the nova.filters.domxml namespace.'''
revised = xml
for filter in filters:
LOG.debug('filtering xml with filter %s',
filter.name)
revised = filter.obj.filter(revised,
instance=instance,
context=context
)
return revised
&lt;/code>&lt;/pre>
&lt;p>The filters are called from the &lt;code>_get_guest_xml&lt;/code> method in
&lt;code>nova/virt/libvirt/driver.py&lt;/code>.&lt;/p>
&lt;h2 id="an-example-filter">An example filter&lt;/h2>
&lt;p>This filter will add an interface to the libvirt &lt;code>default&lt;/code> network to
any instance created by Nova:&lt;/p>
&lt;pre>&lt;code>from lxml import etree
class AddNetworkFilter (object):
def filter(self, xml,
instance=None,
context=None):
doc = etree.fromstring(xml)
network = etree.fromstring('''
&amp;lt;interface type=&amp;quot;network&amp;quot;&amp;gt;
&amp;lt;source network=&amp;quot;default&amp;quot;/&amp;gt;
&amp;lt;model type=&amp;quot;virtio&amp;quot;/&amp;gt;
&amp;lt;/interface&amp;gt;
''')
devices = doc.xpath('/domain/devices')[0]
devices.append(network)
return etree.tostring(doc, pretty_print=True)
&lt;/code>&lt;/pre>
&lt;p>You can find this in my &lt;a href="https://github.com/larsks/demo_nova_filters/">demo_nova_filters&lt;/a> repository, along with a
few other trivial examples. The above filter is registered via the
&lt;code>entry_points&lt;/code> section of the &lt;code>setup.py&lt;/code> file:&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env python
import setuptools
setuptools.setup(
name=&amp;quot;demo_nova_filters&amp;quot;,
version=1,
packages=['demo_nova_filters'],
entry_points={
'nova.filters.domxml': [
'prettyprint=demo_nova_filters.prettyprint:PrettyPrintFilter',
'novideo=demo_nova_filters.novideo:NoVideoFilter',
'addnetwork=demo_nova_filters.addnetwork:AddNetworkFilter',
]
},
)
&lt;/code>&lt;/pre>
&lt;p>And that&amp;rsquo;s it. This is almost entirely untested. While it works in
some cases it doesn&amp;rsquo;t work in all cases, and it&amp;rsquo;s unlikely that I&amp;rsquo;m
going to update this to work with any future version of Nova. This
was really just an exercise in curiosity. Enjoy!&lt;/p></content></item><item><title>Docker vs. PrivateTmp</title><link>https://blog.oddbit.com/post/2015-01-18-docker-vs-privatetmp/</link><pubDate>Sun, 18 Jan 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-01-18-docker-vs-privatetmp/</guid><description>While working with Docker the other day, I ran into an undesirable interaction between Docker and systemd services that utilize the PrivateTmp directive.
The PrivateTmp directive, if true, &amp;ldquo;sets up a new file system namespace for the executed processes and mounts private /tmp and /var/tmp directories inside it that is not shared by processes outside of the namespace&amp;rdquo;. This is a great idea from a security perspective, but can cause some unanticipated consequences.</description><content>&lt;p>While working with Docker &lt;a href="https://blog.oddbit.com/post/2015-01-17-running-novalibvirt-and-novado/">the other day&lt;/a>, I ran into an
undesirable interaction between Docker and &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a> services that
utilize the &lt;code>PrivateTmp&lt;/code> directive.&lt;/p>
&lt;p>The &lt;a href="http://www.freedesktop.org/software/systemd/man/systemd.exec.html#PrivateTmp=">PrivateTmp&lt;/a> directive, if &lt;code>true&lt;/code>, &amp;ldquo;sets up a new file system
namespace for the executed processes and mounts private &lt;code>/tmp&lt;/code> and
&lt;code>/var/tmp&lt;/code> directories inside it that is not shared by processes outside
of the namespace&amp;rdquo;. This is a great idea from a &lt;a href="https://danwalsh.livejournal.com/51459.html">security
perspective&lt;/a>, but can cause some unanticipated consequences.&lt;/p>
&lt;h2 id="the-problem-in-a-nutshell">The problem in a nutshell&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Start a Docker container:&lt;/p>
&lt;pre>&lt;code> # cid=$(docker run -d larsks/thttpd)
# echo $cid
e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>See the &lt;code>devicemapper&lt;/code> mountpoint created by Docker for the
container:&lt;/p>
&lt;pre>&lt;code> # grep devicemapper/mnt /proc/mounts
/dev/mapper/docker-253:6-98310-e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 /var/lib/docker/devicemapper/mnt/e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 ext4 rw,context=&amp;quot;system_u:object_r:svirt_sandbox_file_t:s0:c261,c1018&amp;quot;,relatime,discard,stripe=16,data=ordered 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Now restart a service &amp;ndash; any service! &amp;ndash; that has
&lt;code>PrivateTmp=true&lt;/code>:&lt;/p>
&lt;pre>&lt;code> # systemctl restart systemd-machined
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Get the PID for that service:&lt;/p>
&lt;pre>&lt;code> # systemctl status systemd-machined | grep PID
Main PID: 18698 (systemd-machine
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>And see that the mount created by the Docker &amp;ldquo;devicemapper&amp;rdquo; storage
driver is visible inside the mount namespace for this process:&lt;/p>
&lt;pre>&lt;code> # grep devicemapper/mnt /proc/18698/mounts
/dev/mapper/docker-253:6-98310-e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 /var/lib/docker/devicemapper/mnt/e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 ext4 rw,context=&amp;quot;system_u:object_r:svirt_sandbox_file_t:s0:c261,c1018&amp;quot;,relatime,discard,stripe=16,data=ordered 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Attempt to destroy the container:&lt;/p>
&lt;pre>&lt;code> # docker rm -f $cid
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Watch Docker fail to destroy the container because it is unable to
remove the mountpoint directory:&lt;/p>
&lt;pre>&lt;code> Jan 17 22:43:03 pk115wp-lkellogg docker-1.4.1-dev[18239]:
time=&amp;quot;2015-01-17T22:43:03-05:00&amp;quot; level=&amp;quot;error&amp;quot; msg=&amp;quot;Handler for DELETE
/containers/{name:.*} returned error: Cannot destroy container e68df3f45d61:
Driver devicemapper failed to remove root filesystem
e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62: Device is
Busy&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Because while that mount is gone from the global namespace:&lt;/p>
&lt;pre>&lt;code> # grep devicemapper/mnt /proc/mounts
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>It still exists inside the mount namespace for the service we restarted:&lt;/p>
&lt;pre>&lt;code># grep devicemapper/mnt /proc/18698/mounts
/dev/mapper/docker-253:6-98310-e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 /var/lib/docker/devicemapper/mnt/e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 ext4 rw,context=&amp;quot;system_u:object_r:svirt_sandbox_file_t:s0:c261,c1018&amp;quot;,relatime,discard,stripe=16,data=ordered 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>To resolve this problem, restart the service holding the mount open:&lt;/p>
&lt;pre>&lt;code># systemctl restart systemd-machined
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;p>Now the mountpoint can be deleted.&lt;/p>
&lt;h2 id="its-not-just-docker">It&amp;rsquo;s not just Docker&lt;/h2>
&lt;p>While I ran into this problem while working with Docker, there is
nothing particularly Docker-specific about the problem. You can
replicate this behavior by hand without involving either &lt;code>systemd&lt;/code> or
Docker:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create a parent mountpoint, and make it private:&lt;/p>
&lt;pre>&lt;code> # mkdir /tmp/parent /tmp/parent-backing
# mount --bind --make-private /tmp/parent-backing /tmp/parent
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Create a private mount on a directory &lt;em>inside&lt;/em> &lt;code>/tmp/parent&lt;/code>:&lt;/p>
&lt;pre>&lt;code> # mkdir /tmp/testmount /tmp/parent/mnt
# mount --bind --make-private /tmp/testmount /tmp/parent/mnt
# grep /tmp/parent/mnt /proc/self/mounts
tmpfs /tmp/parent/mnt tmpfs rw,seclabel 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>In another window, create a new mount namespace using &lt;code>unshare&lt;/code>:&lt;/p>
&lt;pre>&lt;code> # unshare -m env PS1='unshare# ' bash
unshare#
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Unmount &lt;code>/tmp/parent/mnt&lt;/code> in the global namespace:&lt;/p>
&lt;pre>&lt;code> # umount /tmp/parent/mnt
# grep /tmp/parent/mnt /proc/self/mounts
#
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Try to delete the mountpoint directory:&lt;/p>
&lt;pre>&lt;code> # rmdir /tmp/parent/mnt
rmdir: failed to remove ‘/tmp/parent/mnt’: Device or resource busy
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>See that the mount still exists in your &lt;code>unshare&lt;/code> namespace:&lt;/p>
&lt;pre>&lt;code> unshare# grep /tmp/parent/mnt /proc/self/mounts
tmpfs /tmp/parent/mnt tmpfs rw,seclabel 0 0
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="so-whats-going-on-here">So what&amp;rsquo;s going on here?&lt;/h2>
&lt;p>To understand what&amp;rsquo;s going on in these examples, you probably want to
start by at least glancing through the &lt;a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt">sharedsubtree.txt&lt;/a> kernel
documentation.&lt;/p>
&lt;p>The Docker &lt;code>devicemapper&lt;/code> driver creates a &lt;em>private&lt;/em> mount on
&lt;code>/var/lib/docker/devicemapper&lt;/code>. A &lt;em>private&lt;/em> mount is one that does
not propagate mount operations between parent and child mount
namespaces.&lt;/p>
&lt;p>Container filesystems are mounted underneath
&lt;code>/var/lib/docker/devicemapper/mnt&lt;/code>, e.g:&lt;/p>
&lt;pre>&lt;code> /dev/mapper/docker-253:6-98310-e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 /var/lib/docker/devicemapper/mnt/e68df3f45d6151259ce84a0e467a3117840084e99ef3bbc654b33f08d2d6dd62 ext4 rw,context=&amp;quot;system_u:object_r:svirt_sandbox_file_t:s0:c261,c1018&amp;quot;,relatime,discard,stripe=16,data=ordered 0 0
&lt;/code>&lt;/pre>
&lt;p>When you create a new mount namespace as a child of the global mount
namespace, either via the &lt;code>unshare&lt;/code> command or by starting a systemd
service with &lt;code>PrivateTmp=true&lt;/code>, it inherits these private mounts.
When Docker unmounts the the container filesystem in the global
namespace, the fact that the &lt;code>/var/lib/docker/devicemapper&lt;/code> mountpoint
is marked &lt;em>private&lt;/em> means that the unmount operation does not
propagate to other namespaces.&lt;/p>
&lt;h2 id="the-solution">The solution&lt;/h2>
&lt;p>The simplest solution to this problem is to set the &lt;code>MountFlags=slave&lt;/code>
option in the &lt;code>docker.service&lt;/code> file:&lt;/p>
&lt;pre>&lt;code>MountFlags=slave
&lt;/code>&lt;/pre>
&lt;p>This will cause SystemD to run Docker in a cloned mount namespace and
sets the &lt;code>MS_SLAVE&lt;/code> flag on all mountpoints; it is effectively
equivalent to:&lt;/p>
&lt;pre>&lt;code># unshare -m
# mount --make-rslave /
&lt;/code>&lt;/pre>
&lt;p>With this change, mounts performed by Docker will not be visible in
the global mount namespace, and they will thus not propagate into the
mount namespaces of other services.&lt;/p>
&lt;h2 id="not-necessarily-the-solution">Not necessarily the solution&lt;/h2>
&lt;p>There was an &lt;a href="http://pkgs.fedoraproject.org/cgit/docker-io.git/commit/?id=6c9e373ee06cb1aee07d3cae426c46002663010d">attempt to fix this problem&lt;/a> committed to the Fedora
&lt;code>docker-io&lt;/code> package that set &lt;code>MountFlags=private&lt;/code>. This will prevent
the symptoms I originally encountered, in which Docker is unable to
remove a mountpoint because it is still held open by another mount
namespace&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;but it will result in behavior that might be confusing to a system
administrator. Specifically, mounts made in the global mount
namespace after Docker starts will not be visible to Docker
containers. This means that if you were to make a remote filesystem
available on your Docker host:&lt;/p>
&lt;pre>&lt;code># mount my-fileserver:/vol/webcontent /srv/content
&lt;/code>&lt;/pre>
&lt;p>And then attempt to bind that into a Docker container as a volume:&lt;/p>
&lt;pre>&lt;code># docker run -v /srv/content:/content larsks/thttpd -d /content
&lt;/code>&lt;/pre>
&lt;p>Your content would not be visible. The mount of
&lt;code>my-fileserver:/vol/webcontent&lt;/code> would not propagate from the global
namespace into the Docker mount namespace because of the &lt;em>private&lt;/em>
flag.&lt;/p>
&lt;h2 id="thanks">Thanks&lt;/h2>
&lt;p>I had some help figuring this out. Thanks to &lt;a href="https://en.wikipedia.org/wiki/Lennart_Poettering">Lennart Poettering&lt;/a>,
Andrey Borzenkov, and &lt;a href="http://blog.verbum.org/">Colin Walters&lt;/a>.&lt;/p></content></item><item><title>Running nova-libvirt and nova-docker on the same host</title><link>https://blog.oddbit.com/post/2015-01-17-running-novalibvirt-and-novado/</link><pubDate>Sat, 17 Jan 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-01-17-running-novalibvirt-and-novado/</guid><description>I regularly use OpenStack on my laptop with libvirt as my hypervisor. I was interested in experimenting with recent versions of the nova-docker driver, but I didn&amp;rsquo;t have a spare system available on which to run the driver, and I use my regular nova-compute service often enough that I didn&amp;rsquo;t want to simply disable it temporarily in favor of nova-docker.
NB As pointed out by gustavo in the comments, running two neutron-openvswitch-agents on the same host &amp;ndash; as suggested in this article &amp;ndash; is going to lead to nothing but sadness and doom.</description><content>&lt;p>I regularly use &lt;a href="http://www.openstack.org/">OpenStack&lt;/a> on my laptop with &lt;a href="http://www.libvirt.org/">libvirt&lt;/a> as my
hypervisor. I was interested in experimenting with recent versions of
the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> driver, but I didn&amp;rsquo;t have a spare system available
on which to run the driver, and I use my regular &lt;code>nova-compute&lt;/code> service
often enough that I didn&amp;rsquo;t want to simply disable it temporarily in
favor of &lt;code>nova-docker&lt;/code>.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong> As pointed out by &lt;em>gustavo&lt;/em> in the comments, running two
&lt;code>neutron-openvswitch-agents&lt;/code> on the same host &amp;ndash; as suggested in this
article &amp;ndash; is going to lead to nothing but sadness and doom. So
kids, don&amp;rsquo;t try this at home. I&amp;rsquo;m leaving the article here because I
think it still has some interesting bits.&lt;/p>
&lt;hr>
&lt;p>I guess the simplest solution would be to spin up a vm on which to run
&lt;code>nova-docker&lt;/code>, but why use a simple solution when there are things to
be learned? I wanted to know if it were possible (and if so, how) to
run both hypervisors on the same physical host.&lt;/p>
&lt;p>The naive solution would be to start up another instance of
&lt;code>nova-compute&lt;/code> configured to use the Docker driver. Unfortunately,
Nova only permits a single service instance per &amp;ldquo;host&amp;rdquo;, so starting up
the second instance of &lt;code>nova-compute&lt;/code> would effectively &amp;ldquo;mask&amp;rdquo; the
original one.&lt;/p>
&lt;p>Fortunately, Nova&amp;rsquo;s definition of what constitutes a &amp;ldquo;host&amp;rdquo; is
somewhat flexible. Nova supports a &lt;code>host&lt;/code> configuration key in
&lt;code>nova.conf&lt;/code> that will cause Nova to identify the host on which it is
running using your explicitly configured value, rather than your
system hostname. We can take advantage of this to get a second
&lt;code>nova-compute&lt;/code> instance running on the same system.&lt;/p>
&lt;h2 id="install-nova-docker">Install nova-docker&lt;/h2>
&lt;p>We&amp;rsquo;ll start by installing the &lt;code>nova-docker&lt;/code> driver from
&lt;a href="https://github.com/stackforge/nova-docker">https://github.com/stackforge/nova-docker&lt;/a>. If you&amp;rsquo;re running the
Juno release of OpenStack (which I am), you&amp;rsquo;re going to want to use
the &lt;code>stable/juno&lt;/code> branch of the &lt;code>nova-docker&lt;/code> repository. So:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/stackforge/nova-docker
$ cd nova-docker
$ git checkout stable/juno
$ sudo python setup.py install
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll want to read the project&amp;rsquo;s &lt;a href="https://github.com/stackforge/nova-docker/blob/master/README.rst">README&lt;/a> for complete installation
instructions.&lt;/p>
&lt;h2 id="configure-nova-docker">Configure nova-docker&lt;/h2>
&lt;p>Now, rather than configuring &lt;code>/etc/nova/nova.conf&lt;/code>, we&amp;rsquo;re going to
create a new configuration file, &lt;code>/etc/nova/nova-docker.conf&lt;/code>, with
only the configuration keys that differ from our primary Nova
configuration:&lt;/p>
&lt;pre>&lt;code>[DEFAULT]
host=nova-docker
compute_driver=novadocker.virt.docker.DockerDriver
log_file=/var/log/nova/nova-docker.log
state_path=/var/lib/nova-docker
&lt;/code>&lt;/pre>
&lt;p>You can see that we&amp;rsquo;ve set the value of &lt;code>host&lt;/code> to &lt;code>nova-docker&lt;/code>, to
differentiate this &lt;code>nova-compute&lt;/code> service from the &lt;code>libvirt&lt;/code>-backed
one that is already running. We&amp;rsquo;ve provided the service with a
dedicated log file and state directory to prevent conflicts with the
already-running &lt;code>nova-compute&lt;/code> service.&lt;/p>
&lt;p>To use this configuration file, we&amp;rsquo;ll launch a new instance of the
&lt;code>nova-compute&lt;/code> service pointing at both the original configuration
file, &lt;code>/etc/nova/nova.conf&lt;/code>, as well as this &lt;code>nova-docker&lt;/code>
configuration file. The command line would look something like:&lt;/p>
&lt;pre>&lt;code>nova-compute --config-file /etc/nova/nova.conf \
--config-file /etc/nova/nova-docker.conf
&lt;/code>&lt;/pre>
&lt;p>The ordering of configuration files on the command line is
significant: later configuration files will override values from
earlier files.&lt;/p>
&lt;p>I&amp;rsquo;m running &lt;a href="http://www.fedora.org/">Fedora&lt;/a> 21 on my laptop, which uses &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a>, so I
created a modified version of the &lt;code>openstack-nova-compute.service&lt;/code>
unit on my system, and saved it as
&lt;code>/etc/systemd/system/openstack-nova-docker.service&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=OpenStack Nova Compute Server (Docker)
After=syslog.target network.target
[Service]
Environment=LIBGUESTFS_ATTACH_METHOD=appliance
Type=notify
Restart=always
User=nova
ExecStart=/usr/bin/nova-compute --config-file /etc/nova/nova.conf --config-file /etc/nova/nova-docker.conf
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;p>And then activated the service;&lt;/p>
&lt;pre>&lt;code># systemctl enable openstack-nova-docker
# systemctl start openstack-nova-docker
&lt;/code>&lt;/pre>
&lt;p>Now, if I run &lt;code>nova service-list&lt;/code> with administrator credentials, I
can see both &lt;code>nova-compute&lt;/code> instances:&lt;/p>
&lt;pre>&lt;code>+----+------------------+------------------+----------+---------+-------...
| Id | Binary | Host | Zone | Status | State ...
+----+------------------+------------------+----------+---------+-------...
| 1 | nova-consoleauth | host.example.com | internal | enabled | up ...
| 2 | nova-scheduler | host.example.com | internal | enabled | up ...
| 3 | nova-conductor | host.example.com | internal | enabled | up ...
| 5 | nova-cert | host.example.com | internal | enabled | up ...
| 6 | nova-compute | host.example.com | nova | enabled | up ...
| 7 | nova-compute | nova-docker | nova | enabled | up ...
+----+------------------+------------------+----------+---------+-------...
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-docker-container-take-1">Booting a Docker container (take 1)&lt;/h2>
&lt;p>Let&amp;rsquo;s try starting a Docker container using the new &lt;code>nova-compute&lt;/code>
service. We&amp;rsquo;ll first need to load a Docker image into Glance (you
followed the &lt;code>nova-docker&lt;/code> &lt;a href="https://github.com/stackforge/nova-docker#1-enable-the-driver-in-glances-configuration">instructions for configuring
Glance&lt;/a>, right?). We&amp;rsquo;ll use my &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">larsks/thttpd&lt;/a> image,
because it&amp;rsquo;s very small and doesn&amp;rsquo;t require any configuration:&lt;/p>
&lt;pre>&lt;code>$ docker pull larsks/thttpd
$ docker save larsks/thttpd |
glance image-create --is-public True --container-format docker \
--disk-format raw --name larsks/thttpd
&lt;/code>&lt;/pre>
&lt;p>(Note that you will probably require administrative credentials to load
this image into Glance.)&lt;/p>
&lt;p>Now that we have an appropriate image available we can try booting a container:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image larsks/thttpd --flavor m1.small test1
&lt;/code>&lt;/pre>
&lt;p>If we wait a moment and then run &lt;code>nova list&lt;/code>, we see:&lt;/p>
&lt;pre>&lt;code>| 9a783952-a888-4fcd-8f5d-cd9291ed1969 | test1 | ERROR | spawning ...
&lt;/code>&lt;/pre>
&lt;p>&lt;a href="http://www.sadtrombone.com/">What happened?&lt;/a> Looking at the appropriate log file
(&lt;code>/var/log/nova/nova-docker.log&lt;/code>), we find:&lt;/p>
&lt;pre>&lt;code>Cannot setup network: Unexpected vif_type=binding_failed
Traceback (most recent call last):
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/driver.py&amp;quot;, line 367, in _start_container
self.plug_vifs(instance, network_info)
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/driver.py&amp;quot;, line 187, in plug_vifs
self.vif_driver.plug(instance, vif)
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/vifs.py&amp;quot;, line 63, in plug
_(&amp;quot;Unexpected vif_type=%s&amp;quot;) % vif_type)
NovaException: Unexpected vif_type=binding_failed
&lt;/code>&lt;/pre>
&lt;p>The message &lt;code>vif_type=binding_failed&lt;/code> is Nova&amp;rsquo;s way of saying &amp;ldquo;I have
no idea what happened, go ask Neutron&amp;rdquo;. Looking in Neutron&amp;rsquo;s
&lt;code>/var/log/neutron/server.log&lt;/code>, we find:&lt;/p>
&lt;pre>&lt;code>Failed to bind port 82c07caa-b2c2-45e9-955d-e8b35112437c on host
nova-docker
&lt;/code>&lt;/pre>
&lt;p>And this tells us our problem: we have told our &lt;code>nova-docker&lt;/code> service
that it is running on a host called &amp;ldquo;nova-docker&amp;rdquo;, and Neutron doesn&amp;rsquo;t
know anything about that host.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong>&lt;/p>
&lt;p>If you were to try to delete this failed instance, you would find that
it is un-deletable. In the end, I was only able to delete it by
directly editing the &lt;code>nova&lt;/code> database using &lt;a href="delete-deleting-instances.sql">this sql script&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="adding-a-neutron-agent">Adding a Neutron agent&lt;/h2>
&lt;p>We&amp;rsquo;re going to need to set up an instance of
&lt;code>neutron-openvswitch-agent&lt;/code> to service network requests on our
&amp;ldquo;nova-docker&amp;rdquo; host. Like Nova, Neutron also supports a &lt;code>host&lt;/code>
configuration key, so we&amp;rsquo;re going to pursue a solution similar to what
we used with Nova by creating a new configuration file,
&lt;code>/etc/neutron/ovs-docker.conf&lt;/code>, with the following content:&lt;/p>
&lt;pre>&lt;code>[DEFAULT]
host = nova-docker
&lt;/code>&lt;/pre>
&lt;p>And then we&amp;rsquo;ll set up the corresponding service by dropping the
following into &lt;code>/etc/systemd/system/docker-openvswitch-agent.service&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=OpenStack Neutron Open vSwitch Agent (Docker)
After=syslog.target network.target
[Service]
Type=simple
User=neutron
ExecStart=/usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini --config-file /etc/neutron/ovs-docker.conf --log-file /var/log/neutron/docker-openvswitch-agent.log
PrivateTmp=true
KillMode=process
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong>&lt;/p>
&lt;p>While working on this configuration I ran into an undesirable
interaction between Docker and &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a>&amp;rsquo;s &lt;code>PrivateTmp&lt;/code> directive.&lt;/p>
&lt;p>This directive causes the service to run with
a private &lt;a href="http://lwn.net/Articles/531114/">mount namespace&lt;/a> such that &lt;code>/tmp&lt;/code> for the service is not
the same as &lt;code>/tmp&lt;/code> for other services. This is a great idea from a
security perspective, but can cause problems in the following
scenario:&lt;/p>
&lt;ol>
&lt;li>Start a Docker container with &lt;code>nova boot ...&lt;/code>&lt;/li>
&lt;li>Restart any service that uses the &lt;code>PrivateTmp&lt;/code> directive&lt;/li>
&lt;li>Attempt to delete the Docker container with &lt;code>nova delete ...&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>Docker will fail to destroy the container because the private
namespace created by the &lt;code>PrivateTmp&lt;/code> directive preserves a reference
to the Docker &lt;code>devicemapper&lt;/code> mount in
&lt;code>/var/lib/docker/devicemapper/mnt/...&lt;/code> that was active at the time the
service was restarted. To recover from this situation, you will need
to restart whichever service is still holding a reference to the
Docker mounts.&lt;/p>
&lt;p>I have &lt;a href="http://lists.freedesktop.org/archives/systemd-devel/2015-January/027162.html">posted to the systemd-devel&lt;/a> mailing
list to see if there are any solutions to this behavior. As I note in
that email, this behavior appears to be identical to that described in
Fedora bug &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=851970">851970&lt;/a>, which was closed two years ago.&lt;/p>
&lt;p>&lt;strong>Update&lt;/strong> I wrote a &lt;a href="https://blog.oddbit.com/post/2015-01-18-docker-vs-privatetmp/">separate post&lt;/a> about this issue, which
includes some discussion about what&amp;rsquo;s going on and a solution.&lt;/p>
&lt;hr>
&lt;p>If we activate this service&amp;hellip;&lt;/p>
&lt;pre>&lt;code># systemctl enable docker-openvswitch-agent
# systemctl start docker-openvswitch-agent
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then run &lt;code>neutron agent-list&lt;/code> with administrative credentials,
we&amp;rsquo;ll see the new agent:&lt;/p>
&lt;pre>&lt;code>$ neutron agent-list
+--------------------------------------+--------------------+------------------+-------+...
| id | agent_type | host | alive |...
+--------------------------------------+--------------------+------------------+-------+...
| 2e40062a-1c30-46a3-8719-3ce93a56b4ce | Open vSwitch agent | nova-docker | :-) |...
| 63edb2a4-f980-4f88-b9c0-9610a1b20f13 | L3 agent | host.example.com | :-) |...
| 8482c5c3-208c-4145-9f7d-606be3da11ed | Loadbalancer agent | host.example.com | :-) |...
| 9922ed54-00fa-41d4-96e8-ac8af8c291fd | Open vSwitch agent | host.example.com | :-) |...
| b8becb9c-7290-42be-9faf-fd3baeea3dcf | Metadata agent | host.example.com | :-) |...
| c46be41b-e93a-40ab-a37e-4d67b770a3df | DHCP agent | host.example.com | :-) |...
+--------------------------------------+--------------------+------------------+-------+...
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-docker-container-take-2">Booting a Docker container (take 2)&lt;/h2>
&lt;p>Now that we have both the &lt;code>nova-docker&lt;/code> service running and a
corresponding &lt;code>neutron-openvswitch-agent&lt;/code> available, let&amp;rsquo;s try
starting our container one more time:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image larsks/thttpd --flavor m1.small test1
$ nova list
+--------------------------------------+---------+--------+...
| ID | Name | Status |...
+--------------------------------------+---------+--------+...
| 642b7d61-9189-40ea-86f5-2424c3c86028 | test1 | ACTIVE |...
+--------------------------------------+---------+--------+...
&lt;/code>&lt;/pre>
&lt;p>If we assign a floating IP address:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-create ext-nat
+-----------------+-----------+----------+---------+
| Ip | Server Id | Fixed Ip | Pool |
+-----------------+-----------+----------+---------+
| 192.168.200.211 | - | - | ext-nat |
+-----------------+-----------+----------+---------+
$ nova floating-ip-associate test1 192.168.200.211
&lt;/code>&lt;/pre>
&lt;p>We can then browse to &lt;code>http://192.168.200.211&lt;/code> and see the sample
page:&lt;/p>
&lt;pre>&lt;code>$ curl http://192.168.200.211/
.
.
.
____ _ _ _ _
/ ___|___ _ __ __ _ _ __ __ _| |_ _ _| | __ _| |_(_) ___ _ __ ___
| | / _ \| '_ \ / _` | '__/ _` | __| | | | |/ _` | __| |/ _ \| '_ \/ __|
| |__| (_) | | | | (_| | | | (_| | |_| |_| | | (_| | |_| | (_) | | | \__ \
\____\___/|_| |_|\__, |_| \__,_|\__|\__,_|_|\__,_|\__|_|\___/|_| |_|___/
|___/
.
.
.
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-libvirt-instance">Booting a libvirt instance&lt;/h2>
&lt;p>To show that we really are running two hypervisors on the same host,
we can launch a traditional &lt;code>libvirt&lt;/code> instance alongside our Docker
container:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image cirros --flavor m1.small --key-name lars test2
&lt;/code>&lt;/pre>
&lt;p>Wait a bit, then:&lt;/p>
&lt;pre>&lt;code>$ nova list
+--------------------------------------+-------+--------+...
| ID | Name | Status |...
+--------------------------------------+-------+--------+...
| 642b7d61-9189-40ea-86f5-2424c3c86028 | test1 | ACTIVE |...
| 7fec33c9-d50f-477e-957c-a05ee9bd0b0b | test2 | ACTIVE |...
+--------------------------------------+-------+--------+...
&lt;/code>&lt;/pre></content></item><item><title>Building a minimal web server for testing Kubernetes</title><link>https://blog.oddbit.com/post/2015-01-04-building-a-minimal-web-server-/</link><pubDate>Sun, 04 Jan 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-01-04-building-a-minimal-web-server-/</guid><description>I have recently been doing some work with Kubernetes, and wanted to put together a minimal image with which I could test service and pod deployment. Size in this case was critical: I wanted something that would download quickly when initially deployed, because I am often setting up and tearing down Kubernetes as part of my testing (and some of my test environments have poor external bandwidth).
Building thttpd My go-to minimal webserver is thttpd.</description><content>&lt;p>I have recently been doing some work with &lt;a href="https://github.com/googlecloudplatform/kubernetes">Kubernetes&lt;/a>, and wanted
to put together a minimal image with which I could test service and
pod deployment. Size in this case was critical: I wanted something
that would download quickly when initially deployed, because I am
often setting up and tearing down Kubernetes as part of my testing
(and some of my test environments have poor external bandwidth).&lt;/p>
&lt;h2 id="building-thttpd">Building thttpd&lt;/h2>
&lt;p>My go-to minimal webserver is &lt;a href="http://acme.com/software/thttpd/">thttpd&lt;/a>. For the normal case,
building the software is a simple matter of &lt;code>./configure&lt;/code> followed by
&lt;code>make&lt;/code>. This gets you a dynamically linked binary; using &lt;code>ldd&lt;/code> you
could build a Docker image containing only the necessary shared
libraries:&lt;/p>
&lt;pre>&lt;code>$ mkdir thttpd-root thttpd-root/lib64
$ cp thttpd thttpd-root/
$ cd thttpd-root
$ cp $(ldd thttpd | awk '$3 ~ &amp;quot;/&amp;quot; {print $3}') lib64/
$ cp /lib64/ld-linux-x86-64.so.2 lib64/
&lt;/code>&lt;/pre>
&lt;p>Which gets us:&lt;/p>
&lt;pre>&lt;code>$ find * -type f
lib64/ld-linux-x86-64.so.2
lib64/libdl.so.2
lib64/libc.so.6
lib64/libcrypt.so.1
lib64/libfreebl3.so
thttpd
&lt;/code>&lt;/pre>
&lt;p>However, if we try to run &lt;code>thttpd&lt;/code> via a &lt;code>chroot&lt;/code> into this directory,
it will fail:&lt;/p>
&lt;pre>&lt;code>$ sudo chroot $PWD /thttpd -D
/thttpd: unknown user - 'nobody'
&lt;/code>&lt;/pre>
&lt;p>A little &lt;code>strace&lt;/code> will show us what&amp;rsquo;s going on:&lt;/p>
&lt;pre>&lt;code>$ sudo strace chroot $PWD /thttpd -D
[...]
open(&amp;quot;/etc/nsswitch.conf&amp;quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
open(&amp;quot;/lib64/libnss_compat.so.2&amp;quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
[...]
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s looking for an &lt;a href="https://en.wikipedia.org/wiki/Name_Service_Switch">NSS&lt;/a> configuration and related libraries. So
let&amp;rsquo;s give it what it wants:&lt;/p>
&lt;pre>&lt;code>$ mkdir etc
$ cat &amp;gt; etc/nsswitch.conf &amp;lt;&amp;lt;EOF
passwd: files
group: files
EOF
$ grep nobody /etc/passwd &amp;gt; etc/passwd
$ grep nobody /etc/group &amp;gt; etc/group
$ cp /lib64/libnss_files.so.2 lib64/
&lt;/code>&lt;/pre>
&lt;p>And now:&lt;/p>
&lt;pre>&lt;code>$ sudo chroot $PWD /thttpd -D
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and it keeps running. This gives a filesystem that is almost
exactly 3MB in size. Can we do better?&lt;/p>
&lt;h2 id="building-a-static-binary">Building a static binary&lt;/h2>
&lt;p>In theory, building a static binary should be as simple as:&lt;/p>
&lt;pre>&lt;code>$ make CCOPT='-O2 -static'
&lt;/code>&lt;/pre>
&lt;p>But on my Fedora 21 system, this gets me several warnings:&lt;/p>
&lt;pre>&lt;code>thttpd.c:(.text.startup+0xf81): warning: Using 'initgroups' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
thttpd.c:(.text.startup+0x146d): warning: Using 'getpwnam' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
thttpd.c:(.text.startup+0x65d): warning: Using 'getaddrinfo' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking
&lt;/code>&lt;/pre>
&lt;p>And then a bunch of errors:&lt;/p>
&lt;pre>&lt;code>/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/libcrypt.a(md5-crypt.o): In function `__md5_crypt_r':
(.text+0x11c): undefined reference to `NSSLOW_Init'
/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/libcrypt.a(md5-crypt.o): In function `__md5_crypt_r':
(.text+0x136): undefined reference to `NSSLOWHASH_NewContext'
/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/libcrypt.a(md5-crypt.o): In function `__md5_crypt_r':
(.text+0x14a): undefined reference to `NSSLOWHASH_Begin'
/usr/lib/gcc/x86_64-redhat-linux/4.9.2/../../../../lib64/libcrypt.a(md5-crypt.o): In function `__md5_crypt_r':
[...]
&lt;/code>&lt;/pre>
&lt;p>Fortunately (?), this is a distribution-specific problem. Building
&lt;code>thttpd&lt;/code> inside an Ubuntu Docker container seems to work fine:&lt;/p>
&lt;pre>&lt;code>$ docker run -it --rm -v $PWD:/src ubuntu
root@1e126269241c:/# apt-get update; apt-get -y install make gcc
root@1e126269241c:/# make -C /src CCOPT='-O2 -static'
root@1e126269241c:/# exit
&lt;/code>&lt;/pre>
&lt;p>Now we have a statically built binary:&lt;/p>
&lt;pre>&lt;code>$ file thttpd
thttpd: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked, for GNU/Linux 2.6.24, BuildID[sha1]=bb211a88e9e1d51fa2e937b2b7ea892d87a287d5, not stripped
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s rebuild our &lt;code>chroot&lt;/code> environment:&lt;/p>
&lt;pre>&lt;code>$ rm -rf thttpd-root
$ mkdir thttpd-root thttpd-root/lib64
$ cp thttpd thttpd-root/
$ cd thttpd-root
&lt;/code>&lt;/pre>
&lt;p>And try running &lt;code>thttpd&lt;/code> again:&lt;/p>
&lt;pre>&lt;code>$ sudo chroot $PWD /thttpd -D
/thttpd: unknown user - 'nobody'
&lt;/code>&lt;/pre>
&lt;p>Bummer. It looks like the NSS libraries are still biting us, and it
looks as if statically compiling code that uses NSS &lt;a href="https://stackoverflow.com/questions/3430400/linux-static-linking-is-dead">may be tricky&lt;/a>.
Fortunately, it&amp;rsquo;s relatively simple to patch out the parts of the
&lt;code>thttpd&lt;/code> code that are trying to switch to another uid/gid. The
following &lt;a href="https://github.com/larsks/docker-image-thttpd/blob/master/builder/thttpd-runasroot.patch">patch&lt;/a> will do the trick:&lt;/p>
&lt;pre>&lt;code>diff --git a/thttpd.c b/thttpd.c
index fe21b44..397feb1 100644
--- a/thttpd.c
+++ b/thttpd.c
@@ -400,22 +400,6 @@ main( int argc, char** argv )
if ( throttlefile != (char*) 0 )
read_throttlefile( throttlefile );
- /* If we're root and we're going to become another user, get the uid/gid
- ** now.
- */
- if ( getuid() == 0 )
- {
- pwd = getpwnam( user );
- if ( pwd == (struct passwd*) 0 )
- {
- syslog( LOG_CRIT, &amp;quot;unknown user - '%.80s'&amp;quot;, user );
- (void) fprintf( stderr, &amp;quot;%s: unknown user - '%s'\n&amp;quot;, argv0, user );
- exit( 1 );
- }
- uid = pwd-&amp;gt;pw_uid;
- gid = pwd-&amp;gt;pw_gid;
- }
-
/* Log file. */
if ( logfile != (char*) 0 )
{
@@ -441,17 +425,6 @@ main( int argc, char** argv )
(void) fprintf( stderr, &amp;quot;%s: logfile is not an absolute path, you may not be able to re-open it\n&amp;quot;, argv0 );
}
(void) fcntl( fileno( logfp ), F_SETFD, 1 );
- if ( getuid() == 0 )
- {
- /* If we are root then we chown the log file to the user we'll
- ** be switching to.
- */
- if ( fchown( fileno( logfp ), uid, gid ) &amp;lt; 0 )
- {
- syslog( LOG_WARNING, &amp;quot;fchown logfile - %m&amp;quot; );
- perror( &amp;quot;fchown logfile&amp;quot; );
- }
- }
}
}
else
@@ -680,41 +653,6 @@ main( int argc, char** argv )
stats_bytes = 0;
stats_simultaneous = 0;
- /* If we're root, try to become someone else. */
- if ( getuid() == 0 )
- {
- /* Set aux groups to null. */
- if ( setgroups( 0, (const gid_t*) 0 ) &amp;lt; 0 )
- {
- syslog( LOG_CRIT, &amp;quot;setgroups - %m&amp;quot; );
- exit( 1 );
- }
- /* Set primary group. */
- if ( setgid( gid ) &amp;lt; 0 )
- {
- syslog( LOG_CRIT, &amp;quot;setgid - %m&amp;quot; );
- exit( 1 );
- }
- /* Try setting aux groups correctly - not critical if this fails. */
- if ( initgroups( user, gid ) &amp;lt; 0 )
- syslog( LOG_WARNING, &amp;quot;initgroups - %m&amp;quot; );
-#ifdef HAVE_SETLOGIN
- /* Set login name. */
- (void) setlogin( user );
-#endif /* HAVE_SETLOGIN */
- /* Set uid. */
- if ( setuid( uid ) &amp;lt; 0 )
- {
- syslog( LOG_CRIT, &amp;quot;setuid - %m&amp;quot; );
- exit( 1 );
- }
- /* Check for unnecessary security exposure. */
- if ( ! do_chroot )
- syslog(
- LOG_WARNING,
- &amp;quot;started as root without requesting chroot(), warning only&amp;quot; );
- }
-
/* Initialize our connections table. */
connects = NEW( connecttab, max_connects );
if ( connects == (connecttab*) 0 )
&lt;/code>&lt;/pre>
&lt;p>After patching this and re-building thttpd in the Ubuntu container, we
have a functioning statically linked binary:&lt;/p>
&lt;pre>&lt;code>$ ./thttpd -D -l /dev/stderr -p 8080
127.0.0.1 - - [04/Jan/2015:16:44:26 -0500] &amp;quot;GET / HTTP/1.1&amp;quot; 200 1351 &amp;quot;&amp;quot; &amp;quot;curl/7.37.0&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>That line of output represents me running &lt;code>curl&lt;/code> in another window.&lt;/p>
&lt;h2 id="automating-the-process">Automating the process&lt;/h2>
&lt;p>I have put together an environment to perform the above steps and
build a minimal Docker image with the resulting binary. You can find
the code at &lt;a href="https://github.com/larsks/docker-image-thttpd">https://github.com/larsks/docker-image-thttpd&lt;/a>.&lt;/p>
&lt;p>If you check out the code:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/larsks/docker-image-thttpd
$ cd docker-image-thttpd
&lt;/code>&lt;/pre>
&lt;p>And run &lt;code>make&lt;/code>, this will:&lt;/p>
&lt;ol>
&lt;li>build an Ubuntu-based image with scripts in place to produce a
statically-linked thttpd,&lt;/li>
&lt;li>Boot a container from that image and drop the static &lt;code>thttpd&lt;/code>
binary into a local directory, and&lt;/li>
&lt;li>Produce a minimal Docker image containing just &lt;code>thttpd&lt;/code> and a
simple &lt;code>index.html&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>The final image is just over 1MB in size, and downloads to a new
Kubernetes environment in seconds. You can grab the finished image
via:&lt;/p>
&lt;pre>&lt;code>docker pull larsks/thttpd
&lt;/code>&lt;/pre>
&lt;p>(Or you can grab the above repository from GitHub and build it
yourself locally).&lt;/p></content></item><item><title>Accessing the serial console of your Nova servers</title><link>https://blog.oddbit.com/post/2014-12-22-accessing-the-serial-console-o/</link><pubDate>Mon, 22 Dec 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-12-22-accessing-the-serial-console-o/</guid><description>One of the new features available in the Juno release of OpenStack is support for serial console access to your Nova servers. This post looks into how to configure the serial console feature and then how to access the serial consoles of your Nova servers.
Configuring serial console support In previous release of OpenStack, read-only access to the serial console of your servers was available through the os-getConsoleOutput server action (exposed via nova console-log on the command line).</description><content>&lt;p>One of the new features available in the Juno release of OpenStack is
support for &lt;a href="https://blueprints.launchpad.net/nova/+spec/serial-ports">serial console access to your Nova
servers&lt;/a>. This post looks into how to configure the
serial console feature and then how to access the serial consoles of
your Nova servers.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="configuring-serial-console-support">Configuring serial console support&lt;/h2>
&lt;p>In previous release of OpenStack, read-only access to the serial
console of your servers was available through the
&lt;code>os-getConsoleOutput&lt;/code> server action (exposed via &lt;code>nova console-log&lt;/code> on
the command line). Most cloud-specific Linux images are configured
with a command line that includes something like &lt;code>console=tty0 console=ttyS0,115200n81&lt;/code>, which ensures that kernel output and other
messages are available on the serial console. This is a useful
mechanism for diagnosing problems in the event that you do not have
network access to a server.&lt;/p>
&lt;p>In Juno, you can exchange this read-only view of the console for
read-write access by setting &lt;code>enabled=true&lt;/code> in the &lt;code>[serial_console]&lt;/code>
section of your &lt;code>nova.conf&lt;/code> file:&lt;/p>
&lt;pre>&lt;code>[serial_console]
enabled=true
&lt;/code>&lt;/pre>
&lt;p>This enables the new &lt;code>os-getSerialConsole&lt;/code> server action.&lt;/p>
&lt;p>Much like the configuration for graphical console access, you will also
probably need to provide values for &lt;code>base_url&lt;/code>, &lt;code>listen&lt;/code>, and
&lt;code>proxyclient_address&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[serial_console]
enabled=true
# Location of serial console proxy. (string value)
base_url=ws://127.0.0.1:6083/
# IP address on which instance serial console should listen
# (string value)
listen=127.0.0.1
# The address to which proxy clients (like nova-serialproxy)
# should connect (string value)
proxyclient_address=127.0.0.1
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>base_url&lt;/code> setting is what gets passed to clients, so this will
probably be the address of one of your &amp;ldquo;front-end&amp;rdquo; controllers (e.g.,
wherever you are running other public APIs or services like Horizon).&lt;/p>
&lt;p>The &lt;code>listen&lt;/code> address is used by &lt;code>nova-compute&lt;/code> to control on which
address the virtual console will listen (this can be set to &lt;code>0.0.0.0&lt;/code>
to listen on all available addresses). The &lt;code>proxyclient_address&lt;/code>
controls to which address the &lt;code>nova-serialproxy&lt;/code> service will connect.&lt;/p>
&lt;p>In other words: a remote client request a serial console will receive
a websocket URL prefixed by &lt;code>base_url&lt;/code>. This URL will connect the
client to the &lt;code>nova-serialproxy&lt;/code> service. The &lt;code>nova-serialproxy&lt;/code>
service will look up the &lt;code>proxyclient_address&lt;/code> associated with the
requested server, and will connect to the appropriate port at that
address.&lt;/p>
&lt;p>Enabling serial console support will result in an entry similar to the
following in the XML description of libvirt guests started by Nova:&lt;/p>
&lt;pre>&lt;code>&amp;lt;console type='tcp'&amp;gt;
&amp;lt;source mode='bind' host='127.0.0.1' service='10000'/&amp;gt;
&amp;lt;protocol type='raw'/&amp;gt;
&amp;lt;target type='serial' port='0'/&amp;gt;
&amp;lt;alias name='serial0'/&amp;gt;
&amp;lt;/console&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="accessing-the-serial-console">Accessing the serial console&lt;/h2>
&lt;p>You can use the &lt;code>nova get-serial-proxy&lt;/code> command to retrieve the
websocket URL for a server&amp;rsquo;s serial console, like this:&lt;/p>
&lt;pre>&lt;code>$ nova get-serial-console my-server
+--------+-----------------------------------------------------------------+
| Type | Url |
+--------+-----------------------------------------------------------------+
| serial | ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d |
+--------+-----------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>Or through the REST API like this:&lt;/p>
&lt;pre>&lt;code>curl -i 'http://127.0.0.1:8774/v2/&amp;lt;tenant_uuid&amp;gt;/servers/&amp;lt;server_uuid&amp;gt;/action' \
-X POST \
-H &amp;quot;Accept: application/json&amp;quot; \
-H &amp;quot;Content-Type: application/json&amp;quot; \
-H &amp;quot;X-Auth-Project-Id: &amp;lt;project_id&amp;gt;&amp;quot; \
-H &amp;quot;X-Auth-Token: &amp;lt;auth_token&amp;gt;&amp;quot; \
-d '{&amp;quot;os-getSerialConsole&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;serial&amp;quot;}}'
&lt;/code>&lt;/pre>
&lt;p>But now that you have a websocket URL, what do you do with it? It
turns out that there aren&amp;rsquo;t all that many out-of-the-box tools that
will let you connect interactively to this URL from the command line.
While I&amp;rsquo;m sure that a future version of Horizon will provide a
web-accessible console access mechanism, it is often convenient to
have a command-line tool for this sort of thing because that permits
you to log or otherwise process the output.&lt;/p>
&lt;p>Fortunately, it&amp;rsquo;s not too difficult to write a simple client. The
Python &lt;code>websocket-client&lt;/code> module has the necessary support; given the
above URL, you can open a connection like this:&lt;/p>
&lt;pre>&lt;code>import websocket
ws = websocket.create_connection(
'ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d',
subprotocols=['binary', 'base64'])
&lt;/code>&lt;/pre>
&lt;p>This gets you a &lt;code>WebSocket&lt;/code> object with &lt;code>.send&lt;/code> and &lt;code>.recv&lt;/code> methods
for sending and receiving data (and a &lt;code>.fileno&lt;/code> method for use in
event loops).&lt;/p>
&lt;h2 id="i-was-told-there-would-be-no-programming">I was told there would be no programming&lt;/h2>
&lt;p>If you don&amp;rsquo;t feel like writing your own websocket client, have no
fear! I have put together a simple client called &lt;a href="http://github.com/larsks/novaconsole/">novaconsole&lt;/a>.
Assuming that you have valid credentials in your environment, you can
provide it with a server name or UUID:&lt;/p>
&lt;pre>&lt;code>$ novaconsole my-server
&lt;/code>&lt;/pre>
&lt;p>You can also provide a verbatim websocket URL (in which case you don&amp;rsquo;t
need to bother with OpenStack authentication):&lt;/p>
&lt;pre>&lt;code>$ novaconsole --url ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d
&lt;/code>&lt;/pre>
&lt;p>In either case, you will have an interactive session to the specified
serial console. You can exit the session by typing &lt;code>~.&lt;/code> at the
beginning of a line.&lt;/p>
&lt;p>You can only have a single active console connection at a time. Other
connections will block until you disconnect from the active session.&lt;/p>
&lt;h2 id="but-everything-is-not-roses-and-sunshine">But everything is not roses and sunshine&lt;/h2>
&lt;p>One disadvantage to the serial console support is that it &lt;em>replaces&lt;/em>
the console log available via &lt;code>nova console-log&lt;/code>. This means that if,
for example, a server were to encounter problems configuring
networking and emit errors on the console, you would not be able to
see this information unless you happened to be connected to the
console at the time the errors were generated.&lt;/p>
&lt;p>It would be nice to have both mechanisms available &amp;ndash; serial console
support for interactive access, and console logs for retroactive
debugging.&lt;/p></content></item><item><title>Cloud-init and the case of the changing hostname</title><link>https://blog.oddbit.com/post/2014-12-10-cloudinit-and-the-case-of-the-/</link><pubDate>Wed, 10 Dec 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-12-10-cloudinit-and-the-case-of-the-/</guid><description>Setting the stage I ran into a problem earlier this week deploying RDO Icehouse under RHEL 6. My target systems were a set of libvirt guests deployed from the RHEL 6 KVM guest image, which includes cloud-init in order to support automatic configuration in cloud environments. I take advantage of this when using libvirt by attaching a configuration drive so that I can pass in ssh keys and a user-data script.</description><content>&lt;h2 id="setting-the-stage">Setting the stage&lt;/h2>
&lt;p>I ran into a problem earlier this week deploying RDO Icehouse under
RHEL 6. My target systems were a set of libvirt guests deployed from
the RHEL 6 KVM guest image, which includes &lt;a href="https://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> in order to
support automatic configuration in cloud environments. I take
advantage of this when using &lt;code>libvirt&lt;/code> by attaching a configuration
drive so that I can pass in ssh keys and a &lt;code>user-data&lt;/code> script.&lt;/p>
&lt;p>Once the systems were up, I used &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a> to deploy OpenStack
onto a single controller and two compute nodes, and at the conclusion
of the &lt;code>packstack&lt;/code> run everything was functioning correctly. Running
&lt;code>neutron agent-list&lt;/code> showed all agents in good order:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+--------------------+------------+-------+----------------+
| id | agent_type | host | alive | admin_state_up |
+--------------------------------------+--------------------+------------+-------+----------------+
| 0d51d200-d902-4e05-847a-858b69c03088 | DHCP agent | controller | :-) | True |
| 192f76e9-a816-4bd9-8a90-a263a1d54031 | Open vSwitch agent | compute-0 | :-) | True |
| 3d97d7ba-1b1f-43f8-9582-f860fbfe50df | Open vSwitch agent | controller | :-) | True |
| 54d387a6-dca1-4ace-8c1b-7788fb0bc090 | Metadata agent | controller | :-) | True |
| 92fc83bf-0995-43c3-92d1-70002c734604 | L3 agent | controller | :-) | True |
| e06575c2-43b3-4691-80bc-454f501debfe | Open vSwitch agent | compute-1 | :-) | True |
+--------------------------------------+--------------------+------------+-------+----------------+
&lt;/code>&lt;/pre>
&lt;h2 id="a-problem-rears-its-ugly-head">A problem rears its ugly head&lt;/h2>
&lt;p>After rebooting the system, I found that I was missing an expected
Neutron router namespace. Specifically, given:&lt;/p>
&lt;pre>&lt;code># neutron router-list
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| id | name | external_gateway_info |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| e83eec10-0de2-4bfa-8e58-c1bcbe702f51 | router1 | {&amp;quot;network_id&amp;quot;: &amp;quot;b53a9ecd-01fc-4bee-b20d-8fbe0cd2e010&amp;quot;, &amp;quot;enable_snat&amp;quot;: true} |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>I expected to see:&lt;/p>
&lt;pre>&lt;code># ip netns
qrouter-e83eec10-0de2-4bfa-8e58-c1bcbe702f51
&lt;/code>&lt;/pre>
&lt;p>But the &lt;code>qrouter&lt;/code> namespace was missing.&lt;/p>
&lt;p>The output of &lt;code>neutron agent-list&lt;/code> shed some light on the problem:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+--------------------+------------------------+-------+----------------+
| id | agent_type | host | alive | admin_state_up |
+--------------------------------------+--------------------+------------------------+-------+----------------+
| 0832e8f3-61f9-49cf-b49c-886cc94d3d28 | Metadata agent | controller.localdomain | :-) | True |
| 0d51d200-d902-4e05-847a-858b69c03088 | DHCP agent | controller | xxx | True |
| 192f76e9-a816-4bd9-8a90-a263a1d54031 | Open vSwitch agent | compute-0 | :-) | True |
| 3be34828-ca8d-4638-9b3a-4e2f688a9ca9 | L3 agent | controller.localdomain | :-) | True |
| 3d97d7ba-1b1f-43f8-9582-f860fbfe50df | Open vSwitch agent | controller | xxx | True |
| 54d387a6-dca1-4ace-8c1b-7788fb0bc090 | Metadata agent | controller | xxx | True |
| 87b53741-f28b-4582-9ea8-6062ab9962e9 | Open vSwitch agent | controller.localdomain | :-) | True |
| 92fc83bf-0995-43c3-92d1-70002c734604 | L3 agent | controller | xxx | True |
| e06575c2-43b3-4691-80bc-454f501debfe | Open vSwitch agent | compute-1 | :-) | True |
| e327b7f9-c9ce-49f8-89c1-b699d9f7d253 | DHCP agent | controller.localdomain | :-) | True |
+--------------------------------------+--------------------+------------------------+-------+----------------+
&lt;/code>&lt;/pre>
&lt;p>There were two sets of Neutron agents registered using different
hostnames &amp;ndash; one set using the short name of the host, and the other
set using the fully qualified hostname.&lt;/p>
&lt;h2 id="whats-up-with-that">What&amp;rsquo;s up with that?&lt;/h2>
&lt;p>In the &lt;code>cc_set_hostname.py&lt;/code> module, &lt;code>cloud-init&lt;/code> performs the
following operation:&lt;/p>
&lt;pre>&lt;code>(hostname, fqdn) = util.get_hostname_fqdn(cfg, cloud)
try:
log.debug(&amp;quot;Setting the hostname to %s (%s)&amp;quot;, fqdn, hostname)
cloud.distro.set_hostname(hostname, fqdn)
except Exception:
util.logexc(log, &amp;quot;Failed to set the hostname to %s (%s)&amp;quot;, fqdn,
hostname)
raise
&lt;/code>&lt;/pre>
&lt;p>It starts by retrieving the hostname (both the qualified and
unqualified version) from the cloud environment, and then calls
&lt;code>cloud.distro.set_hostname(hostname, fqdn)&lt;/code>. This ends up calling:&lt;/p>
&lt;pre>&lt;code>def set_hostname(self, hostname, fqdn=None):
writeable_hostname = self._select_hostname(hostname, fqdn)
self._write_hostname(writeable_hostname, self.hostname_conf_fn)
self._apply_hostname(hostname)
&lt;/code>&lt;/pre>
&lt;p>Where, on a RHEL system, &lt;code>_select_hostname&lt;/code> is:&lt;/p>
&lt;pre>&lt;code>def _select_hostname(self, hostname, fqdn):
# See: http://bit.ly/TwitgL
# Should be fqdn if we can use it
if fqdn:
return fqdn
return hostname
&lt;/code>&lt;/pre>
&lt;p>So:&lt;/p>
&lt;ul>
&lt;li>&lt;code>cloud-init&lt;/code> sets &lt;code>writeable_hostname&lt;/code> to the fully qualified name
of the system (assuming it is available).&lt;/li>
&lt;li>&lt;code>cloud-init&lt;/code> writes the fully qualified hostname to &lt;code>/etc/sysconfig/network&lt;/code>.&lt;/li>
&lt;li>&lt;code>cloud-init&lt;/code> sets the hostname to the &lt;em>unqualified&lt;/em> hostname&lt;/li>
&lt;/ul>
&lt;p>The result is that your system will probably have a different hostname
after your first reboot, which throws off Neutron.&lt;/p>
&lt;h2 id="and-they-all-lived-happily-ever-after">And they all lived happily ever after?&lt;/h2>
&lt;p>It turns out this bug was reported upstream back in October of 2013 as
&lt;a href="https://bugs.launchpad.net/cloud-init/+bug/1246485">bug 1246485&lt;/a>, and while there are patches available the bug has
been marked as &amp;ldquo;low&amp;rdquo; priority and has been fixed. There are patches
attached to the bug report that purport to fix the problem.&lt;/p></content></item><item><title>Starting systemd services without blocking</title><link>https://blog.oddbit.com/post/2014-12-02-starting-systemd-services-with/</link><pubDate>Tue, 02 Dec 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-12-02-starting-systemd-services-with/</guid><description>Recently, I&amp;rsquo;ve been playing around with Fedora Atomic and Kubernetes. I ran into a frustrating problem in which I would attempt to start a service from within a script launched by cloud-init, only to have have systemctl block indefinitely because the service I was attempting to start was dependent on cloud-init finishing first.
It turns out that systemctl has a flag meant exactly for this situation:
--no-block Do not synchronously wait for the requested operation to finish.</description><content>&lt;p>Recently, I&amp;rsquo;ve been playing around with &lt;a href="https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/">Fedora Atomic and
Kubernetes&lt;/a>. I ran into a frustrating problem in which I
would attempt to start a service from within a script launched by
&lt;a href="http://cloudinit.readthedocs.org/">cloud-init&lt;/a>, only to have have &lt;code>systemctl&lt;/code> block indefinitely
because the service I was attempting to start was dependent on
&lt;code>cloud-init&lt;/code> finishing first.&lt;/p>
&lt;p>It turns out that &lt;code>systemctl&lt;/code> has a flag meant exactly for this
situation:&lt;/p>
&lt;pre>&lt;code> --no-block
Do not synchronously wait for the requested operation to finish. If
this is not specified, the job will be verified, enqueued and
systemctl will wait until it is completed. By passing this
argument, it is only verified and enqueued.
&lt;/code>&lt;/pre>
&lt;p>Replacing &lt;code>systemctl start &amp;lt;service&amp;gt;&lt;/code> with &lt;code>systemctl start --no-block &amp;lt;service&amp;gt;&lt;/code> has solved that particular problem.&lt;/p></content></item><item><title>Fedora Atomic, OpenStack, and Kubernetes (oh my)</title><link>https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/</link><pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/</guid><description>While experimenting with Fedora Atomic, I was looking for an elegant way to automatically deploy Atomic into an OpenStack environment and then automatically schedule some Docker containers on the Atomic host. This post describes my solution.
Like many other cloud-targeted distributions, Fedora Atomic runs cloud-init when the system boots. We can take advantage of this to configure the system at first boot by providing a user-data blob to Nova when we boot the instance.</description><content>&lt;p>While experimenting with &lt;a href="http://www.projectatomic.io/">Fedora Atomic&lt;/a>, I was looking for an
elegant way to automatically deploy Atomic into an &lt;a href="http://openstack.org/">OpenStack&lt;/a>
environment and then automatically schedule some &lt;a href="http://docker.com/">Docker&lt;/a> containers
on the Atomic host. This post describes my solution.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Like many other cloud-targeted distributions, Fedora Atomic runs
&lt;a href="http://cloudinit.readthedocs.org/">cloud-init&lt;/a> when the system boots. We can take advantage of this
to configure the system at first boot by providing a &lt;code>user-data&lt;/code> blob
to Nova when we boot the instance. A &lt;code>user-data&lt;/code> blob can be as
simple as a shell script, and while we could arguably mash everything
into a single script it wouldn&amp;rsquo;t be particularly maintainable or
flexible in the face of different pod/service/etc descriptions.&lt;/p>
&lt;p>In order to build a more flexible solution, we&amp;rsquo;re going to take
advantage of the following features:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Support for &lt;a href="http://cloudinit.readthedocs.org/en/latest/topics/format.html#mime-multi-part-archive">multipart MIME archives&lt;/a>.&lt;/p>
&lt;p>Cloud-init allows you to pass in multiple files via &lt;code>user-data&lt;/code> by
encoding them as a multipart MIME archive.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Support for a &lt;a href="http://cloudinit.readthedocs.org/en/latest/topics/format.html#part-handler">custom part handler&lt;/a>.&lt;/p>
&lt;p>Cloud-init recognizes a number of specific MIME types (such as
&lt;code>text/cloud-config&lt;/code> or &lt;code>text/x-shellscript&lt;/code>). We can provide a
custom part handler that will be used to handle MIME types not
intrinsincally supported by &lt;code>cloud-init&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="a-custom-part-handler-for-kubernetes-configurations">A custom part handler for Kubernetes configurations&lt;/h2>
&lt;p>I have written a &lt;a href="https://github.com/larsks/atomic-kubernetes-tools/blob/master/kube-part-handler.py">custom part handler&lt;/a> that knows
about the following MIME types:&lt;/p>
&lt;ul>
&lt;li>&lt;code>text/x-kube-pod&lt;/code>&lt;/li>
&lt;li>&lt;code>text/x-kube-service&lt;/code>&lt;/li>
&lt;li>&lt;code>text/x-kube-replica&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>When the part handler is first initialized it will ensure the
Kubernetes is started. If it is provided with a document matching one
of the above MIME types, it will pass it to the appropriate &lt;code>kubecfg&lt;/code>
command to create the objects in Kubernetes.&lt;/p>
&lt;h2 id="creating-multipart-mime-archives">Creating multipart MIME archives&lt;/h2>
&lt;p>I have also created a &lt;a href="https://github.com/larsks/atomic-kubernetes-tools/blob/master/write-mime-multipart.py">modified version&lt;/a> of the standard
&lt;code>write-multipart-mime.py&lt;/code> Python script. This script will inspect the
first lines of files to determine their content type; in addition to
the standard &lt;code>cloud-init&lt;/code> types (like &lt;code>#cloud-config&lt;/code> for a
&lt;code>text/cloud-config&lt;/code> type file), this script recognizes:&lt;/p>
&lt;ul>
&lt;li>&lt;code>#kube-pod&lt;/code> for &lt;code>text/x-kube-pod&lt;/code>&lt;/li>
&lt;li>&lt;code>#kube-service&lt;/code> for &lt;code>text/x-kube-service&lt;/code>&lt;/li>
&lt;li>&lt;code>#kube-replica&lt;/code> for &lt;code>text/x-kube-replca&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>That is, a simple pod description might look something like:&lt;/p>
&lt;pre>&lt;code>#kube-pod
id: dbserver
desiredState:
manifest:
version: v1beta1
id: dbserver
containers:
- image: mysql
name: dbserver
env:
- name: MYSQL_ROOT_PASSWORD
value: secret
&lt;/code>&lt;/pre>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>Assuming that the pod description presented in the previous section is
stored in a file named &lt;code>dbserver.yaml&lt;/code>, we can bundle that file up
with our custom part handler like this:&lt;/p>
&lt;pre>&lt;code>$ write-mime-multipart.py \
kube-part-handler.py dbserver.yaml &amp;gt; userdata
&lt;/code>&lt;/pre>
&lt;p>We would then launch a Nova instance using the &lt;code>nova boot&lt;/code> command,
providing the generated &lt;code>userdata&lt;/code> file as an argument to the
&lt;code>user-data&lt;/code> command:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image fedora-atomic --key-name mykey \
--flavor m1.small --user-data userdata my-atomic-server
&lt;/code>&lt;/pre>
&lt;p>You would obviously need to substitute values for &lt;code>--image&lt;/code> and
&lt;code>--key-name&lt;/code> that are appropriate for your environment.&lt;/p>
&lt;h2 id="details-details">Details, details&lt;/h2>
&lt;p>If you are experimenting with Fedora Atomic 21, you may find out that
the above example doesn&amp;rsquo;t work &amp;ndash; the official &lt;code>mysql&lt;/code> image generates
an selinux error. We can switch selinux to permissive mode by putting
the following into a file called &lt;code>disable-selinux.sh&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
setenforce 0
sed -i '/^SELINUX=/ s/=.*/=permissive/' /etc/selinux/config
&lt;/code>&lt;/pre>
&lt;p>And then including that in our MIME archive:&lt;/p>
&lt;pre>&lt;code>$ write-mime-multipart.py \
kube-part-handler.py disable-selinux.sh dbserver.yaml &amp;gt; userdata
&lt;/code>&lt;/pre>
&lt;h2 id="a-brief-demonstration">A brief demonstration&lt;/h2>
&lt;p>If we launch an instance as described in the previous section and then
log in, we should find that the pod has already been scheduled:&lt;/p>
&lt;pre>&lt;code># kubecfg list pods
ID Image(s) Host Labels Status
---------- ---------- ---------- ---------- ----------
dbserver mysql / Waiting
&lt;/code>&lt;/pre>
&lt;p>At this point, &lt;code>docker&lt;/code> needs to pull the &lt;code>mysql&lt;/code> image locally, so
this step can take a bit depending on the state of your local internet
connection.&lt;/p>
&lt;p>Running &lt;code>docker ps&lt;/code> at this point will yield:&lt;/p>
&lt;pre>&lt;code># docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
3561e39f198c kubernetes/pause:latest &amp;quot;/pause&amp;quot; 46 seconds ago Up 43 seconds k8s--net.d96a64a9--dbserver.etcd--3d30eac0_-_745c_-_11e4_-_b32a_-_fa163e6e92ce--d872be51
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>pause&lt;/code> image here is a Kubernetes detail that is used to
configure the networking for a pod (in the Kubernetes world, a pod is
a group of linked containers that share a common network namespace).&lt;/p>
&lt;p>After a few minutes, you should eventually see:&lt;/p>
&lt;pre>&lt;code># docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
644c8fc5a79c mysql:latest &amp;quot;/entrypoint.sh mysq 3 minutes ago Up 3 minutes k8s--dbserver.fd48803d--dbserver.etcd--3d30eac0_-_745c_-_11e4_-_b32a_-_fa163e6e92ce--58794467
3561e39f198c kubernetes/pause:latest &amp;quot;/pause&amp;quot; 5 minutes ago Up 5 minutes k8s--net.d96a64a9--dbserver.etcd--3d30eac0_-_745c_-_11e4_-_b32a_-_fa163e6e92ce--d872be51
&lt;/code>&lt;/pre>
&lt;p>And &lt;code>kubecfg&lt;/code> should show the pod as running:&lt;/p>
&lt;pre>&lt;code># kubecfg list pods
ID Image(s) Host Labels Status
---------- ---------- ---------- ---------- ----------
dbserver mysql 127.0.0.1/ Running
&lt;/code>&lt;/pre>
&lt;h2 id="problems-problems">Problems, problems&lt;/h2>
&lt;p>This works and is I think a relatively elegant solution. However,
there are some drawbacks. In particular, the custom part handler
runs fairly early in the &lt;code>cloud-init&lt;/code> process, which means that it
cannot depend on changes implemented by &lt;code>user-data&lt;/code> scripts (because
these run much later).&lt;/p>
&lt;p>A better solution might be to have the custom part handler simply
write the Kubernetes configs into a directory somewhere, and then
install a service that launches after Kubernetes and (a) watches that
directory for files, then (b) passes the configuration to Kubernetes
and deletes (or relocates) the file.&lt;/p></content></item><item><title>Creating a Windows image for OpenStack</title><link>https://blog.oddbit.com/post/2014-11-15-creating-a-windows-image-for-o/</link><pubDate>Sat, 15 Nov 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-11-15-creating-a-windows-image-for-o/</guid><description>If you want to build a Windows image for use in your OpenStack environment, you can follow the example in the official documentation, or you can grab a Windows 2012r2 evaluation pre-built image from the nice folks at CloudBase.
The CloudBase-provided image is built using a set of scripts and configuration files that CloudBase has made available on GitHub.
The CloudBase repository is an excellent source of information, but I wanted to understand the process myself.</description><content>&lt;p>If you want to build a Windows image for use in your OpenStack
environment, you can follow &lt;a href="http://docs.openstack.org/image-guide/content/windows-image.html">the example in the official
documentation&lt;/a>, or you can grab a Windows 2012r2
evaluation &lt;a href="http://www.cloudbase.it/ws2012r2/">pre-built image&lt;/a> from the nice folks at &lt;a href="http://www.cloudbase.it/">CloudBase&lt;/a>.&lt;/p>
&lt;p>The CloudBase-provided image is built using a set of scripts and
configuration files that CloudBase has &lt;a href="https://github.com/cloudbase/windows-openstack-imaging-tools/">made available on
GitHub&lt;/a>.&lt;/p>
&lt;p>The CloudBase repository is an excellent source of information, but I
wanted to understand the process myself. This post describes the
process I went through to establish an automated process for
generating a Windows image suitable for use with OpenStack.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="unattended-windows-installs">Unattended windows installs&lt;/h2>
&lt;p>The Windows installer supports &lt;a href="http://technet.microsoft.com/en-us/library/ff699026.aspx">fully automated installations&lt;/a> through
the use of an answer file, or &amp;ldquo;unattend&amp;rdquo; file, that provides
information to the installer that would otherwise be provided
manually. The installer will look in &lt;a href="http://technet.microsoft.com/en-us/library/cc749415%28v=ws.10%29.aspx">a number of places&lt;/a> to find
this file. For our purposes, the important fact is that the installer
will look for a file named &lt;code>autounattend.xml&lt;/code> in the root of all
available read/write or read-only media. We&amp;rsquo;ll take advantage of this
by creating a file &lt;code>config/autounattend.xml&lt;/code>, and then generating an
ISO image like this:&lt;/p>
&lt;pre>&lt;code>mkisofs -J -r -o config.iso config
&lt;/code>&lt;/pre>
&lt;p>And we&amp;rsquo;ll attach this ISO to a vm later on in order to provide the
answer file to the installer.&lt;/p>
&lt;p>So, what goes into this answer file?&lt;/p>
&lt;p>The answer file is an XML document enclosed in an
&lt;code>&amp;lt;unattend&amp;gt;..&amp;lt;/unattend&amp;gt;&lt;/code> element. In order to provide all the
expected XML namespaces that may be used in the document, you would
typically start with something like this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version=&amp;quot;1.0&amp;quot; ?&amp;gt;
&amp;lt;unattend
xmlns=&amp;quot;urn:schemas-microsoft-com:unattend&amp;quot;
xmlns:ms=&amp;quot;urn:schemas-microsoft-com:asm.v3&amp;quot;
xmlns:wcm=&amp;quot;http://schemas.microsoft.com/WMIConfig/2002/State&amp;quot;&amp;gt;
&amp;lt;!-- your content goes here --&amp;gt;
&amp;lt;/unattend&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Inside this &lt;code>&amp;lt;unattend&amp;gt;&lt;/code> element you will put one or more &lt;code>&amp;lt;settings&amp;gt;&lt;/code>
elements, corresponding to the different &lt;a href="http://technet.microsoft.com/en-us/library/cc766245%28v=ws.10%29.aspx">configuration passes&lt;/a> of the
installer:&lt;/p>
&lt;pre>&lt;code>&amp;lt;settings pass=&amp;quot;specialize&amp;quot;&amp;gt;
&amp;lt;/settings&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The available configuration passes are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc749062%28v=ws.10%29.aspx">auditSystem&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc722343%28v=ws.10%29.aspx">auditUser&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc766229%28v=ws.10%29.aspx">generalize&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc749001%28v=ws.10%29.aspx">offlineServicing&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc748990%28v=ws.10%29.aspx">oobeSystem&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc722130%28v=ws.10%29.aspx">specialize&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc766028%28v=ws.10%29.aspx">windowsPE&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Of these, the most interesting for our use will be:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>windowsPE&lt;/code> &amp;ndash; used to install device drivers for use within the
installer environment. We will use this to install the VirtIO
drivers necessary to make VirtIO devices visible to the Windows
installer.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>specialize&lt;/code> &amp;ndash; In this pass, the installer applies machine-specific
configuration. This is typically used to configure networking,
locale settings, and most other things.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>oobeSystem&lt;/code> &amp;ndash; In this pass, the installer configures things that
happen at first boot. We use this to step to install some
additional software and run &lt;a href="http://technet.microsoft.com/en-us/library/cc721940%28v=ws.10%29.aspx">sysprep&lt;/a> in order to prepare the
image for use in OpenStack.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Inside each &lt;code>&amp;lt;settings&amp;gt;&lt;/code> element we will place one or more
&lt;code>&amp;lt;component&amp;gt;&lt;/code> elements that will apply specific pieces of
configuration. For example, the following &lt;code>&amp;lt;component&amp;gt;&lt;/code> configures
language and keyboard settings in the installer:&lt;/p>
&lt;pre>&lt;code>&amp;lt;settings pass=&amp;quot;windowsPE&amp;quot;&amp;gt;
&amp;lt;component name=&amp;quot;Microsoft-Windows-International-Core-WinPE&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;SetupUILanguage&amp;gt;
&amp;lt;UILanguage&amp;gt;en-US&amp;lt;/UILanguage&amp;gt;
&amp;lt;/SetupUILanguage&amp;gt;
&amp;lt;InputLocale&amp;gt;en-US&amp;lt;/InputLocale&amp;gt;
&amp;lt;UILanguage&amp;gt;en-US&amp;lt;/UILanguage&amp;gt;
&amp;lt;SystemLocale&amp;gt;en-US&amp;lt;/SystemLocale&amp;gt;
&amp;lt;UserLocale&amp;gt;en-US&amp;lt;/UserLocale&amp;gt;
&amp;lt;/component&amp;gt;
&amp;lt;/settings&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>&lt;a href="http://technet.microsoft.com/">Technet&lt;/a> provides documentation on the &lt;a href="http://technet.microsoft.com/en-us/library/ff699038.aspx">available components&lt;/a>.&lt;/p>
&lt;h2 id="cloud-init-for-windows">Cloud-init for Windows&lt;/h2>
&lt;p>&lt;a href="http://cloudinit.readthedocs.org/en/latest/">Cloud-init&lt;/a> is a tool that will configure a virtual instance when
it first boots, using metadata provided by the cloud service provider.
For example, when booting a Linux instance under OpenStack,
&lt;code>cloud-init&lt;/code> will contact the OpenStack metadata service at
http://169.254.169.254/ in order to retrieve things like the system
hostname, SSH keys, and so forth.&lt;/p>
&lt;p>While &lt;code>cloud-init&lt;/code> has support for Linux and BSD, it does not support
Windows. The folks at &lt;a href="http://www.cloudbase.it/">Cloudbase&lt;/a> have produced &lt;a href="http://www.cloudbase.it/cloud-init-for-windows-instances/">cloudbase-init&lt;/a>
in order to fill this gap. Once installed, the &lt;code>cloudbase-init&lt;/code> tool
will, upon first booting a system:&lt;/p>
&lt;ul>
&lt;li>Configure the network using information provided in the cloud
metadata&lt;/li>
&lt;li>Set the system hostname&lt;/li>
&lt;li>Create an initial user account (by default &amp;ldquo;Admin&amp;rdquo;) with a randomly
generated password (see below for details)&lt;/li>
&lt;li>Install your public key, if provided&lt;/li>
&lt;li>Execute a script provided via cloud &lt;code>user-data&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="passwords-and-ssh-keys">Passwords and ssh keys&lt;/h3>
&lt;p>While &lt;code>cloudbase-init&lt;/code> will install your SSH public key (by default
into &lt;code>/Users/admin/.ssh/authorized_keys&lt;/code>), Windows does not ship with
an SSH server and cloudbase-init does not install one. So what is it
doing with the public key?&lt;/p>
&lt;p>While you could arrange to install an ssh server that would make use
of the key, &lt;code>cloudbase-init&lt;/code> uses it for a completely unrelated
purpose: encrypting the randomly generated password. This encrypted
password is then passed back to OpenStack, where you can retrieve it
using the &lt;code>nova get-password&lt;/code> command, and decrypt it using the
corresponding SSH private key.&lt;/p>
&lt;p>Running &lt;code>nova get-password myinstance&lt;/code> will return something like:&lt;/p>
&lt;pre>&lt;code>w+In/P6+FeE8nv45oCjc5/Bohq4adqzoycwb9hOy9dlmuYbz0hiV923WW0fL
7hvQcZnWqGY7xLNnbJAeRFiSwv/MWvF3Sq8T0/IWhi6wBhAiVOxM95yjwIit
/L1Fm0TBARjoBuo+xq44YHpep1qzh4frsOo7TxvMHCOtibKTaLyCsioHjRaQ
dHk+uVFM1E0VIXyiqCdj421JoJzg32DqqeQTJJMqT9JiOL3FT26Y4XkVyJvI
vtUCQteIbd4jFtv3wEErJZKHgxHTLEYK+h67nTA4rXpvYVyKw9F8Qwj7JBTj
UJqp1syEqTR5/DUHYS+NoSdONUa+K7hhtSSs0bS1ghQuAdx2ifIA7XQ5eMRS
sXC4JH3d+wwtq4OmYYSOQkjmpKD8s5d4TgtG2dK8/l9B/1HTXa6qqcOw9va7
oUGGws3XuFEVq9DYmQ5NF54N7FU7NVl9UuRW3WTf4Q3q8VwJ4tDrmFSct6oG
2liJ8s7ybbW5PQU/lJe0gGBGGFzo8c+Rur17nsZ01+309JPEUKqUQT/uEg55
ziOo8uAwPvInvPkbxjH5doH79t47Erb3cK44kuqZy7J0RdDPtPr2Jel4NaSt
oCs+P26QF2NVOugsY9O/ugYfZWoEMUZuiwNWCWBqrIohB8JHcItIBQKBdCeY
7ORjotJU+4qAhADgfbkTqwo=
&lt;/code>&lt;/pre>
&lt;p>Providing your secret key as an additional parameter will decrypt the
password:&lt;/p>
&lt;pre>&lt;code>$ nova get-password myinstance ~/.ssh/id_rsa
fjgJmUB7fXF6wo
&lt;/code>&lt;/pre>
&lt;p>With an appropriately configured image, you could connect using an RDP
client and log in as the &amp;ldquo;Admin&amp;rdquo; user using that password.&lt;/p>
&lt;h3 id="passwords-without-ssh-keys">Passwords without ssh keys&lt;/h3>
&lt;p>If you do not provide your instance with an SSH key you will not be
able to retrieve the randomly generated password. However, if you can
get console access to your instance (e.g., via the Horizon dashboard),
you can log in as the &amp;ldquo;Administrator&amp;rdquo; user, at which point you will be
prompted to set an initial password for that account.&lt;/p>
&lt;h3 id="logging">Logging&lt;/h3>
&lt;p>You can find logs for &lt;code>cloudbase-init&lt;/code> in &lt;code>c:\program files (x86)\cloudbase solutions\cloudbase-init\log\cloudbase-init.log&lt;/code>.&lt;/p>
&lt;p>If appropriately configured, &lt;code>cloudbase-init&lt;/code> will also log to the
virtual serial port. This log is available in OpenStack by running
&lt;code>nova console-log &amp;lt;instance&amp;gt;&lt;/code>. For example:&lt;/p>
&lt;pre>&lt;code>$ nova console-log my-windows-server
2014-11-19 04:10:45.887 1272 INFO cloudbaseinit.init [-] Metadata service loaded: 'HttpService'
2014-11-19 04:10:46.339 1272 INFO cloudbaseinit.init [-] Executing plugin 'MTUPlugin'
2014-11-19 04:10:46.371 1272 INFO cloudbaseinit.init [-] Executing plugin 'NTPClientPlugin'
2014-11-19 04:10:46.387 1272 INFO cloudbaseinit.init [-] Executing plugin 'SetHostNamePlugin'
.
.
.
&lt;/code>&lt;/pre>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>I have an &lt;a href="https://github.com/larsks/windows-openstack-image/blob/master/install">install script&lt;/a> that drives the process, but it&amp;rsquo;s
ultimately just a wrapper for &lt;code>virt-install&lt;/code> and results in the
following invocation:&lt;/p>
&lt;pre>&lt;code>exec virt-install -n ws2012 -r 2048 \
-w network=default,model=virtio \
--disk path=$TARGET_IMAGE,bus=virtio \
--cdrom $WINDOWS_IMAGE \
--disk path=$VIRTIO_IMAGE,device=cdrom \
--disk path=$CONFIG_IMAGE,device=cdrom \
--os-type windows \
--os-variant win2k8 \
--vnc \
--console pty
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>TARGET_IMAGE&lt;/code> is the name of a pre-existing &lt;code>qcow2&lt;/code> image onto
which we will install Windows, &lt;code>WINDOWS_IMAGE&lt;/code> is the path to an ISO
containing Windows Server 2012r2, &lt;code>VIRTIO_IMAGE&lt;/code> is the path to an ISO
containing VirtIO drivers for Windows (available from the &lt;a href="https://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/bin/">Fedora
project&lt;/a>), and &lt;code>CONFIG_IMAGE&lt;/code> is a path to the ISO containing our
&lt;code>autounattend.xml&lt;/code> file.&lt;/p>
&lt;p>The fully commented &lt;a href="https://github.com/larsks/windows-openstack-image/blob/master/config/autounattend.xml">autounattend.xml&lt;/a> file, along with the script
mentioned above, are available in my &lt;a href="https://github.com/larsks/windows-openstack-image/">windows-openstack-image&lt;/a>
repository on GitHub.&lt;/p>
&lt;h2 id="the-answer-file-in-detail">The answer file in detail&lt;/h2>
&lt;h3 id="windowspe">windowsPE&lt;/h3>
&lt;p>In the &lt;a href="http://technet.microsoft.com/en-us/library/cc766028%28v=ws.10%29.aspx">windowsPE&lt;/a> phase, we start by configuring the installer locale
settings:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-International-Core-WinPE&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;SetupUILanguage&amp;gt;
&amp;lt;UILanguage&amp;gt;en-US&amp;lt;/UILanguage&amp;gt;
&amp;lt;/SetupUILanguage&amp;gt;
&amp;lt;InputLocale&amp;gt;en-US&amp;lt;/InputLocale&amp;gt;
&amp;lt;UILanguage&amp;gt;en-US&amp;lt;/UILanguage&amp;gt;
&amp;lt;SystemLocale&amp;gt;en-US&amp;lt;/SystemLocale&amp;gt;
&amp;lt;UserLocale&amp;gt;en-US&amp;lt;/UserLocale&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And installing the VirtIO drviers using the &lt;a href="http://technet.microsoft.com/en-us/library/ff715623.aspx">Microsoft-Windows-PnpCustomizationsWinPE&lt;/a> component:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-PnpCustomizationsWinPE&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot; language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot; processorArchitecture=&amp;quot;amd64&amp;quot;&amp;gt;
&amp;lt;DriverPaths&amp;gt;
&amp;lt;PathAndCredentials wcm:action=&amp;quot;add&amp;quot; wcm:keyValue=&amp;quot;1&amp;quot;&amp;gt;
&amp;lt;Path&amp;gt;d:\win8\amd64&amp;lt;/Path&amp;gt;
&amp;lt;/PathAndCredentials&amp;gt;
&amp;lt;/DriverPaths&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>This assumes that the VirtIO image is mounted as drive &lt;code>d:&lt;/code>.&lt;/p>
&lt;p>With the drivers installed, we can then call the
&lt;a href="http://technet.microsoft.com/en-us/library/ff715827.aspx">Microsoft-Windows-Setup&lt;/a> component to configure the disks and
install Windows. We start by configuring the product key:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-Setup&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;&amp;gt;
&amp;lt;UserData&amp;gt;
&amp;lt;AcceptEula&amp;gt;true&amp;lt;/AcceptEula&amp;gt;
&amp;lt;ProductKey&amp;gt;
&amp;lt;WillShowUI&amp;gt;OnError&amp;lt;/WillShowUI&amp;gt;
&amp;lt;Key&amp;gt;INSERT-PRODUCT-KEY-HERE&amp;lt;/Key&amp;gt;
&amp;lt;/ProductKey&amp;gt;
&amp;lt;/UserData&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And then configure the disk with a single partition (that will grow to
fill all the available space) which we then format with NTFS:&lt;/p>
&lt;pre>&lt;code> &amp;lt;DiskConfiguration&amp;gt;
&amp;lt;WillShowUI&amp;gt;OnError&amp;lt;/WillShowUI&amp;gt;
&amp;lt;Disk wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;DiskID&amp;gt;0&amp;lt;/DiskID&amp;gt;
&amp;lt;WillWipeDisk&amp;gt;true&amp;lt;/WillWipeDisk&amp;gt;
&amp;lt;CreatePartitions&amp;gt;
&amp;lt;CreatePartition wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;Order&amp;gt;1&amp;lt;/Order&amp;gt;
&amp;lt;Extend&amp;gt;true&amp;lt;/Extend&amp;gt;
&amp;lt;Type&amp;gt;Primary&amp;lt;/Type&amp;gt;
&amp;lt;/CreatePartition&amp;gt;
&amp;lt;/CreatePartitions&amp;gt;
&amp;lt;ModifyPartitions&amp;gt;
&amp;lt;ModifyPartition wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;Format&amp;gt;NTFS&amp;lt;/Format&amp;gt;
&amp;lt;Order&amp;gt;1&amp;lt;/Order&amp;gt;
&amp;lt;PartitionID&amp;gt;1&amp;lt;/PartitionID&amp;gt;
&amp;lt;Label&amp;gt;System&amp;lt;/Label&amp;gt;
&amp;lt;/ModifyPartition&amp;gt;
&amp;lt;/ModifyPartitions&amp;gt;
&amp;lt;/Disk&amp;gt;
&amp;lt;/DiskConfiguration&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We provide information about what to install:&lt;/p>
&lt;pre>&lt;code> &amp;lt;ImageInstall&amp;gt;
&amp;lt;OSImage&amp;gt;
&amp;lt;WillShowUI&amp;gt;Never&amp;lt;/WillShowUI&amp;gt;
&amp;lt;InstallFrom&amp;gt;
&amp;lt;MetaData&amp;gt;
&amp;lt;Key&amp;gt;/IMAGE/Name&amp;lt;/Key&amp;gt;
&amp;lt;Value&amp;gt;Windows Server 2012 R2 SERVERSTANDARDCORE&amp;lt;/Value&amp;gt;
&amp;lt;/MetaData&amp;gt;
&amp;lt;/InstallFrom&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And where we would like it installed:&lt;/p>
&lt;pre>&lt;code> &amp;lt;InstallTo&amp;gt;
&amp;lt;DiskID&amp;gt;0&amp;lt;/DiskID&amp;gt;
&amp;lt;PartitionID&amp;gt;1&amp;lt;/PartitionID&amp;gt;
&amp;lt;/InstallTo&amp;gt;
&amp;lt;/OSImage&amp;gt;
&amp;lt;/ImageInstall&amp;gt;
&lt;/code>&lt;/pre>
&lt;h3 id="specialize">specialize&lt;/h3>
&lt;p>In the &lt;a href="http://technet.microsoft.com/en-us/library/cc722130%28v=ws.10%29.aspx">specialize&lt;/a> phase, we start by setting the system name to a
randomly generated value using the &lt;a href="http://technet.microsoft.com/en-us/library/ff715801.aspx">Microsoft-Windows-Shell-Setup&lt;/a>
component:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-Shell-Setup&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot; language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot; processorArchitecture=&amp;quot;amd64&amp;quot;&amp;gt;
&amp;lt;ComputerName&amp;gt;*&amp;lt;/ComputerName&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We enable remote desktop because in an OpenStack environment this will
probably be the preferred mechanism with which to connect to the host
(but see &lt;a href="http://www.cloudbase.it/windows-without-passwords-in-openstack/">this document&lt;/a> for an alternative mechanism).&lt;/p>
&lt;p>First, we need to permit terminal server connections:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-TerminalServices-LocalSessionManager&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;fDenyTSConnections&amp;gt;false&amp;lt;/fDenyTSConnections&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And we do not want to require network-level authentication prior to
connecting:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-TerminalServices-RDP-WinStationExtensions&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;UserAuthentication&amp;gt;0&amp;lt;/UserAuthentication&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We will also need to open the necessary firewall group:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Networking-MPSSVC-Svc&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;FirewallGroups&amp;gt;
&amp;lt;FirewallGroup wcm:action=&amp;quot;add&amp;quot; wcm:keyValue=&amp;quot;RemoteDesktop&amp;quot;&amp;gt;
&amp;lt;Active&amp;gt;true&amp;lt;/Active&amp;gt;
&amp;lt;Profile&amp;gt;all&amp;lt;/Profile&amp;gt;
&amp;lt;Group&amp;gt;@FirewallAPI.dll,-28752&amp;lt;/Group&amp;gt;
&amp;lt;/FirewallGroup&amp;gt;
&amp;lt;/FirewallGroups&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Finally, we use the &lt;a href="http://technet.microsoft.com/en-us/library/ff716283.aspx">Microsoft-Windows-Deployment&lt;/a> component to configure the Windows firewall to permit ICMP traffic:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-Deployment&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot; versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;RunSynchronous&amp;gt;
&amp;lt;RunSynchronousCommand wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;Order&amp;gt;3&amp;lt;/Order&amp;gt;
&amp;lt;Path&amp;gt;netsh advfirewall firewall add rule name=ICMP protocol=icmpv4 dir=in action=allow&amp;lt;/Path&amp;gt;
&amp;lt;/RunSynchronousCommand&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And to download the &lt;code>cloudbase-init&lt;/code> installer and make it available
for later steps:&lt;/p>
&lt;pre>&lt;code> &amp;lt;RunSynchronousCommand wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;Order&amp;gt;5&amp;lt;/Order&amp;gt;
&amp;lt;Path&amp;gt;powershell -NoLogo -Command &amp;quot;(new-object System.Net.WebClient).DownloadFile('https://www.cloudbase.it/downloads/CloudbaseInitSetup_Beta_x64.msi', 'c:\Windows\Temp\cloudbase.msi')&amp;quot;&amp;lt;/Path&amp;gt;
&amp;lt;/RunSynchronousCommand&amp;gt;
&amp;lt;/RunSynchronous&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re using &lt;a href="http://technet.microsoft.com/en-us/scriptcenter/powershell.aspx">Powershell&lt;/a> here because it has convenient methods
available for downloading URLs to local files. This is roughly
equivalent to using &lt;code>curl&lt;/code> on a Linux system.&lt;/p>
&lt;h3 id="oobesystem">oobeSystem&lt;/h3>
&lt;p>In the &lt;a href="http://technet.microsoft.com/en-us/library/cc748990%28v=ws.10%29.aspx">oobeSystem&lt;/a> phase, we configure an automatic login for the
Administrator user:&lt;/p>
&lt;pre>&lt;code> &amp;lt;UserAccounts&amp;gt;
&amp;lt;AdministratorPassword&amp;gt;
&amp;lt;Value&amp;gt;Passw0rd&amp;lt;/Value&amp;gt;
&amp;lt;PlainText&amp;gt;true&amp;lt;/PlainText&amp;gt;
&amp;lt;/AdministratorPassword&amp;gt;
&amp;lt;/UserAccounts&amp;gt;
&amp;lt;AutoLogon&amp;gt;
&amp;lt;Password&amp;gt;
&amp;lt;Value&amp;gt;Passw0rd&amp;lt;/Value&amp;gt;
&amp;lt;PlainText&amp;gt;true&amp;lt;/PlainText&amp;gt;
&amp;lt;/Password&amp;gt;
&amp;lt;Enabled&amp;gt;true&amp;lt;/Enabled&amp;gt;
&amp;lt;LogonCount&amp;gt;50&amp;lt;/LogonCount&amp;gt;
&amp;lt;Username&amp;gt;Administrator&amp;lt;/Username&amp;gt;
&amp;lt;/AutoLogon&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>This automatic login only happens once, because we configure
&lt;code>FirstLogonCommands&lt;/code> that will first install &lt;code>cloudbase-init&lt;/code>:&lt;/p>
&lt;pre>&lt;code> &amp;lt;FirstLogonCommands&amp;gt;
&amp;lt;SynchronousCommand wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;CommandLine&amp;gt;msiexec /i c:\windows\temp\cloudbase.msi /qb /l*v c:\windows\temp\cloudbase.log LOGGINGSERIALPORTNAME=COM1&amp;lt;/CommandLine&amp;gt;
&amp;lt;Order&amp;gt;1&amp;lt;/Order&amp;gt;
&amp;lt;/SynchronousCommand&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And will then run &lt;code>sysprep&lt;/code> to generalize the system (which will,
among other things, lose the administrator password):&lt;/p>
&lt;pre>&lt;code> &amp;lt;SynchronousCommand wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;CommandLine&amp;gt;c:\windows\system32\sysprep\sysprep /generalize /oobe /shutdown&amp;lt;/CommandLine&amp;gt;
&amp;lt;Order&amp;gt;2&amp;lt;/Order&amp;gt;
&amp;lt;/SynchronousCommand&amp;gt;
&amp;lt;/FirstLogonCommands&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The system will shut down when &lt;code>sysprep&lt;/code> is complete, leaving you with a
Windows image suitable for uploading into OpenStack:&lt;/p>
&lt;pre>&lt;code>glance image-create --name ws2012 \
--disk-format qcow2 \
--container-format bare \
--file ws2012.qcow2
&lt;/code>&lt;/pre>
&lt;h2 id="troubleshooting">Troubleshooting&lt;/h2>
&lt;p>If you run into problems with an unattended Windows installation:&lt;/p>
&lt;p>During the first stage of the installer, you can look in the
&lt;code>x:\windows\panther&lt;/code> directory for &lt;code>setupact.log&lt;/code> and &lt;code>setuperr.log&lt;/code>,
which will have information about the early install process. The &lt;code>x:&lt;/code>
drive is temporary, and files here will be discarded when the system
reboots.&lt;/p>
&lt;p>Subsequent installer stages will log to
&lt;code>c:\windows\panther\&lt;/code>.&lt;/p>
&lt;p>If you are unfamiliar with Windows, the &lt;code>type&lt;/code> command can be used
very much like the &lt;code>cat&lt;/code> command on Linux, and the &lt;code>more&lt;/code> command
provides paging as you would expect. The &lt;code>notepad&lt;/code> command will open
a GUI text editor/viewer.&lt;/p>
&lt;p>You can emulate the &lt;code>tail&lt;/code> command using &lt;code>powershell&lt;/code>; to see the last
10 lines of a file:&lt;/p>
&lt;pre>&lt;code>C:\&amp;gt; powershell -command &amp;quot;Get-Content setupact.log -Tail 10&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Technet has a &lt;a href="http://technet.microsoft.com/en-us/library/hh825073.aspx">Deployment Troubleshooting and Log Files&lt;/a>
document that discusses in more detail what is logged and where to
find it.&lt;/p></content></item><item><title>Building Docker images with Puppet</title><link>https://blog.oddbit.com/post/2014-10-22-building-docker-images-with-pu/</link><pubDate>Wed, 22 Oct 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-10-22-building-docker-images-with-pu/</guid><description>I like Docker, but I&amp;rsquo;m not a huge fan of using shell scripts for complex system configuration&amp;hellip;and Dockerfiles are basically giant shell scripts.
I was curious whether or not it would be possible to use Puppet during the docker build process. As a test case, I used the ssh module included in the openstack-puppet-modules package.
I started with a manifest like this (in puppet/node.pp):
class { 'ssh': } And a Dockerfile like this:</description><content>&lt;p>I like &lt;a href="http://docker.com/">Docker&lt;/a>, but I&amp;rsquo;m not a huge fan of using shell scripts for
complex system configuration&amp;hellip;and Dockerfiles are basically giant
shell scripts.&lt;/p>
&lt;p>I was curious whether or not it would be possible to use Puppet during
the &lt;code>docker build&lt;/code> process. As a test case, I used the
&lt;a href="https://github.com/saz/puppet-ssh">ssh&lt;/a> module included in the openstack-puppet-modules package.&lt;/p>
&lt;p>I started with a manifest like this (in &lt;code>puppet/node.pp&lt;/code>):&lt;/p>
&lt;pre>&lt;code>class { 'ssh': }
&lt;/code>&lt;/pre>
&lt;p>And a Dockerfile like this:&lt;/p>
&lt;pre>&lt;code>FROM larsks/rdo-puppet-base
COPY puppet /puppet
RUN cd /puppet; puppet apply \
--modulepath /usr/share/openstack-puppet/modules \
node.pp
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>larsks/rdo-puppet-base&lt;/code> module includes &amp;ldquo;puppet&amp;rdquo; and all the Puppet
modules required by RDO (installed in
&lt;code>/usr/share/openstack-puppet/modules&lt;/code>).&lt;/p>
&lt;p>Running &lt;code>docker build&lt;/code> with this &lt;code>Dockerfile&lt;/code> results in:&lt;/p>
&lt;pre>&lt;code>Error: Could not run: Could not retrieve facts for
a9cde05eb735.example.com: no address for
a9cde05eb735.example.com
&lt;/code>&lt;/pre>
&lt;p>Puppet is trying to determine the FQDN of the container, and is then
trying to determine the canonical ip address of the container. This is
never going to work, absent some mechanism that automatically
registers DNS entries when you boot containers (e.g., &lt;a href="https://github.com/crosbymichael/skydock">skydock&lt;/a>).&lt;/p>
&lt;p>The obvious way to fix this would be to modify &lt;code>/etc/hosts&lt;/code> and add
the calculated fqdn to the entry for &lt;code>localhost&lt;/code>, but &lt;code>/etc/hosts&lt;/code>
inside Docker containers is read-only.&lt;/p>
&lt;p>Since Puppet is using Facter to get information about the host, I
looked into whether or not it would be possible (and convenient) to
override Facter generated facts. It turns out that it &lt;a href="http://www.puppetcookbook.com/posts/override-a-facter-fact.html">is relatively
easy&lt;/a>; just set &lt;code>FACTER_&amp;lt;fact_name&amp;gt;&lt;/code> in the environment.
For example:&lt;/p>
&lt;pre>&lt;code>FACTER_fqdn=localhost
&lt;/code>&lt;/pre>
&lt;p>I modified the Dockerfile to look like this:&lt;/p>
&lt;pre>&lt;code>FROM larsks/rdo-puppet-base
COPY puppet /puppet
RUN cd /puppet; FACTER_fqdn=localhost puppet apply \
--modulepath=/usr/share/openstack-puppet/modules \
node.pp
&lt;/code>&lt;/pre>
&lt;p>Running this yields:&lt;/p>
&lt;pre>&lt;code>Error: Could not start Service[sshd]: Execution of '/sbin/service
sshd start' returned 1: Redirecting to /bin/systemctl start sshd.service
Failed to get D-Bus connection: No connection to service manager.
Wrapped exception:
Execution of '/sbin/service sshd start' returned 1: Redirecting to
/bin/systemctl start sshd.service
Failed to get D-Bus connection: No connection to service manager.
&lt;/code>&lt;/pre>
&lt;p>This is happening because the Puppet module is trying to manipulate
the corresponding service resource, but there is no service manager
(e.g., &amp;ldquo;systemd&amp;rdquo; or &amp;ldquo;upstart&amp;rdquo;, etc) inside the container.&lt;/p>
&lt;p>Some modules provide a module parameter to disable service management,
but that solution isn&amp;rsquo;t available in this module. Instead, I created
a &amp;ldquo;dummy&amp;rdquo; service provider. The &amp;ldquo;code&amp;rdquo; (or lack thereof) looks like
this:&lt;/p>
&lt;pre>&lt;code>Puppet::Type.type(:service).provide :dummy, :parent =&amp;gt; :base do
desc &amp;quot;Dummy service provider&amp;quot;
def startcmd
true;
end
def stopcmd
true;
end
def restartcmd
true
end
def statuscmd
true
end
end
&lt;/code>&lt;/pre>
&lt;p>I dropped this into a &lt;code>dummy_service&lt;/code> puppet module with the
following structure:&lt;/p>
&lt;pre>&lt;code>dummy_service/
lib/
puppet/
provider/
service/
dummy.rb
&lt;/code>&lt;/pre>
&lt;p>I installed the whole thing into &lt;code>/usr/share/puppet/modules&lt;/code> in the
base image (&lt;code>larsks/rdo-puppet-base&lt;/code>) by adding the following to the
relevant &lt;code>Dockerfile&lt;/code>:&lt;/p>
&lt;pre>&lt;code>COPY dummy_service /usr/share/puppet/modules/dummy_service
&lt;/code>&lt;/pre>
&lt;p>I modified the &lt;code>Dockerfile&lt;/code> for my ssh image to look like this:&lt;/p>
&lt;pre>&lt;code>FROM larsks/rdo-puppet-base
COPY puppet /puppet
RUN cd /puppet; \
FACTER_fqdn=localhost \
puppet apply \
--modulepath=/usr/share/openstack-puppet/modules:/usr/share/puppet/modules \
node.pp
&lt;/code>&lt;/pre>
&lt;p>And finally I modified &lt;code>node.pp&lt;/code> to look like this:&lt;/p>
&lt;pre>&lt;code>Service {
provider =&amp;gt; dummy,
}
class { 'ssh': }
&lt;/code>&lt;/pre>
&lt;p>This sets the default &lt;code>provider&lt;/code> for &lt;code>service&lt;/code> resources to &lt;code>dummy&lt;/code>.&lt;/p>
&lt;p>With these changes, the &lt;code>docker build&lt;/code> operation completes
successfully:&lt;/p>
&lt;pre>&lt;code>Sending build context to Docker daemon 49.15 kB
Sending build context to Docker daemon
Step 0 : FROM larsks/rdo-puppet-base
---&amp;gt; 2554b6fb47bb
Step 1 : COPY puppet /puppet
---&amp;gt; Using cache
---&amp;gt; bf867271fd0f
Step 2 : RUN cd /puppet; FACTER_fqdn=localhost puppet apply --modulepath=/usr/share/openstack-puppet/modules:/usr/share/puppet/modules node.pp
---&amp;gt; Running in 91b08a7a0ff5
Notice: Compiled catalog for c6f07ae86c40.redhat.com in environment production in 0.58 seconds
Notice: /Stage[main]/Ssh::Server::Install/Package[openssh-server]/ensure: created
Notice: /Stage[main]/Ssh::Client::Config/File[/etc/ssh/ssh_config]/content: content changed '{md5}e233b9bb27ac15b968d8016d7be7d7ce' to '{md5}34815c31785be0c717f766e8d2c8d4d7'
Notice: Finished catalog run in 47.61 seconds
---&amp;gt; e830e6adce26
Removing intermediate container 91b08a7a0ff5
Successfully built e830e6adce26
&lt;/code>&lt;/pre>
&lt;p>Obviously, in order to turn this into a functional module you would
need to add an appropriate &lt;code>CMD&lt;/code> or &lt;code>ENTRYPOINT&lt;/code> script to make it
generate host keys and start &lt;code>sshd&lt;/code>, but I think this successfully
demonstrates what is necessary to make a stock Puppet module run
as part of the &lt;code>docker build&lt;/code> process.&lt;/p></content></item><item><title>Docker networking with dedicated network containers</title><link>https://blog.oddbit.com/post/2014-10-06-docker-networking-with-dedicat/</link><pubDate>Mon, 06 Oct 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-10-06-docker-networking-with-dedicat/</guid><description>The current version of Docker has a very limited set of networking options:
bridge &amp;ndash; connect a container to the Docker bridge host &amp;ndash; run the container in the global network namespace container:xxx &amp;ndash; connect a container to the network namespace of another container none &amp;ndash; do not configure any networking If you need something more than that, you can use a tool like pipework to provision additional network interfaces inside the container, but this leads to a synchronization problem: pipework can only be used after your container is running.</description><content>&lt;p>The current version of Docker has a very limited set of networking
options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>bridge&lt;/code> &amp;ndash; connect a container to the Docker bridge&lt;/li>
&lt;li>&lt;code>host&lt;/code> &amp;ndash; run the container in the global network namespace&lt;/li>
&lt;li>&lt;code>container:xxx&lt;/code> &amp;ndash; connect a container to the network namespace of
another container&lt;/li>
&lt;li>&lt;code>none&lt;/code> &amp;ndash; do not configure any networking&lt;/li>
&lt;/ul>
&lt;p>If you need something more than that, you can use a tool like
&lt;a href="https://github.com/jpetazzo/pipework">pipework&lt;/a> to provision additional network interfaces inside the
container, but this leads to a synchronization problem: &lt;code>pipework&lt;/code> can
only be used after your container is running. This means that when
starting your container, you must have logic that will wait until the
necessary networking is available before starting your service.&lt;/p>
&lt;p>The &lt;a href="https://github.com/GoogleCloudPlatform/kubernetes">kubernetes&lt;/a> project uses a clever solution to this problem:&lt;/p>
&lt;p>Begin by starting a no-op container &amp;ndash; that is, a container that does
not run any services &amp;ndash; with &lt;code>--net=none&lt;/code>. It needs to run
&lt;em>something&lt;/em>; otherwise it will exit. The &lt;code>kubernetes/pause&lt;/code> image
implements an extremely minimal &amp;ldquo;do nothing but wait&amp;rdquo; solution.&lt;/p>
&lt;p>Once you have this no-op container running, you can set up the
corresponding network namespace to meet your requirements. For
example, you can create a &lt;code>veth&lt;/code> device pair and place one end in the
interface and attach another to a bridge on your system. &lt;a href="https://github.com/jpetazzo/pipework">Pipework&lt;/a>
can help with this, but you can also perform all the &lt;a href="https://blog.oddbit.com/post/2014-08-11-four-ways-to-connect-a-docker/">changes by
hand&lt;/a>&lt;/p>
&lt;p>Once your networking is configured, start your actual service
container with &lt;code>--net=container:&amp;lt;id-of-noop-container&amp;gt;&lt;/code>. Your service
container will start with your configured network environment.&lt;/p>
&lt;p>You could, I suppose, decide to link &lt;em>every&lt;/em> service container with
it&amp;rsquo;s own network container, but that would get messy. Kubernetes
groups containers together into &amp;ldquo;pods&amp;rdquo;, in which all containers in a
pod share the same network namespace, which reduces the number of
&amp;ldquo;networking containers&amp;rdquo; necessary for services that have the same
networking requirements.&lt;/p>
&lt;p>This solution &amp;ndash; linking your service container with a no-op container
used to implement networking &amp;ndash; solves the problems identified at the
beginning of this post: because you can perform all your network
configuration prior to starting your service, your service container
does not need any special logic to deal with interfaces that will be
created after the container starts. The networking will already be
in place when the service starts.&lt;/p>
&lt;p>Docker issue &lt;a href="https://github.com/docker/docker/issues/7455">7455&lt;/a> proposes a docker-native solution that would
accomplish largely the same thing without requiring the separate
networking container (by permitting you to pre-configure a network
namespace and then pass that to docker using something like
&lt;code>--net=netns:&amp;lt;name-of-network-namespace&amp;gt;&lt;/code>).&lt;/p></content></item><item><title>Integrating custom code with Nova using hooks</title><link>https://blog.oddbit.com/post/2014-09-27-integrating-custom-code-with-n/</link><pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-27-integrating-custom-code-with-n/</guid><description>Would you like to run some custom Python code when Nova creates and destroys virtual instances on your compute hosts? This is possible using Nova&amp;rsquo;s support for hooks, but the existing documentation is somewhat short on examples, so I&amp;rsquo;ve spent some time trying to get things working.
The demo_nova_hooks repository contains a working example of the techniques discussed in this article.
What&amp;rsquo;s a hook? A Nova &amp;ldquo;hook&amp;rdquo; is a mechanism that allows you to attach a class of your own design to a particular function or method call in Nova.</description><content>&lt;p>Would you like to run some custom Python code when Nova creates and
destroys virtual instances on your compute hosts? This is possible
using Nova&amp;rsquo;s support for &lt;a href="http://docs.openstack.org/developer/nova/devref/hooks.html">hooks&lt;/a>, but the existing documentation is
somewhat short on examples, so I&amp;rsquo;ve spent some time trying to get
things working.&lt;/p>
&lt;p>The &lt;a href="https://github.com/larsks/demo_nova_hooks">demo_nova_hooks&lt;/a> repository contains a working example of the
techniques discussed in this article.&lt;/p>
&lt;h2 id="whats-a-hook">What&amp;rsquo;s a hook?&lt;/h2>
&lt;p>A Nova &amp;ldquo;hook&amp;rdquo; is a mechanism that allows you to attach a class of your
own design to a particular function or method call in Nova. Your
class should define a &lt;code>pre&lt;/code> method (that will be called before the
method is called) and &lt;code>post&lt;/code> function (that will be called after the
method is called):&lt;/p>
&lt;pre>&lt;code>class YourHookClass(object):
def pre(self, *args, **kwargs):
....
def post(self, rv, *args, **kwargs):
....
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>pre&lt;/code> method will be called with the positional and keyword
arguments being passed to the hooked function. The &lt;code>post&lt;/code> method
receives the return value of the called method in addition to the
positional and keyword arguments.&lt;/p>
&lt;p>You connect your code to available hooks using &lt;a href="https://pythonhosted.org/setuptools/setuptools.html">Setuptools entry
points&lt;/a>. For example, assuming that the above code lived in
module named &lt;code>your_package.hooks&lt;/code>, you might have the following in the
corresponding &lt;code>setup.py&lt;/code> file:&lt;/p>
&lt;pre>&lt;code>entry_points = {
'nova.hooks': [
'create_instance=your_package.hooks:YourHookClass',
]
},
&lt;/code>&lt;/pre>
&lt;h2 id="what-hooks-are-available">What hooks are available?&lt;/h2>
&lt;p>The Nova code (as of &lt;a href="https://github.com/openstack/nova/commit/81b1babcd9699118f57d5055ff9045e275b536b5">81b1bab&lt;/a>) defines three hooks:&lt;/p>
&lt;ul>
&lt;li>&lt;code>create_instance&lt;/code>&lt;/li>
&lt;li>&lt;code>delete_instances&lt;/code>&lt;/li>
&lt;li>&lt;code>instance_network_info&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="create_instance">create_instance&lt;/h3>
&lt;p>The &lt;code>create_instance&lt;/code> hook is attached to the Nova API &lt;code>create&lt;/code>
function, and will receive the following arguments:&lt;/p>
&lt;pre>&lt;code>def create(self, context, instance_type,
image_href, kernel_id=None, ramdisk_id=None,
min_count=None, max_count=None,
display_name=None, display_description=None,
key_name=None, key_data=None, security_group=None,
availability_zone=None, user_data=None, metadata=None,
injected_files=None, admin_password=None,
block_device_mapping=None, access_ip_v4=None,
access_ip_v6=None, requested_networks=None, config_drive=None,
auto_disk_config=None, scheduler_hints=None, legacy_bdm=True,
shutdown_terminate=False, check_server_group_quota=False):
&lt;/code>&lt;/pre>
&lt;p>When called, &lt;code>self&lt;/code> is a &lt;code>nova.compute.api.API&lt;/code> object, &lt;code>context&lt;/code> is a
&lt;code>nova.context.RequestContext&lt;/code> object, &lt;code>instance_type&lt;/code> is a dictionary
containing information about the selected flavor, and &lt;code>image_href&lt;/code> is
an image UUID.&lt;/p>
&lt;p>During my testing, the &lt;code>instance_type&lt;/code> dictionary looked like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>{'created_at': None,
'deleted': 0L,
'deleted_at': None,
'disabled': False,
'ephemeral_gb': 0L,
'extra_specs': {},
'flavorid': u'2',
'id': 5L,
'is_public': True,
'memory_mb': 2048L,
'name': u'm1.small',
'root_gb': 20L,
'rxtx_factor': 1.0,
'swap': 0L,
'updated_at': None,
'vcpu_weight': None,
'vcpus': 1L}
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;corresponding to the &lt;code>m1.small&lt;/code> flavor on my system.&lt;/p>
&lt;h3 id="delete_instance">delete_instance&lt;/h3>
&lt;p>The &lt;code>delete_instance&lt;/code> hook is attached to the &lt;code>_delete_instance&lt;/code>
method in the &lt;code>nova.compute.manager.ComputeManager&lt;/code> class, which is
called whenever Nova needs to delete an instance. The hook will
receive the following arguments:&lt;/p>
&lt;pre>&lt;code>def _delete_instance(self, context, instance, bdms, quotas):
&lt;/code>&lt;/pre>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>&lt;code>self&lt;/code> is a &lt;code>nova.compute.manager.ComputeManager&lt;/code> object,&lt;/li>
&lt;li>&lt;code>context&lt;/code> is a &lt;code>nova.context.RequestContext&lt;/code>,&lt;/li>
&lt;li>&lt;code>instance&lt;/code> is a &lt;code>nova.objects.instance.Instance&lt;/code> object&lt;/li>
&lt;li>&lt;code>bdms&lt;/code> is a &lt;code>nova.objects.block_device.BlockDeviceMappingList&lt;/code>
object, and&lt;/li>
&lt;li>&lt;code>quotas&lt;/code> is a &lt;code>nova.objects.quotas.Quotas&lt;/code> object&lt;/li>
&lt;/ul>
&lt;h3 id="instance_network_info">instance_network_info&lt;/h3>
&lt;p>The &lt;code>instance_network_info&lt;/code> hook is attached to the
&lt;code>update_instance_cache_with_nw_info&lt;/code> function in
&lt;code>nova.network.base_api.py&lt;/code>. The hook will receive the following
arguments:&lt;/p>
&lt;pre>&lt;code>def update_instance_cache_with_nw_info(impl, context, instance,
nw_info=None, update_cells=True):
&lt;/code>&lt;/pre>
&lt;p>I am not running Nova Network in my environment, so I have not
examined this hook in any additional detail.&lt;/p>
&lt;h2 id="a-working-example">A working example&lt;/h2>
&lt;p>The &lt;a href="https://github.com/larsks/demo_nova_hooks">demo_nova_hooks&lt;/a> repository implements simple logging-only
implementations of &lt;code>create_instance&lt;/code> and &lt;code>delete_instance&lt;/code> hooks. You
can install this code, restart Nova services, boot an instances, and
verify that the code has executed by looking at the logs generated in
&lt;code>/var/log/nova&lt;/code>.&lt;/p></content></item><item><title>Stupid command line tricks: Quickly share screen captures</title><link>https://blog.oddbit.com/post/2014-09-23-stupid-cli-quickly-share-scree/</link><pubDate>Tue, 23 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-23-stupid-cli-quickly-share-scree/</guid><description>Sometimes you want to quickly share a screenshot with someone. Here&amp;rsquo;s my favorite mechanism, which assumes you have installed both curl and the ImageMagick suite.
$ import png:- | curl -T- -s chunk.io http://chunk.io/f/76ea98ea081748e19de4507fde3c2c65 When you run this command, you cursor will change into crosshairs. Click on a window, and this will grab a png image of that window and send it to chunk.io using curl.
You&amp;rsquo;ll get back a URL that you can use to share the image with people.</description><content>&lt;p>Sometimes you want to quickly share a screenshot with someone. Here&amp;rsquo;s
my favorite mechanism, which assumes you have installed both &lt;code>curl&lt;/code>
and the &lt;a href="http://www.imagemagick.org/">ImageMagick&lt;/a> suite.&lt;/p>
&lt;pre>&lt;code>$ import png:- | curl -T- -s chunk.io
http://chunk.io/f/76ea98ea081748e19de4507fde3c2c65
&lt;/code>&lt;/pre>
&lt;p>When you run this command, you cursor will change into crosshairs.
Click on a window, and this will grab a png image of that window and
send it to &lt;a href="http://chunk.io/">chunk.io&lt;/a> using curl.&lt;/p>
&lt;p>You&amp;rsquo;ll get back a URL that you can use to share the image with people.&lt;/p></content></item><item><title>Heat Hangout</title><link>https://blog.oddbit.com/post/2014-09-05-heat-hangout/</link><pubDate>Fri, 05 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-05-heat-hangout/</guid><description>I ran a Google Hangout this morning on Deploying with Heat. You can find the slides for the presentation on line here, and the Heat templates (as well as slide sources) are available on github.
If you have any questions about the presentation, please feel free to ping me on irc (larsks).</description><content>&lt;p>I ran a Google Hangout this morning on &lt;a href="https://plus.google.com/events/c9u4sjn7ksb8jrmma7vd25aok94">Deploying with Heat&lt;/a>. You
can find the slides for the presentation on line &lt;a href="http://oddbit.com/rdo-hangout-heat-intro/#/">here&lt;/a>, and the
Heat templates (as well as slide sources) are available &lt;a href="https://github.com/larsks/rdo-hangout-heat-intro/">on
github&lt;/a>.&lt;/p>
&lt;p>If you have any questions about the presentation, please feel free to
ping me on irc (&lt;code>larsks&lt;/code>).&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/qH-qYE1Kmpg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></content></item><item><title>Visualizing Heat stacks</title><link>https://blog.oddbit.com/post/2014-09-02-visualizing-heat-stacks/</link><pubDate>Tue, 02 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-02-visualizing-heat-stacks/</guid><description>I spent some time today learning about Heat autoscaling groups, which are incredibly nifty but a little opaque from the Heat command line, since commands such as heat resource-list don&amp;rsquo;t recurse into nested stacks. It is possible to introspect these resources (you can pass the physical resource id of a nested stack to heat resource-list, for example)&amp;hellip;
&amp;hellip;but I really like visualizing things, so I wrote a quick hack called dotstack that will generate dot language output from a Heat stack.</description><content>&lt;p>I spent some time today learning about Heat &lt;a href="https://wiki.openstack.org/wiki/Heat/AutoScaling">autoscaling groups&lt;/a>,
which are incredibly nifty but a little opaque from the Heat command
line, since commands such as &lt;code>heat resource-list&lt;/code> don&amp;rsquo;t recurse into
nested stacks. It is possible to introspect these resources (you can
pass the physical resource id of a nested stack to &lt;code>heat resource-list&lt;/code>, for example)&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;but I really like visualizing things, so I wrote a quick hack
called &lt;a href="http://github.com/larsks/dotstack">dotstack&lt;/a> that will generate &lt;a href="http://en.wikipedia.org/wiki/DOT_(graph_description_language)">dot&lt;/a> language output from a
Heat stack. You can process this with &lt;a href="http://www.graphviz.org/">Graphviz&lt;/a> to produce output
like this, in which graph nodes are automatically colorized by
resource type:&lt;/p>
&lt;figure class="left" >
&lt;img src="sample.svg" />
&lt;/figure>
&lt;p>Or like this, in which each node contains information about its
resource type and physical resource id:&lt;/p>
&lt;figure class="left" >
&lt;img src="sample-detailed.svg" />
&lt;/figure>
&lt;p>The source code is available on &lt;a href="http://github.com/larsks/dotstack">github&lt;/a>.&lt;/p></content></item><item><title>Docker plugin bugs</title><link>https://blog.oddbit.com/post/2014-09-01-docker-plugin-bugs/</link><pubDate>Mon, 01 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-01-docker-plugin-bugs/</guid><description>This is a companion to my article on the Docker plugin for Heat.
While writing that article, I encountered a number of bugs in the Docker plugin and elsewhere. I&amp;rsquo;ve submitted patches for most of the issues I encountered:
Bugs in the Heat plugin https://bugs.launchpad.net/heat/+bug/1364017
docker plugin fails to delete a container resource in CREATE_FAILED state.
https://bugs.launchpad.net/heat/+bug/1364041
docker plugin volumes_from parameter should be a list.
https://bugs.launchpad.net/heat/+bug/1364039
docker plugin volumes_from parameter results in an error</description><content>&lt;p>This is a companion to my &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/">article on the Docker plugin for Heat&lt;/a>.&lt;/p>
&lt;p>While writing that article, I encountered a number of bugs in the
Docker plugin and elsewhere. I&amp;rsquo;ve submitted patches for most of the
issues I encountered:&lt;/p>
&lt;h2 id="bugs-in-the-heat-plugin">Bugs in the Heat plugin&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364017">https://bugs.launchpad.net/heat/+bug/1364017&lt;/a>&lt;/p>
&lt;p>docker plugin fails to delete a container resource in
&lt;code>CREATE_FAILED&lt;/code> state.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364041">https://bugs.launchpad.net/heat/+bug/1364041&lt;/a>&lt;/p>
&lt;p>docker plugin &lt;code>volumes_from&lt;/code> parameter should be a list.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364039">https://bugs.launchpad.net/heat/+bug/1364039&lt;/a>&lt;/p>
&lt;p>docker plugin &lt;code>volumes_from&lt;/code> parameter results in an error&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364019">https://bugs.launchpad.net/heat/+bug/1364019&lt;/a>&lt;/p>
&lt;p>docker plugin does not actually remove containers on delete&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugs-in-docker-python-module">Bugs in docker Python module&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/docker/docker-py/pull/310">https://github.com/docker/docker-py/pull/310&lt;/a>&lt;/p>
&lt;p>allow ports to be specified as &lt;code>port/proto&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul></content></item><item><title>Annotated documentation for DockerInc::Docker::Container</title><link>https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/</guid><description>This is a companion to my article on the Docker plugin for Heat.
DockerInc::Docker::Container Properties cmd : List
Command to run after spawning the container.
Optional property.
Example:
cmd: [ 'thttpd', '-C', '/etc/thttpd.conf', '-D', '-c', '*.cgi'] dns : List
Set custom DNS servers.
Example:
dns: - 8.8.8.8 - 8.8.4.4 docker_endopint : String
Docker daemon endpoint. By default the local Docker daemon will be used.
Example:
docker_endpoint: tcp://192.168.1.100:2375 env : String</description><content>&lt;p>This is a companion to my &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/">article on the Docker plugin for Heat&lt;/a>.&lt;/p>
&lt;h2 id="dockerincdockercontainer">DockerInc::Docker::Container&lt;/h2>
&lt;h3 id="properties">Properties&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;code>cmd&lt;/code> : List&lt;/p>
&lt;p>Command to run after spawning the container.&lt;/p>
&lt;p>Optional property.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> cmd: [ 'thttpd', '-C', '/etc/thttpd.conf', '-D', '-c', '*.cgi']
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>dns&lt;/code> : List&lt;/p>
&lt;p>Set custom DNS servers.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> dns:
- 8.8.8.8
- 8.8.4.4
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>docker_endopint&lt;/code> : String&lt;/p>
&lt;p>Docker daemon endpoint. By default the local Docker daemon will
be used.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> docker_endpoint: tcp://192.168.1.100:2375
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>env&lt;/code> : String&lt;/p>
&lt;p>Set environment variables.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> env:
- MYSQL_ROOT_PASSWORD=secret
- &amp;quot;ANOTHER_VARIABLE=something long with spaces&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hostname&lt;/code> : String&lt;/p>
&lt;p>Hostname of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> hostname: mywebserver
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>image&lt;/code> : String&lt;/p>
&lt;p>Image name to boot.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> image: mysql
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>links&lt;/code> : Mapping&lt;/p>
&lt;p>Links to other containers.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> links:
name_in_this_container: name_of_that_container
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>memory&lt;/code> : Number&lt;/p>
&lt;p>Memory limit in bytes.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> # 512 MB
memory: 536870912
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>name&lt;/code> : String&lt;/p>
&lt;p>Name of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> name: dbserver
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>open_stdin&lt;/code> : Boolean&lt;/p>
&lt;p>True to open &lt;code>stdin&lt;/code>.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> open_stdin: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>port_bindings&lt;/code> : Map&lt;/p>
&lt;p>TCP/UDP port bindings.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> # bind port 8080 in the container to port 80 on the host
port_bindings:
8080: 80
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>port_specs&lt;/code> : List&lt;/p>
&lt;p>List of TCP/UDP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> port_specs:
- 80
- 53/udp
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>privileged&lt;/code> : Boolean&lt;/p>
&lt;p>Enable extended privileges.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> privileged: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>stdin_once&lt;/code> : Boolean&lt;/p>
&lt;p>If &lt;code>true&lt;/code>, close &lt;code>stdin&lt;/code> after the one attached client disconnects.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> stdin_once: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>tty&lt;/code> : Boolean&lt;/p>
&lt;p>Allocate a pseudo-tty if &lt;code>true&lt;/code>.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> tty: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>user&lt;/code> : String&lt;/p>
&lt;p>Username or UID for running the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> username: apache
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>volumes&lt;/code> : Map&lt;/p>
&lt;p>Create a bind mount.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> volumes:
/var/tmp/data_on_host: /srv/data_in_container
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>volumes_from&lt;/code> : String&lt;/p>
&lt;p>&lt;em>This option is broken in the current version of the Docker
plugin.&lt;/em>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="attributes">Attributes&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;code>info&lt;/code> : Map&lt;/p>
&lt;p>Information about the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> info:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;info&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> {
&amp;quot;HostsPath&amp;quot;: &amp;quot;/var/lib/docker/containers/d6d84d1bbf2984fa3e04cea36c8d10d27d318b6d96b57c41fca2cbc1da23bf71/hosts&amp;quot;,
&amp;quot;Created&amp;quot;: &amp;quot;2014-09-01T14:21:02.7577874Z&amp;quot;,
&amp;quot;Image&amp;quot;: &amp;quot;a950533b3019d8f6dfdcb8fdc42ef810b930356619b3e4786d4f2acec514238d&amp;quot;,
&amp;quot;Args&amp;quot;: [
&amp;quot;mysqld&amp;quot;,
&amp;quot;--datadir=/var/lib/mysql&amp;quot;,
&amp;quot;--user=mysql&amp;quot;
],
&amp;quot;Driver&amp;quot;: &amp;quot;devicemapper&amp;quot;,
&amp;quot;HostConfig&amp;quot;: {
&amp;quot;CapDrop&amp;quot;: null,
&amp;quot;PortBindings&amp;quot;: {
&amp;quot;3306/tcp&amp;quot;: [
{
&amp;quot;HostPort&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;HostIp&amp;quot;: &amp;quot;&amp;quot;
}
]
},
&amp;quot;NetworkMode&amp;quot;: &amp;quot;&amp;quot;,
.
.
.
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>logs&lt;/code> : String&lt;/p>
&lt;p>Logs from the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> logs:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;logs&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>logs_head&lt;/code> : String&lt;/p>
&lt;p>Most recent log line from the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> logs:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;logs_head&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;2014-09-01 14:21:04 0 [Warning] TIMESTAMP with implicit DEFAULT
value is deprecated. Please use --explicit_defaults_for_timestamp
server option (see documentation for more details).&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_gateway&lt;/code> : String&lt;/p>
&lt;p>IP address of the network gateway for the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_gateway:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_gateway&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;172.17.42.1&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_info&lt;/code> : Map&lt;/p>
&lt;p>Information about the network configuration of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_info:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_info&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> {
&amp;quot;Bridge&amp;quot;: &amp;quot;docker0&amp;quot;,
&amp;quot;TcpPorts&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;PortMapping&amp;quot;: null,
&amp;quot;IPPrefixLen&amp;quot;: 16,
&amp;quot;UdpPorts&amp;quot;: &amp;quot;&amp;quot;,
&amp;quot;IPAddress&amp;quot;: &amp;quot;172.17.0.10&amp;quot;,
&amp;quot;Gateway&amp;quot;: &amp;quot;172.17.42.1&amp;quot;,
&amp;quot;Ports&amp;quot;: {
&amp;quot;3306/tcp&amp;quot;: [
{
&amp;quot;HostPort&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;HostIp&amp;quot;: &amp;quot;0.0.0.0&amp;quot;
}
]
}
}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_ip&lt;/code> : String&lt;/p>
&lt;p>IP address assigned to the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_ip:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_ip&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;172.17.0.10&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_tcp_ports&lt;/code> : String&lt;/p>
&lt;p>A comma delimited list of TCP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_tcp_ports:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_tcp_ports&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;8443,8080&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_udp_ports&lt;/code> : String&lt;/p>
&lt;p>A comma delimited list of TCP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_udp_ports:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_udp_ports&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;8443,8080&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul></content></item><item><title>Docker plugin for OpenStack Heat</title><link>https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/</guid><description>I have been looking at both Docker and OpenStack recently. In my last post I talked a little about the Docker driver for Nova; in this post I&amp;rsquo;ll be taking an in-depth look at the Docker plugin for Heat, which has been available since the Icehouse release but is surprisingly under-documented.
The release announcement on the Docker blog includes an example Heat template, but it is unfortunately grossly inaccurate and has led many people astray.</description><content>&lt;p>I have been looking at both Docker and OpenStack recently. In my &lt;a href="https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/">last
post&lt;/a> I talked a little about the &lt;a href="https://github.com/stackforge/nova-docker">Docker driver for Nova&lt;/a>; in
this post I&amp;rsquo;ll be taking an in-depth look at the Docker plugin for
Heat, which has been available &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">since the Icehouse release&lt;/a> but is
surprisingly under-documented.&lt;/p>
&lt;p>The &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">release announcement&lt;/a> on the Docker blog includes an
example Heat template, but it is unfortunately grossly inaccurate and
has led many people astray. In particular:&lt;/p>
&lt;ul>
&lt;li>It purports to but does not actually install Docker, due to a basic
&lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a> syntax error, and&lt;/li>
&lt;li>Even if you were to fix that problem, the lack of synchronization
between the two resources in the template would mean that you would
never be able to successfully launch a container.&lt;/li>
&lt;/ul>
&lt;p>In this post, I will present a fully functional example that will work
with the Icehouse release of Heat. We will install the Docker plugin
for Heat, then write a template that will (a) launch a Fedora 20
server and automatically install Docker, and then (b) use the Docker
plugin to launch some containers on that server.&lt;/p>
&lt;p>The &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a> referenced in this article can be found on GitHub:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/heat-docker-example">https://github.com/larsks/heat-docker-example&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="installing-the-docker-plugin">Installing the Docker plugin&lt;/h2>
&lt;p>The first thing we need to do is install the Docker plugin. I am
running &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> packages for Icehouse locally, which do not include
the Docker plugin. We&amp;rsquo;r going to install the plugin from the Heat
sources.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Download the Heat repository:&lt;/p>
&lt;pre>&lt;code> $ git clone https://github.com/openstack/heat.git
Cloning into 'heat'...
remote: Counting objects: 50382, done.
remote: Compressing objects: 100% (22/22), done.
remote: Total 50382 (delta 7), reused 1 (delta 0)
Receiving objects: 100% (50382/50382), 19.84 MiB | 1.81 MiB/s, done.
Resolving deltas: 100% (34117/34117), done.
Checking connectivity... done.
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>This will result in a directory called &lt;code>heat&lt;/code> in your current
working directory. Change into this directory:&lt;/p>
&lt;pre>&lt;code> $ cd heat
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Patch the Docker plugin.&lt;/p>
&lt;p>You have now checked out the &lt;code>master&lt;/code> branch of the Heat
repository; this is the most recent code committed to the project.
At this point we could check out the &lt;code>stable/icehouse&lt;/code> branch of
the repository to get the version of the plugin released at the
same time as the version of Heat that we&amp;rsquo;re running, but we would
find that the Docker plugin was, at that point in time, somewhat
crippled; in particular:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It does not support mapping container ports to host ports, so
there is no easy way to expose container services for external
access, and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It does not know how to automatically &lt;code>pull&lt;/code> missing images, so
you must arrange to run &lt;code>docker pull&lt;/code> a priori for each image you
plan to use in your Heat template.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>That would make us sad, so instead we&amp;rsquo;re going to use the plugin
from the &lt;code>master&lt;/code> branch, which only requires a trivial change in
order to work with the Icehouse release of Heat.&lt;/p>
&lt;p>Look at the file
&lt;code>contrib/heat_docker/heat_docker/resources/docker_container.py&lt;/code>.
Locate the following line:&lt;/p>
&lt;pre>&lt;code> attributes_schema = {
&lt;/code>&lt;/pre>
&lt;p>Add a line immediately before that so that the file look like
this:&lt;/p>
&lt;pre>&lt;code> attributes.Schema = lambda x: x
attributes_schema = {
&lt;/code>&lt;/pre>
&lt;p>If you&amp;rsquo;re curious, here is what we accomplished with that
additional line:&lt;/p>
&lt;p>The code following that point contains multiple stanzas of the
form:&lt;/p>
&lt;pre>&lt;code> INFO: attributes.Schema(
_('Container info.')
),
&lt;/code>&lt;/pre>
&lt;p>In Icehouse, the &lt;code>heat.engine.attributes&lt;/code> module does not have a
&lt;code>Schema&lt;/code> class so this fails. Our patch above adds a module
member named &lt;code>Schema&lt;/code> that simply returns it&amp;rsquo;s arguments (that
is, it is an identity function).&lt;/p>
&lt;p>(&lt;strong>NB&lt;/strong>: At the time this was written, Heat&amp;rsquo;s &lt;code>master&lt;/code> branch was
at &lt;code>a767880&lt;/code>.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Install the Docker plugin into your Heat plugin directory, which
on my system is &lt;code>/usr/lib/heat&lt;/code> (you can set this explicitly using
the &lt;code>plugin_dirs&lt;/code> directive in &lt;code>/etc/heat/heat.conf&lt;/code>):&lt;/p>
&lt;pre>&lt;code> $ rsync -a --exclude=tests/ contrib/heat_docker/heat_docker \
/usr/lib/heat
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re excluding the &lt;code>tests&lt;/code> directory here because it has
additional prerequisites that aren&amp;rsquo;t operationally necessary but
that will prevent Heat from starting up if they are missing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Restart your &lt;code>heat-engine&lt;/code> service. On Fedora, that would be:&lt;/p>
&lt;pre>&lt;code> # systemctl restart openstack-heat-engine
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Verify that the new &lt;code>DockerInc::Docker::Container&lt;/code> resource is
available:&lt;/p>
&lt;pre>&lt;code> $ heat resource-type-list | grep Docker
| DockerInc::Docker::Container |
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="templates-installing-docker">Templates: Installing docker&lt;/h2>
&lt;p>We would like our template to automatically install Docker on a Nova
server. The example in the &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">Docker blog&lt;/a> mentioned earlier
attempts to do this by setting the &lt;code>user_data&lt;/code> parameter of a
&lt;code>OS::Nova::Server&lt;/code> resource like this:&lt;/p>
&lt;pre>&lt;code>user_data: #include https://get.docker.io
&lt;/code>&lt;/pre>
&lt;p>Unfortunately, an unquoted &lt;code>#&lt;/code> introduces a comment in &lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a>, so
this is completely ignored. It would be written more correctly like
this (the &lt;code>|&lt;/code> introduces a block of literal text):&lt;/p>
&lt;pre>&lt;code>user_data: |
#include https://get.docker.io
&lt;/code>&lt;/pre>
&lt;p>Or possibly like this, although this would restrict you to a single
line and thus wouldn&amp;rsquo;t be used much in practice:&lt;/p>
&lt;pre>&lt;code>user_data: &amp;quot;#include https://get.docker.io&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And, all other things being correct, this would install Docker on a
system&amp;hellip;but would not necessarily start it, nor would it configure
Docker to listen on a TCP socket. On my Fedora system, I ended up
creating the following &lt;code>user_data&lt;/code> script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
yum -y upgrade
# I have occasionally seen 'yum install' fail with errors
# trying to contact mirrors. Because it can be a pain to
# delete and re-create the stack, just loop here until it
# succeeds.
while :; do
yum -y install docker-io
[ -x /usr/bin/docker ] &amp;amp;&amp;amp; break
sleep 5
done
# Add a tcp socket for docker
cat &amp;gt; /etc/systemd/system/docker-tcp.socket &amp;lt;&amp;lt;EOF
[Unit]
Description=Docker remote access socket
[Socket]
ListenStream=2375
BindIPv6Only=both
Service=docker.service
[Install]
WantedBy=sockets.target
EOF
# Start and enable the docker service.
for sock in docker.socket docker-tcp.socket; do
systemctl start $sock
systemctl enable $sock
done
&lt;/code>&lt;/pre>
&lt;p>This takes care of making sure our packages are current, installing
Docker, and arranging for it to listen on a tcp socket. For that last
bit, we&amp;rsquo;re creating a new &lt;code>systemd&lt;/code> socket file
(&lt;code>/etc/systemd/system/docker-tcp.socket&lt;/code>), which means that &lt;code>systemd&lt;/code>
will actually open the socket for listening and start &lt;code>docker&lt;/code> if
necessary when a client connects.&lt;/p>
&lt;h2 id="templates-synchronizing-resources">Templates: Synchronizing resources&lt;/h2>
&lt;p>In our Heat template, we are starting a Nova server that will run
Docker, and then we are instantiating one or more Docker containers
that will run on this server. This means that timing is suddenly very
important. If we use the &lt;code>user_data&lt;/code> script as presented in the
previous section, we would probably end up with an error like this in
our &lt;code>heat-engine.log&lt;/code>:&lt;/p>
&lt;pre>&lt;code>2014-08-29 17:10:37.598 15525 TRACE heat.engine.resource ConnectionError:
HTTPConnectionPool(host='192.168.200.11', port=2375): Max retries exceeded
with url: /v1.12/containers/create (Caused by &amp;lt;class 'socket.error'&amp;gt;:
[Errno 113] EHOSTUNREACH)
&lt;/code>&lt;/pre>
&lt;p>This happens because it takes &lt;em>time&lt;/em> to install packages. Absent any
dependencies, Heat creates resources in parallel, so Heat is happily
trying to spawn our Docker containers when our server is still
fetching the Docker package.&lt;/p>
&lt;p>Heat does have a &lt;code>depends_on&lt;/code> property that can be applied to
resources. For example, if we have:&lt;/p>
&lt;pre>&lt;code>docker_server:
type: &amp;quot;OS::Nova::Server&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>We can make a Docker container depend on that resource:&lt;/p>
&lt;pre>&lt;code>docker_container_mysql:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
depends_on:
- docker_server
&lt;/code>&lt;/pre>
&lt;p>Looks good, but this does not, in fact, help us. From Heat&amp;rsquo;s
perspective, the dependency is satisfied as soon as the Nova server
&lt;em>boots&lt;/em>, so really we&amp;rsquo;re back where we started.&lt;/p>
&lt;p>The Heat solution to this is the &lt;code>AWS::CloudFormation::WaitCondition&lt;/code>
resource (and its boon companion, the and
&lt;code>AWS::CloudFormation::WaitConditionHandle&lt;/code> resource). A
&lt;code>WaitCondition&lt;/code> is a resource this is not &amp;ldquo;created&amp;rdquo; until it has
received an external signal. We define a wait condition like this:&lt;/p>
&lt;pre>&lt;code>docker_wait_handle:
type: &amp;quot;AWS::CloudFormation::WaitConditionHandle&amp;quot;
docker_wait_condition:
type: &amp;quot;AWS::CloudFormation::WaitCondition&amp;quot;
depends_on:
- docker_server
properties:
Handle:
get_resource: docker_wait_handle
Timeout: &amp;quot;6000&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And then we make our container depend on the wait condition:&lt;/p>
&lt;pre>&lt;code>docker_container_mysql:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
depends_on:
- docker_wait_condition
&lt;/code>&lt;/pre>
&lt;p>With this in place, Heat will not attempt to create the Docker
container until we signal the wait condition resource. In order to do
that, we need to modify our &lt;code>user_data&lt;/code> script to embed the
notification URL generated by heat. We&amp;rsquo;ll use both the &lt;code>get_resource&lt;/code>
and &lt;code>str_replace&lt;/code> &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#intrinsic-functions">intrinsic function&lt;/a> in order to generate the appropriate
script:&lt;/p>
&lt;pre>&lt;code> user_data:
# We're using Heat's 'str_replace' function in order to
# substitute into this script the Heat-generated URL for
# signaling the docker_wait_condition resource.
str_replace:
template: |
#!/bin/sh
yum -y upgrade
# I have occasionally seen 'yum install' fail with errors
# trying to contact mirrors. Because it can be a pain to
# delete and re-create the stack, just loop here until it
# succeeds.
while :; do
yum -y install docker-io
[ -x /usr/bin/docker ] &amp;amp;&amp;amp; break
sleep 5
done
# Add a tcp socket for docker
cat &amp;gt; /etc/systemd/system/docker-tcp.socket &amp;lt;&amp;lt;EOF
[Unit]
Description=Docker remote access socket
[Socket]
ListenStream=2375
BindIPv6Only=both
Service=docker.service
[Install]
WantedBy=sockets.target
EOF
# Start and enable the docker service.
for sock in docker.socket docker-tcp.socket; do
systemctl start $sock
systemctl enable $sock
done
# Signal heat that we are finished settings things up.
cfn-signal -e0 --data 'OK' -r 'Setup complete' '$WAIT_HANDLE'
params:
&amp;quot;$WAIT_HANDLE&amp;quot;:
get_resource: docker_wait_handle
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>str_replace&lt;/code> function probably deserves a closer look; the
general format is:&lt;/p>
&lt;pre>&lt;code>str_replace:
template:
params:
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>template&lt;/code> is text content containing 0 or more things to be
replaced, and &lt;code>params&lt;/code> is a list of tokens to search for and replace
in the &lt;code>template&lt;/code>.&lt;/p>
&lt;p>We use &lt;code>str_replace&lt;/code> to substitute the token &lt;code>$WAIT_HANDLE&lt;/code> with the
result of calling &lt;code>get_resource&lt;/code> on our &lt;code>docker_wait_handle&lt;/code> resource.
This results in a URL that contains an EC2-style signed URL that will
deliver the necessary notification to Heat. In this example we&amp;rsquo;re
using the &lt;code>cfn-signal&lt;/code> tool, which is included in the Fedora cloud
images, but you could accomplish the same thing with &lt;code>curl&lt;/code>:&lt;/p>
&lt;pre>&lt;code>curl -X PUT -H 'Content-Type: application/json' \
--data-binary '{&amp;quot;Status&amp;quot;: &amp;quot;SUCCESS&amp;quot;,
&amp;quot;Reason&amp;quot;: &amp;quot;Setup complete&amp;quot;,
&amp;quot;Data&amp;quot;: &amp;quot;OK&amp;quot;, &amp;quot;UniqueId&amp;quot;: &amp;quot;00000&amp;quot;}' \
&amp;quot;$WAIT_HANDLE&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>You need to have correctly configured Heat in order for this to work;
I&amp;rsquo;ve written a short &lt;a href="https://blog.oddbit.com/post/2014-08-30-using-wait-conditions-with-hea/">companion article&lt;/a> that contains a checklist
and pointers to additional documentation to help work around some
common issues.&lt;/p>
&lt;h2 id="templates-defining-docker-containers">Templates: Defining Docker containers&lt;/h2>
&lt;p>&lt;strong>UPDATE&lt;/strong>: I have generated some &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/">annotated documentation for the
Docker plugin&lt;/a>.&lt;/p>
&lt;p>Now that we have arranged for Heat to wait for the server to finish
configuration before starting Docker contains, how do we create a
container? As Scott Lowe noticed in his &lt;a href="http://blog.scottlowe.org/2014/08/22/a-heat-template-for-docker-containers/">blog post about Heat and
Docker&lt;/a>, there is very little documentation available out there
for the Docker plugin (something I am trying to remedy with this blog
post!). Things are not quite as bleak as you might think, because
Heat resources are to a certain extent self-documenting. If you run:&lt;/p>
&lt;pre>&lt;code>$ heat resource-template DockerInc::Docker::Container
&lt;/code>&lt;/pre>
&lt;p>You will get a complete description of the attributes and properties
available in the named resource. The &lt;code>parameters&lt;/code> section is probably
the most descriptive:&lt;/p>
&lt;pre>&lt;code>parameters:
cmd:
Default: []
Description: Command to run after spawning the container.
Type: CommaDelimitedList
dns: {Description: Set custom dns servers., Type: CommaDelimitedList}
docker_endpoint: {Description: Docker daemon endpoint (by default the local docker
daemon will be used)., Type: String}
env: {Description: Set environment variables., Type: CommaDelimitedList}
hostname: {Default: '', Description: Hostname of the container., Type: String}
image: {Description: Image name., Type: String}
links: {Description: Links to other containers., Type: Json}
memory: {Default: 0, Description: Memory limit (Bytes)., Type: Number}
name: {Description: Name of the container., Type: String}
open_stdin:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Open stdin.
Type: String
port_bindings: {Description: TCP/UDP ports bindings., Type: Json}
port_specs: {Description: TCP/UDP ports mapping., Type: CommaDelimitedList}
privileged:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Enable extended privileges.
Type: String
stdin_once:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: If true, close stdin after the 1 attached client disconnects.
Type: String
tty:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Allocate a pseudo-tty.
Type: String
user: {Default: '', Description: Username or UID., Type: String}
volumes:
Default: {}
Description: Create a bind mount.
Type: Json
volumes_from: {Default: '', Description: Mount all specified volumes., Type: String}
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>port_specs&lt;/code> and &lt;code>port_bindings&lt;/code> parameters require a little
additional explanation.&lt;/p>
&lt;p>The &lt;code>port_specs&lt;/code> parameter is a list of (TCP) ports that will be
&amp;ldquo;exposed&amp;rdquo; by the container (similar to the &lt;code>EXPOSE&lt;/code> directive in a
Dockerfile). This corresponds to the &lt;code>PortSpecs&lt;/code> argument in the the
&lt;a href="https://docs.docker.com/reference/api/docker_remote_api_v1.14/#create-a-container">/containers/create&lt;/a> call of the &lt;a href="https://docs.docker.com/reference/api/docker_remote_api/">Docker remote API&lt;/a>.
For example:&lt;/p>
&lt;pre>&lt;code>port_specs:
- 3306
- 53/udp
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>port_bindings&lt;/code> parameter is a mapping that allows you to bind
host ports to ports in the container, similar to the &lt;code>-p&lt;/code> argument to
&lt;code>docker run&lt;/code>. This corresponds to the
&lt;a href="https://docs.docker.com/reference/api/docker_remote_api_v1.14/#start-a-container">/containers/(id)/start&lt;/a> call in the &lt;a href="https://docs.docker.com/reference/api/docker_remote_api/">Docker remote API&lt;/a>.
In the mappings, the key (left-hand side) is the &lt;em>container&lt;/em> port, and
the value (right-hand side) is the &lt;em>host&lt;/em> port.&lt;/p>
&lt;p>For example, to bind container port 3306 to host port 3306:&lt;/p>
&lt;pre>&lt;code>port_bindings:
3306: 3306
&lt;/code>&lt;/pre>
&lt;p>To bind port 9090 in a container to port 80 on the host:&lt;/p>
&lt;pre>&lt;code>port_bindings:
9090: 80
&lt;/code>&lt;/pre>
&lt;p>And in theory, this should also work for UDP ports (but in practice
there is an issue between the Docker plugin and the &lt;code>docker-py&lt;/code> Python
module which makes it impossible to expose UDP ports via &lt;code>port_specs&lt;/code>;
this is fixed in
&lt;a href="https://github.com/docker/docker-py/pull/310" class="pull-request">#310&lt;/a>
on GitHub).&lt;/p>
&lt;pre>&lt;code>port_bindings:
53/udp: 5300
&lt;/code>&lt;/pre>
&lt;p>With all of this in mind, we can create a container resource
definition:&lt;/p>
&lt;pre>&lt;code>docker_dbserver:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
# here's where we set the dependency on the WaitCondition
# resource we mentioned earlier.
depends_on:
- docker_wait_condition
properties:
docker_endpoint:
str_replace:
template: &amp;quot;tcp://$HOST:2375&amp;quot;
params:
&amp;quot;$HOST&amp;quot;:
get_attr:
- docker_server_floating
- floating_ip_address
image: mysql
env:
# The official MySQL docker image expect the database root
# password to be provided in the MYSQL_ROOT_PASSWORD
# environment variable.
- str_replace:
template: MYSQL_ROOT_PASSWORD=$PASSWORD
params:
&amp;quot;$PASSWORD&amp;quot;:
get_param:
mysql_root_password
port_specs:
- 3306
port_bindings:
3306: 3306
&lt;/code>&lt;/pre>
&lt;p>Take a close look at how we&amp;rsquo;re setting the &lt;code>docker_endpoint&lt;/code> property:&lt;/p>
&lt;pre>&lt;code>docker_endpoint:
str_replace:
template: &amp;quot;tcp://$HOST:2375&amp;quot;
params:
&amp;quot;$HOST&amp;quot;:
get_attr:
- docker_server_floating
- floating_ip_address
&lt;/code>&lt;/pre>
&lt;p>This uses the &lt;code>get_attr&lt;/code> function to get the &lt;code>floating_ip_address&lt;/code>
attribute from the &lt;code>docker_server_floating&lt;/code> resource, which you can
find in the &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a>. We take the return value from that
function and use &lt;code>str_replace&lt;/code> to substitute that into the
&lt;code>docker_endpoint&lt;/code> URL.&lt;/p>
&lt;h2 id="the-pudding">The pudding&lt;/h2>
&lt;p>Using the &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a> with an appropriate local environment
file, I can launch this stack by runnign:&lt;/p>
&lt;pre>&lt;code>$ heat stack-create -f docker-server.yml -e local.env docker
&lt;/code>&lt;/pre>
&lt;p>And after a while, I can run&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
&lt;/code>&lt;/pre>
&lt;p>And see that the stack has been created successfully:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+------------+-----------------+----------------------+
| id | stack_name | stack_status | creation_time |
+--------------------------------------+------------+-----------------+----------------------+
| c0fd793e-a1f7-4b35-afa9-12ba1005925a | docker | CREATE_COMPLETE | 2014-08-31T03:01:14Z |
+--------------------------------------+------------+-----------------+----------------------+
&lt;/code>&lt;/pre>
&lt;p>And I can ask for status information on the individual resources in
the stack:&lt;/p>
&lt;pre>&lt;code>$ heat resource-list docker
+------------------------+------------------------------------------+-----------------+
| resource_name | resource_type | resource_status |
+------------------------+------------------------------------------+-----------------+
| fixed_network | OS::Neutron::Net | CREATE_COMPLETE |
| secgroup_db | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| secgroup_docker | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| secgroup_webserver | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| docker_wait_handle | AWS::CloudFormation::WaitConditionHandle | CREATE_COMPLETE |
| extrouter | OS::Neutron::Router | CREATE_COMPLETE |
| fixed_subnet | OS::Neutron::Subnet | CREATE_COMPLETE |
| secgroup_common | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| docker_server_eth0 | OS::Neutron::Port | CREATE_COMPLETE |
| extrouter_inside | OS::Neutron::RouterInterface | CREATE_COMPLETE |
| docker_server | OS::Nova::Server | CREATE_COMPLETE |
| docker_server_floating | OS::Neutron::FloatingIP | CREATE_COMPLETE |
| docker_wait_condition | AWS::CloudFormation::WaitCondition | CREATE_COMPLETE |
| docker_webserver | DockerInc::Docker::Container | CREATE_COMPLETE |
| docker_dbserver | DockerInc::Docker::Container | CREATE_COMPLETE |
+------------------------+------------------------------------------+-----------------+
&lt;/code>&lt;/pre>
&lt;p>I can run &lt;code>nova list&lt;/code> and see information about my running Nova
server:&lt;/p>
&lt;pre>&lt;code>+--------...+-----------------...+------------------------------------------------------------+
| ID ...| Name ...| Networks |
+--------...+-----------------...+------------------------------------------------------------+
| 301c5ec...| docker-docker_se...| docker-fixed_network-whp3fxhohkxk=10.0.0.2, 192.168.200.46 |
+--------...+-----------------...+------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>I can point a Docker client at the remote address and see the running
containers:&lt;/p>
&lt;pre>&lt;code>$ docker-1.2 -H tcp://192.168.200.46:2375 ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
f2388c871b20 mysql:5 /entrypoint.sh mysql 5 minutes ago Up 5 minutes 0.0.0.0:3306-&amp;gt;3306/tcp grave_almeida
9596cbe51291 larsks/simpleweb:latest /bin/sh -c '/usr/sbi 11 minutes ago Up 11 minutes 0.0.0.0:80-&amp;gt;80/tcp hungry_tesla
&lt;/code>&lt;/pre>
&lt;p>And I can point a &lt;code>mysql&lt;/code> client at the remote address and access the
database server:&lt;/p>
&lt;pre>&lt;code>$ mysql -h 192.168.200.46 -u root -psecret mysql
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
[...]
MySQL [mysql]&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="when-things-go-wrong">When things go wrong&lt;/h2>
&lt;p>Your &lt;code>heat-engine&lt;/code> log, generally &lt;code>/var/log/heat/engine.log&lt;/code>, is going
to be your best source of information if things go wrong. The &lt;code>heat stack-show&lt;/code> command will generally provide useful fault information if
your stack ends up in the &lt;code>CREATE_FAILED&lt;/code> (or &lt;code>DELETE_FAILED&lt;/code>) state.&lt;/p></content></item><item><title>Using wait conditions with Heat</title><link>https://blog.oddbit.com/post/2014-08-30-using-wait-conditions-with-hea/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-30-using-wait-conditions-with-hea/</guid><description>This post accompanies my article on the Docker plugin for Heat.
In order for WaitCondition resources to operate correctly in Heat, you will need to make sure that that you have:
Created the necessary Heat domain and administrative user in Keystone, Configured appropriate values in heat.conf for stack_user_domain, stack_domain_admin, and stack_domain_admin_password. Configured an appropriate value in heat.conf for heat_waitcondition_server_url. On a single-system install this will often be pointed by default at 127.</description><content>&lt;p>This post accompanies my &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/">article on the Docker plugin for
Heat&lt;/a>.&lt;/p>
&lt;p>In order for &lt;code>WaitCondition&lt;/code> resources to operate correctly in Heat, you
will need to make sure that that you have:&lt;/p>
&lt;ul>
&lt;li>Created the necessary Heat domain and administrative user in
Keystone,&lt;/li>
&lt;li>Configured appropriate values in &lt;code>heat.conf&lt;/code> for
&lt;code>stack_user_domain&lt;/code>, &lt;code>stack_domain_admin&lt;/code>, and
&lt;code>stack_domain_admin_password&lt;/code>.&lt;/li>
&lt;li>Configured an appropriate value in &lt;code>heat.conf&lt;/code> for
&lt;code>heat_waitcondition_server_url&lt;/code>. On a single-system install this
will often be pointed by default at &lt;code>127.0.0.1&lt;/code>, which, hopefully for
obvious reasons, won&amp;rsquo;t be of any use to your Nova servers.&lt;/li>
&lt;li>Enabled the &lt;code>heat-api-cfn&lt;/code> service,&lt;/li>
&lt;li>Configured your firewall to permit access to the CFN service (which
runs on port 8000).&lt;/li>
&lt;/ul>
&lt;p>Steve Hardy has a blog post on &lt;a href="http://hardysteven.blogspot.co.uk/2014/04/heat-auth-model-updates-part-2-stack.html">stack domain users&lt;/a> that goes into
detail on configuring authentication for Heat and Keystone.&lt;/p></content></item><item><title>nova-docker and environment variables</title><link>https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/</link><pubDate>Thu, 28 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/</guid><description>I&amp;rsquo;ve been playing with Docker a bit recently, and decided to take a look at the nova-docker driver for OpenStack.
The nova-docker driver lets Nova, the OpenStack Compute service, spawn Docker containers instead of hypervisor-based servers. For certain workloads, this leads to better resource utilization than you would get with a hypervisor-based solution, while at the same time givin you better support for multi-tenancy and flexible networking than you get with Docker by itself.</description><content>&lt;p>I&amp;rsquo;ve been playing with &lt;a href="https://docker.com/">Docker&lt;/a> a bit recently, and decided to take
a look at the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> driver for &lt;a href="http://openstack.org/">OpenStack&lt;/a>.&lt;/p>
&lt;p>The &lt;code>nova-docker&lt;/code> driver lets Nova, the OpenStack Compute service,
spawn Docker containers instead of hypervisor-based servers. For
certain workloads, this leads to better resource utilization than you
would get with a hypervisor-based solution, while at the same time
givin you better support for multi-tenancy and flexible networking
than you get with Docker by itself.&lt;/p>
&lt;p>The &lt;a href="https://wiki.openstack.org/wiki/Docker">Docker driver wiki&lt;/a> was mostly sufficient for getting the
&lt;code>nova-docker&lt;/code> driver installed in my existing OpenStack deployment,
although I did make a few &lt;a href="https://wiki.openstack.org/w/index.php?title=Docker&amp;amp;diff=61664&amp;amp;oldid=58546">small changes&lt;/a> to the wiki to reflect
some missing steps. Other than that, the installation was relatively
simple and I was soon able to spin up Docker containers using &lt;code>nova boot ...&lt;/code>&lt;/p>
&lt;p>The one problem I encountered is that it is not possible to pass
environment variable to Docker containers via the &lt;code>nova-docker&lt;/code>
driver. Many existing images (such as the &lt;a href="https://registry.hub.docker.com/_/mysql/">official MySQL image&lt;/a>)
expect configuration information to be passed in using environment
variables; for example, the &lt;code>mysql&lt;/code> image expects to be started like
this:&lt;/p>
&lt;pre>&lt;code>docker run --name some-mysql \
-e MYSQL_ROOT_PASSWORD=mysecretpassword -d mysql
&lt;/code>&lt;/pre>
&lt;p>I have proposed a &lt;a href="https://review.openstack.org/#/c/117583/">patch&lt;/a> to the &lt;code>nova-docker&lt;/code> driver that permits
one to provide environment variables via the Nova metadata service.
With this patch in place, I would start the &lt;code>mysql&lt;/code> container like
this:&lt;/p>
&lt;pre>&lt;code>nova boot --image mysql --flavor m1.small \
--meta ENV_MYSQL_ROOT_PASSWORD=mysecretpassword \
some-mysql
&lt;/code>&lt;/pre>
&lt;p>That is, the driver looks for metadata items that begin with &lt;code>ENV_&lt;/code>
and transforms these into Docker environment variables after stripping
&lt;code>ENV_&lt;/code> from the name.&lt;/p>
&lt;p>While this patch works great in my testing environment, it&amp;rsquo;s unlikely
to get accepted. Generally, the metadata provided by Nova belongs to
the tenant and is not meant to be operationally significant to the
compute driver itself.&lt;/p>
&lt;p>It sounds as if there is a lot of work going on right now regarding
container support in OpenStack, so it is very likely that a better
solution will show up in the near future.&lt;/p>
&lt;p>In the absence of that support, I hope others find this patch helpful.&lt;/p></content></item><item><title>lvcache: a tool for managing LVM caches</title><link>https://blog.oddbit.com/post/2014-08-16-lvcache-a-tool-for-managing-lv/</link><pubDate>Sat, 16 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-16-lvcache-a-tool-for-managing-lv/</guid><description>Until recently I had a bcache based setup on my laptop, but when forced by circumstance to reinstall everything I spent some time looking for alternatives that were less disruptive to configure on an existing system.
I came across Richard Jones&amp;rsquo; article discussing the recent work to integrate dm-cache into LVM. Unlike bcache and unlike using dm-cache directly, the integration with LVM makes it easy to associate devices with an existing logical volume.</description><content>&lt;p>Until recently I had a &lt;a href="http://bcache.evilpiepirate.org/">bcache&lt;/a> based setup on my laptop, but when
forced by circumstance to reinstall everything I spent some time
looking for alternatives that were less disruptive to configure on an
existing system.&lt;/p>
&lt;p>I came across &lt;a href="http://rwmj.wordpress.com/2014/05/22/using-lvms-new-cache-feature/">Richard Jones&amp;rsquo; article&lt;/a> discussing the recent work to
integrate &lt;a href="https://en.wikipedia.org/wiki/Dm-cache">dm-cache&lt;/a> into &lt;a href="http://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)">LVM&lt;/a>. Unlike &lt;em>bcache&lt;/em> and unlike using
&lt;em>dm-cache&lt;/em> directly, the integration with LVM makes it easy to
associate devices with an existing logical volume.&lt;/p>
&lt;p>I have put together a small tool called &lt;a href="https://github.com/larsks/lvcache">lvcache&lt;/a> that simplies the
process of:&lt;/p>
&lt;ul>
&lt;li>Creating and attaching cache volumes&lt;/li>
&lt;li>Detaching and removing cache volumes&lt;/li>
&lt;li>Getting cache statistics for logical volumes&lt;/li>
&lt;li>Listing the cache status of all logical volumes&lt;/li>
&lt;/ul>
&lt;p>With &lt;code>lvcache&lt;/code> installed, you can run (as root) the following command
to create a new cache volume that is 20% the size of your origin
volume and attach it to the specified origin volume:&lt;/p>
&lt;pre>&lt;code># lvcache create myvg/home
&lt;/code>&lt;/pre>
&lt;p>You can control the size of the cache LV relative to the origin
volume. To create a cache LV that is 40% the size of the origin
volume:&lt;/p>
&lt;pre>&lt;code># lvcache create -% 40 myvg/home
&lt;/code>&lt;/pre>
&lt;p>You can query &lt;code>dm-setup&lt;/code> for cache statistics with the &lt;code>status&lt;/code>
command (the &lt;code>-H&lt;/code> translates raw bytes counts into human readable
numbers with SI suffixes):&lt;/p>
&lt;pre>&lt;code># lvcache status -H myvg/home
+-----------------------+------------------+
| Field | Value |
+-----------------------+------------------+
| cached | True |
| size | 32G |
| cache_lv | home_cache |
| cache_lv_size | 6G |
| metadata_lv | home_cache_cmeta |
| metadata_lv_size | 8M |
| cache_block_size | 128 |
| cache_utilization | 0/98304 |
| cache_utilization_pct | 0.0 |
| demotions | 0 |
| dirty | 0 |
| end | 62914560 |
| features | 1 |
| md_block_size | 8 |
| md_utilization | 200/2048 |
| md_utilization_pct | 9.765625 |
| promotions | 0 |
| read_hits | 0 |
| read_misses | 0 |
| segment_type | cache |
| start | 0 |
| write_hits | 0 |
| write_misses | 0 |
+-----------------------+------------------+
&lt;/code>&lt;/pre>
&lt;p>Because &lt;code>lvcache&lt;/code> is using the &lt;a href="http://cliff.readthedocs.org/en/latest/">cliff&lt;/a> framework, it is very easy to
extract individual values from this list for graphing or monitoring
purposes:&lt;/p>
&lt;pre>&lt;code># lvcache status tank.home -f value -c md_utilization_pct
9.765625
&lt;/code>&lt;/pre>
&lt;p>Or:&lt;/p>
&lt;pre>&lt;code># lvcache status tank.home -f shell
cached=&amp;quot;True&amp;quot;
size=&amp;quot;32G&amp;quot;
cache_lv=&amp;quot;nova_cache&amp;quot;
cache_lv_size=&amp;quot;6G&amp;quot;
metadata_lv=&amp;quot;nova_cache_cmeta&amp;quot;
metadata_lv_size=&amp;quot;8M&amp;quot;
cache_block_size=&amp;quot;128&amp;quot;
cache_utilization=&amp;quot;0/98304&amp;quot;
cache_utilization_pct=&amp;quot;0.0&amp;quot;
demotions=&amp;quot;0&amp;quot;
dirty=&amp;quot;0&amp;quot;
end=&amp;quot;62914560&amp;quot;
features=&amp;quot;1&amp;quot;
md_block_size=&amp;quot;8&amp;quot;
md_utilization=&amp;quot;200/2048&amp;quot;
md_utilization_pct=&amp;quot;9.765625&amp;quot;
promotions=&amp;quot;0&amp;quot;
read_hits=&amp;quot;0&amp;quot;
read_misses=&amp;quot;0&amp;quot;
segment_type=&amp;quot;cache&amp;quot;
start=&amp;quot;0&amp;quot;
write_hits=&amp;quot;0&amp;quot;
write_misses=&amp;quot;0&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>This is a very rough tool right now, but it seems to get the job done
on my system. If you do find this useful, let me know!&lt;/p></content></item><item><title>Four ways to connect a docker container to a local network</title><link>https://blog.oddbit.com/post/2014-08-11-four-ways-to-connect-a-docker/</link><pubDate>Mon, 11 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-11-four-ways-to-connect-a-docker/</guid><description>Update (2018-03-22) Since I wrote this document back in 2014, Docker has developed the macvlan network driver. That gives you a supported mechanism for direct connectivity to a local layer 2 network. I&amp;rsquo;ve written an article about working with the macvlan driver.
This article discusses four ways to make a Docker container appear on a local network. These are not suggested as practical solutions, but are meant to illustrate some of the underlying network technology available in Linux.</description><content>&lt;p>&lt;strong>Update (2018-03-22)&lt;/strong> Since I wrote this document back in 2014,
Docker has developed the &lt;a href="https://docs.docker.com/network/macvlan/">macvlan network
driver&lt;/a>. That gives you a
&lt;em>supported&lt;/em> mechanism for direct connectivity to a local layer 2
network. I&amp;rsquo;ve &lt;a href="https://blog.oddbit.com/2018/03/12/using-docker-macvlan-networks/">written an article about working with the macvlan
driver&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>This article discusses four ways to make a Docker container appear on
a local network. These are not suggested as practical solutions, but
are meant to illustrate some of the underlying network technology
available in Linux.&lt;/p>
&lt;p>If you were actually going to use one of these solutions as anything
other than a technology demonstration, you might look to the &lt;a href="https://github.com/jpetazzo/pipework">pipework&lt;/a> script, which can automate many of these configurations.&lt;/p>
&lt;h2 id="goals-and-assumptions">Goals and Assumptions&lt;/h2>
&lt;p>In the following examples, we have a host with address 10.12.0.76 on
the 10.12.0.0/21 network. We are creating a Docker container that we
want to expose as 10.12.0.117.&lt;/p>
&lt;p>I am running Fedora 20 with Docker 1.1.2. This means, in particular,
that my &lt;code>utils-linux&lt;/code> package is recent enough to include the
&lt;a href="http://man7.org/linux/man-pages/man1/nsenter.1.html">nsenter&lt;/a> command. If you don&amp;rsquo;t have that handy, there is a
convenient Docker recipe to build it for you at &lt;a href="https://github.com/jpetazzo/nsenter">jpetazzo/nsenter&lt;/a>
on GitHub.&lt;/p>
&lt;h2 id="a-little-help-along-the-way">A little help along the way&lt;/h2>
&lt;p>In this article we will often refer to the PID of a docker container.
In order to make this convenient, drop the following into a script
called &lt;code>docker-pid&lt;/code>, place it somewhere on your &lt;code>PATH&lt;/code>, and make it
executable:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
exec docker inspect --format '{{ .State.Pid }}' &amp;quot;$@&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>This allows us to conveniently get the PID of a docker container by
name or ID:&lt;/p>
&lt;pre>&lt;code>$ docker-pid web
22041
&lt;/code>&lt;/pre>
&lt;p>In a script called &lt;code>docker-ip&lt;/code>, place the following:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
exec docker inspect --format '{{ .NetworkSettings.IPAddress }}' &amp;quot;$@&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And now we can get the ip address of a container like this:&lt;/p>
&lt;pre>&lt;code>$ docker-ip web
172.17.0.4
&lt;/code>&lt;/pre>
&lt;h2 id="using-nat">Using NAT&lt;/h2>
&lt;p>This uses the standard Docker network model combined with NAT rules on
your host to redirect inbound traffic to/outbound traffic from the
appropriate IP address.&lt;/p>
&lt;p>Assign our target address to your host interface:&lt;/p>
&lt;pre>&lt;code># ip addr add 10.12.0.117/21 dev em1
&lt;/code>&lt;/pre>
&lt;p>Start your docker container, using the &lt;code>-p&lt;/code> option to bind exposed
ports to an ip address and port on the host:&lt;/p>
&lt;pre>&lt;code># docker run -d --name web -p 10.12.0.117:80:80 larsks/simpleweb
&lt;/code>&lt;/pre>
&lt;p>With this command, Docker will set up the &lt;a href="https://docs.docker.com/articles/networking/">standard network&lt;/a> model:&lt;/p>
&lt;ul>
&lt;li>It will create a &lt;a href="http://lwn.net/Articles/232688/">veth&lt;/a> interface pair.&lt;/li>
&lt;li>Connect one end to the &lt;code>docker0&lt;/code> bridge.&lt;/li>
&lt;li>Place the other inside the container namespace as &lt;code>eth0&lt;/code>.&lt;/li>
&lt;li>Assign an ip address from the network used by the &lt;code>docker0&lt;/code> bridge.&lt;/li>
&lt;/ul>
&lt;p>Because we added &lt;code>-p 10.12.0.117:80:80&lt;/code> to our command line, Docker
will also create the following rule in the &lt;code>nat&lt;/code> table &lt;code>DOCKER&lt;/code>
chain (which is run from the &lt;code>PREROUTING&lt;/code> chain):&lt;/p>
&lt;pre>&lt;code>-A DOCKER -d 10.12.0.117/32 ! -i docker0 -p tcp -m tcp
--dport 80 -j DNAT --to-destination 172.17.0.4:80
&lt;/code>&lt;/pre>
&lt;p>This matches traffic TO our target address (&lt;code>-d 10.12.0.117/32&lt;/code>) not
originating on the &lt;code>docker0&lt;/code> bridge (&lt;code>! -i docker0&lt;/code>) destined for
&lt;code>tcp&lt;/code> port &lt;code>80&lt;/code> (&lt;code>-p tcp -m tcp --dport 80&lt;/code>). Matching traffic has
it&amp;rsquo;s destination set to the address of our docker container (&lt;code>-j DNAT --to-destination 172.17.0.4:80&lt;/code>).&lt;/p>
&lt;p>From a host elsewhere on the network, we can now access the web server
at our selected ip address:&lt;/p>
&lt;pre>&lt;code>$ curl http://10.12.0.117/hello.html
Hello world
&lt;/code>&lt;/pre>
&lt;p>If our container were to initiate a network connection with another
system, that connection would appear to originate with ip address of
our &lt;em>host&lt;/em>. We can fix that my adding a &lt;code>SNAT&lt;/code> rule to the
&lt;code>POSTROUTING&lt;/code> chain to modify the source address:&lt;/p>
&lt;pre>&lt;code># iptables -t nat -I POSTROUTING -s $(docker-ip web) \
-j SNAT --to-source 10.12.0.117
&lt;/code>&lt;/pre>
&lt;p>Note here the use of &lt;code>-I POSTROUTING&lt;/code>, which places the rule at the
&lt;em>top&lt;/em> of the &lt;code>POSTROUTING&lt;/code> chain. This is necessary because, by
default, Docker has already added the following rule to the top of the
&lt;code>POSTROUTING&lt;/code> chain:&lt;/p>
&lt;pre>&lt;code>-A POSTROUTING -s 172.17.0.0/16 ! -d 172.17.0.0/16 -j MASQUERADE
&lt;/code>&lt;/pre>
&lt;p>Because this &lt;code>MASQUERADE&lt;/code> rule matches traffic from any container, we
need to place our rule earlier in the &lt;code>POSTROUTING&lt;/code> chain for it to
have any affect.&lt;/p>
&lt;p>With these rules in place, traffic to 10.12.0.117 (port 80) is
directed to our &lt;code>web&lt;/code> container, and traffic &lt;em>originating&lt;/em> in the web
container will appear to come from 10.12.0.117.&lt;/p>
&lt;h2 id="with-linux-bridge-devices">With Linux Bridge devices&lt;/h2>
&lt;p>The previous example was relatively easy to configure, but has a few
shortcomings. If you need to configure an interface using DHCP, or if
you have an application that needs to be on the same layer 2 broadcast
domain as other devices on your network, NAT rules aren&amp;rsquo;t going to
work out.&lt;/p>
&lt;p>This solution uses a Linux bridge device, created using &lt;code>brctl&lt;/code>, to
connect your containers directly to a physical network.&lt;/p>
&lt;p>Start by creating a new bridge device. In this example, we&amp;rsquo;ll create
one called &lt;code>br-em1&lt;/code>:&lt;/p>
&lt;pre>&lt;code># brctl addbr br-em1
# ip link set br-em1 up
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re going to add &lt;code>em1&lt;/code> to this bridge, and move the ip address from
&lt;code>em1&lt;/code> onto the bridge.&lt;/p>
&lt;p>&lt;strong>WARNING&lt;/strong>: This is not something you should do remotely, especially
for the first time, and making this persistent varies from
distribution to distribution, so this will not be a persistent
configuration.&lt;/p>
&lt;p>Look at the configuration of interface &lt;code>em1&lt;/code> and note the existing ip
address:&lt;/p>
&lt;pre>&lt;code># ip addr show em1
2: em1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc mq master br-em1 state UP group default qlen 1000
link/ether 00:1d:09:63:71:30 brd ff:ff:ff:ff:ff:ff
inet 10.12.0.76/21 scope global br-em1
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>Look at your current routes and note the default route:&lt;/p>
&lt;pre>&lt;code># ip route
default via 10.12.7.254 dev em1
10.12.0.0/21 dev em1 proto kernel scope link src 10.12.0.76
&lt;/code>&lt;/pre>
&lt;p>Now, add this device to your bridge:&lt;/p>
&lt;pre>&lt;code># brctl addif br-em1 em1
&lt;/code>&lt;/pre>
&lt;p>Configure the bridge with the address that used to belong to
&lt;code>em1&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ip addr del 10.12.0.76/21 dev em1
# ip addr add 10.12.0.76/21 dev br-em1
&lt;/code>&lt;/pre>
&lt;p>And move the default route to the bridge:&lt;/p>
&lt;pre>&lt;code># ip route del default
# ip route add default via 10.12.7.254 dev br-em1
&lt;/code>&lt;/pre>
&lt;p>If you were doing this remotely; you would do this all in one line
like this:&lt;/p>
&lt;pre>&lt;code># ip addr add 10.12.0.76/21 dev br-em1; \
ip addr del 10.12.0.76/21 dev em1; \
brctl addif br-em1 em1; \
ip route del default; \
ip route add default via 10.12.7.254 dev br-em1
&lt;/code>&lt;/pre>
&lt;p>At this point, verify that you still have network connectivity:&lt;/p>
&lt;pre>&lt;code># curl http://google.com/
&amp;lt;HTML&amp;gt;&amp;lt;HEAD&amp;gt;&amp;lt;meta http-equiv=&amp;quot;content-type&amp;quot; content=&amp;quot;text/html;charset=utf-8&amp;quot;&amp;gt;
[...]
&lt;/code>&lt;/pre>
&lt;p>Start up the web container:&lt;/p>
&lt;pre>&lt;code># docker run -d --name web larsks/simpleweb
&lt;/code>&lt;/pre>
&lt;p>This will give us the normal &lt;code>eth0&lt;/code> interface inside the container,
but we&amp;rsquo;re going to ignore that and add a new one.&lt;/p>
&lt;p>Create a &lt;a href="http://lwn.net/Articles/232688/">veth&lt;/a> interface pair:&lt;/p>
&lt;pre>&lt;code># ip link add web-int type veth peer name web-ext
&lt;/code>&lt;/pre>
&lt;p>Add the &lt;code>web-ext&lt;/code> link to the &lt;code>br-eth0&lt;/code> bridge:&lt;/p>
&lt;pre>&lt;code># brctl addif br-em1 web-ext
&lt;/code>&lt;/pre>
&lt;p>And add the &lt;code>web-int&lt;/code> interface to the namespace of the container:&lt;/p>
&lt;pre>&lt;code># ip link set netns $(docker-pid web) dev web-int
&lt;/code>&lt;/pre>
&lt;p>Next, we&amp;rsquo;ll use the &lt;a href="http://man7.org/linux/man-pages/man1/nsenter.1.html">nsenter&lt;/a> command (part of the &lt;code>util-linux&lt;/code> package) to run some commands inside the &lt;code>web&lt;/code> container. Start by bringing up the link inside the container:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip link set web-int up
&lt;/code>&lt;/pre>
&lt;p>Assign our target ip address to the interface:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip addr add 10.12.0.117/21 dev web-int
&lt;/code>&lt;/pre>
&lt;p>And set a new default route inside the container:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip route del default
# nsenter -t $(docker-pid web) -n ip route add default via 10.12.7.254 dev web-int
&lt;/code>&lt;/pre>
&lt;p>Again, we can verify from another host that the web server is
available at 10.12.0.117:&lt;/p>
&lt;pre>&lt;code>$ curl http://10.12.0.117/hello.html
Hello world
&lt;/code>&lt;/pre>
&lt;p>Note that in this example we have assigned a static ip address, but we
could just have easily acquired an address using DHCP. After running:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip link set web-int up
&lt;/code>&lt;/pre>
&lt;p>We can run:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n -- dhclient -d web-int
Internet Systems Consortium DHCP Client 4.2.6
Copyright 2004-2014 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/
Listening on LPF/web-int/6e:f0:a8:c6:f0:43
Sending on LPF/web-int/6e:f0:a8:c6:f0:43
Sending on Socket/fallback
DHCPDISCOVER on web-int to 255.255.255.255 port 67 interval 4 (xid=0x3aaab45b)
DHCPREQUEST on web-int to 255.255.255.255 port 67 (xid=0x3aaab45b)
DHCPOFFER from 10.12.7.253
DHCPACK from 10.12.7.253 (xid=0x3aaab45b)
bound to 10.12.6.151 -- renewal in 714 seconds.
&lt;/code>&lt;/pre>
&lt;h2 id="with-open-vswitch-bridge-devices">With Open vSwitch Bridge devices&lt;/h2>
&lt;p>This process is largely the same as in the previous example, but we
use &lt;a href="http://openvswitch.org/">Open vSwitch&lt;/a> instead of the legacy Linux bridge devices.
These instructions assume that you have already installed and started
Open vSwitch on your system.&lt;/p>
&lt;p>Create an OVS bridge using the &lt;code>ovs-vsctl&lt;/code> command:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-br br-em1
# ip link set br-em1 up
&lt;/code>&lt;/pre>
&lt;p>And add your external interface:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-port br-em1 em1
&lt;/code>&lt;/pre>
&lt;p>And then proceed as in the previous set of instructions.&lt;/p>
&lt;p>The equivalent all-in-one command is:&lt;/p>
&lt;pre>&lt;code># ip addr add 10.12.0.76/21 dev br-em1; \
ip addr del 10.12.0.76/21 dev em1; \
ovs-vsctl add-port br-em1 em1; \
ip route del default; \
ip route add default via 10.12.7.254 dev br-em1
&lt;/code>&lt;/pre>
&lt;p>Once that completes, your openvswitch configuration should look like
this:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl show
0b1d5895-88e6-42e5-a1da-ad464c75198c
Bridge &amp;quot;br-em1&amp;quot;
Port &amp;quot;br-em1&amp;quot;
Interface &amp;quot;br-em1&amp;quot;
type: internal
Port &amp;quot;em1&amp;quot;
Interface &amp;quot;em1&amp;quot;
ovs_version: &amp;quot;2.1.2&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>To add the &lt;code>web-ext&lt;/code> interface to the bridge, run:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-port br-em1 web-ext
&lt;/code>&lt;/pre>
&lt;p>Instead of:&lt;/p>
&lt;pre>&lt;code># brctl addif br-em1 web-ext
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>WARNING&lt;/strong>: The Open vSwitch configuration persists between reboots.
This means that when your system comes back up, &lt;code>em1&lt;/code> will still be a
member of &lt;code>br-em&lt;/code>, which will probably result in no network
connectivity for your host.&lt;/p>
&lt;p>Before rebooting your system, make sure to &lt;code>ovs-vsctl del-port br-em1 em1&lt;/code>.&lt;/p>
&lt;h2 id="with-macvlan-devices">With macvlan devices&lt;/h2>
&lt;p>This process is similar to the previous two, but instead of using a
bridge device we will create a &lt;a href="http://backreference.org/2014/03/20/some-notes-on-macvlanmacvtap/">macvlan&lt;/a>, which is a virtual network
interface associated with a physical interface. Unlike the previous
two solutions, this does not require any interruption to your primary
network interface.&lt;/p>
&lt;p>Start by creating a docker container as in the previous examples:&lt;/p>
&lt;pre>&lt;code># docker run -d --name web larsks/simpleweb
&lt;/code>&lt;/pre>
&lt;p>Create a &lt;code>macvlan&lt;/code> interface associated with your physical interface:&lt;/p>
&lt;pre>&lt;code># ip link add em1p0 link em1 type macvlan mode bridge
&lt;/code>&lt;/pre>
&lt;p>This creates a new &lt;code>macvlan&lt;/code> interface named &lt;code>em1p0&lt;/code> (but you can
name it anything you want) associated with interface &lt;code>em1&lt;/code>. We are
setting it up in &lt;code>bridge&lt;/code> mode, which permits all &lt;code>macvlan&lt;/code> interfaces
to communicate with eachother.&lt;/p>
&lt;p>Add this interface to the container&amp;rsquo;s network namespace:&lt;/p>
&lt;pre>&lt;code># ip link set netns $(docker-pid web) em1p0
&lt;/code>&lt;/pre>
&lt;p>Bring up the link:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip link set em1p0 up
&lt;/code>&lt;/pre>
&lt;p>And configure the ip address and routing:&lt;/p>
&lt;pre>&lt;code># nsenter -t $(docker-pid web) -n ip route del default
# nsenter -t $(docker-pid web) -n ip addr add 10.12.0.117/21 dev em1p0
# nsenter -t $(docker-pid web) -n ip route add default via 10.12.7.254 dev em1p0
&lt;/code>&lt;/pre>
&lt;p>And demonstrate that &lt;em>from another host&lt;/em> the web server is available
at 10.12.0.117:&lt;/p>
&lt;pre>&lt;code>$ curl http://10.12.0.117/hello.html
Hello world
&lt;/code>&lt;/pre>
&lt;p>But note that if you were to try the same thing on the host, you would
get:&lt;/p>
&lt;pre>&lt;code>curl: (7) Failed connect to 10.12.0.117:80; No route to host
&lt;/code>&lt;/pre>
&lt;p>The &lt;em>host&lt;/em> is unable to communicate with &lt;code>macvlan&lt;/code> devices via the
primary interface. You can create &lt;em>another&lt;/em> &lt;code>macvlan&lt;/code> interface on
the host, give it an address on the appropriate network, and then set
up routes to your containers via that interface:&lt;/p>
&lt;pre>&lt;code># ip link add em1p1 link em1 type macvlan mode bridge
# ip addr add 10.12.6.144/21 dev em1p1
# ip route add 10.12.0.117 dev em1p1
&lt;/code>&lt;/pre></content></item><item><title>gpio-watch: Run scripts in response to GPIO signals</title><link>https://blog.oddbit.com/post/2014-07-26-gpiowatch-run-scripts-in-respo/</link><pubDate>Sat, 26 Jul 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-07-26-gpiowatch-run-scripts-in-respo/</guid><description>For a small project I&amp;rsquo;m working on I needed to attach a few buttons to a Raspberry Pi and have some code execute in response to the button presses.
Normally I would reach for Python for a simple project like this, but constraints of the project made it necessary to implement something in C with minimal dependencies. I didn&amp;rsquo;t want to write something that was tied closely to my project&amp;hellip;</description><content>&lt;p>For a small project I&amp;rsquo;m working on I needed to attach a few buttons to
a &lt;a href="http://raspberrypi.org/">Raspberry Pi&lt;/a> and have some code execute in response to the
button presses.&lt;/p>
&lt;p>Normally I would reach for &lt;a href="http://python.org/">Python&lt;/a> for a simple project like this,
but constraints of the project made it necessary to implement
something in C with minimal dependencies. I didn&amp;rsquo;t want to write
something that was tied closely to my project&amp;hellip;&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;figure class="left" >
&lt;img src="http://imgs.xkcd.com/comics/the_general_problem.png" />
&lt;/figure>
&lt;p>&amp;hellip;so I ended up writing &lt;a href="https://github.com/larsks/gpio-watch">gpio-watch&lt;/a>, a simple tool for connecting
shell scripts (or any other executable) to GPIO events. There are a
few ways to interact with GPIO on the Raspberry Pi. For the fastest
possible performance, you will need to interact directly with the
underlying hardware using, e.g., something like &lt;a href="http://hertaville.com/2014/07/07/rpimmapgpio/">direct register
access&lt;/a>. Since I was only responding to button presses I opted
to take advantage of the &lt;a href="https://www.kernel.org/doc/Documentation/gpio/sysfs.txt">GPIO sysfs interface&lt;/a>, which exposes
the GPIO pins via the filesystem.&lt;/p>
&lt;p>To access a GPIO pin using the &lt;code>sysfs&lt;/code> interface:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You write the GPIO number to &lt;code>/sys/class/gpio/export&lt;/code>. This will
result in a new directory named &lt;code>gpio&amp;lt;pin&amp;gt;&lt;/code> appearing in
&lt;code>/sys/class/gpio&lt;/code> (where &lt;code>&amp;lt;pin&amp;gt;&lt;/code> is the GPIO number you have exported).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Inside &lt;code>/sys/class/gpio/gpio&amp;lt;pin&amp;gt;&lt;/code>, there are a number of files:&lt;/p>
&lt;ul>
&lt;li>&lt;code>direction&lt;/code> is used to configure the GPIO as an input (write &lt;code>in&lt;/code>)
or output (write &lt;code>out&lt;/code>).&lt;/li>
&lt;li>&lt;code>edge&lt;/code> is used to control which edge of a signal generates
interrupts. The options are &lt;code>rising&lt;/code>, &lt;code>falling&lt;/code>, &lt;code>both&lt;/code>, or
&lt;code>none&lt;/code>.&lt;/li>
&lt;li>&lt;code>value&lt;/code> contains the current value of the GPIO pin.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Once you have properly configure a pin, you can monitor the &lt;code>value&lt;/code>
file for events (see below).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>We can use the &lt;code>poll()&lt;/code> or &lt;code>select()&lt;/code> system calls to monitor events
on &lt;code>/sys/class/gpio/gpio&amp;lt;pin&amp;gt;/value&lt;/code>. For example, to wait for a signal
on GPIO 23 (assuming that we have correctly configured the &lt;code>direction&lt;/code>
and &lt;code>edge&lt;/code> values):&lt;/p>
&lt;pre>&lt;code>#include &amp;lt;poll.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
void poll_pin() {
struct pollfd fdlist[1];
int fd;
fd = open(&amp;quot;/sys/class/gpio/gpio23/value&amp;quot;, O_RDONLY);
fdlist[0].fd = fd;
fdlist[0].events = POLLPRI;
while (1) {
int err;
char buf[3];
err = poll(fdlist, 1, -1);
if (-1 == err) {
perror(&amp;quot;poll&amp;quot;);
return;
}
err = read(fdlist[0].fd, buf, 2);
printf(&amp;quot;event on pin 23!\n&amp;quot;);
}
}
int main(int argc, char *argv[]) {
poll_pin();
}
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>gpio-watch&lt;/code> command wraps this all up in a convenient package
that lets you do something like this:&lt;/p>
&lt;pre>&lt;code>gpio-watch -e rising 18 23 24
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-e rising&lt;/code> option means that we are watching for rising signals
on all three pins. You can also trigger on different parts of the
signal for each pin:
gpio-watch 18:rising 23:both 24:falling&lt;/p>
&lt;p>When &lt;code>gpio-watch&lt;/code> sees an event on a pin, it looks for
&lt;code>/etc/gpio-scripts/&amp;lt;pin&amp;gt;&lt;/code> (e.g., &lt;code>/etc/gpio-scripts/23&lt;/code>), and then runs:&lt;/p>
&lt;pre>&lt;code>/etc/gpio-scripts/&amp;lt;pin&amp;gt; &amp;lt;pin&amp;gt; &amp;lt;value&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Since the script is passed the pin number as the first argument, you
can use a single script to handle events on multiple pins (by
symlinking the script to the appropriate name).&lt;/p>
&lt;h2 id="mechanical-switches">Mechanical switches&lt;/h2>
&lt;p>There is some special code in &lt;code>gpio-watch&lt;/code> for handling mechanical
buttons. The &lt;code>switch&lt;/code> edge mode&amp;hellip;&lt;/p>
&lt;pre>&lt;code>gpio-watch 23:switch
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;enables some simple &lt;a href="https://en.wikipedia.org/wiki/Switch#Contact_bounce">de-bouncing&lt;/a> logic. This causes
&lt;code>gpio-watch&lt;/code> to monitor both rising and falling events on this pin,
but the events scripts will only trigger on the falling edge event,
which must occur more than &lt;code>DEBOUNCE_INTERVAL&lt;/code> after the rising edge
event. In other words, you must both press and release the button for
the event to fire, and the debounce logic should avoid firing the
event multiple times due to contact bounce.&lt;/p>
&lt;p>As an example, assume we have a script &lt;code>/etc/gpio-scripts/23&lt;/code> that
looks like this:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
echo &amp;quot;Something happened! Pin=$1, value=$2&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>If I run &lt;code>gpio-watch&lt;/code> to monitor the falling signal edge and press a
button attached to pin 23 three times, I see:&lt;/p>
&lt;pre>&lt;code>$ gpio-watch 23:falling
Something happened! Pin=23, value=0
Something happened! Pin=23, value=0
Something happened! Pin=23, value=0
Something happened! Pin=23, value=1
Something happened! Pin=23, value=0
&lt;/code>&lt;/pre>
&lt;p>Whereas if I use &lt;code>switch&lt;/code> mode, I see:&lt;/p>
&lt;pre>&lt;code>$ gpio-watch 23:switch
Something happened! Pin=23, value=0
Something happened! Pin=23, value=0
Something happened! Pin=23, value=0
&lt;/code>&lt;/pre>
&lt;h2 id="use-the-source-luke">Use the source, Luke!&lt;/h2>
&lt;p>The source is available &lt;a href="https://github.com/larsks/gpio-watch">on gitub&lt;/a>. To get started, clone
the repository with &lt;code>git&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/larsks/gpio-watch.git
&lt;/code>&lt;/pre>
&lt;p>And then build the source using &lt;code>make&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ cd gpio-watch
$ make
cc -c -o main.o main.c
cc -c -o gpio.o gpio.c
cc -c -o fileutil.o fileutil.c
cc -c -o logging.o logging.c
cc -o gpio-watch main.o gpio.o fileutil.o logging.o -lrt
&lt;/code>&lt;/pre>
&lt;p>There is basic documentation in &lt;code>README.md&lt;/code> in the distribution. If
you run into any problems, feel free to &lt;a href="https://github.com/larsks/gpio-watch/issues/new">open a new issue&lt;/a>.&lt;/p></content></item><item><title>Tracking down a kernel bug with git bisect</title><link>https://blog.oddbit.com/post/2014-07-21-tracking-down-a-kernel-bug-wit/</link><pubDate>Mon, 21 Jul 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-07-21-tracking-down-a-kernel-bug-wit/</guid><description>After a recent upgrade of my Fedora 20 system to kernel 3.15.mumble, I started running into a problem (BZ 1121345) with my Docker containers. Operations such as su or runuser would fail with the singularly unhelpful System error message:
$ docker run -ti fedora /bin/bash bash-4.2# su -c 'uptime' su: System error Hooking up something (like, say, socat unix-listen:/dev/log -) to /dev/log revealed that the system was logging:
Jul 19 14:31:18 su: PAM audit_log_acct_message() failed: Operation not permitted Downgrading the kernel to 3.</description><content>&lt;p>After a recent upgrade of my Fedora 20 system to kernel 3.15.mumble, I
started running into a problem (&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1121345">BZ 1121345&lt;/a>) with my &lt;a href="https://www.docker.com/">Docker&lt;/a>
containers. Operations such as &lt;code>su&lt;/code> or &lt;code>runuser&lt;/code> would fail with the
singularly unhelpful &lt;code>System error&lt;/code> message:&lt;/p>
&lt;pre>&lt;code>$ docker run -ti fedora /bin/bash
bash-4.2# su -c 'uptime'
su: System error
&lt;/code>&lt;/pre>
&lt;p>Hooking up something (like, say, &lt;code>socat unix-listen:/dev/log -&lt;/code>) to
&lt;code>/dev/log&lt;/code> revealed that the system was logging:&lt;/p>
&lt;pre>&lt;code>Jul 19 14:31:18 su: PAM audit_log_acct_message() failed: Operation not permitted
&lt;/code>&lt;/pre>
&lt;p>Downgrading the kernel to 3.14 immediately resolved the problem,
suggesting that this was at least partly a kernel issue. This seemed
like a great opportunity to play with the &lt;a href="http://git-scm.com/docs/git-bisect">git bisect&lt;/a> command,
which uses a binary search to find which commit introduced a
particular problem.&lt;/p>
&lt;p>Unfortunately, between the version I knew to work correctly (3.14) and
the version I knew to have a problem (3.15) there were close to 15,000
commits, which seemed like a large space to search by hand.&lt;/p>
&lt;p>Fortunately, &lt;code>git bisect&lt;/code> can be easily automated via &lt;code>git bisect run&lt;/code>
subcommand, which after checking out a commit will run a script to
determine if the current commit is &amp;ldquo;good&amp;rdquo; or &amp;ldquo;bad&amp;rdquo;. So all I have to
do is write a script&amp;hellip;that&amp;rsquo;s not so bad!&lt;/p>
&lt;figure class="left" >
&lt;img src="ha-ha.jpg" />
&lt;/figure>
&lt;p>It actually ended up being somewhat tricky.&lt;/p>
&lt;h2 id="testing-kernels-is-hard">Testing kernels is hard&lt;/h2>
&lt;p>In order to test for this problem, I would need to use arbitrary
kernels generated during the &lt;code>git bisect&lt;/code> operation to boot a system
functional enough to run docker, and then run docker and somehow
communicate the result of that test back to the build environment.&lt;/p>
&lt;p>I started with the &lt;a href="http://fedoraproject.org/get-fedora#clouds">Fedora 20 cloud image&lt;/a>, which is nice and
small but still the same platform as my laptop on which I was
experiencing the problem. I would need to correct a few things before
moving forward:&lt;/p>
&lt;p>The Fedora cloud images (a) do not support password authentication and
(b) expect a datasource to be available to &lt;a href="http://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> (without
which you get errors on the console and potentially a delay waiting
for the &lt;code>login:&lt;/code> prompt), so prior to using the image in this test I
made some changes by mounting it locally:&lt;/p>
&lt;pre>&lt;code># modprobe nbd max_part=8
# qemu-nbd -c /dev/nbd0 Fedora-x86_64-20-20140407-sda.qcow2
# mount /dev/nbd0p1 /mnt
# systemd-nspawn -D /mnt
&lt;/code>&lt;/pre>
&lt;p>And then:&lt;/p>
&lt;ul>
&lt;li>I set a password for the &lt;code>root&lt;/code> account and&lt;/li>
&lt;li>I removed the &lt;code>cloud-init&lt;/code> package.&lt;/li>
&lt;/ul>
&lt;p>For this test I would be using the &lt;code>qemu-system-x86_64&lt;/code> command
directly, rather than working through &lt;code>libvirt&lt;/code> (&lt;code>qemu&lt;/code> has options
for convenient debugging with &lt;code>gdb&lt;/code>, and is also able to access the
filesystem as the calling &lt;code>uid&lt;/code> whereas &lt;code>libvirt&lt;/code> is typically running
as another user).&lt;/p>
&lt;p>I would need to perform an initial &lt;code>docker pull&lt;/code> in the image, which
meant I was going to need a functioning network, so first I had to set
up a network environment for qemu.&lt;/p>
&lt;h3 id="network-configuration">Network configuration&lt;/h3>
&lt;p>I created a bridge interface named &lt;code>qemu0&lt;/code> to be used by &lt;code>qemu&lt;/code>. I added
to &lt;code>/etc/sysconfig/network-scripts/ifcfg-qemu0&lt;/code> the following:&lt;/p>
&lt;pre>&lt;code>DEVICE=qemu0
TYPE=Bridge
ONBOOT=yes
BOOTPROTO=none
STP=no
NAME=&amp;quot;Bridge qemu0&amp;quot;
IPADDR=192.168.210.1
NETMASK=255.255.255.0
&lt;/code>&lt;/pre>
&lt;p>This is largely equivalent to the following, but persists after reboot:&lt;/p>
&lt;pre>&lt;code>brctl addbr qemu0
ip addr add 192.168.210.1/24 dev qemu0
ip link set qemu0 up
&lt;/code>&lt;/pre>
&lt;p>I created a &lt;a href="https://www.kernel.org/doc/Documentation/networking/tuntap.txt">tap&lt;/a> interface named &lt;code>linux0&lt;/code>:&lt;/p>
&lt;pre>&lt;code>ip tuntap add dev linux0 mode tap user lars
&lt;/code>&lt;/pre>
&lt;p>And added it to the bridge:&lt;/p>
&lt;pre>&lt;code>brctl addif qemu0 linux0
&lt;/code>&lt;/pre>
&lt;p>I also started up &lt;code>dnsmasq&lt;/code> process listening on &lt;code>qemu0&lt;/code> to provide
DNS lookup and DHCP service to qemu instances attached to this bridge.
The &lt;code>dnsmasq&lt;/code> configuration looked like this:&lt;/p>
&lt;pre>&lt;code>listen-address=192.168.210.1
bind-interfaces
dhcp-range=192.168.210.10,192.168.210.254
&lt;/code>&lt;/pre>
&lt;h3 id="running-qemu">Running qemu&lt;/h3>
&lt;p>With the network environment set up, I needed to figure out an
appropriate qemu command line. This is what I finally ended up with,
in a script called &lt;code>boot-kernel&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
qemu-system-x86_64 -m 1024M \
-drive file=fedora.img,if=virtio \
-append &amp;quot;console=hvc0 root=/dev/vda1 selinux=0 $BOOT_ARGS&amp;quot; \
-initrd initrd.img \
-kernel arch/x86_64/boot/bzImage \
-machine accel=kvm \
-netdev tap,id=net0,ifname=linux0,script=no,downscript=no \
-device virtio-net,netdev=net0,mac=52:54:00:c0:ff:ee \
-chardev stdio,id=stdio,mux=on \
-device virtio-serial-pci \
-device virtconsole,chardev=stdio \
-mon chardev=stdio \
-fsdev local,id=fs0,path=$PWD,security_model=none \
-device virtio-9p-pci,fsdev=fs0,mount_tag=kernel_src \
-display none \
$QEMU_ARGS
&lt;/code>&lt;/pre>
&lt;p>These lines set up the networking:&lt;/p>
&lt;pre>&lt;code> -netdev tap,id=net0,ifname=linux0,script=no,downscript=no \
-device virtio-net,netdev=net0,mac=52:54:00:c0:ff:ee \
&lt;/code>&lt;/pre>
&lt;p>These lines set up console on &lt;code>stdin&lt;/code>/&lt;code>stdout&lt;/code> and multiplex the
console with the qemu monitor:&lt;/p>
&lt;pre>&lt;code> -chardev stdio,id=stdio,mux=on \
-device virtio-serial-pci \
-device virtconsole,chardev=stdio \
-mon chardev=stdio \
&lt;/code>&lt;/pre>
&lt;p>These lines set up access to the current working directory as a &lt;code>9p&lt;/code>
filesystem:&lt;/p>
&lt;pre>&lt;code> -fsdev local,id=fs0,path=$PWD,security_model=none \
-device virtio-9p-pci,fsdev=fs0,mount_tag=kernel_src \
&lt;/code>&lt;/pre>
&lt;p>Within the qemu instance, this lets me access my working directory with:&lt;/p>
&lt;pre>&lt;code>mount -t 9p kernel_src /mnt
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>$BOOT_ARGS&lt;/code> and &lt;code>$QEMU_ARGS&lt;/code> in the script allow me to modify the
behavior of the script by setting environment variables when calling
it, like this:&lt;/p>
&lt;pre>&lt;code>QEMU_ARGS=&amp;quot;-s&amp;quot; sh boot-kernel
&lt;/code>&lt;/pre>
&lt;h3 id="first-boot">First boot&lt;/h3>
&lt;p>I tried to boot the image using my existing kernel and initrd from
&lt;code>/boot&lt;/code>, and ran into a problem:&lt;/p>
&lt;pre>&lt;code>[ 184.060756] dracut-initqueue[218]: Warning: Could not boot.
[ 184.062855] dracut-initqueue[218]: Warning: /dev/ssd/root does not exist
Starting Dracut Emergency Shell...
Warning: /dev/ssd/root does not exist
Generating &amp;quot;/run/initramfs/rdsosreport.txt&amp;quot;
Entering emergency mode. Exit the shell to continue.
&lt;/code>&lt;/pre>
&lt;p>The what now? &lt;code>/dev/ssd/root&lt;/code> is the root device for my host system,
but wasn&amp;rsquo;t anywhere in the kernel command line I used when booting
qemu. It turns out that this was embedded in the initrd image in
&lt;code>/etc/cmdline.d/90lvm.conf&lt;/code>. After removing that file from the
image&amp;hellip;&lt;/p>
&lt;pre>&lt;code># mkdir initrd
# cd initrd
# zcat /boot/initramfs-3.15.6-200.fc20.x86_64.img | cpio -id
# rm -rf etc/cmdline.d
# find . -print | cpio -o -Hcrc | gzip &amp;gt; ../initrd.img
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;I was able to boot successfully and log in.&lt;/p>
&lt;h3 id="i-bet-you-thought-we-were-done">I bet you thought we were done!&lt;/h3>
&lt;p>Modern systems are heavily modular. Without access to a module tree
matching the kernel, I would be unable to successfully boot the
system, let alone use Docker. Looking at which modules were loaded
when I ran &lt;code>docker&lt;/code> with the above image, I set up a custom kernel
configuration that would permit me to boot and run docker without
requiring any loadable modules. This would allow me to use the same
image for each kernel without needing to re-populate it with modules
each time I built a kernel.&lt;/p>
&lt;p>The kernel configuration I ended up with is available &lt;a href="kernel-config.txt">here&lt;/a>.&lt;/p>
&lt;h3 id="testing-docker">Testing docker&lt;/h3>
&lt;p>The last step in this process is putting together something that tests
&lt;code>docker&lt;/code> and exposes the result of that test to the build environment.
I added the following script to the image as &lt;code>/root/docker-test&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
grep NO_DOCKER_TEST /proc/cmdline &amp;amp;&amp;amp; exit 0
if [ -d /mnt/test_result ]; then
docker run --rm -i fedora sh -c 'su -c true &amp;amp;&amp;amp; echo OKAY || echo FAILED' \
&amp;gt; /mnt/test_result/stdout \
2&amp;gt; /mnt/test_result/stderr
poweroff
fi
&lt;/code>&lt;/pre>
&lt;p>This relies on the following entry in &lt;code>/etc/fstab&lt;/code>:&lt;/p>
&lt;pre>&lt;code>kernel_src /mnt 9p defaults 0 0
&lt;/code>&lt;/pre>
&lt;p>That mounts the build directory as a &lt;code>9p&lt;/code> filesystem on &lt;code>/mnt&lt;/code>. This
allows us to write out test results to, e.g.,
&lt;code>/mnt/test_result/stdout&lt;/code> and have that appear in the &lt;code>test_result&lt;/code>
directory inside the kernel source.&lt;/p>
&lt;p>This script is run at the end of the boot process via an entry in
&lt;code>/etc/rc.d/rc.local&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
sh /root/docker-test
&lt;/code>&lt;/pre>
&lt;p>Running the &lt;code>boot-kernel&lt;/code> script without additional configuration will
cause the image to boot up, run the docker test, and then exit.&lt;/p>
&lt;h2 id="running-git-bisect">Running git-bisect&lt;/h2>
&lt;p>At this point we have just about everything we need to start running
&lt;code>git bisect&lt;/code>. For the initial run, I&amp;rsquo;m going to use git tag &lt;code>v3.14&lt;/code>
as the &amp;ldquo;known good&amp;rdquo; commit and &lt;code>v3.15&lt;/code> as the &amp;ldquo;known bad&amp;rdquo; commit, so
we start &lt;code>git bisect&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code>$ git bisect start v3.15 v3.14
&lt;/code>&lt;/pre>
&lt;p>Then we run &lt;code>git bisect run sh bisect-test&lt;/code>, where &lt;code>bisect-test&lt;/code> is
the following shell script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
# Rebuild the kernel
make olddefconfig
make -j8
# Clear out old test results and run the test
rm -f test_result/{stdout,stderr}
sh boot-kernel
# Report results to git-bisect
if grep OKAY test_result/stdout; then
exit 0
else
exit 1
fi
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then we go out for a cup of coffee or something, because that&amp;rsquo;s
going to take a while.&lt;/p>
&lt;h2 id="keep-digging-watson">Keep digging, Watson&lt;/h2>
&lt;p>The initial run of &lt;code>git bisect&lt;/code> narrowed the change down to the
&lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b7d3622">following commit&lt;/a>:&lt;/p>
&lt;pre>&lt;code>commit b7d3622a39fde7658170b7f3cf6c6889bb8db30d
Merge: f3411cb d8ec26d
Author: Eric Paris &amp;lt;eparis@redhat.com&amp;gt;
Date: Fri Mar 7 11:41:32 2014 -0500
Merge tag 'v3.13' into for-3.15
Linux 3.13
Conflicts:
include/net/xfrm.h
Simple merge where v3.13 removed 'extern' from definitions and the audit
tree did s/u32/unsigned int/ to the same definitions.
&lt;/code>&lt;/pre>
&lt;p>As you can see (from the &lt;code>Merge:&lt;/code> header), this is a merge commit, in
which an entire set of changes was joined into the &lt;code>master&lt;/code> branch.
So while this commit is technically the first commit in which this
problem appears in the &lt;code>master&lt;/code> branch&amp;hellip;it is not actually the commit
that introduced the problem.&lt;/p>
&lt;p>I was in luck, though, because looking at the history for the left
side of this branch (starting with &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f3411cb">f3411cb&lt;/a>) showed a series of
patches to the audit subsystem:&lt;/p>
&lt;pre>&lt;code>$ git log --oneline f3411cb
f3411cb audit: whitespace fix in kernel-parameters.txt
8626877 audit: fix location of __net_initdata for audit_net_ops
4f06632 audit: remove pr_info for every network namespace
262fd3a audit: Modify a set of system calls in audit class definitions
3e1d0bb audit: Convert int limit uses to u32
d957f7b audit: Use more current logging style
b8dbc32 audit: Use hex_byte_pack_upper
06bdadd audit: correct a type mismatch in audit_syscall_exit()
1ce319f audit: reorder AUDIT_TTY_SET arguments
0e23bac audit: rework AUDIT_TTY_SET to only grab spin_lock once
3f0c5fa audit: remove needless switch in AUDIT_SET
70249a9 audit: use define's for audit version
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;etc.&lt;/p>
&lt;p>I picked as a starting point the merge commit previous to &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f3411cb">f3411cb&lt;/a>:&lt;/p>
&lt;pre>&lt;code>$ git log --merges -1
commit fc582aef7dcc27a7120cf232c1e76c569c7b6eab
Merge: 9175c9d 5e01dc7
Author: Eric Paris &amp;lt;eparis@redhat.com&amp;gt;
Date: Fri Nov 22 18:57:08 2013 -0500
Merge tag 'v3.12'
Linux 3.12
Conflicts:
fs/exec.c
&lt;/code>&lt;/pre>
&lt;p>And ran &lt;code>git bisect&lt;/code> again from &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=fc582ae">that commit&lt;/a> through to &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f3411cb">f3411cb&lt;/a>:&lt;/p>
&lt;pre>&lt;code>$ git bisect start f3411cb fc582ae
$ git bisect run sh bisect-test
&lt;/code>&lt;/pre>
&lt;p>Which ultimately ended up with &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=33faba7">this commit&lt;/a>:&lt;/p>
&lt;pre>&lt;code>33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb is the first bad commit
commit 33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb
Author: Richard Guy Briggs &amp;lt;rgb@redhat.com&amp;gt;
Date: Tue Jul 16 13:18:45 2013 -0400
audit: listen in all network namespaces
Convert audit from only listening in init_net to use register_pernet_subsys()
to dynamically manage the netlink socket list.
Signed-off-by: Richard Guy Briggs &amp;lt;rgb@redhat.com&amp;gt;
Signed-off-by: Eric Paris &amp;lt;eparis@redhat.com&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Running &lt;code>git bisect log&lt;/code> shows us what revisions were checked as part
of this process:&lt;/p>
&lt;pre>&lt;code># bad: [f3411cb2b2e396a41ed3a439863f028db7140a34] audit: whitespace fix in kernel-parameters.txt
# good: [fc582aef7dcc27a7120cf232c1e76c569c7b6eab] Merge tag 'v3.12'
git bisect start 'f3411cb' 'fc582ae'
# bad: [ff235f51a138fc61e1a22dcb8b072d9c78c2a8cc] audit: Added exe field to audit core dump signal log
git bisect bad ff235f51a138fc61e1a22dcb8b072d9c78c2a8cc
# bad: [51cc83f024ee51de9da70c17e01ec6de524f5906] audit: add audit_backlog_wait_time configuration option
git bisect bad 51cc83f024ee51de9da70c17e01ec6de524f5906
# bad: [ae887e0bdcddb9d7acd8f1eb7b7795b438aa4950] audit: make use of remaining sleep time from wait_for_auditd
git bisect bad ae887e0bdcddb9d7acd8f1eb7b7795b438aa4950
# good: [2f2ad1013322c8f6c40fc6dafdbd32442fa730ad] audit: restore order of tty and ses fields in log output
git bisect good 2f2ad1013322c8f6c40fc6dafdbd32442fa730ad
# bad: [e789e561a50de0aaa8c695662d97aaa5eac9d55f] audit: reset audit backlog wait time after error recovery
git bisect bad e789e561a50de0aaa8c695662d97aaa5eac9d55f
# bad: [33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb] audit: listen in all network namespaces
git bisect bad 33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb
# first bad commit: [33faba7fa7f2288d2f8aaea95958b2c97bf9ebfb] audit: listen in all network namespaces
&lt;/code>&lt;/pre>
&lt;p>The commit found by &lt;code>git bisect&lt;/code> seems like a reasonable candidate;
it&amp;rsquo;s a patch against the audit subsystem and has something to do with
namespaces, which are central to Docker&amp;rsquo;s proper operation.&lt;/p>
&lt;h2 id="debugging-the-problem">Debugging the problem&lt;/h2>
&lt;p>We can boot the kernel built from 33faba7 with the &lt;code>boot-kernel&lt;/code>
script, adding the &lt;code>-s&lt;/code> argument to qemu to start a &lt;code>gdbserver&lt;/code> on
port &lt;code>1234&lt;/code>:&lt;/p>
&lt;pre>&lt;code>sh BOOT_ARGS=NO_DOCKER_TEST QEMU_ARGS=&amp;quot;-s&amp;quot; boot-kernel
&lt;/code>&lt;/pre>
&lt;blockquote>
&lt;p>A caveat about attaching to qemu with gdb: qemu has a &lt;code>-S&lt;/code> option
that will cause the virtual machine to halt at startup, such that
you can attach before it starts booting and &amp;ndash; in theory &amp;ndash; set
breakpoints in the early boot process. In practice this doesn&amp;rsquo;t
work well at all (possibly because the vm switches from 32- to
64-bit operation during the boot process, which makes gdb unhappy).
You&amp;rsquo;re better off attaching after the kernel has booted.&lt;/p>
&lt;/blockquote>
&lt;p>In another window, we attach &lt;code>gdb&lt;/code> to the running &lt;code>qemu&lt;/code> process:&lt;/p>
&lt;pre>&lt;code>$ gdb vmlinux
Reading symbols from vmlinux...done.
(gdb) target remote :1234
Remote debugging using :1234
native_safe_halt () at /home/lars/src/linux/arch/x86/include/asm/irqflags.h:50
50 }
(gdb)
&lt;/code>&lt;/pre>
&lt;p>I know we&amp;rsquo;re getting the &lt;code>EPERM&lt;/code> in response to sending audit
messages. Looking through the code in &lt;code>kernel/audit.c&lt;/code>, the
&lt;code>audit_receive_msg&lt;/code> seems like a reasonable place to start poking
about. At the beginning of &lt;code>audit_receive_msg&lt;/code>, I see the following
code:&lt;/p>
&lt;pre>&lt;code>err = audit_netlink_ok(skb, msg_type);
if (err)
return err;
&lt;/code>&lt;/pre>
&lt;p>So let&amp;rsquo;s set a breakpoint there if &lt;code>audit_netlink_ok()&lt;/code> returns an
error:&lt;/p>
&lt;pre>&lt;code>(gdb) br kernel/audit.c:752 if (err != 0)
&lt;/code>&lt;/pre>
&lt;p>And let our qemu process continue running:&lt;/p>
&lt;pre>&lt;code>(gdb) continue
Continuing.
&lt;/code>&lt;/pre>
&lt;p>Inside the qemu instance I start docker:&lt;/p>
&lt;pre>&lt;code>-bash-4.2# docker run -it fedora /bin/su -c uptime
&lt;/code>&lt;/pre>
&lt;p>And eventually &lt;code>gdb&lt;/code> hits the breakpoint:&lt;/p>
&lt;pre>&lt;code>Breakpoint 1, audit_receive_msg (nlh=0xffff88003819a400,
skb=0xffff880038044300) at kernel/audit.c:752
752 if (err)
&lt;/code>&lt;/pre>
&lt;p>If I look at the value of &lt;code>err&lt;/code> at this point:&lt;/p>
&lt;pre>&lt;code>(gdb) print err
$1 = -1
&lt;/code>&lt;/pre>
&lt;p>That it is, in fact, &lt;code>-EPERM&lt;/code>, which suggests we&amp;rsquo;re on the right
track. Taking a closer look at &lt;code>audit_netlink_ok()&lt;/code>, it&amp;rsquo;s obvious
that there are only three places where it can return &lt;code>-EPERM&lt;/code>. I
tried setting some breakpoint in this function but they weren&amp;rsquo;t
working correctly, probably due to to optimizations performed when
compiling the kernel. So instead of &lt;code>gdb&lt;/code>, in this step we just add a
bunch of &lt;code>pr_err()&lt;/code> statements to print out debugging information on
the console:&lt;/p>
&lt;pre>&lt;code>if ((current_user_ns() != &amp;amp;init_user_ns) ||
(task_active_pid_ns(current) != &amp;amp;init_pid_ns)) {
pr_err(&amp;quot;currnet_user_ns() check failed\n&amp;quot;);
return -EPERM;
}
.
.
.
case AUDIT_MAKE_EQUIV:
if (!capable(CAP_AUDIT_CONTROL)) {
pr_err(&amp;quot;CAP_AUDIT_CONTROL check failed\n&amp;quot;);
err = -EPERM;
}
break;
case AUDIT_USER:
.
.
.
case AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:
case AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:
if (!capable(CAP_AUDIT_WRITE)) {
pr_err(&amp;quot;CAP_AUDIT_WRITE check failed\n&amp;quot;);
err = -EPERM;
}
break;
&lt;/code>&lt;/pre>
&lt;p>With these in place, if I run the &lt;code>docker&lt;/code> command again I see:&lt;/p>
&lt;pre>&lt;code>[ 12.239860] currnet_user_ns() check failed
su: System error
&lt;/code>&lt;/pre>
&lt;p>It looks like we&amp;rsquo;ve found out where it&amp;rsquo;s failing! Of course, we&amp;rsquo;re
checking code right now that is several commits behind v3.15, so let&amp;rsquo;s
take a look the same function in the 3.15 release:&lt;/p>
&lt;pre>&lt;code>$ git checkout v3.15
&lt;/code>&lt;/pre>
&lt;p>Looking at &lt;code>audit_netlink_ok&lt;/code> in &lt;code>kernel/audit.c&lt;/code>, it looks as if that
initial check has changed:&lt;/p>
&lt;pre>&lt;code> /* Only support initial user namespace for now. */
/*
* We return ECONNREFUSED because it tricks userspace into thinking
* that audit was not configured into the kernel. Lots of users
* configure their PAM stack (because that's what the distro does)
* to reject login if unable to send messages to audit. If we return
* ECONNREFUSED the PAM stack thinks the kernel does not have audit
* configured in and will let login proceed. If we return EPERM
* userspace will reject all logins. This should be removed when we
* support non init namespaces!!
*/
if (current_user_ns() != &amp;amp;init_user_ns)
return -ECONNREFUSED;
&lt;/code>&lt;/pre>
&lt;p>So let&amp;rsquo;s insert our print statements into this version of the code and
see if we get the same behavior:&lt;/p>
&lt;pre>&lt;code>if (current_user_ns() != &amp;amp;init_user_ns) {
pr_err(&amp;quot;current_user-ns() check failed\n&amp;quot;);
return -ECONNREFUSED;
}
.
.
.
case AUDIT_MAKE_EQUIV:
/* Only support auditd and auditctl in initial pid namespace
* for now. */
if ((task_active_pid_ns(current) != &amp;amp;init_pid_ns)) {
pr_err(&amp;quot;init_pid_ns check failed\n&amp;quot;);
return -EPERM;
}
if (!netlink_capable(skb, CAP_AUDIT_CONTROL)) {
pr_err(&amp;quot;CAP_AUDIT_CONTROL check failed\n&amp;quot;);
err = -EPERM;
}
break;
.
.
.
case AUDIT_USER:
case AUDIT_FIRST_USER_MSG ... AUDIT_LAST_USER_MSG:
case AUDIT_FIRST_USER_MSG2 ... AUDIT_LAST_USER_MSG2:
if (!netlink_capable(skb, CAP_AUDIT_WRITE)) {
pr_err(&amp;quot;CAP_AUDIT_WRITE check failed\n&amp;quot;);
err = -EPERM;
}
break;
&lt;/code>&lt;/pre>
&lt;p>Running the v3.15 kernel, I see:&lt;/p>
&lt;pre>&lt;code>[ 26.273992] audit: CAP_AUDIT_WRITE check failed
su: System error
&lt;/code>&lt;/pre>
&lt;p>So it looks like the intial failure in &lt;code>audit_netlink_ok()&lt;/code> was fixed,
but we&amp;rsquo;re stilling failing the &lt;code>CAP_AUDIT_WRITE&lt;/code> check.&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>What&amp;rsquo;s going on here?&lt;/p>
&lt;p>Prior to &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=33faba7">33faba7&lt;/a>, audit messages were only accepted in the main
network namespace. Inside other network namespaces, processes sending
audit messages would simply receive &lt;code>ECONNREFUSED&lt;/code>. For example, this
is the result of using &lt;code>strace&lt;/code> on that &lt;code>docker run&lt;/code> command in a
pre-&lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=33faba7">33faba7&lt;/a> kernel:&lt;/p>
&lt;pre>&lt;code>539 sendto(3, &amp;quot;...authentication acct=\&amp;quot;root\&amp;quot; exe=\&amp;quot;/usr/bin/su\&amp;quot; hostname=? a&amp;quot;...,
112, 0, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = -1 ECONNREFUSED (Connection refused)
&lt;/code>&lt;/pre>
&lt;p>With &lt;a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=33faba7">33faba7&lt;/a>, audit messages are now accepted inside network
namespaces. This means that instead of simply getting &lt;code>ECONNREFUSED&lt;/code>,
messages must pass the kernel capability check. I spoke with some of
the audit subsystem maintainers (including Richard Guy Briggs, the
author of this patch series), and the general consensus is that &amp;ldquo;if
you want to write audit messages you need &lt;code>CAP_AUDIT_WRITE&lt;/code>&amp;rdquo;.&lt;/p>
&lt;p>So while this patch did change the behavior of the kernel from the
perspective of container tools such as Docker, the fix needs to be in
the tool creating the namespaces.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>This issue was reported against Fedora in &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1121345">BZ 1121345&lt;/a> and &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1119849">BZ
1119849&lt;/a>. This issue was also reported against Docker in &lt;a href="https://github.com/dotcloud/docker/issues/6345">GHI 6345&lt;/a>
and &lt;a href="https://github.com/dotcloud/docker/issues/7123">GHI 7123&lt;/a>.&lt;/p>
&lt;p>This problem has been corrected upstream in
&lt;a href="https://github.com/dotcloud/docker/pull/7179" class="pull-request">#7179&lt;/a>
.&lt;/p>
&lt;p>Package &lt;a href="https://admin.fedoraproject.org/updates/FEDORA-2014-8877/docker-io-1.0.0-9.fc20">docker-io-1.0.0-9.fc20&lt;/a>, which includes
the above fix, is now available for Fedora 20 (and Fedora 19).&lt;/p></content></item><item><title>Booting an instance with multiple fixed addresses</title><link>https://blog.oddbit.com/post/2014-05-28-booting-an-instance-with-multi/</link><pubDate>Wed, 28 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-28-booting-an-instance-with-multi/</guid><description>This article expands on my answer to Add multiple specific IPs to instance, a question posted to ask.openstack.org.
In order to serve out SSL services from an OpenStack instance, you will generally want one local ip address for each SSL virtual host you support. It is possible to create an instance with multiple fixed addresses, but there are a few complications to watch out for.
Assumptions This article assumes that the following resources exist:</description><content>&lt;p>This article expands on my answer to &lt;a href="https://ask.openstack.org/en/question/30690/add-multiple-specific-ips-to-instance/">Add multiple specific IPs to
instance&lt;/a>, a question posted to &lt;a href="https://ask.openstack.org/">ask.openstack.org&lt;/a>.&lt;/p>
&lt;p>In order to serve out SSL services from an OpenStack instance, you
will generally want one local ip address for each SSL virtual host you
support. It is possible to create an instance with multiple fixed
addresses, but there are a few complications to watch out for.&lt;/p>
&lt;h1 id="assumptions">Assumptions&lt;/h1>
&lt;p>This article assumes that the following resources exist:&lt;/p>
&lt;ul>
&lt;li>a private network &lt;code>net0&lt;/code>.&lt;/li>
&lt;li>a private network &lt;code>net0-subnet0&lt;/code>, associated with &lt;code>net0&lt;/code>, assigned
the range &lt;code>10.0.0.0/24&lt;/code>.&lt;/li>
&lt;li>a public network &lt;code>external&lt;/code> assigned the range &lt;code>192.168.200.0/24&lt;/code>.&lt;/li>
&lt;li>an image named &lt;code>fedora-20-x86_64&lt;/code>, with hopefully self-evident
contents.&lt;/li>
&lt;/ul>
&lt;h1 id="creating-a-port">Creating a port&lt;/h1>
&lt;p>Start by creating a port in Neutron:&lt;/p>
&lt;pre>&lt;code>$ neutron port-create net0 \
--fixed-ip subnet_id=net0-subnet0 \
--fixed-ip subnet_id=net0-subnet0
&lt;/code>&lt;/pre>
&lt;p>This will create a neutron port to which have been allocated to fixed
ip addresses from &lt;code>net0-subnet0&lt;/code>:&lt;/p>
&lt;pre>&lt;code>+-----------------------+----------------------------------------------------------------------------------+
| Field | Value |
+-----------------------+----------------------------------------------------------------------------------+
| admin_state_up | True |
| allowed_address_pairs | |
| binding:vnic_type | normal |
| device_id | |
| device_owner | |
| fixed_ips | {&amp;quot;subnet_id&amp;quot;: &amp;quot;f8ca90fd-cb82-4218-9627-6fa66e4c9c3c&amp;quot;, &amp;quot;ip_address&amp;quot;: &amp;quot;10.0.0.18&amp;quot;} |
| | {&amp;quot;subnet_id&amp;quot;: &amp;quot;f8ca90fd-cb82-4218-9627-6fa66e4c9c3c&amp;quot;, &amp;quot;ip_address&amp;quot;: &amp;quot;10.0.0.19&amp;quot;} |
| id | 3c564dd5-fd45-4f61-88df-715f71667b3b |
| mac_address | fa:16:3e:e1:15:7f |
| name | |
| network_id | bb4e5e37-74e1-41bd-880e-b59e94236c5e |
| security_groups | 52f7a87c-380f-4a07-a6ff-d64be495f25b |
| status | DOWN |
| tenant_id | 4dfe8e38f68449b6a0c9cd73037726f7 |
+-----------------------+----------------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>If you want, you can specify an explicit set of addresses rather than
having neutron allocate them for you:&lt;/p>
&lt;pre>&lt;code>$ neutron port-create net0 \
--fixed-ip subnet_id=net0-subnet0,ip_address=10.0.0.18 \
--fixed-ip subnet_id=net0-subnet0,ip_address=10.0.0.19
&lt;/code>&lt;/pre>
&lt;h1 id="boot-an-instance">Boot an instance&lt;/h1>
&lt;p>You can boot an instance using this port using the &lt;code>port-id=...&lt;/code>
parameter to the &lt;code>--nic&lt;/code> option:&lt;/p>
&lt;pre>&lt;code>$ nova boot \
--nic port-id=3c564dd5-fd45-4f61-88df-715f71667b3b \
--flavor m1.tiny \
--image fedora-20-x86_64 \
--key-name lars test0
&lt;/code>&lt;/pre>
&lt;p>This is where the first complication arises: the instance will boot
and receive a DHCP lease for one of the fixed addresses you created,
but you don&amp;rsquo;t know which one. This isn&amp;rsquo;t an insurmountable problem;
you can assign floating ips to each one and then try logging in to
both and see which works.&lt;/p>
&lt;p>Rather than playing network roulette, you can pass in a script via the
&lt;code>--user-data&lt;/code> option that will take care of configuring the network
correctly. For example, something like this:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
cat &amp;gt; /etc/sysconfig/network-scripts/ifcfg-eth0 &amp;lt;&amp;lt;EOF
DEVICE=eth0
BOOTPROTO=none
IPADDR=10.0.0.18
NETMASK=255.255.255.0
GATEWAY=10.0.0.1
ONBOOT=yes
EOF
cat &amp;gt; /etc/sysconfig/network-scripts/ifcfg-eth0:0 &amp;lt;&amp;lt;EOF
DEVICE=eth0:0
BOOTPROTO=none
IPADDR=10.0.0.19
NETMASK=255.255.255.0
GATEWAY=10.0.0.1
ONBOOT=yes
EOF
ifdown eth0
ifup eth0
ifup eth0:0
&lt;/code>&lt;/pre>
&lt;p>And boot the instance like this:&lt;/p>
&lt;pre>&lt;code>$ nova boot --nic port-id=3c564dd5-fd45-4f61-88df-715f71667b3b \
--flavor m1.tiny --image fedora-20-x86_64 --key-name lars \
--user-data userdata.txt test0
&lt;/code>&lt;/pre>
&lt;p>Assuming that your image uses &lt;a href="http://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> or something similar, it
should execute the &lt;code>user-data&lt;/code> script at boot and set up the
persistent network configuration.&lt;/p>
&lt;p>At this stage, you can verify that both addresses have been assigned
by using the &lt;code>ip netns&lt;/code> command to run &lt;code>ping&lt;/code> inside an appropriate
namespace. Something like:&lt;/p>
&lt;pre>&lt;code>$ sudo ip netns exec qdhcp-bb4e5e37-74e1-41bd-880e-b59e94236c5e ping -c1 10.0.0.18
PING 10.0.0.18 (10.0.0.18) 56(84) bytes of data.
64 bytes from 10.0.0.18: icmp_seq=1 ttl=64 time=1.60 ms
--- 10.0.0.18 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.606/1.606/1.606/0.000 ms
$ sudo ip netns exec qdhcp-bb4e5e37-74e1-41bd-880e-b59e94236c5e ping -c1 10.0.0.19
PING 10.0.0.19 (10.0.0.19) 56(84) bytes of data.
64 bytes from 10.0.0.19: icmp_seq=1 ttl=64 time=1.60 ms
--- 10.0.0.19 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.701/1.701/1.701/0.000 ms
&lt;/code>&lt;/pre>
&lt;p>This assumes that the UUID of the &lt;code>net0&lt;/code> network is &lt;code>bb4e5e37-74e1-41bd-880e-b59e94236c5e&lt;/code>. On your system, the namespace will be something different.&lt;/p>
&lt;h1 id="assign-floating-ips">Assign floating ips&lt;/h1>
&lt;p>Assign a floating ip address to each of the fixed addresses. You will
need to use the &lt;code>--fixed-address&lt;/code> option to &lt;code>nova add-floating-ip&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova add-floating-ip --fixed-address 10.0.0.19 test0 192.168.200.6
$ nova add-floating-ip --fixed-address 10.0.0.18 test0 192.168.200.4
&lt;/code>&lt;/pre>
&lt;p>With these changes in place, the system is accessible via either
address:&lt;/p>
&lt;pre>&lt;code>$ ssh fedora@192.168.200.4 uptime
14:51:52 up 4 min, 0 users, load average: 0.00, 0.02, 0.02
$ ssh fedora@192.168.200.6 uptime
14:51:54 up 4 min, 0 users, load average: 0.00, 0.02, 0.02
&lt;/code>&lt;/pre>
&lt;p>And looking at the network configuration on the system, we can see
that both addresses have been assigned to &lt;code>eth0&lt;/code> as expected:&lt;/p>
&lt;pre>&lt;code>$ ssh fedora@192.168.200.4 /sbin/ip a
[...]
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
link/ether fa:16:3e:bf:f9:6a brd ff:ff:ff:ff:ff:ff
inet 10.0.0.18/24 brd 10.0.0.255 scope global eth0
valid_lft forever preferred_lft forever
inet 10.0.0.19/24 brd 10.0.0.255 scope global secondary eth0:0
valid_lft forever ...
&lt;/code>&lt;/pre></content></item><item><title>Multiple external networks with a single L3 agent</title><link>https://blog.oddbit.com/post/2014-05-28-multiple-external-networks-wit/</link><pubDate>Wed, 28 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-28-multiple-external-networks-wit/</guid><description>In the old days (so, like, last year), Neutron supported a single external network per L3 agent. You would run something like this&amp;hellip;
$ neutron net-create external --router:external=true &amp;hellip;and neutron would map this to the bridge defined in external_network_bridge in /etc/neutron/l3_agent.ini. If you wanted to support more than a single external network, you would need to run multiple L3 agents, each with a unique value for external_network_bridge.
There is now a better option available.</description><content>&lt;p>In the old days (so, like, last year), Neutron supported a single
external network per L3 agent. You would run something like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>$ neutron net-create external --router:external=true
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and neutron would map this to the bridge defined in
&lt;code>external_network_bridge&lt;/code> in &lt;code>/etc/neutron/l3_agent.ini&lt;/code>. If you
wanted to support more than a single external network, you would need
to run multiple L3 agents, each with a unique value for
&lt;code>external_network_bridge&lt;/code>.&lt;/p>
&lt;p>There is now a better option available.&lt;/p>
&lt;h2 id="assumptions">Assumptions&lt;/h2>
&lt;p>In this post, I&amp;rsquo;m assuming:&lt;/p>
&lt;ul>
&lt;li>You&amp;rsquo;re using the ML2 plugin for Neutron.&lt;/li>
&lt;li>You&amp;rsquo;re using the Open vSwitch mechanism driver for the ML2 plugin&lt;/li>
&lt;li>You have &lt;code>eth1&lt;/code> and &lt;code>eth2&lt;/code> connected directly to networks that you
would like to make available as external networks in OpenStack.&lt;/li>
&lt;/ul>
&lt;h2 id="create-your-bridges">Create your bridges&lt;/h2>
&lt;p>For each external network you wish to support, create a new OVS
bridge. For example, assuming that we want to make a network attached
to &lt;code>eth1&lt;/code> and a network attached to &lt;code>eth2&lt;/code> available to tenants:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-br br-eth1
# ovs-vsctl add-port br-eth1 eth1
# ovs-vsctl add-br br-eth2
# ovs-vsctl add-port br-eth2 eth2
&lt;/code>&lt;/pre>
&lt;p>Realistically, you would accomplish this via your system&amp;rsquo;s native
network configuration mechanism, but I&amp;rsquo;m going to gloss over that
detail for now.&lt;/p>
&lt;h2 id="configure-the-l3-agent">Configure the L3 Agent&lt;/h2>
&lt;p>Start with the following comment in &lt;code>l3_agent.ini&lt;/code>:&lt;/p>
&lt;pre>&lt;code># When external_network_bridge is set, each L3 agent can be associated
# with no more than one external network. This value should be set to the UUID
# of that external network. To allow L3 agent support multiple external
# networks, both the external_network_bridge and gateway_external_network_id
# must be left empty.
&lt;/code>&lt;/pre>
&lt;p>Following those instructions, make sure that both
&lt;code>external_network_bridge&lt;/code> and &lt;code>gateway_external_network_id&lt;/code> are unset
in &lt;code>l3_agent.ini&lt;/code>.&lt;/p>
&lt;h2 id="configure-the-ml2-plugin">Configure the ML2 Plugin&lt;/h2>
&lt;p>We are creating &amp;ldquo;flat&amp;rdquo; networks in this example, so we need to make
sure that we can create flat networks. Make sure that the
&lt;code>type_drivers&lt;/code> parameter of the &lt;code>[ml2]&lt;/code> section of your plugin
configuration includes &lt;code>flat&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[ml2]
type_drivers = local,flat,gre,vxlan
&lt;/code>&lt;/pre>
&lt;p>In the &lt;code>[ml2_type_flat]&lt;/code> section, need to create a list of physical
network names that can be used to create flat networks. If you want
all physical networks to be available for flat networks, you can use
&lt;code>*&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[ml2_type_flat]
flat_networks = *
&lt;/code>&lt;/pre>
&lt;p>Both of these changes probably go in &lt;code>/etc/neutron/plugin.ini&lt;/code>, but
&lt;em>may&lt;/em> going elsewhere depending on how your system is configured.&lt;/p>
&lt;h2 id="configure-the-open-vswitch-agent">Configure the Open vSwitch Agent&lt;/h2>
&lt;p>For each bridge, you will need to add entries to both the
&lt;code>network_vlan_ranges&lt;/code> and &lt;code>bridge_mappings&lt;/code> parameters of the &lt;code>[ovs]&lt;/code>
section of your plugin configuration. For the purposes of this post,
that means:&lt;/p>
&lt;pre>&lt;code>[ovs]
network_vlan_ranges = physnet1,physnet2
bridge_mappings = physnet1:br-eth1,physnet2:br-eth2
&lt;/code>&lt;/pre>
&lt;p>This will probably go in &lt;code>/etc/neutron/plugin.ini&lt;/code>. Specifically, it
needs to go wherever your &lt;code>neutron-openvswitch-agent&lt;/code> process is
looking for configuration information. So you if you see this:&lt;/p>
&lt;pre>&lt;code>$ ps -fe | grep openvswitch-agent
neutron 12529 1 0 09:50 ? 00:00:08 /usr/bin/python /usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini --log-file /var/log/neutron/openvswitch-agent.log
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;then you would make the changes to &lt;code>/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini&lt;/code>.&lt;/p>
&lt;h2 id="restart-neutron">Restart Neutron&lt;/h2>
&lt;p>You will need to restart both the l3 agent and the openvswitch agent.
If you&amp;rsquo;re on a recent Fedora/RHEL/CentOS, you can restart all Neutron
services like this:&lt;/p>
&lt;pre>&lt;code># openstack-service restart neutron
&lt;/code>&lt;/pre>
&lt;h2 id="inspect-your-open-vswitch-configuration">Inspect your Open vSwitch Configuration&lt;/h2>
&lt;p>As root, run &lt;code>ovs-vsctl show&lt;/code>. You should see something like this:&lt;/p>
&lt;pre>&lt;code>f4a4312b-307e-4c3c-b728-9434000a34ff
Bridge br-int
Port br-int
Interface br-int
type: internal
Port &amp;quot;int-br-eth2&amp;quot;
Interface &amp;quot;int-br-eth2&amp;quot;
Port int-br-ex
Interface int-br-ex
Port &amp;quot;int-br-eth1&amp;quot;
Interface &amp;quot;int-br-eth1&amp;quot;
Bridge &amp;quot;br-eth2&amp;quot;
Port &amp;quot;br-eth2&amp;quot;
Interface &amp;quot;br-eth2&amp;quot;
type: internal
Port &amp;quot;phy-br-eth2&amp;quot;
Interface &amp;quot;phy-br-eth2&amp;quot;
Port &amp;quot;eth2&amp;quot;
Interface &amp;quot;eth2&amp;quot;
Bridge &amp;quot;br-eth1&amp;quot;
Port &amp;quot;br-eth1&amp;quot;
Interface &amp;quot;br-eth1&amp;quot;
type: internal
Port &amp;quot;phy-br-eth1&amp;quot;
Interface &amp;quot;phy-br-eth1&amp;quot;
Port &amp;quot;eth1&amp;quot;
Interface &amp;quot;eth1&amp;quot;
ovs_version: &amp;quot;2.0.1&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Here you can see the OVS bridge &lt;code>br-eth1&lt;/code> and &lt;code>br-eth2&lt;/code>, each with the
appropriate associated physical interface and links to the integration
bridge, &lt;code>br-int&lt;/code>.&lt;/p>
&lt;h2 id="create-your-external-networks">Create your external networks&lt;/h2>
&lt;p>With admin credentials, use the &lt;code>net-create&lt;/code> and &lt;code>subnet-create&lt;/code>
commands to create the appropiate networks:&lt;/p>
&lt;pre>&lt;code>$ neutron net-create external1 -- --router:external=true \
--provider:network_type=flat \
--provider:physical_network=physnet1
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | True |
| id | 23f4b5f6-14fd-4bab-a8b0-445257bbc0d1 |
| name | external1 |
| provider:network_type | flat |
| provider:physical_network | physnet1 |
| provider:segmentation_id | |
| router:external | True |
| shared | False |
| status | ACTIVE |
| subnets | |
| tenant_id | 6f736b1361b74789a81d4d53d88be3c5 |
+---------------------------+--------------------------------------+
$ neutron subnet-create --disable-dhcp external1 10.1.0.0/24
+------------------+--------------------------------------------+
| Field | Value |
+------------------+--------------------------------------------+
| allocation_pools | {&amp;quot;start&amp;quot;: &amp;quot;10.1.0.2&amp;quot;, &amp;quot;end&amp;quot;: &amp;quot;10.1.0.254&amp;quot;} |
| cidr | 10.1.0.0/24 |
| dns_nameservers | |
| enable_dhcp | False |
| gateway_ip | 10.1.0.1 |
| host_routes | |
| id | 363ba289-a989-4acb-ac3b-ffaeb90796fc |
| ip_version | 4 |
| name | |
| network_id | 23f4b5f6-14fd-4bab-a8b0-445257bbc0d1 |
| tenant_id | 6f736b1361b74789a81d4d53d88be3c5 |
+------------------+--------------------------------------------+
$ neutron net-create external2 -- --router:external=true \
--provider:network_type=flat \
--provider:physical_network=physnet2
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | True |
| id | 762be5de-31a2-46b8-925c-0967871f8181 |
| name | external2 |
| provider:network_type | flat |
| provider:physical_network | physnet2 |
| provider:segmentation_id | |
| router:external | True |
| shared | False |
| status | ACTIVE |
| subnets | |
| tenant_id | 6f736b1361b74789a81d4d53d88be3c5 |
+---------------------------+--------------------------------------+
$ neutron subnet-create --disable-dhcp external2 10.2.0.0/24
+------------------+--------------------------------------------+
| Field | Value |
+------------------+--------------------------------------------+
| allocation_pools | {&amp;quot;start&amp;quot;: &amp;quot;10.2.0.2&amp;quot;, &amp;quot;end&amp;quot;: &amp;quot;10.2.0.254&amp;quot;} |
| cidr | 10.2.0.0/24 |
| dns_nameservers | |
| enable_dhcp | False |
| gateway_ip | 10.2.0.1 |
| host_routes | |
| id | edffc5c6-0e16-4da0-8eba-9d79ab9fd2fe |
| ip_version | 4 |
| name | |
| network_id | 762be5de-31a2-46b8-925c-0967871f8181 |
| tenant_id | 6f736b1361b74789a81d4d53d88be3c5 |
+------------------+--------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>This assumes that &lt;code>eth1&lt;/code> is connected to a network using
&lt;code>10.1.0.0/24&lt;/code> and &lt;code>eth2&lt;/code> is connected to a network using
&lt;code>10.2.0.0/24&lt;/code>, and that each network has a gateway sitting at the
corresponding &lt;code>.1&lt;/code> address.&lt;/p>
&lt;p>And you&amp;rsquo;re all set!&lt;/p></content></item><item><title>Video: Configuring OpenStack's external bridge on a single-interface system</title><link>https://blog.oddbit.com/post/2014-05-27-configuring-openstacks-externa/</link><pubDate>Tue, 27 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-27-configuring-openstacks-externa/</guid><description>I&amp;rsquo;ve just put a video on Youtube that looks at the steps required to set up the external bridge (br-ex) on a single-interface system:</description><content>&lt;p>I&amp;rsquo;ve just put a video on Youtube that looks at the steps required to
set up the external bridge (&lt;code>br-ex&lt;/code>) on a single-interface system:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8zFQG5mKwPk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></content></item><item><title>Open vSwitch and persistent MAC addresses</title><link>https://blog.oddbit.com/post/2014-05-23-open-vswitch-and-persistent-ma/</link><pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-23-open-vswitch-and-persistent-ma/</guid><description>Normally I like to post solutions, but today&amp;rsquo;s post is about a vexing problem to which I have not been able to find a solution.
This started as a simple attempt to set up external connectivity on an all-in-one Icehouse install deployed on an OpenStack instance. I wanted to add eth0 to br-ex in order to model a typical method for providing external connectivity, but I ran into a very odd problem: the system would boot and work fine for a few seconds, but would then promptly lose network connectivity.</description><content>&lt;p>Normally I like to post solutions, but today&amp;rsquo;s post is about a
vexing problem to which I have not been able to find a solution.&lt;/p>
&lt;p>This started as a simple attempt to set up external connectivity on
an all-in-one Icehouse install deployed on an OpenStack instance. I
wanted to add &lt;code>eth0&lt;/code> to &lt;code>br-ex&lt;/code> in order to model a typical method for
providing external connectivity, but I ran into a very odd problem:
the system would boot and work fine for a few seconds, but would then
promptly lose network connectivity.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The immediate cause was that the MAC address on &lt;code>br-ex&lt;/code> was changing.
I was setting the MAC explicitly in the configuration file:&lt;/p>
&lt;pre>&lt;code># cat ifcfg-br-ex
DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
ONBOOT=yes
OVSBOOTPROTO=dhcp
OVSDHCPINTERFACES=eth0
MACADDR=fa:16:3e:ef:91:ec
&lt;/code>&lt;/pre>
&lt;p>This was required in this case in order to make the MAC-address
filters on the host happy. When booting an instance, Neutron sets up
a rule like this:&lt;/p>
&lt;pre>&lt;code>-A neutron-openvswi-s55439d7d-a -s 10.0.0.8/32 -m mac --mac-source FA:16:3E:EF:91:EC -j RETURN
-A neutron-openvswi-s55439d7d-a -j DROP
&lt;/code>&lt;/pre>
&lt;p>But things quickly got weird. Some testing demonstrated that the MAC
address was changing when starting &lt;code>neutron-openvswitch-agent&lt;/code>, but a
thorough inspection of the code didn&amp;rsquo;t yield any obvious culprits for
this behavior.&lt;/p>
&lt;p>I liberally sprinkled the agent with the following (incrementing the
argument to &lt;code>echo&lt;/code> each time to uniquely identify each message):&lt;/p>
&lt;pre>&lt;code>os.system('echo 1 &amp;gt;&amp;gt; /tmp/ovs.log; ip link show dev br-ex &amp;gt;&amp;gt; /tmp/ovs.log')
&lt;/code>&lt;/pre>
&lt;p>It turns out that the MAC address on &lt;code>br-ex&lt;/code> was changing&amp;hellip;when
Neutron was deleting a port on &lt;code>br-int&lt;/code>. Specifically, at &lt;a href="https://github.com/openstack/neutron/blob/423ca756af10e10398636d6d34a7594a4fd4bc87/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py#L909">this
line&lt;/a> in &lt;code>ovs_neutron_agent.py&lt;/code>:&lt;/p>
&lt;pre>&lt;code>self.int_br.delete_port(int_veth_name)
&lt;/code>&lt;/pre>
&lt;p>After some additional testing, it turns out that just about &lt;em>any&lt;/em> OVS
operation causes an explicit MAC address to disappear. For example,
create a new OVS bridge:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-br br-test0
# ip link show dev br-test0
9: br-test0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether ba:cb:48:b9:6a:43 brd ff:ff:ff:ff:ff:ff
&lt;/code>&lt;/pre>
&lt;p>Then set the MAC address:&lt;/p>
&lt;pre>&lt;code># ip link set br-test0 addr c0:ff:ee:ee:ff:0c
# ip link show br-test0
8: br-test0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether c0:ff:ee:ee:ff:0c brd ff:ff:ff:ff:ff:ff
&lt;/code>&lt;/pre>
&lt;p>Now create a new bridge:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-br br-test1
&lt;/code>&lt;/pre>
&lt;p>And inspect the MAC address on the first bridge:&lt;/p>
&lt;pre>&lt;code># ip link show dev br-test0
9: br-test0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether ba:cb:48:b9:6a:43 brd ff:ff:ff:ff:ff:ff
&lt;/code>&lt;/pre>
&lt;p>In other words, creating a new bridge caused the MAC address on
&lt;code>br-ex&lt;/code> to revert. Other operations (e.g., deleting a port on an
unrelated switch) will cause the same behavior.&lt;/p>
&lt;p>I&amp;rsquo;ve seen this behavior on both versions &lt;code>1.11.0&lt;/code> and &lt;code>2.0.1&lt;/code>.&lt;/p>
&lt;p>So far everyone I&amp;rsquo;ve asked about this behavior has been stumped. If I
am able to figure out what&amp;rsquo;s going on I will update this post. Thanks
for reading!&lt;/p></content></item><item><title>Solved: Open vSwitch and persistent MAC addresses</title><link>https://blog.oddbit.com/post/2014-05-23-solved-open-vswitch-and-persis/</link><pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-23-solved-open-vswitch-and-persis/</guid><description>In my previous post I discussed a problem I was having setting a persistent MAC address on an OVS bridge device. It looks like the short answer is, &amp;ldquo;don&amp;rsquo;t use ip link set ...&amp;rdquo; for this purpose.
You can set the bridge MAC address via ovs-vsctl like this:
ovs-vsctl set bridge br-ex other-config:hwaddr=$MACADDR So I&amp;rsquo;ve updated my ifconfig-br-ex to look like this:
DEVICE=br-ex DEVICETYPE=ovs TYPE=OVSBridge ONBOOT=yes OVSBOOTPROTO=dhcp OVSDHCPINTERFACES=eth0 MACADDR=fa:16:3e:ef:91:ec OVS_EXTRA=&amp;quot;set bridge br-ex other-config:hwaddr=$MACADDR&amp;quot; The OVS_EXTRA parameter gets passed to the add-br call like this:</description><content>&lt;p>In my &lt;a href="https://blog.oddbit.com/2014/05/23/open-vswitch-and-persistent-ma/">previous post&lt;/a> I discussed a problem I was having setting a
persistent MAC address on an OVS bridge device. It looks like the
short answer is, &amp;ldquo;don&amp;rsquo;t use &lt;code>ip link set ...&lt;/code>&amp;rdquo; for this purpose.&lt;/p>
&lt;p>You can set the bridge MAC address via &lt;code>ovs-vsctl&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code>ovs-vsctl set bridge br-ex other-config:hwaddr=$MACADDR
&lt;/code>&lt;/pre>
&lt;p>So I&amp;rsquo;ve updated my &lt;code>ifconfig-br-ex&lt;/code> to look like this:&lt;/p>
&lt;pre>&lt;code>DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
ONBOOT=yes
OVSBOOTPROTO=dhcp
OVSDHCPINTERFACES=eth0
MACADDR=fa:16:3e:ef:91:ec
OVS_EXTRA=&amp;quot;set bridge br-ex other-config:hwaddr=$MACADDR&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>OVS_EXTRA&lt;/code> parameter gets passed to the &lt;code>add-br&lt;/code> call like this:&lt;/p>
&lt;pre>&lt;code>ovs-vsctl --may-exist add-br br-ex -- set bridge br-ex other-config:hwaddr=$MACADDR
&lt;/code>&lt;/pre>
&lt;p>And unlike using &lt;code>ip link set&lt;/code>, this seems to stick.&lt;/p></content></item><item><title>Sharing a terminal session with termshare</title><link>https://blog.oddbit.com/post/2014-05-21-sharing-a-terminal-session-wit/</link><pubDate>Wed, 21 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-21-sharing-a-terminal-session-wit/</guid><description>Termshare is a tool for sharing your terminal in a browser session. It supports both read-only and read-write sessions, and unlike many other tools it does not require any software installation on the remote side. This makes it tremendously handy for:
Streaming terminal demonstrations to a diverse audience, or Sharing a terminal session with someone without needing to much about with ssh, tmux, screen, etc. I&amp;rsquo;ve successfully used Termshare under both Fedora (19 and 20) and CentOS.</description><content>&lt;p>&lt;a href="https://github.com/progrium/termshare">Termshare&lt;/a> is a tool for sharing your terminal in a browser
session. It supports both read-only and read-write sessions, and
unlike many other tools it does not require any software installation
on the remote side. This makes it tremendously handy for:&lt;/p>
&lt;ul>
&lt;li>Streaming terminal demonstrations to a diverse audience, or&lt;/li>
&lt;li>Sharing a terminal session with someone without needing to much
about with ssh, tmux, screen, etc.&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;ve successfully used &lt;a href="https://github.com/progrium/termshare">Termshare&lt;/a> under both Fedora (19 and 20) and
CentOS. To get started on these platforms, you&amp;rsquo;ll need to install the
&lt;a href="http://golang.org/">Go&lt;/a> language, &lt;a href="http://git-scm.org/">git&lt;/a> for cloning the termshare repository, and
&lt;a href="http://mercurial.selenic.com/">mercurial&lt;/a> to support installation of some Go libraries:&lt;/p>
&lt;pre>&lt;code># yum -y install golang git hg
&lt;/code>&lt;/pre>
&lt;p>Then, clone the &lt;a href="https://github.com/progrium/termshare">Termshare&lt;/a> repository:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/progrium/termshare.git
$ cd termshare
&lt;/code>&lt;/pre>
&lt;p>Install the Go dependencies:&lt;/p>
&lt;pre>&lt;code>$ mkdir gopath
$ export GOPATH=$PWD/gopath
$ go get
&lt;/code>&lt;/pre>
&lt;p>This will yield&amp;hellip;&lt;/p>
&lt;pre>&lt;code>go install: no install location for directory /home/lars/src/termshare outside GOPATH
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;but you can ignore that. Then build the software:&lt;/p>
&lt;pre>&lt;code>$ make
&lt;/code>&lt;/pre>
&lt;p>And you&amp;rsquo;re done! To start a read-only session, run &lt;code>./termshare&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ ./termshare
_ _
| |_ ___ _ __ _ __ ___ ___| |__ __ _ _ __ ___
| __/ _ \ '__| '_ ` _ \/ __| '_ \ / _` | '__/ _ \
| || __/ | | | | | | \__ \ | | | (_| | | | __/
\__\___|_| |_| |_| |_|___/_| |_|\__,_|_| \___|
Running this open source service supported 100% by community.
Donate: https://www.gittip.com/termshare
Session URL: https://termsha.re/12345678-ee85-49ba-66ce-987654321abc
&lt;/code>&lt;/pre>
&lt;p>Pass the session URL to people you want to view your terminal. Run
&lt;code>./termshare -c&lt;/code> if you want someone to be able to control your
terminal as well as view it.&lt;/p></content></item><item><title>Fedora and OVS Bridge Interfaces</title><link>https://blog.oddbit.com/post/2014-05-20-fedora-and-ovs-bridge-interfac/</link><pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-20-fedora-and-ovs-bridge-interfac/</guid><description>I run OpenStack on my laptop, and I&amp;rsquo;ve been chasing down a pernicious problem with OVS bridge interfaces under both F19 and F20. My OpenStack environment relies on an OVS bridge device named br-ex for external connectivity and for making services available to OpenStack instances, but after rebooting, br-ex was consistently unconfigured, which caused a variety of problems.
This is the network configuration file for br-ex on my system:
DEVICE=br-ex DEVICETYPE=ovs TYPE=OVSBridge BOOTPROT=static IPADDR=192.</description><content>&lt;p>I run OpenStack on my laptop, and I&amp;rsquo;ve been chasing down a pernicious
problem with OVS bridge interfaces under both F19 and F20. My
OpenStack environment relies on an OVS bridge device named &lt;code>br-ex&lt;/code> for
external connectivity and for making services available to OpenStack
instances, but after rebooting, &lt;code>br-ex&lt;/code> was consistently unconfigured,
which caused a variety of problems.&lt;/p>
&lt;p>This is the network configuration file for &lt;code>br-ex&lt;/code> on my system:&lt;/p>
&lt;pre>&lt;code>DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROT=static
IPADDR=192.168.200.1
NETMASK=255.255.255.0
ONBOOT=yes
NM_CONTROLLED=no
ZONE=openstack
&lt;/code>&lt;/pre>
&lt;p>Running &lt;code>ifup br-ex&lt;/code> would also fail to configure the interface, but
running &lt;code>ifdown br-ex; ifup br-ex&lt;/code> would configure things
appropriately.&lt;/p>
&lt;p>I finally got fed up with this behavior and spent some time chasing
down the problem, and this is what I found:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Calling &lt;code>ifup br-ex&lt;/code> passes control to
&lt;code>/etc/sysconfig/network-scripts/ifup-ovs&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ifup-ovs&lt;/code> calls the &lt;code>check_device_down&lt;/code> function from
&lt;code>network-functions&lt;/code>, which looks like:&lt;/p>
&lt;pre>&lt;code> check_device_down ()
{
[ ! -d /sys/class/net/$1 ] &amp;amp;&amp;amp; return 0
if LC_ALL=C ip -o link show dev $1 2&amp;gt;/dev/null | grep -q &amp;quot;,UP&amp;quot; ; then
return 1
else
return 0
fi
}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>This returns failure (=1) if the interface flags contain &lt;code>,UP&lt;/code>.
Unfortunately, since information about this device is stored
persistently in &lt;code>ovsdb&lt;/code>, the device is already &lt;code>UP&lt;/code> when &lt;code>ifup&lt;/code> is
called, which causes &lt;code>ifup-ovs&lt;/code> to skip further device
configuration. The logic that calls &lt;code>check_device_down&lt;/code> looks like
this:&lt;/p>
&lt;pre>&lt;code>if check_device_down &amp;quot;${DEVICE}&amp;quot;; then
ovs-vsctl -t ${TIMEOUT} -- --may-exist add-br &amp;quot;$DEVICE&amp;quot; $OVS_OPTIONS \
${OVS_EXTRA+-- $OVS_EXTRA} \
${STP+-- set bridge &amp;quot;$DEVICE&amp;quot; stp_enable=&amp;quot;${STP}&amp;quot;}
else
OVSBRIDGECONFIGURED=&amp;quot;yes&amp;quot;
fi
&lt;/code>&lt;/pre>
&lt;p>This sets &lt;code>OVSBRIDGECONFIGURED&lt;/code> if it believes the device is &lt;code>UP&lt;/code>,
which causes &lt;code>ifup-ovs&lt;/code> to skip the call to &lt;code>ifup-eth&lt;/code> to configure
the interface:&lt;/p>
&lt;pre>&lt;code>if [ &amp;quot;${OVSBOOTPROTO}&amp;quot; != &amp;quot;dhcp&amp;quot; ] &amp;amp;&amp;amp; [ -z &amp;quot;${OVSINTF}&amp;quot; ] &amp;amp;&amp;amp; \
[ &amp;quot;${OVSBRIDGECONFIGURED}&amp;quot; != &amp;quot;yes&amp;quot; ]; then
${OTHERSCRIPT} ${CONFIG}
fi
&lt;/code>&lt;/pre>
&lt;p>I have found that the simplest solution to this problem is to disable
the logic that sets &lt;code>OVSBRIDGECONFIGURED&lt;/code>, by changing this:&lt;/p>
&lt;pre>&lt;code>else
OVSBRIDGECONFIGURED=&amp;quot;yes&amp;quot;
fi
&lt;/code>&lt;/pre>
&lt;p>To this:&lt;/p>
&lt;pre>&lt;code>else
: OVSBRIDGECONFIGURED=&amp;quot;yes&amp;quot;
fi
&lt;/code>&lt;/pre>
&lt;p>With this change in place, &lt;code>br-ex&lt;/code> is correctly configured after a
reboot.&lt;/p></content></item><item><title>Firewalld, NetworkManager, and OpenStack</title><link>https://blog.oddbit.com/post/2014-05-20-firewalld-and-openstack/</link><pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-20-firewalld-and-openstack/</guid><description>These are my notes on making OpenStack play well with firewalld and NetworkManager.
NetworkManager By default, NetworkManager attempts to start a DHCP client on every new available interface. Since booting a single instance in OpenStack can result in the creation of several virtual interfaces, this results in a lot of:
May 19 11:58:24 pk115wp-lkellogg NetworkManager[1357]: &amp;lt;info&amp;gt; Activation (qvb512640bd-ee) starting connection 'Wired connection 2' You can disable this behavior by adding the following to /etc/NetworkManager/NetworkManager.</description><content>&lt;p>These are my notes on making OpenStack play well with &lt;a href="https://fedoraproject.org/wiki/FirewallD">firewalld&lt;/a>
and &lt;a href="https://wiki.gnome.org/Projects/NetworkManager">NetworkManager&lt;/a>.&lt;/p>
&lt;h2 id="networkmanager">NetworkManager&lt;/h2>
&lt;p>By default, NetworkManager attempts to start a DHCP client on every
new available interface. Since booting a single instance in OpenStack
can result in the creation of several virtual interfaces, this results
in a lot of:&lt;/p>
&lt;pre>&lt;code>May 19 11:58:24 pk115wp-lkellogg NetworkManager[1357]: &amp;lt;info&amp;gt;
Activation (qvb512640bd-ee) starting connection 'Wired connection 2'
&lt;/code>&lt;/pre>
&lt;p>You can disable this behavior by adding the following to
&lt;code>/etc/NetworkManager/NetworkManager.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[main]
no-auto-default=*
&lt;/code>&lt;/pre>
&lt;p>From &lt;code>NetworkManager.conf(5)&lt;/code>:&lt;/p>
&lt;blockquote>
&lt;p>Comma-separated list of devices for which NetworkManager shouldn&amp;rsquo;t
create default wired connection (Auto eth0). By default,
NetworkManager creates a temporary wired connection for any
Ethernet device that is managed and doesn&amp;rsquo;t have a connection
configured. List a device in this option to inhibit creating the
default connection for the device. May have the special value * to
apply to all devices.&lt;/p>
&lt;/blockquote>
&lt;h2 id="firewalld">FirewallD&lt;/h2>
&lt;p>&lt;a href="https://fedoraproject.org/wiki/FirewallD">FirewallD&lt;/a> is the firewall manager recently introduced in Fedora
(and soon to be appearing in RHEL 7).&lt;/p>
&lt;p>I start by creating a new zone named &lt;code>openstack&lt;/code> by creating the file
&lt;code>/etc/firewalld/zones/openstack.xml&lt;/code> with the following content:&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;zone&amp;gt;
&amp;lt;short&amp;gt;OpenStack&amp;lt;/short&amp;gt;
&amp;lt;description&amp;gt;For OpenStack services&amp;lt;/description&amp;gt;
&amp;lt;/zone&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>After populating this file, you need to run &lt;code>firewall-cmd --reload&lt;/code>
to make the zone available. Note that if you&amp;rsquo;re already running
OpenStack this will hose any rules set up by Neutron or Nova, so
you&amp;rsquo;ll probably want to restart those services:&lt;/p>
&lt;pre>&lt;code># openstack-service restart nova neutron
&lt;/code>&lt;/pre>
&lt;p>I then add &lt;code>br-ex&lt;/code> to this zone, where &lt;code>br-ex&lt;/code> is the OVS bridge my
OpenStack environment uses for external connectivity:&lt;/p>
&lt;pre>&lt;code># echo ZONE=openstack &amp;gt;&amp;gt; /etc/sysconfig/network-scripts/ifcfg-br-ex
&lt;/code>&lt;/pre>
&lt;p>I run a &lt;code>dnsmasq&lt;/code> instance on my laptop to which I expect OpenStack
instances to connect, so I need to add the &lt;code>dns&lt;/code> service to this zone:&lt;/p>
&lt;pre>&lt;code># firewall-cmd --zone openstack --add-service dns
# firewall-cmd --zone openstack --add-service dns --permanent
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m running &lt;code>firewall-cmd&lt;/code> twice here: the first time modifies the
currently running configuration, while the second makes the change
persistent across reboots.&lt;/p>
&lt;p>On my laptop, I handle external connectivity through NAT rather than
placing floating ips on a &amp;ldquo;real&amp;rdquo; network. To make this work, I add my
ethernet and wireless interfaces to the &lt;code>external&lt;/code> zone, which already
has ip masquerading enabled, by adding a &lt;code>ZONE&lt;/code> directive to the
appropriate interface configuration file:&lt;/p>
&lt;pre>&lt;code># echo ZONE=external &amp;gt;&amp;gt; /etc/sysconfig/network-scripts/ifcfg-em1
&lt;/code>&lt;/pre>
&lt;p>After a reboot, things look like this:&lt;/p>
&lt;pre>&lt;code># firewall-cmd --get-active-zones
openstack
interfaces: br-ex
external
interfaces: em1
public
interfaces: int-br-ex phy-br-ex qvb58cc67ca-06 qvo58cc67ca-06
# firewall-cmd --zone openstack --list-services
dns
&lt;/code>&lt;/pre></content></item><item><title>Flat networks with ML2 and OpenVSwitch</title><link>https://blog.oddbit.com/post/2014-05-19-flat-networks-with-ml-and-open/</link><pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-19-flat-networks-with-ml-and-open/</guid><description>Due to an unfortunate incident involving sleep mode and an overheated backpack I had the &amp;ldquo;opportunity&amp;rdquo; to rebuild my laptop. Since this meant reinstalling OpenStack I used this as an excuse to finally move to the ML2 network plugin for Neutron.
I was attempting to add an external network using the normal incantation:
neutron net-create external -- --router:external=true \ --provider:network_type=flat \ --provider:physical_network=physnet1 While this command completed successfully, I was left without any connectivity between br-int and br-ex, despite having in my /etc/neutron/plugins/ml2/ml2_conf.</description><content>&lt;p>Due to an unfortunate incident involving sleep mode and an overheated
backpack I had the &amp;ldquo;opportunity&amp;rdquo; to rebuild my laptop. Since this meant
reinstalling OpenStack I used this as an excuse to finally move to the ML2
network plugin for Neutron.&lt;/p>
&lt;p>I was attempting to add an external network using the normal incantation:&lt;/p>
&lt;pre>&lt;code>neutron net-create external -- --router:external=true \
--provider:network_type=flat \
--provider:physical_network=physnet1
&lt;/code>&lt;/pre>
&lt;p>While this command completed successfully, I was left without any
connectivity between &lt;code>br-int&lt;/code> and &lt;code>br-ex&lt;/code>, despite having in my
&lt;code>/etc/neutron/plugins/ml2/ml2_conf.ini&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[ml2_type_flat]
flat_networks = *
[ovs]
network_vlan_ranges = physnet1
bridge_mappings = physnet1:br-ex
&lt;/code>&lt;/pre>
&lt;p>The reason this is failing is very simple, but not terribly clear from
the existing documentation. This is how the &lt;code>neutron-server&lt;/code> process
is running:&lt;/p>
&lt;pre>&lt;code>/usr/bin/python /usr/bin/neutron-server \
--config-file /usr/share/neutron/neutron-dist.conf \
--config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/plugin.ini \
--log-file /var/log/neutron/server.log
&lt;/code>&lt;/pre>
&lt;p>This is how the &lt;code>neutron-openvswitch-agent&lt;/code> process is running:&lt;/p>
&lt;pre>&lt;code>/usr/bin/python /usr/bin/neutron-openvswitch-agent \
--config-file /usr/share/neutron/neutron-dist.conf \
--config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini \
--log-file /var/log/neutron/openvswitch-agent.log
&lt;/code>&lt;/pre>
&lt;p>Note in particular that &lt;code>neutron-server&lt;/code> is looking at
&lt;code>/etc/neutron/plugin.ini&lt;/code>, which is a symlink to
&lt;code>/etc/neutron/plugins/ml2/ml2_conf.ini&lt;/code>, while
&lt;code>neutron-openvswitch-agent&lt;/code> is looking explicitly at
&lt;code>/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini&lt;/code>. The
physical network configuration needs to go into the
&lt;code>ovs_neutron_plugin.ini&lt;/code> configuration file.&lt;/p></content></item><item><title>Extending Puppet</title><link>https://blog.oddbit.com/post/2014-04-16-article-on-extending-puppet/</link><pubDate>Wed, 16 Apr 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-04-16-article-on-extending-puppet/</guid><description>I wanted to learn about writing custom Puppet types and providers. The official documentation is a little sparse, but I finally stumbled upon the following series of articles by Gary Larizza that provide a great deal of insight into the process and a bunch of example code:
Fun With Puppet Providers Who Abstracted My Ruby? Seriously, What Is This Provider Doing?</description><content>&lt;p>I wanted to learn about writing custom Puppet types and providers.
The official documentation is a little sparse, but I finally stumbled
upon the following series of articles by &lt;a href="http://garylarizza.com/">Gary Larizza&lt;/a> that provide
a great deal of insight into the process and a bunch of example code:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://garylarizza.com/blog/2013/11/25/fun-with-providers/">Fun With Puppet Providers&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://garylarizza.com/blog/2013/11/26/fun-with-providers-part-2/">Who Abstracted My Ruby?&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://garylarizza.com/blog/2013/12/15/seriously-what-is-this-provider-doing/">Seriously, What Is This Provider Doing?&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Multinode OpenStack with Packstack</title><link>https://blog.oddbit.com/post/2014-02-27-multinode-packstack/</link><pubDate>Thu, 27 Feb 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-02-27-multinode-packstack/</guid><description>I was the presenter for this morning&amp;rsquo;s RDO hangout, where I ran through a simple demonstration of setting up a multinode OpenStack deployment using packstack.
The slides are online here.
Here&amp;rsquo;s the video (also available on the event page):</description><content>&lt;p>I was the presenter for this morning&amp;rsquo;s &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> hangout, where I ran
through a simple demonstration of setting up a multinode OpenStack
deployment using &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a>.&lt;/p>
&lt;p>The slides are online &lt;a href="http://goo.gl/Yvmd0P">here&lt;/a>.&lt;/p>
&lt;p>Here&amp;rsquo;s the video (also available on the &lt;a href="https://plus.google.com/events/cm9ff549vmsim737lj7hopk4gao">event page&lt;/a>):&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/DGf-ny25OAw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></content></item><item><title>Show OVS external-ids</title><link>https://blog.oddbit.com/post/2014-01-19-show-ovs-externalids/</link><pubDate>Sun, 19 Jan 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-01-19-show-ovs-externalids/</guid><description>This is just here as a reminder for me:
An OVS interface has a variety of attributes associated with it, including an external-id field that can be used to associate resources outside of OpenVSwitch with the interface. You can view this field with the following command:
$ ovs-vsctl --columns=name,external-ids list Interface Which on my system, with a single virtual instance, looks like this:
# ovs-vsctl --columns=name,external-ids list Interface . . .</description><content>&lt;p>This is just here as a reminder for me:&lt;/p>
&lt;p>An OVS interface has a variety of attributes associated with it, including an
&lt;code>external-id&lt;/code> field that can be used to associate resources outside of
OpenVSwitch with the interface. You can view this field with the following
command:&lt;/p>
&lt;pre>&lt;code>$ ovs-vsctl --columns=name,external-ids list Interface
&lt;/code>&lt;/pre>
&lt;p>Which on my system, with a single virtual instance, looks like this:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl --columns=name,external-ids list Interface
.
.
.
name : &amp;quot;qvo519d7cc4-75&amp;quot;
external_ids : {attached-mac=&amp;quot;fa:16:3e:f7:75:b0&amp;quot;, iface-id=&amp;quot;519d7cc4-7593-4944-af7b-4056436f2d66&amp;quot;, iface-status=active, vm-uuid=&amp;quot;0330b084-03db-4d42-a231-2cd6ad89515b&amp;quot;}
.
.
.
&lt;/code>&lt;/pre>
&lt;p>Note the information contained here:&lt;/p>
&lt;ul>
&lt;li>&lt;code>attached-mac&lt;/code> is the MAC address of the device attached to this interface.&lt;/li>
&lt;li>&lt;code>vm-uuid&lt;/code> is the libvirt UUID for the instance attached to this interface&amp;hellip;&lt;/li>
&lt;li>&amp;hellip;which also happens to be the Nova UUID for the instance.&lt;/li>
&lt;/ul>
&lt;p>So we can pass that UUID to &lt;code>virsh dumpxml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ virsh dumpxml 0330b084-03db-4d42-a231-2cd6ad89515b
&amp;lt;domain type='kvm' id='150'&amp;gt;
&amp;lt;name&amp;gt;instance-0000009c&amp;lt;/name&amp;gt;
&amp;lt;uuid&amp;gt;0330b084-03db-4d42-a231-2cd6ad89515b&amp;lt;/uuid&amp;gt;
&amp;lt;memory unit='KiB'&amp;gt;6144000&amp;lt;/memory&amp;gt;
&amp;lt;currentMemory unit='KiB'&amp;gt;6144000&amp;lt;/currentMemory&amp;gt;
&amp;lt;vcpu placement='static'&amp;gt;1&amp;lt;/vcpu&amp;gt;
.
.
.
&lt;/code>&lt;/pre>
&lt;p>Or to &lt;code>nova show&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova show 0330b084-03db-4d42-a231-2cd6ad89515b
+--------------------------------------+----------------------------------------------------------+
| Property | Value |
+--------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
.
.
.
&lt;/code>&lt;/pre></content></item><item><title>Stupid OpenStack Tricks</title><link>https://blog.oddbit.com/post/2014-01-16-stupid-openstack-tricks/</link><pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-01-16-stupid-openstack-tricks/</guid><description>I work with several different OpenStack installations. I usually work on the command line, sourcing in an appropriate stackrc with credentials as necessary, but occasionally I want to use the dashboard for something.
For all of the deployments with which I work, the keystone endpoint is on the same host as the dashboard. So rather than trying to remember which dashboard url I want for the environment I&amp;rsquo;m currently using on the command line, I put together this shell script:</description><content>&lt;p>I work with several different OpenStack installations. I usually work
on the command line, sourcing in an appropriate &lt;code>stackrc&lt;/code> with
credentials as necessary, but occasionally I want to use the dashboard
for something.&lt;/p>
&lt;p>For all of the deployments with which I work, the keystone endpoint is
on the same host as the dashboard. So rather than trying to remember
which dashboard url I want for the environment I&amp;rsquo;m currently using on
the command line, I put together this shell script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
url=${OS_AUTH_URL%:*}/
exec xdg-open $url
&lt;/code>&lt;/pre>
&lt;p>This takes the value of your &lt;code>OS_AUTH_URL&lt;/code> environment variable,
strips off everything after the port specification, and passes that to
your default browser.&lt;/p></content></item><item><title>Direct access to Nova metadata</title><link>https://blog.oddbit.com/post/2014-01-14-direct-access-to-nova-metadata/</link><pubDate>Tue, 14 Jan 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-01-14-direct-access-to-nova-metadata/</guid><description>When you boot a virtual instance under OpenStack, your instance has access to certain instance metadata via the Nova metadata service, which is canonically available at http://169.254.169.254/.
In an environment running Neutron, a request from your instance must traverse a number of steps:
From the instance to a router, Through a NAT rule in the router namespace, To an instance of the neutron-ns-metadata-proxy, To the actual Nova metadata service When there are problem accessing the metadata, it can be helpful to verify that the metadata service itself is configured correctly and returning meaningful information.</description><content>&lt;p>When you boot a virtual instance under &lt;a href="http://www.openstack.org/">OpenStack&lt;/a>, your instance
has access to certain &lt;a href="http://docs.openstack.org/admin-guide-cloud/content//section_metadata-service.html">instance metadata&lt;/a> via the Nova metadata service,
which is canonically available at &lt;a href="http://169.254.169.254/">http://169.254.169.254/&lt;/a>.&lt;/p>
&lt;p>In an environment running &lt;a href="https://wiki.openstack.org/wiki/Neutron">Neutron&lt;/a>, a request from your instance
must traverse a number of steps:&lt;/p>
&lt;ul>
&lt;li>From the instance to a router,&lt;/li>
&lt;li>Through a NAT rule in the router namespace,&lt;/li>
&lt;li>To an instance of the neutron-ns-metadata-proxy,&lt;/li>
&lt;li>To the actual Nova metadata service&lt;/li>
&lt;/ul>
&lt;p>When there are problem accessing the metadata, it can be helpful to
verify that the metadata service itself is configured correctly and
returning meaningful information.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Naively trying to contact the Nova metadata service listening on port
8775 will, not unexpectedly, fail:&lt;/p>
&lt;pre>&lt;code>$ curl http://localhost:8775/latest/meta-data/
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;400 Bad Request&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;400 Bad Request&amp;lt;/h1&amp;gt;
X-Instance-ID header is missing from request.&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;
&amp;lt;/body&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>You can grab the UUID of a running instance with &lt;code>nova list&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova list
+--------------------------------------+-------...
| ID | Name ...
+--------------------------------------+-------...
| 32d0524b-314d-4594-b3a3-607e3f2354f8 | test0 ...
+--------------------------------------+-------...
&lt;/code>&lt;/pre>
&lt;p>You can retry your request with an appropraite &lt;code>X-Instance-ID&lt;/code> header
(&lt;code>-H 'x-instance-id: 32d0524b-314d-4594-b3a3-607e3f2354f8'&lt;/code>), but
ultimately (after also adding the tenant id), you&amp;rsquo;ll find that you
need to add an &lt;code>x-instance-id-signature&lt;/code> header. If you investigate
the &lt;a href="https://github.com/openstack/nova/blob/master/nova/api/metadata/handler.py">Nova source code&lt;/a>, you&amp;rsquo;ll find that this header is calculated
via an HMAC over the instance ID and a shared secret:&lt;/p>
&lt;pre>&lt;code>expected_signature = hmac.new(
CONF.neutron_metadata_proxy_shared_secret,
instance_id,
hashlib.sha256).hexdigest()
&lt;/code>&lt;/pre>
&lt;p>You can get the shared secret from &lt;code>/etc/nova/nova.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code># grep shared_secret /etc/nova/nova.conf
neutron_metadata_proxy_shared_secret=deadbeef2eb84d8d
&lt;/code>&lt;/pre>
&lt;p>And insert that into the previous Python code:&lt;/p>
&lt;pre>&lt;code>Python 2.7.5 (default, Nov 12 2013, 16:18:42)
[GCC 4.8.2 20131017 (Red Hat 4.8.2-1)] on linux2
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
&amp;gt;&amp;gt;&amp;gt; import hmac
&amp;gt;&amp;gt;&amp;gt; import hashlib
&amp;gt;&amp;gt;&amp;gt; hmac.new('deadbeef2eb84d8d',
&amp;gt;&amp;gt;&amp;gt; '32d0524b-314d-4594-b3a3-607e3f2354f8',
&amp;gt;&amp;gt;&amp;gt; hashlib.sha256).hexdigest()
'6bcbe3885ae7efc49cef35b438efe29c95501f4a720a0c53ed000d8fcf04a605'
&amp;gt;&amp;gt;&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And now make a request directly to the metadata service:&lt;/p>
&lt;pre>&lt;code>$ curl \
-H 'x-instance-id: 32d0524b-314d-4594-b3a3-607e3f2354f8' \
-H 'x-tenant-id: 28a490a0f8b28800181ce490a74df8d2' \
-H 'x-instance-id-signature: 6bcbe3885ae7efc49cef35b438efe29c95501f4a720a0c53ed000d8fcf04a605' \
http://localhost:8775/latest/meta-data
ami-id
ami-launch-index
ami-manifest-path
block-device-mapping/
hostname
instance-action
instance-id
instance-type
kernel-id
local-hostname
local-ipv4
placement/
public-hostname
public-ipv4
public-keys/
ramdisk-id
reservation-id
&lt;/code>&lt;/pre>
&lt;p>And you&amp;rsquo;re done!&lt;/p></content></item><item><title>RDO Bug Triage</title><link>https://blog.oddbit.com/post/2014-01-13-rdo-bug-triage/</link><pubDate>Mon, 13 Jan 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-01-13-rdo-bug-triage/</guid><description>This Wednesday, January 15, at 14:00 UTC (that&amp;rsquo;s 9AM US/Eastern, or date -d &amp;quot;14:00 UTC&amp;quot; in your local timezone) I will be helping out with the RDO bug triage day. We&amp;rsquo;ll be trying to validate all the untriaged bugs opened against RDO.
Feel free to drop by on #rdo and help out or ask questions.</description><content>&lt;p>This Wednesday, January 15, at 14:00 UTC (that&amp;rsquo;s 9AM US/Eastern, or
&lt;code>date -d &amp;quot;14:00 UTC&amp;quot;&lt;/code> in your local timezone) I will be helping out
with the
&lt;a href="http://openstack.redhat.com/">RDO&lt;/a> &lt;a href="http://openstack.redhat.com/RDO-BugTriage">bug triage day&lt;/a>. We&amp;rsquo;ll be trying to validate all the
&lt;a href="http://goo.gl/NqW2LN">untriaged bugs&lt;/a> opened against RDO.&lt;/p>
&lt;p>Feel free to drop by on &lt;code>#rdo&lt;/code> and help out or ask questions.&lt;/p></content></item><item><title>Visualizing Neutron Networking with GraphViz</title><link>https://blog.oddbit.com/post/2013-12-23-visualizing-network-with-graph/</link><pubDate>Mon, 23 Dec 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-12-23-visualizing-network-with-graph/</guid><description>I&amp;rsquo;ve put together a few tools to help gather information about your Neutron and network configuration and visualize it in different ways. All of these tools are available as part of my neutron-diag repository on GitHub.
In this post I&amp;rsquo;m going to look at a tool that will help you visualize the connectivity of network devices on your system.
mk-network-dot There are a lot of devices involved in your Neutron network configuration.</description><content>&lt;p>I&amp;rsquo;ve put together a few tools to help gather information about your
Neutron and network configuration and visualize it in different ways.
All of these tools are available as part of my &lt;a href="http://github.com/larsks/neutron-diag/">neutron-diag&lt;/a>
repository on GitHub.&lt;/p>
&lt;p>In this post I&amp;rsquo;m going to look at a tool that will help you visualize
the connectivity of network devices on your system.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="mk-network-dot">mk-network-dot&lt;/h2>
&lt;p>There are a lot of devices involved in your Neutron network
configuration. Information originating in one of your instances has
two traverse &lt;em>at least&lt;/em> seven network devices before seeing the light
of day. Understanding how everything connects is critical if you&amp;rsquo;re
trying to debug problems in your envionment.&lt;/p>
&lt;p>The &lt;code>mk-network-dot&lt;/code> tool interrogates your system for information
about network devices and generates &lt;a href="http://en.wikipedia.org/wiki/DOT_%28graph_description_language%29">dot format&lt;/a> output showing how
everything connects. You can use &lt;a href="http://www.graphviz.org/">GraphViz&lt;/a> to render this into a
variety of output formats. The script must be run as &lt;code>root&lt;/code>, so I
usually do something like this:&lt;/p>
&lt;pre>&lt;code>sudo sh mk-network-dot | dot -Tsvg -o network.svg
&lt;/code>&lt;/pre>
&lt;p>The &lt;em>dot&lt;/em> language is a language designed for describing graphs, and
the syntax looks something like this:&lt;/p>
&lt;pre>&lt;code>digraph example {
A -&amp;gt; B
A -&amp;gt; C
C -&amp;gt; D
B -&amp;gt; D
}
&lt;/code>&lt;/pre>
&lt;p>Which would produce output like this:&lt;/p>
&lt;p>&lt;img src="dot-example.svg" alt="Dot output example">&lt;/p>
&lt;p>When run on my laptop, with a simple all-in-one configuration and five
instances across two networks, the result of running &lt;code>mk-network-dot&lt;/code>
looks like this:&lt;/p>
&lt;figure class="left" >
&lt;img src="network.svg" />
&lt;/figure>
&lt;p>There are a few caveats with this tool:&lt;/p>
&lt;ul>
&lt;li>As of this writing, it doesn&amp;rsquo;t know about either bond interfaces or
VLAN interfaces.&lt;/li>
&lt;li>It&amp;rsquo;s had only limited testing.&lt;/li>
&lt;/ul>
&lt;p>If you try this out and something doesn&amp;rsquo;t work as you expect, please
open a new issues on the &lt;a href="https://github.com/larsks/neutron-diag/issues">GitHub issues page&lt;/a>.&lt;/p></content></item><item><title>An introduction to OpenStack Heat</title><link>https://blog.oddbit.com/post/2013-12-06-an-introduction-to-openstack-h/</link><pubDate>Fri, 06 Dec 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-12-06-an-introduction-to-openstack-h/</guid><description>Heat is a template-based orchestration mechanism for use with OpenStack. With Heat, you can deploy collections of resources &amp;ndash; networks, servers, storage, and more &amp;ndash; all from a single, parameterized template.
In this article I will introduce Heat templates and the heat command line client.
Writing templates Because Heat began life as an analog of AWS CloudFormation, it supports the template formats used by the CloudFormation (CFN) tools. It also supports its own native template format, called HOT (&amp;ldquo;Heat Orchestration Templates&amp;rdquo;).</description><content>&lt;p>&lt;a href="https://wiki.openstack.org/wiki/Heat">Heat&lt;/a> is a template-based orchestration mechanism for use with
OpenStack. With Heat, you can deploy collections of resources &amp;ndash;
networks, servers, storage, and more &amp;ndash; all from a single,
parameterized template.&lt;/p>
&lt;p>In this article I will introduce Heat templates and the &lt;code>heat&lt;/code> command
line client.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="writing-templates">Writing templates&lt;/h2>
&lt;p>Because Heat began life as an analog of AWS &lt;a href="http://aws.amazon.com/cloudformation/">CloudFormation&lt;/a>, it
supports the template formats used by the CloudFormation (CFN) tools.
It also supports its own native template format, called HOT (&amp;ldquo;Heat
Orchestration Templates&amp;rdquo;). In this article I will be using the HOT
template syntax, which is fully specified on &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html">the OpenStack
website&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>NB: Heat is under active development, and there are a variety of
discussions going on right now regarding the HOT specification. I
will try to keep this post up-to-date as the spec evolves.&lt;/p>
&lt;/blockquote>
&lt;p>A HOT template is written using &lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a> syntax and has three major
sections:&lt;/p>
&lt;ul>
&lt;li>&lt;em>parameters&lt;/em> &amp;ndash; these are input parameters that you provide when you
deploy from the template.&lt;/li>
&lt;li>&lt;em>resources&lt;/em> &amp;ndash; these are things created by the template.&lt;/li>
&lt;li>&lt;em>outputs&lt;/em> &amp;ndash; these are output parameters generated by Heat and
available to you via the API.&lt;/li>
&lt;/ul>
&lt;h3 id="parameters">Parameters&lt;/h3>
&lt;p>The &lt;code>parameters&lt;/code> section defines the list of available parameters.
For each parameter, you define a data type, an optional default value
(that will be used if you do not otherwise specify a value for the
parameter), an optional description, constraints to validate the data,
and so forth. The definition from &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html">the spec&lt;/a> looks like this:&lt;/p>
&lt;pre>&lt;code>parameters:
&amp;lt;param name&amp;gt;:
type: &amp;lt;string | number | json | comma_delimited_list&amp;gt;
description: &amp;lt;description of the parameter&amp;gt;
default: &amp;lt;default value for parameter&amp;gt;
hidden: &amp;lt;true | false&amp;gt;
constraints:
&amp;lt;parameter constraints&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>A simple example might look like this:&lt;/p>
&lt;pre>&lt;code>parameters:
flavor:
type: string
default: m1.small
constraints:
- allowed_values: [m1.nano, m1.tiny, m1.small, m1.large]
description: Value must be one of 'm1.tiny', 'm1.small' or 'm1.large'
&lt;/code>&lt;/pre>
&lt;p>This defines one parameter named &lt;code>flavor&lt;/code> with a default value of
&lt;code>m1.small&lt;/code>. Any value passed in when you deploy from this template
must match of one the values in the &lt;code>allowed_values&lt;/code> constraint.&lt;/p>
&lt;h3 id="resources">Resources&lt;/h3>
&lt;p>The &lt;code>resources&lt;/code> section of your template defines the items that will
be created by Heat when you deploy from your template. This may
include storage, networks, ports, routers, security groups, firewall
rules, or any other of the &lt;a href="http://docs.openstack.org/developer/heat/template_guide/openstack.html">many available resources&lt;/a>.&lt;/p>
&lt;p>The definition of this section from &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html">the spec&lt;/a> looks like
this:&lt;/p>
&lt;pre>&lt;code>resources:
&amp;lt;resource ID&amp;gt;:
type: &amp;lt;resource type&amp;gt;
properties:
&amp;lt;property name&amp;gt;: &amp;lt;property value&amp;gt;
# more resource specific metadata
&lt;/code>&lt;/pre>
&lt;p>Here&amp;rsquo;s a simple example that would create a single server:&lt;/p>
&lt;pre>&lt;code>resources:
instance0:
type: OS::Nova::Server
properties:
name: instance0
image: cirros
flavor: m1.tiny
key_name: mykey
&lt;/code>&lt;/pre>
&lt;p>The complete list of resources and their available properties can be
found &lt;a href="http://docs.openstack.org/developer/heat/template_guide/openstack.html">in the documentation&lt;/a>.&lt;/p>
&lt;p>You&amp;rsquo;ll notice that the above example is static: it will always result
in an instance using the &lt;code>cirros&lt;/code> image and the &lt;code>m1.tiny&lt;/code> flavor.
This isn&amp;rsquo;t terribly useful, so let&amp;rsquo;s redefine this example assuming
that we have available the &lt;code>parameter&lt;/code> section from the previous
example:&lt;/p>
&lt;pre>&lt;code>resources:
instance0:
type: OS::Nova::Server
properties:
name: instance0
image: cirros
flavor: {get_param: flavor}
key_name: mykey
&lt;/code>&lt;/pre>
&lt;p>Here we are using the &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#get-param">get_param&lt;/a> &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#intrinsic-functions">intrinsic function&lt;/a> to
retrieve an insert the value of the &lt;code>flavor&lt;/code> parameter.&lt;/p>
&lt;h3 id="outputs">Outputs&lt;/h3>
&lt;p>The &lt;code>outputs&lt;/code> section of your template defines parameters that will be
available to you (via the API or command line client) after your stack
has been deployed. This may include things like this ip addresses
assigned to your instances. The &lt;code>outputs&lt;/code> section definition is:&lt;/p>
&lt;pre>&lt;code>outputs:
&amp;lt;parameter name&amp;gt;:
description: &amp;lt;description&amp;gt;
value: &amp;lt;parameter value&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>In order to make the &lt;code>outputs&lt;/code> section useful, we&amp;rsquo;ll need another
template function, &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#get-attr">get_attr&lt;/a>. Where &lt;code>get_param&lt;/code> accesses values from
your &lt;code>parameters&lt;/code> section, &lt;code>get_attr&lt;/code> accesses attributes of your
resources. For example:&lt;/p>
&lt;pre>&lt;code>outputs:
instance_ip:
value: {get_attr: [instance0, first_address]}
&lt;/code>&lt;/pre>
&lt;p>You will again want to refer to the &lt;a href="http://docs.openstack.org/developer/heat/template_guide/openstack.html">list of resource types&lt;/a>
for a list of available attributes.&lt;/p>
&lt;h3 id="putting-it-all-together">Putting it all together&lt;/h3>
&lt;p>Using the above information, let&amp;rsquo;s put together a slightly more
complete template. This example will:&lt;/p>
&lt;ul>
&lt;li>Deploy a single instance&lt;/li>
&lt;li>Assign it a floating ip address&lt;/li>
&lt;li>Ensure ssh access via an ssh key published to Nova&lt;/li>
&lt;/ul>
&lt;p>We&amp;rsquo;ll get the flavor name, image name, key name, and network
information from user-provided parameters.&lt;/p>
&lt;p>Since this is a complete example, we need to add the
&lt;code>heat_template_version&lt;/code> key to our template:&lt;/p>
&lt;pre>&lt;code>heat_template_version: 2013-05-23
&lt;/code>&lt;/pre>
&lt;p>And a description provides useful documentation:&lt;/p>
&lt;pre>&lt;code>description: &amp;gt;
A simple HOT template for demonstrating Heat.
&lt;/code>&lt;/pre>
&lt;p>We define parameters for the key name, flavor, and image, as well as
network ids for address provisioning:&lt;/p>
&lt;pre>&lt;code>parameters:
key_name:
type: string
default: lars
description: Name of an existing key pair to use for the instance
flavor:
type: string
description: Instance type for the instance to be created
default: m1.small
constraints:
- allowed_values: [m1.nano, m1.tiny, m1.small, m1.large]
description: Value must be one of 'm1.tiny', 'm1.small' or 'm1.large'
image:
type: string
default: cirros
description: ID or name of the image to use for the instance
private_net_id:
type: string
description: Private network id
private_subnet_id:
type: string
description: Private subnet id
public_net_id:
type: string
description: Public network id
&lt;/code>&lt;/pre>
&lt;p>In the &lt;code>resources&lt;/code> section, we define a single instance of
&lt;code>OS::Nova::Server&lt;/code>, attach to it an instance of &lt;code>OS::Neutron::Port&lt;/code>,
and attach to that port an instance of &lt;code>OS::Neutron::FloatingIP&lt;/code>.
Note that use of the &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#get-resource">get_resource&lt;/a> function here to refer to a
resource defined elsewhere in the template:&lt;/p>
&lt;pre>&lt;code>resources:
instance0:
type: OS::Nova::Server
properties:
name: instance0
image: { get_param: image }
flavor: { get_param: flavor }
key_name: { get_param: key_name }
networks:
- port: { get_resource: instance0_port0 }
instance0_port0:
type: OS::Neutron::Port
properties:
network_id: { get_param: private_net_id }
security_groups:
- default
fixed_ips:
- subnet_id: { get_param: private_subnet_id }
instance0_public:
type: OS::Neutron::FloatingIP
properties:
floating_network_id: { get_param: public_net_id }
port_id: { get_resource: instance0_port0 }
&lt;/code>&lt;/pre>
&lt;p>As outputs we provide the fixed and floating ip addresses assigned to
our instance:&lt;/p>
&lt;pre>&lt;code>outputs:
instance0_private_ip:
description: IP address of instance0 in private network
value: { get_attr: [ instance0, first_address ] }
instance0_public_ip:
description: Floating IP address of instance0 in public network
value: { get_attr: [ instance0_public, floating_ip_address ] }
&lt;/code>&lt;/pre>
&lt;h2 id="filling-in-the-blanks">Filling in the blanks&lt;/h2>
&lt;p>Now that we have a complete template, what do we do with it?&lt;/p>
&lt;p>When you deploy from a template, you need to provide values for any
parameters required by the template (and you may also want to override
default values). You can do this using the &lt;code>-P&lt;/code> (aka &lt;code>--parameter&lt;/code>)
command line option, which takes a semicolon-delimited list of
&lt;code>name=value&lt;/code> pairs:&lt;/p>
&lt;pre>&lt;code>heat stack-create -P 'param1=value1;param2=value2' ...
&lt;/code>&lt;/pre>
&lt;p>While this works, it&amp;rsquo;s not terribly useful, especially as the
parameter list grows long. You can also provide parameters via an
&lt;a href="https://wiki.openstack.org/wiki/Heat/Environments">environment file&lt;/a>, which a YAML configuration file containing a
&lt;code>parameters&lt;/code> key (you can use environment files for other things, too,
but here we&amp;rsquo;re going to focus on their use for template parameters).
A sample file, equivalent to arguments to &lt;code>-P&lt;/code> in the above command
line, might look like:&lt;/p>
&lt;pre>&lt;code>parameters:
param1: value1
param2: value2
&lt;/code>&lt;/pre>
&lt;p>An environment file appropriate to our example template from the
previous section might look like this:&lt;/p>
&lt;pre>&lt;code>parameters:
image: fedora-19-x86_64
flavor: m1.small
private_net_id: 99ab8ebf-ad2f-4a4b-9890-fee37cea4254
private_subnet_id: ed8ad5f5-4c47-4204-9ca3-1b3bc4de286d
public_net_id: 7e687cc3-8155-4ec2-bd11-ba741ecbf4f0
&lt;/code>&lt;/pre>
&lt;p>You would, of course, need to replace the network ids with ones
appropriate to your environment.&lt;/p>
&lt;h2 id="command-line-client">Command line client&lt;/h2>
&lt;p>With the template in a file called &lt;code>template.yml&lt;/code> and the parameters
in a file called &lt;code>environment.yml&lt;/code>, we could deploy an instance like
this:&lt;/p>
&lt;pre>&lt;code>heat stack-create -f template.yml \
-e environment.yml mystack
&lt;/code>&lt;/pre>
&lt;p>This would, assuming no errors, create a stack called &lt;code>mystack&lt;/code>. You
can view the status of your stacks with the &lt;code>stack-list&lt;/code> subcommand:&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
+-------+------------+-----------------+----------------------+
| id | stack_name | stack_status | creation_time |
+-------+------------+-----------------+----------------------+
| 0...6 | mystack | CREATE_COMPLETE | 2013-12-06T21:37:32Z |
+-------+------------+-----------------+----------------------+
&lt;/code>&lt;/pre>
&lt;p>You can view detailed information about your stack &amp;ndash; including the
values of your outputs &amp;ndash; using the &lt;code>stack-show&lt;/code> subcommand:&lt;/p>
&lt;pre>&lt;code>$ heat stack-show mystack
&lt;/code>&lt;/pre>
&lt;p>(The output is a little too verbose to include here.)&lt;/p>
&lt;p>If you want more convenient access to the values of your outputs,
you&amp;rsquo;re going to have to either make direct use of the Heat &lt;a href="http://api.openstack.org/api-ref-orchestration.html">REST
API&lt;/a>, or wait for my &lt;a href="https://review.openstack.org/#/c/60591/">proposed change&lt;/a> to the
&lt;a href="https://launchpad.net/python-heatclient">python-heatclient&lt;/a> package, which will add the &lt;code>output-list&lt;/code> and
&lt;code>output-get&lt;/code> subcommands:&lt;/p>
&lt;pre>&lt;code>$ heat output-list mystack
instance0_private_ip
instance0_public_ip
$ heat output-get mystack instance0_public_ip
192.168.122.203
&lt;/code>&lt;/pre>
&lt;h2 id="working-with-neutron">Working with Neutron&lt;/h2>
&lt;p>If you are using Heat in an environment that uses &lt;a href="https://wiki.openstack.org/wiki/Neutron">Neutron&lt;/a> for
networking you may need to take a few additional steps. By default,
your virtual instances will not be associated with &lt;em>any&lt;/em> security
groups, which means that they will have neither outbound or inbound
network connectivity. This is in contrast to instances started using
the &lt;code>nova boot&lt;/code> command, which will automatically be members of the
&lt;code>default&lt;/code> security group.&lt;/p>
&lt;p>In order to provide your instances with appropriate network
connectivity, you will need to associate each &lt;code>OS::Neutron::Port&lt;/code>
resource in your template with one or more security groups. For
example, the following configuration snippet would create a port
&lt;code>instance0_port0&lt;/code> and assign it to the &lt;code>default&lt;/code> and &lt;code>webserver&lt;/code>
security groups:&lt;/p>
&lt;pre>&lt;code>instance0_port0:
type: OS::Neutron::Port
properties:
network_id: { get_param: private_net_id }
security_groups:
- default
- webserver
fixed_ips:
- subnet_id: { get_param: private_subnet_id }
&lt;/code>&lt;/pre>
&lt;p>For this to work, you will need to be running a (very) recent version
of Heat. Until commit &lt;a href="https://github.com/openstack/heat/commit/902154c">902154c&lt;/a>, Heat was unable to look up Neutron security
groups by name. It worked fine if you specified security groups by
UUID:&lt;/p>
&lt;pre>&lt;code>security_groups:
- 4296f3ff-9dc0-4b0b-a633-c30eacc8493d
- 8c49cd42-7c42-4a1f-af1d-492a0687fc12
&lt;/code>&lt;/pre>
&lt;h2 id="see-also">See also&lt;/h2>
&lt;ul>
&lt;li>The Heat project maintains a &lt;a href="https://github.com/openstack/heat-templates">repository of example
templates&lt;/a>.&lt;/li>
&lt;/ul></content></item><item><title>A Python interface to signalfd() using FFI</title><link>https://blog.oddbit.com/post/2013-11-28-a-python-interface-to-signalfd/</link><pubDate>Thu, 28 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-28-a-python-interface-to-signalfd/</guid><description>I just recently learned about the signalfd(2) system call, which was introduced to the Linux kernel back in 2007:
signalfd() creates a file descriptor that can be used to accept signals targeted at the caller. This provides an alternative to the use of a signal handler or sigwaitinfo(2), and has the advantage that the file descriptor may be monitored by select(2), poll(2), and epoll(7).
The traditional asynchronous delivery mechanism can be tricky to get right, whereas this provides a convenient fd interface that integrates nicely with your existing event-based code.</description><content>&lt;p>I just recently learned about the &lt;code>signalfd(2)&lt;/code> system call, which was
introduced to the Linux kernel &lt;a href="http://lwn.net/Articles/225714/">back in 2007&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>signalfd() creates a file descriptor that can be used to accept
signals targeted at the caller. This provides an alternative to
the use of a signal handler or sigwaitinfo(2), and has the
advantage that the file descriptor may be monitored by select(2),
poll(2), and epoll(7).&lt;/p>
&lt;/blockquote>
&lt;p>The traditional asynchronous delivery mechanism can be tricky to get
right, whereas this provides a convenient fd interface that integrates
nicely with your existing event-based code.&lt;/p>
&lt;p>I was interested in using &lt;code>signalfd()&lt;/code> in some Python code, but Python
does not expose this system call through any of the standard
libraries. There are a variety of ways one could add support,
including:&lt;/p>
&lt;ul>
&lt;li>Writing a Python module in C&lt;/li>
&lt;li>Using the &lt;code>ctypes&lt;/code> module (which I played with &lt;a href="https://blog.oddbit.com/post/2010-08-10-python-ctypes-module/">a few years ago&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>However, I decided to use this as an excuse to learn about the
&lt;a href="https://pypi.python.org/pypi/cffi">cffi&lt;/a> module. You can find the complete code in my
&lt;a href="https://github.com/larsks/python-signalfd">python-signalfd&lt;/a> repository and an explanation of the process
below.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The &lt;a href="http://cffi.readthedocs.org/">cffi documentation&lt;/a> lists a number of principles the project
tries to follow; the first two read as follows:&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>The goal is to call C code from Python. You should be able to do so without learning a 3rd language: every alternative requires you to learn their own language (Cython, SWIG) or API (ctypes)&amp;hellip;&lt;/li>
&lt;li>Keep all the Python-related logic in Python so that you don&amp;rsquo;t need to write much C code (unlike CPython native C extensions).&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>In practice, what this means is that if the C API documentation for a
function looks like this:&lt;/p>
&lt;pre>&lt;code>unsigned int sleep(unsigned int seconds);
&lt;/code>&lt;/pre>
&lt;p>Then you can make this function available in Python like this:&lt;/p>
&lt;pre>&lt;code>from cffi import FFI
ffi = FFI()
crt = ffi.dlopen(None)
ffi.cdef('unsigned int sleep(unsigned int seconds);')
&lt;/code>&lt;/pre>
&lt;p>And to use it:&lt;/p>
&lt;pre>&lt;code>crt.sleep(10)
&lt;/code>&lt;/pre>
&lt;p>While this works great for a simple function like &lt;code>sleep&lt;/code>, it gets
slightly more complicated when you function prototype looks like this:&lt;/p>
&lt;pre>&lt;code> #include &amp;lt;sys/signalfd.h&amp;gt;
int signalfd(int fd, const sigset_t *mask, int flags);
&lt;/code>&lt;/pre>
&lt;p>If you try what seems obvious given the above example:&lt;/p>
&lt;pre>&lt;code>ffi.cdef('''
#include &amp;lt;sys/signalfd.h&amp;gt;
int signalfd(int fd, const sigset_t *mask, int flags);
''')
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll run into an error:&lt;/p>
&lt;pre>&lt;code>cffi.api.CDefError: cannot parse &amp;quot;#include &amp;lt;sys/signalfd.h&amp;gt;&amp;quot;
:3: Directives not supported yet
&lt;/code>&lt;/pre>
&lt;p>You can try that without the &lt;code>#include&lt;/code> statement, but you&amp;rsquo;ll just get
a new error:&lt;/p>
&lt;pre>&lt;code>cffi.api.CDefError: cannot parse &amp;quot;int signalfd(int fd, const sigset_t *mask, int flags);&amp;quot;
:3:37: before: *
&lt;/code>&lt;/pre>
&lt;p>What all this means is that you need to translate &lt;code>sigset_t&lt;/code> into
standard C types. You could go digging through include files in
&lt;code>/usr/include&lt;/code>, but an easier method is to create a small C source
file like this:&lt;/p>
&lt;pre>&lt;code>#include &amp;lt;sys/signalfd.h&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And then run it through the preprocessor:&lt;/p>
&lt;pre>&lt;code>$ gcc -E sourcefile.c
&lt;/code>&lt;/pre>
&lt;p>Inspecting the output of this command reveals that &lt;code>sigset_t&lt;/code> is a
typedef for &lt;code>__sigset_t&lt;/code>, and that &lt;code>__sigset_t&lt;/code> looks like this:&lt;/p>
&lt;pre>&lt;code>typedef struct
{
unsigned long int __val[(1024 / (8 * sizeof (unsigned long int)))];
} __sigset_t;
typedef __sigset_t sigset_t;
&lt;/code>&lt;/pre>
&lt;p>If you plug this into your &lt;code>cdef&lt;/code>:&lt;/p>
&lt;pre>&lt;code>ffi.cdef('''
typedef struct
{
unsigned long int __val[(1024 / (8 * sizeof (unsigned long int)))];
} __sigset_t;
typedef __sigset_t sigset_t;
int signalfd(int fd, const sigset_t *mask, int flags);
''')
&lt;/code>&lt;/pre>
&lt;p>You end up with the following:&lt;/p>
&lt;pre>&lt;code>cffi.api.FFIError: unsupported non-constant or not immediately constant expression
&lt;/code>&lt;/pre>
&lt;p>This happens because of the &lt;code>sizeof()&lt;/code> expression in the &lt;code>struct&lt;/code>. We
need to replace that with an actual size. We can use the
&lt;code>ffi.sizeof()&lt;/code> method to accomplish the same thing, like this:&lt;/p>
&lt;pre>&lt;code>ffi.cdef('''
typedef struct
{
unsigned long int __val[%d];
} __sigset_t;
typedef __sigset_t sigset_t;
int signalfd (int fd, const sigset_t * mask, int flags);
''' % ( 1024 / (8 * ffi.sizeof('''unsigned long int''') )))
&lt;/code>&lt;/pre>
&lt;p>This will load without error. You can create a variable suitable for
passing as the &lt;code>mask&lt;/code> parameter to &lt;code>signalfd&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code>&amp;gt;&amp;gt;&amp;gt; mask = ffi.new('sigset_t *')
&amp;gt;&amp;gt;&amp;gt; mask
&amp;lt;cdata 'struct $__sigset_t *' owning 128 bytes&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The trick, of course, is populating that variable correctly. I ended
up just implementing all of the &lt;code>sigsetops&lt;/code> functions, which, having
already set up the &lt;code>sigset_t&lt;/code> structure, meant just adding this:&lt;/p>
&lt;pre>&lt;code>ffi.cdef('''
int sigemptyset(sigset_t *set);
int sigfillset(sigset_t *set);
int sigaddset(sigset_t *set, int signum);
int sigdelset(sigset_t *set, int signum);
int sigismember(const sigset_t *set, int signum);
int sigprocmask(int how, const sigset_t *set, sigset_t *oldset);
''')
&lt;/code>&lt;/pre>
&lt;p>Now we&amp;rsquo;re all set to call these functions through the &lt;code>crt&lt;/code> variable
we created ealier (by calling &lt;code>ffi.dlopen(None)&lt;/code>):&lt;/p>
&lt;pre>&lt;code>&amp;gt;&amp;gt;&amp;gt; import signal
&amp;gt;&amp;gt;&amp;gt; mask = ffi.new('sigset_t *')
&amp;gt;&amp;gt;&amp;gt; crt.sigemptyset(mask)
0
&amp;gt;&amp;gt;&amp;gt; crt.sigismember(mask, signal.SIGINT)
0
&amp;gt;&amp;gt;&amp;gt; crt.sigaddset(mask, signal.SIGINT)
0
&amp;gt;&amp;gt;&amp;gt; crt.sigismember(mask, signal.SIGINT)
1
&lt;/code>&lt;/pre>
&lt;p>And finally, we can all &lt;code>signalfd()&lt;/code>:&lt;/p>
&lt;pre>&lt;code>&amp;gt;&amp;gt;&amp;gt; crt.sigprocmask(0, mask, ffi.NULL)
0
&amp;gt;&amp;gt;&amp;gt; fd = crt.signalfd(-1, mask, 0)
&amp;gt;&amp;gt;&amp;gt; from select import poll
&amp;gt;&amp;gt;&amp;gt; p = poll()
&amp;gt;&amp;gt;&amp;gt; p.register(fd)
&amp;gt;&amp;gt;&amp;gt; p.poll()
^C[(3, 1)]
&amp;gt;&amp;gt;&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>In case it&amp;rsquo;s not obvious from the above example, when I typed
&lt;code>CONTROL-C&lt;/code> on my keyboard, sending a &lt;code>SIGINT&lt;/code> to the Python shell, it
caused the &lt;code>p.poll()&lt;/code> method to exit, reporting activity on fd 3
(which is the fd we were given by &lt;code>signalfd()&lt;/code>). We call
&lt;code>sigprocmask(2)&lt;/code> to prevent the normal asynchronous delivery of
signals, which would otherwise result in Python handling the &lt;code>SIGINT&lt;/code>
and generating a &lt;code>KeyboardInterrupt&lt;/code> exception.&lt;/p>
&lt;p>You can find this all packaged up nicely with a slightly more pythonic
interface in my &lt;a href="https://github.com/larsks/python-signalfd">python-signalfd&lt;/a> repository on GitHub.&lt;/p>
&lt;hr>
&lt;p>&lt;a href="http://gabrbedd.wordpress.com/">Gabe&amp;rsquo;s Geek Log&lt;/a> has an &lt;a href="http://gabrbedd.wordpress.com/2013/07/29/handling-signals-with-signalfd/">article about signalfd&lt;/a> that is also
worth reading.&lt;/p></content></item><item><title>Long polling with Javascript and Python</title><link>https://blog.oddbit.com/post/2013-11-23-long-polling-with-ja/</link><pubDate>Sat, 23 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-23-long-polling-with-ja/</guid><description>In this post I&amp;rsquo;m going to step through an example web chat system implemented in Python (with Bottle and gevent) that uses long polling to implement a simple publish/subscribe mechanism for efficiently updating connected clients.
My pubsub_example repository on GitHub has a complete project that implements the ideas discussed in this article. This project can be deployed directly on OpenShift if you want to try things out on your own. You can also try it out online at http://pubsub.</description><content>&lt;p>In this post I&amp;rsquo;m going to step through an example web chat system
implemented in Python (with &lt;a href="http://bottlepy.org/docs/">Bottle&lt;/a> and &lt;a href="http://www.gevent.org/">gevent&lt;/a>) that uses long
polling to implement a simple publish/subscribe mechanism for
efficiently updating connected clients.&lt;/p>
&lt;p>My &lt;a href="http://github.com/larsks/pusub_example/">pubsub_example&lt;/a> repository on &lt;a href="http://github.com/">GitHub&lt;/a> has a complete
project that implements the ideas discussed in this article. This
project can be deployed directly on &lt;a href="http://openshift.com/">OpenShift&lt;/a> if you want to try
things out on your own. You can also try it out online at
&lt;a href="http://pubsub.example.oddbit.com/">http://pubsub.example.oddbit.com/&lt;/a>.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="what-is-long-polling">What is long polling?&lt;/h2>
&lt;p>Long polling is a technique used in web applications to enable a
low-latency response to server messages without the CPU or traffic
overhead of high frequency polling.&lt;/p>
&lt;p>A client makes a request to the web server, but rather than responding
immediately the server holds the connection (for a potentially lengthy
time), and only response when data is available. The client will
react to this data and then restart the poll and wait for more data.&lt;/p>
&lt;h2 id="long-polling-with-jquery">Long polling with Jquery&lt;/h2>
&lt;p>I&amp;rsquo;ve opted to use &lt;a href="http://jquery.com/">jquery&lt;/a> as part of my client-side implementation,
because I&amp;rsquo;ve used it a little in the past and it simplifies things a
great deal.&lt;/p>
&lt;p>There are a number of articles out there that describe the client-side
implementation of long polling using jquery. &lt;a href="http://techoctave.com/c7/posts/60-simple-long-polling-example-with-javascript-and-jquery">This&lt;/a>
article from Seventh Octave gives what is a typical example:&lt;/p>
&lt;pre>&lt;code>(function poll(){
$.ajax({
url: &amp;quot;/poll&amp;quot;,
success: function (data) {
do_something_with(data);
},
dataType: &amp;quot;json&amp;quot;,
complete: poll,
timeout: 30000
});
})();
&lt;/code>&lt;/pre>
&lt;p>This defines a function &lt;code>poll()&lt;/code> that gets called automatically.
Because I&amp;rsquo;m not a JavaScript person it took me a moment to figure out
exactly how that worked. So that you are less mysified than I: The
basic structure of this function is:&lt;/p>
&lt;pre>&lt;code>(function poll() {...})();
&lt;/code>&lt;/pre>
&lt;p>Since the &lt;code>function&lt;/code> keyword returns a value (a reference to the
defined function), we can call that reference immediately. This
construct is entirely equivalent to:&lt;/p>
&lt;pre>&lt;code>function poll() {...}
poll();
&lt;/code>&lt;/pre>
&lt;p>When called, it fires off an asynchronous AJAX request to &lt;code>/poll&lt;/code> on
our server. If the server sends a response, the browser will call the
function in the &lt;code>success&lt;/code> attribute, which will presumably do
something useful with the response data.&lt;/p>
&lt;p>It&amp;rsquo;s important not to gloss over the fact that the &lt;a href="http://api.jquery.com/jQuery.ajax/">ajax&lt;/a> method is
called asynchronously. The &lt;code>poll()&lt;/code> function returns immediately when
it is called, allowing your client to continue processing your
script(s).&lt;/p>
&lt;p>The callable in the &lt;code>complete&lt;/code> attribute will be called at the end of
the AJAX request, regardless of whether or not the request was
successful. The use of the &lt;code>timeout&lt;/code> attribute ensures that this code
will not poll more frequently than once every 30 seconds (this helps
prevent the code from monopolizing the CPU by polling too frequently
if an error is causing the AJAX request to return immediately).&lt;/p>
&lt;h2 id="a-simple-web-page">A simple web page&lt;/h2>
&lt;p>Our chat application is going be built around a very simple web page
with two fields (one for a &amp;ldquo;nickname&amp;rdquo;, and one for entering messages
to send) and a container for printing messages received from the
server. Stripped of headers and extraneous content, it looks like
this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;form id=&amp;quot;chatform&amp;quot;&amp;gt;
&amp;lt;table&amp;gt;
&amp;lt;tr&amp;gt;
&amp;lt;td&amp;gt;
&amp;lt;label for=&amp;quot;nick&amp;quot;&amp;gt;Nickname:&amp;lt;/label&amp;gt;
&amp;lt;/td&amp;gt;
&amp;lt;td&amp;gt;
&amp;lt;input id=&amp;quot;nick&amp;quot; size=&amp;quot;10&amp;quot; /&amp;gt;
&amp;lt;/td&amp;gt;
&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;
&amp;lt;td&amp;gt;
&amp;lt;label for=&amp;quot;message&amp;quot;&amp;gt;message:&amp;lt;/label&amp;gt;
&amp;lt;/td&amp;gt;
&amp;lt;td&amp;gt;
&amp;lt;input id=&amp;quot;message&amp;quot; size=&amp;quot;40&amp;quot; /&amp;gt;
&amp;lt;button id=&amp;quot;send&amp;quot;&amp;gt;Send&amp;lt;/button&amp;gt;
&amp;lt;/td&amp;gt;
&amp;lt;/tr&amp;gt;
&amp;lt;/table&amp;gt;
&amp;lt;/form&amp;gt;
&amp;lt;hr /&amp;gt;
&amp;lt;div id=&amp;quot;conversation&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>It doesn&amp;rsquo;t get much simpler than that.&lt;/p>
&lt;p>We&amp;rsquo;ll use &lt;a href="http://jquery.com/">jquery&lt;/a> to attach JavaScript functions to various actions
using constructs such as the following, which attaches the
&lt;code>send_message&lt;/code> to the &lt;code>click&lt;/code> event on an element with id &lt;code>send&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$(&amp;quot;#send&amp;quot;).click(send_message);
&lt;/code>&lt;/pre>
&lt;p>Attaching functions this way, rather than using &lt;code>onclick=&lt;/code> attributes
in our HTML, helps keep the HTML simple.&lt;/p>
&lt;h2 id="making-things-go">Making things go&lt;/h2>
&lt;p>For this simple application, our client is going to need to implement
two basic operations:&lt;/p>
&lt;ul>
&lt;li>Sending messages from the user to the server, and&lt;/li>
&lt;li>Receiving messages from the server and displaying them to the user.&lt;/li>
&lt;/ul>
&lt;p>Polling for new messages is implemented using a function that looks
very much like the sample shown above. The final code looks like
this:&lt;/p>
&lt;pre>&lt;code>function poll() {
var poll_interval=0;
$.ajax({
url: poll_url,
type: 'GET',
dataType: 'json',
success: function(data) {
display_message(data);
poll_interval=0;
},
error: function () {
poll_interval=1000;
},
complete: function () {
setTimeout(poll, poll_interval);
},
});
}
&lt;/code>&lt;/pre>
&lt;p>Upon successfully receiving a message from the server this starts
&lt;code>poll()&lt;/code> again immediately, but in the event of an error this code
waits one second (1000 ms) before initiating a new poll.&lt;/p>
&lt;p>The &lt;code>display_message&lt;/code> function simply updates a &lt;code>&amp;lt;div&amp;gt;&lt;/code> element using
the data supplied by the server:&lt;/p>
&lt;pre>&lt;code>function display_message(data) {
$(&amp;quot;#conversation&amp;quot;).append(&amp;quot;&amp;lt;p&amp;gt;&amp;lt;span class='nick'&amp;gt;&amp;quot;
+ (data['nick'] ? data['nick'] : &amp;quot;&amp;amp;lt;unknown&amp;amp;gt;&amp;quot;)
+ &amp;quot;&amp;lt;/span&amp;gt;: &amp;quot; + data['message'] + &amp;quot;&amp;lt;/p&amp;gt;&amp;quot;);
$(&amp;quot;#conversation&amp;quot;).each(function () {
this.scrollTop = this.scrollHeight;
});
}
&lt;/code>&lt;/pre>
&lt;p>Sending messages is even simpler; we just make a one-off AJAX request
to send the message to the server:&lt;/p>
&lt;pre>&lt;code>function send_message() {
$.ajax({
url: '/pub',
type: 'POST',
dataType: 'json',
data: {
nick: $(&amp;quot;#nick&amp;quot;).val(),
message: $(&amp;quot;#message&amp;quot;).val(),
},
complete: function () {
$(&amp;quot;#message&amp;quot;).val(&amp;quot;&amp;quot;);
},
});
}
&lt;/code>&lt;/pre>
&lt;p>This reads the value of the &lt;code>nick&lt;/code> and &lt;code>message&lt;/code> fields in our HTML
document and then posts the message to the server.&lt;/p>
&lt;p>The complete JavaScript code can be found online as &lt;a href="https://github.com/larsks/pubsub_example/blob/master/static/pubsub.js">pubsub.js&lt;/a>.&lt;/p>
&lt;h2 id="making-bottle-asynchronous">Making Bottle asynchronous&lt;/h2>
&lt;p>I generally lean on &lt;a href="http://bottlepy.org/docs/">Bottle&lt;/a> when writing simple Python web
applications. It&amp;rsquo;s simple to work with, but Bottle&amp;rsquo;s native server is
single-threaded, which makes it ill suited to a long-poll scenario:
when one request is active, all other connections will block until the
first request has been serviced. You can test this out yourself with
the following &lt;a href="https://github.com/larsks/pubsub_example/blob/master/example_blocking.py">simple webapp&lt;/a>:&lt;/p>
&lt;pre>&lt;code>import time
import bottle
@bottle.route('/')
def default():
data = [ 'one', 'two', 'three', 'four' ]
for d in data:
yield d
time.sleep(5)
def main():
bottle.run(port=9090)
if __name__ == '__main__':
main()
&lt;/code>&lt;/pre>
&lt;p>Open two simultaneous connections to this application. In two
dfferent windows, run the following command at approximately the same
time:&lt;/p>
&lt;pre>&lt;code>curl --trace-ascii /dev/stderr http://localhost:9090
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll see that one will block with no activity until the first
completes.&lt;/p>
&lt;p>Fortunately, there are a number of solutions to this issue. I opted
to use Bottle&amp;rsquo;s support for &lt;a href="http://www.gevent.org/">gevent&lt;/a>, an asynchronous networking
library that includes a WSGI server. Using Bottle&amp;rsquo;s &lt;code>gevent&lt;/code> support
is easy; the above code, using the &lt;code>gevent&lt;/code> server, would look
&lt;a href="https://github.com/larsks/pubsub_example/blob/master/example_nonblocking.py">like this&lt;/a>:&lt;/p>
&lt;pre>&lt;code>import time
from gevent import monkey; monkey.patch_all()
import bottle
@bottle.route('/')
def default():
data = [ 'one', 'two', 'three', 'four' ]
for d in data:
yield d
time.sleep(5)
def main():
bottle.run(port=9090, server=&amp;quot;gevent&amp;quot;)
if __name__ == '__main__':
main()
&lt;/code>&lt;/pre>
&lt;p>The &lt;a href="http://www.gevent.org/gevent.monkey.html">monkey.patch_all&lt;/a> routine in the above code is necessary to
patch a number of core Python libraries to work correctly with gevent.&lt;/p>
&lt;p>If you re-run the &lt;code>curl --trace-ascii ...&lt;/code> test from earlier, you&amp;rsquo;ll
see that this webapp will now service multiple requests
simultaneously.&lt;/p>
&lt;h2 id="writing-the-server-receiving-messages">Writing the server: receiving messages&lt;/h2>
&lt;p>The server needs to perform two basic operations:&lt;/p>
&lt;ul>
&lt;li>Receive a message from one client, and&lt;/li>
&lt;li>Broadcast that message to all connected clients.&lt;/li>
&lt;/ul>
&lt;p>Receiving a message is easy (we&amp;rsquo;re just grabbing some data from a
&lt;code>POST&lt;/code> request), but how do we handle the broadcast aspect of things?
In this application, I opted to use &lt;a href="http://zeromq.org/">0MQ&lt;/a>, a communication library
that has been described as &amp;ldquo;&lt;a href="https://speakerdeck.com/methodmissing/zeromq-sockets-on-steroids">sockets on steroids&lt;/a>&amp;rdquo;. In this case,
two features of 0MQ are particularly attractive:&lt;/p>
&lt;ul>
&lt;li>support for publish/subscribe communication patterns, and&lt;/li>
&lt;li>an easy to use &lt;a href="http://api.zeromq.org/2-1:zmq-inproc">in-process message transport&lt;/a>&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note that while you would normally import the Python 0MQ module like
this:&lt;/p>
&lt;pre>&lt;code>import zmq
&lt;/code>&lt;/pre>
&lt;p>When working with &lt;a href="http://www.gevent.org/">gevent&lt;/a> you must do this instead:&lt;/p>
&lt;pre>&lt;code>from zmq import green as zmq
&lt;/code>&lt;/pre>
&lt;p>This imports the &amp;ldquo;green&amp;rdquo; version of 0MQ, which uses non-blocking
operations compatible with &lt;code>gevent&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>We&amp;rsquo;ll start by creating a global 0MQ &lt;code>PUB&lt;/code> socket (called &lt;code>pubsock&lt;/code>)
that will be used as one end of our in-process message bus:&lt;/p>
&lt;pre>&lt;code>ctx = zmq.Context()
pubsock = ctx.socket(zmq.PUB)
pubsock.bind('inproc://pub')
&lt;/code>&lt;/pre>
&lt;p>With this in place, the code for receiving messages is trivial:&lt;/p>
&lt;pre>&lt;code>@app.route('/pub', method='POST')
def pub():
global pubsock
pubsock.send_json({
'message': bottle.request.params.get('message'),
'nick': bottle.request.params.get('nick'),
})
return {'status': 'sent'}
&lt;/code>&lt;/pre>
&lt;p>We grab the &lt;code>message&lt;/code> and &lt;code>nick&lt;/code> parameters from a &lt;code>POST&lt;/code> request and
publish a JSON message onto the message bus.&lt;/p>
&lt;h2 id="writing-the-server-sending-messages">Writing the server: sending messages&lt;/h2>
&lt;p>Having received a message from a client, our task is to send that
message out to all connected clients. Each client connected to our
server will be polling the &lt;code>/sub&lt;/code> endpoint for messages. A simple
implementation of &lt;code>/sub&lt;/code> might look like this:&lt;/p>
&lt;pre>&lt;code>@app.route('/sub')
def sub():
subsock = ctx.socket(zmq.SUB)
subsock.setsockopt(zmq.SUBSCRIBE, '')
subsock.connect('inproc://pub')
msg = subsock.recv_json()
return msg
&lt;/code>&lt;/pre>
&lt;p>This sets up a 0MQ &lt;code>SUB&lt;/code> socket and connects it to the message bus
that we established when we created &lt;code>pubsocket&lt;/code>, earlier. Because
we&amp;rsquo;re using 0MQ&amp;rsquo;s publish-and-subscribe support, any message sent on
&lt;code>pubsocket&lt;/code> will automatically be propagated to any connected
subscribers.&lt;/p>
&lt;p>When each client requests &lt;code>/sub&lt;/code>, Bottle runs this function, which blocks
waiting for messages on the message bus. When a message arrives, the
function returns it to the client and exits. Note there&amp;rsquo;s a little
bit of magic here: when a request handler returns a dictionary to
Bottle, Bottle automatically serializes that as JSON before sending
the response to the client.&lt;/p>
&lt;h2 id="fixing-things-up">Fixing things up&lt;/h2>
&lt;p>While the above code seems to work, there is a potential problem with
the implementation: If a client that has been waiting on &lt;code>/sub&lt;/code>
disconnects, the &lt;code>sub&lt;/code> function will continue to remain blocked on the
call to &lt;code>subsock.recv_json&lt;/code>. This means that the server will hold
open the file descriptor associated with the connection. Once a
message is received, the server will attempt to send a response,
notice that the client has disconnected, and close the file
descriptor. Given a large enough population of clients, the number of
open file descriptors could run into system resource limits. In order
to prevent this situation, we need to react to client
disconnects&amp;hellip;which means that now, instead of just blocking waiting
for messages, we &lt;em>also&lt;/em> need to wait for some notification that a
client has disconnected.&lt;/p>
&lt;p>In a traditional socket program, you might do this with something like
the &lt;code>poll()&lt;/code> or &lt;code>select()&lt;/code> function. Since we&amp;rsquo;re using 0MQ&amp;hellip;we&amp;rsquo;ll
do exactly the same thing, which is one of the reasons 0MQ is fun to
work with. We first need to figure out how to detect client
disconnects. The &lt;a href="http://www.python.org/dev/peps/pep-0333/">WSGI specification&lt;/a> doesn&amp;rsquo;t provide a standard way
to expose the client socket to our application. However, inspection
of the WSGI environment (in &lt;code>bottle.request.environ&lt;/code> reveals that the
&lt;code>wsgi.input&lt;/code> member contains an &lt;code>rfile&lt;/code> attribute, which is exactly
what we need. With that in hand, we set up a polling object to listen
for activity on either the message bus or on the client socket:&lt;/p>
&lt;pre>&lt;code>rfile = bottle.request.environ['wsgi.input'].rfile
poll = zmq.Poller()
poll.register(subsock, zmq.POLLIN)
poll.register(rfile, zmq.POLLIN)
&lt;/code>&lt;/pre>
&lt;p>And now we can block waiting for either event:&lt;/p>
&lt;pre>&lt;code>events = dict(poll.poll())
# This means the client has disconnected.
if rfile.fileno() in events:
return
# If we get this far it's because there's a message
# available.
msg = subsock.recv_json()
return msg
&lt;/code>&lt;/pre>
&lt;h2 id="finishing-up">Finishing up&lt;/h2>
&lt;p>Those are pretty much all the parts necessary to implement a simple
publish/subscribe web application in Python. You can see all the
parts put together into a functioning project in the
&lt;a href="http://github.com/larsks/pusub_example/">pubsub_example&lt;/a> repository, and you can try out the running code at
&lt;a href="http://pubsub.example.oddbit.com">http://pubsub.example.oddbit.com&lt;/a>. The code in the repository is
slightly more complete than the snippets presented in this article.&lt;/p>
&lt;p>If you encounter any problems with the code (or this article), or if
I&amp;rsquo;ve gotten something terribly wrong, please open a new issue
&lt;a href="https://github.com/larsks/pubsub_example/issues">here&lt;/a>.&lt;/p></content></item><item><title>Sockets on OpenShift</title><link>https://blog.oddbit.com/post/2013-11-23-openshift-socket-pro/</link><pubDate>Sat, 23 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-23-openshift-socket-pro/</guid><description>In this article, a followup to my previous post regarding long-poll servers and Python, we investigate the code changes that were necessary to make the code work when deployed on OpenShift.
In the previous post, we implemented IO polling to watch for client disconnects at the same time we were waiting for messages on a message bus:
poll = zmq.Poller() poll.register(subsock, zmq.POLLIN) poll.register(rfile, zmq.POLLIN) events = dict(poll.poll()) . . . If you were to try this at home, you would find that everything worked as described&amp;hellip;but if you were to deploy the same code to OpenShift, you would find that the problem we were trying to solve (the server holding file descriptors open after a client disconnected) would still exist.</description><content>&lt;p>In this article, a followup to my &lt;a href="https://blog.oddbit.com/post/2013-11-23-long-polling-with-ja/">previous post&lt;/a> regarding
long-poll servers and Python, we investigate the code changes that
were necessary to make the code work when deployed on OpenShift.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>In the previous post, we implemented IO polling to watch for client
disconnects at the same time we were waiting for messages on a message
bus:&lt;/p>
&lt;pre>&lt;code>poll = zmq.Poller()
poll.register(subsock, zmq.POLLIN)
poll.register(rfile, zmq.POLLIN)
events = dict(poll.poll())
.
.
.
&lt;/code>&lt;/pre>
&lt;p>If you were to try this at home, you would find that everything worked
as described&amp;hellip;but if you were to deploy the same code to OpenShift,
you would find that the problem we were trying to solve (the server
holding file descriptors open after a client disconnected) would still
exist.&lt;/p>
&lt;p>So, what&amp;rsquo;s going on here? I spent a chunk of time trying to figure
this out myself. I finally found &lt;a href="https://www.openshift.com/blogs/paas-websockets">this post&lt;/a> by
Marak Jelen discussing issues with &lt;a href="http://en.wikipedia.org/wiki/WebSocket">websockets&lt;/a> in OpenShift, which
says, among other things:&lt;/p>
&lt;blockquote>
&lt;p>For OpenShift as a PaaS provider, WebSockets were a big challenge.
The routing layer that sits between the user&amp;rsquo;s browser and your
application must be able to route and handle WebSockets. OpenShift
uses Apache as a reverse proxy server and a main component to route
requests throughout the platform. However, Apache&amp;rsquo;s mod_proxy has
been problematic with WebSockets, so OpenShift implemented a new
Node.js based routing layer that provides scalability and the
possibility to expand features provided to our users.&lt;/p>
&lt;/blockquote>
&lt;p>In order to work around these problems, an alternate &lt;a href="http://nodejs.org/">Node.js&lt;/a> based
front-end has been deployed on port 8000. So if your application is
normally available at &lt;code>http://myapplication-myname.rhcloud.com&lt;/code>, you
can also access it at &lt;code>http://myapplication-myname.rhcloud.com:8000&lt;/code>.&lt;/p>
&lt;p>Not unexpectedly, it seems that the same things that can cause
difficulties with WebSockets connections can also interfere with the
operation of a long-poll server. The root of the problem is that your
service running on OpenShift never receives notifications of client
disconnects. You can see this by opening up a connection to your
service. Assuming that you&amp;rsquo;ve deployed the &lt;a href="https://github.com/larsks/pubsub_example/">pubsub example&lt;/a>, you
can run something like this:&lt;/p>
&lt;pre>&lt;code>$ curl http://myapplication-myname.rhcloud.com/sub
&lt;/code>&lt;/pre>
&lt;p>Leave the connection open and &lt;a href="https://www.openshift.com/developers/remote-access">log in to your OpenShift
instance&lt;/a>. Run &lt;code>netstat&lt;/code> to see the existing connection:&lt;/p>
&lt;pre>&lt;code>$ netstat -tan |
grep $OPENSHIFT_PYTHON_IP |
grep $OPENSHIFT_PYTHON_PORT |
grep ESTABLISHED
tcp 0 0 127.6.26.1:15368 127.6.26.1:8080 ESTABLISHED
tcp 0 0 127.6.26.1:8080 127.6.26.1:15368 ESTABLISHED
&lt;/code>&lt;/pre>
&lt;p>Now close your client, and re-run the &lt;code>netstat&lt;/code> command on your
OpenShift instance. You will find that the client connection from
the front-end proxies to your server is still active. Because the
server never receives any notification that the client has closed the
connection, no amount of &lt;code>select&lt;/code> or &lt;code>poll&lt;/code> or anything else will
solve this problem.&lt;/p>
&lt;p>Now, try the same experiment using port 8000. That is, run:&lt;/p>
&lt;pre>&lt;code>$ curl http://myapplication-myname.rhcloud.com:8000/sub
&lt;/code>&lt;/pre>
&lt;p>Verify that when you close your client, the connection is long evident
in your server. This means that we need to modify our JavaScript code
to poll using port 8000, which is why in &lt;a href="https://github.com/larsks/pubsub_example/blob/master/static/pubsub.js">pubsub.js&lt;/a> you will find
the following:&lt;/p>
&lt;pre>&lt;code>if (using_openshift) {
poll_url = location.protocol + &amp;quot;//&amp;quot; + location.hostname + &amp;quot;:8000/sub&amp;quot;;
} else {
poll_url = &amp;quot;/sub&amp;quot;;
}
&lt;/code>&lt;/pre>
&lt;h2 id="but-wait-theres-more">But wait, there&amp;rsquo;s more!&lt;/h2>
&lt;p>If you were to deploy the above code with no other changes, you would
find a mysterious problem: even though your JavaScript console would
show that your code was successfully polling the server, your client
would never update. This is because by introducing an alternate port
number to the poll operation you are now running afoul of your
brower&amp;rsquo;s &lt;a href="http://en.wikipedia.org/wiki/Same-origin_policy">same origin policy&lt;/a>, a security policy that restricts
JavaScript in your browser from interacting with sites other than the
one from which the script was loaded.&lt;/p>
&lt;p>The &lt;a href="http://en.wikipedia.org/wiki/Cross-origin_resource_sharing">CORS&lt;/a> standard introduces a mechanism to work around this
restriction. An HTTP response can contain additional access control
headers that instruct your browser to permit access to the resource from
a select set of other origins. The header is called
&lt;code>Access-Control-Alliow-Origin&lt;/code>, and you will find it in the &lt;a href="https://github.com/larsks/pubsub_example/">pubsub
example&lt;/a> in &lt;a href="https://github.com/larsks/pubsub_example/blob/master/pubsub.py">pubsub.py&lt;/a>:&lt;/p>
&lt;pre>&lt;code> if using_openshift:
bottle.response.headers['Access-Control-Allow-Origin'] = '*'
&lt;/code>&lt;/pre>
&lt;p>With this header in place, your JavaScript can poll your
OpenShift-hosted application on port 8000 and everything will work as
expected&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;barring bugs in my code, which, if discovered, should be reported
&lt;a href="https://github.com/larsks/pubsub_example/issues">here&lt;/a>.&lt;/p></content></item><item><title>A unified CLI for OpenStack</title><link>https://blog.oddbit.com/post/2013-11-22-a-unified-cli-for-op/</link><pubDate>Fri, 22 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-22-a-unified-cli-for-op/</guid><description>The python-openstackclient project, by Dean Troyer and others, is a new command line tool to replace the existing command line clients (including commands such as nova, keystone, cinder, etc).
This tool solves two problems I&amp;rsquo;ve encountered in the past:
Command line options between different command line clients are sometimes inconsistent.
The output from the legacy command line tools is not designed to be machine parse-able (and yet people do it anyway).</description><content>&lt;p>The &lt;a href="https://github.com/openstack/python-openstackclient">python-openstackclient&lt;/a> project, by &lt;a href="https://github.com/dtroyer">Dean Troyer&lt;/a> and
others, is a new command line tool to replace the existing command
line clients (including commands such as &lt;code>nova&lt;/code>, &lt;code>keystone&lt;/code>, &lt;code>cinder&lt;/code>,
etc).&lt;/p>
&lt;p>This tool solves two problems I&amp;rsquo;ve encountered in the past:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Command line options between different command line clients are
sometimes inconsistent.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The output from the legacy command line tools is not designed to be
machine parse-able (and yet people &lt;a href="https://github.com/openstack/python-openstackclient">do it anyway&lt;/a>).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The new &lt;code>openstack&lt;/code> CLI framework is implement using the &lt;a href="https://github.com/dreamhost/cliff">cliff&lt;/a>
module for Python, which will help enforce a consistent interface to
the various subcommands (because common options can be shared, and
just having everything in the same codebase will help tremendously).
Cliff also provides flexible table formatters. It includes a number
of useful formatters out of the box, and can be extended via
setuptools entry points.&lt;/p>
&lt;p>The &lt;code>csv&lt;/code> formatter can be used to produce machine parse-able output
from list commands. For example:&lt;/p>
&lt;pre>&lt;code>$ openstack -q endpoint list -f csv --quote none
ID,Region,Service Name,Service Type
ba686936d31846f5b226539dba285654,RegionOne,quantum,network
161684fd123740138c8806267c489766,RegionOne,cinder,volume
b2019dbef5f34d1bb809e8e399369782,RegionOne,keystone,identity
4b5dd8c6b961442ba13d6b9d317d718a,RegionOne,swift_s3,s3
ac766707ffa3437eaaeaafa3c3eace08,RegionOne,swift,object-store
e3f7bd37b51341bbaa77f81ba39a3bf2,RegionOne,glance,image
6821fad71a914636af6e98775e52e1ec,RegionOne,nova_ec2,ec2
3b2a90e9f85a468988af763c707961d7,RegionOne,nova,compute
&lt;/code>&lt;/pre>
&lt;p>For &amp;ldquo;show&amp;rdquo; commands, the &lt;code>shell&lt;/code> formatter produces output in
&lt;code>name=value&lt;/code> format, like this:&lt;/p>
&lt;pre>&lt;code>$ openstack -q endpoint show image -f shell --all
adminurl=&amp;quot;http://192.168.122.110:9292&amp;quot;
id=&amp;quot;e3f7bd37b51341bbaa77f81ba39a3bf2&amp;quot;
internalurl=&amp;quot;http://192.168.122.110:9292&amp;quot;
publicurl=&amp;quot;http://192.168.122.110:9292&amp;quot;
region=&amp;quot;RegionOne&amp;quot;
service_id=&amp;quot;14a1479f77274dd485e9fb52af2e1721&amp;quot;
service_name=&amp;quot;glance&amp;quot;
service_type=&amp;quot;image&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>This output could easily be sourced into a shell script.&lt;/p></content></item><item><title>Automatic maintenance of tag feeds</title><link>https://blog.oddbit.com/post/2013-11-22-automatic-maintenanc/</link><pubDate>Fri, 22 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-22-automatic-maintenanc/</guid><description>I recently added some scripts to automatically generate tag feeds for my blog when pushing new content. I&amp;rsquo;m using GitHub Pages to publish everything, so it seemed easiest to make tag generation part of a pre-push hook (new in Git 1.8.2). This hook is run automatically as part of the git push operation, so it&amp;rsquo;s the perfect place to insert generated content that must be kept in sync with posts on the blog.</description><content>&lt;p>I recently added some scripts to automatically generate tag feeds for
my blog when pushing new content. I&amp;rsquo;m using GitHub Pages to publish
everything, so it seemed easiest to make tag generation part of a
&lt;code>pre-push&lt;/code> hook (new in Git 1.8.2). This hook is run automatically as
part of the &lt;code>git push&lt;/code> operation, so it&amp;rsquo;s the perfect place to insert
generated content that must be kept in sync with posts on the blog.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="keeping-things-in-sync">Keeping things in sync&lt;/h2>
&lt;p>The &lt;code>_posts&lt;/code> directory of my blog is a &lt;a href="http://git-scm.com/book/en/Git-Tools-Submodules">git submodule&lt;/a>, which means
it gets updated and pushed asynchronously with respect to the main
repository. We want to make sure that we don&amp;rsquo;t regenerate the tag
feeds if there are either uncomitted changes in &lt;code>_posts&lt;/code> &lt;em>or&lt;/em> if there
are &lt;em>unpushed&lt;/em> changes in &lt;code>_posts&lt;/code>: in either situation, we could
generate a tag feed for tags that weren&amp;rsquo;t actually used in any
published posts.&lt;/p>
&lt;p>The following checks for any uncomitted changes in &lt;code>_posts&lt;/code>:&lt;/p>
&lt;pre>&lt;code>if ! git diff-files --quiet _posts; then
echo &amp;quot;posts are out of sync (skipping tag maintenance)&amp;quot;
exit 0
fi
&lt;/code>&lt;/pre>
&lt;p>This will abort the tag feed generation if any of the following is
true:&lt;/p>
&lt;ul>
&lt;li>&lt;code>_posts&lt;/code> has uncomitted changes&lt;/li>
&lt;li>&lt;code>_posts&lt;/code> has new, untracked content&lt;/li>
&lt;li>&lt;code>_posts&lt;/code> is at a revision that differs from the last comitted
revision in the parent repository.&lt;/li>
&lt;/ul>
&lt;p>This still leaves one possible failure mode: if we commit all changes
in &lt;code>_posts&lt;/code>, and then commit the updated &lt;code>_posts&lt;/code> revision in the
parent repository, all of the previous checks will pass&amp;hellip;but since we
haven&amp;rsquo;t pushed the &lt;code>_posts&lt;/code> repository, we could still be pushing tags
that don&amp;rsquo;t match up with published posts.&lt;/p>
&lt;p>The following check will prevent this situation by checking if the
repository differs from the upstream branch:&lt;/p>
&lt;pre>&lt;code>if ! (cd _posts; git diff-index --quiet origin/posts); then
echo &amp;quot;posts are out of sync (skipping tag maintenance)&amp;quot;
exit 0
fi
&lt;/code>&lt;/pre>
&lt;h2 id="generating-tag-feeds">Generating tag feeds&lt;/h2>
&lt;p>In order to prevent stale tags, we need to delete and regenerate all
the tag feeds. Cleaning up the existing tag feeds is taken care of by
the &lt;code>cleantagfeeds&lt;/code> script:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;cleaning tag feeds&amp;quot;
_oddbit/cleantagfeeds
&lt;/code>&lt;/pre>
&lt;p>Which is really just a wrapper for the following &lt;code>find&lt;/code> commands:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
# Delete tag feeds unless there is a `.keep` file in the
# same directory.
find tag/* -name index.xml \
-execdir sh -c 'test -f .keep || rm -f index.xml' \;
find tag/* -type d -delete
&lt;/code>&lt;/pre>
&lt;p>This will preserve any tag feeds that have a corresponding &lt;code>.keep&lt;/code>
file (just in case we&amp;rsquo;ve done something special that requires manual
intervention) and deletes everything else.&lt;/p>
&lt;p>Generating the tag feeds is taken care of by the &lt;code>gentagfeeds&lt;/code>
script:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;generating tag feeds&amp;quot;
_oddbit/gentagfeeds
&lt;/code>&lt;/pre>
&lt;p>This is a Python program that iterates over all the files in &lt;code>_posts&lt;/code>,
reads in the YAML frontmatter from each one, and then generates a feed
file for each tag using a template.&lt;/p>
&lt;p>Finally, we need to add any changes to the repository. We
unilaterally add the &lt;code>tags/&lt;/code> directory:&lt;/p>
&lt;pre>&lt;code>git add -A tag
&lt;/code>&lt;/pre>
&lt;p>And then see if that got us anything:&lt;/p>
&lt;pre>&lt;code>if ! git diff-index --quiet HEAD -- tag; then
git commit -m 'automatic tag update' tag
fi
&lt;/code>&lt;/pre>
&lt;p>At this point, we&amp;rsquo;ve regenerated all the tag feeds and committed any
new or modified tag feeds to the repository, which will get published
to GitHub as part of the current &lt;code>push&lt;/code> operation.&lt;/p>
&lt;p>The actual feed templates look like this:&lt;/p>
&lt;pre>&lt;code>---
layout: rss
exclude: true
tags:
- {{tag}}
---
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m using a modified version of &lt;a href="https://github.com/thedereck/gh-pages-blog/">gh-pages-blog&lt;/a> in which I have
modified &lt;code>_layouts/rss.xml&lt;/code> to optionally filter posts by tag using
the following template code:&lt;/p>
&lt;p>{% raw %}
.
.
.
{% for p in site.posts %}
{% if page contains &amp;rsquo;tags&amp;rsquo; %}
{% assign selected = false %}
{% for t in p.tags %}
{% if page.tags contains t %}
{% assign selected = true %}
{% endif %}
{% endfor %}&lt;/p>
&lt;pre>&lt;code> {% if selected == false %}
{% continue %}
{% endif %}
{% endif %}
.
.
.
&lt;/code>&lt;/pre>
&lt;p>{% endraw %}&lt;/p>
&lt;p>For each post on the site (&lt;code>site.posts&lt;/code>), this checks for any overlap
between the tags in the post and the tags selected in the tag feed.
While the automatic feeds use only a single tag, this also makes it
possible to create feeds that follow multiple tags.&lt;/p>
&lt;p>All of the code used to implement this is available in the &lt;a href="http://github.com/larsks/blog.oddbit.com/">GitHub
repository for this blog&lt;/a>.&lt;/p></content></item><item><title>Enabled blog comments</title><link>https://blog.oddbit.com/post/2013-11-18-enabled-comments/</link><pubDate>Mon, 18 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-18-enabled-comments/</guid><description>I&amp;rsquo;ve enabled blog comments using Disqus. This is something of an experiment, since (a) I&amp;rsquo;m not really happy with how Disqus is handling user registration these days and (b) I don&amp;rsquo;t know that I have the time to moderate anything. But we&amp;rsquo;ll see.</description><content>&lt;p>I&amp;rsquo;ve enabled blog comments using &lt;a href="http://disqus.com/">Disqus&lt;/a>. This is something of an
experiment, since (a) I&amp;rsquo;m not really happy with how Disqus is handling
user registration these days and (b) I don&amp;rsquo;t know that I have the time
to moderate anything. But we&amp;rsquo;ll see.&lt;/p></content></item><item><title>json-tools: cli for generating and filtering json</title><link>https://blog.oddbit.com/post/2013-11-17-json-tools/</link><pubDate>Sun, 17 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-17-json-tools/</guid><description>Interacting with JSON-based APIs from the command line can be difficult, and OpenStack is filled with REST APIs that consume or produce JSON. I&amp;rsquo;ve just put pair of tools for generating and filtering JSON on the command line, called collectively json-tools.
Both make use of the Python dpath module to populate or filter JSON objects.
The jsong command generates JSON on stdout. You provide /-delimited paths on the command line to represent the JSON structure.</description><content>&lt;p>Interacting with JSON-based APIs from the command line can be
difficult, and OpenStack is filled with REST APIs that consume or
produce JSON. I&amp;rsquo;ve just put pair of tools for generating and
filtering JSON on the command line, called collectively
&lt;a href="http://github.com/larsks/json-tools/">json-tools&lt;/a>.&lt;/p>
&lt;p>Both make use of the Python &lt;a href="https://github.com/akesterson/dpath-python">dpath&lt;/a> module to populate or filter
JSON objects.&lt;/p>
&lt;p>The &lt;code>jsong&lt;/code> command generates JSON on &lt;code>stdout&lt;/code>. You provide &lt;code>/&lt;/code>-delimited paths
on the command line to represent the JSON structure. For example, if
you run:&lt;/p>
&lt;pre>&lt;code>$ jsong auth/passwordCredentials/username=admin \
auth/passwordCredentials/password=secret
&lt;/code>&lt;/pre>
&lt;p>You get:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;auth&amp;quot;: {
&amp;quot;passwordCredentials&amp;quot;: {
&amp;quot;username&amp;quot;: &amp;quot;admin&amp;quot;,
&amp;quot;password&amp;quot;: &amp;quot;secret&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>jsonx&lt;/code> command accepts JSON on &lt;code>stdin&lt;/code> and selects subtrees or
values for output on &lt;code>stdout&lt;/code>. Given the above output, you could
extract the password with:&lt;/p>
&lt;pre>&lt;code>jsonx -v auth/passwordCredentials/password
&lt;/code>&lt;/pre>
&lt;p>Which would give you:&lt;/p>
&lt;pre>&lt;code>secret
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-v&lt;/code> flag here indicates that you only want the values of matched
paths; without the &lt;code>-v&lt;/code> the output would have been:&lt;/p>
&lt;pre>&lt;code>auth/passwordCredentials/password secret
&lt;/code>&lt;/pre>
&lt;p>There are more examples &amp;ndash; including some use of the OpenStack APIs &amp;ndash;
in the &lt;a href="https://github.com/larsks/json-tools/blob/master/README.md">README&lt;/a> document.&lt;/p></content></item><item><title>Quantum in Too Much Detail</title><link>https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/</link><pubDate>Thu, 14 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/</guid><description>I originally posted this article on the RDO website.
The players This document describes the architecture that results from a particular OpenStack configuration, specifically:
Quantum networking using GRE tunnels; A dedicated network controller; A single instance running on a compute host Much of the document will be relevant to other configurations, but details will vary based on your choice of layer 2 connectivity, number of running instances, and so forth.</description><content>&lt;blockquote>
&lt;p>I originally posted this article on
the &lt;a href="http://openstack.redhat.com/Networking_in_too_much_detail">RDO&lt;/a>
website.&lt;/p>
&lt;/blockquote>
&lt;h1 id="the-players">The players&lt;/h1>
&lt;p>This document describes the architecture that results from a
particular OpenStack configuration, specifically:&lt;/p>
&lt;ul>
&lt;li>Quantum networking using GRE tunnels;&lt;/li>
&lt;li>A dedicated network controller;&lt;/li>
&lt;li>A single instance running on a compute host&lt;/li>
&lt;/ul>
&lt;p>Much of the document will be relevant to other configurations, but
details will vary based on your choice of layer 2 connectivity, number
of running instances, and so forth.&lt;/p>
&lt;p>The examples in this document were generated on a system with Quantum
networking but will generally match what you see under Neutron as
well, if you replace &lt;code>quantum&lt;/code> by &lt;code>neutron&lt;/code> in names. The OVS flow
rules under Neutron are somewhat more complex and I will cover those
in another post.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h1 id="the-lay-of-the-land">The lay of the land&lt;/h1>
&lt;p>This is a simplified architecture diagram of network connectivity in a
quantum/neutron managed world:&lt;/p>
&lt;figure class="left" >
&lt;img src="quantum-gre.svg" />
&lt;/figure>
&lt;p>Section names in this document include
parenthetical references to the nodes on the map relevant to that
particular section.&lt;/p>
&lt;h1 id="compute-host-instance-networking-abc">Compute host: instance networking (A,B,C)&lt;/h1>
&lt;p>An outbound packet starts on &lt;code>eth0&lt;/code> of the virtual instance, which is
connected to a &lt;code>tap&lt;/code> device on the host, &lt;code>tap7c7ae61e-05&lt;/code>. This &lt;code>tap&lt;/code>
device is attached to a Linux bridge device, &lt;code>qbr7c7ae61e-05&lt;/code>. What is
this bridge device for? From the &lt;a href="http://docs.openstack.org/network-admin/admin/content/under_the_hood_openvswitch.html">OpenStack Networking Administration
Guide&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Ideally, the TAP device vnet0 would be connected directly to the
integration bridge, br-int. Unfortunately, this isn&amp;rsquo;t possible because
of how OpenStack security groups are currently implemented. OpenStack
uses iptables rules on the TAP devices such as vnet0 to implement
security groups, and Open vSwitch is not compatible with iptables
rules that are applied directly on TAP devices that are connected to
an Open vSwitch port.&lt;/p>
&lt;/blockquote>
&lt;p>Because this bridge device exists primarily to support firewall rules,
I&amp;rsquo;m going to refer to it as the &amp;ldquo;firewall bridge&amp;rdquo;.&lt;/p>
&lt;p>If you examine the firewall rules on your compute host, you will find
that there are several rules associated with this &lt;code>tap&lt;/code> device:&lt;/p>
&lt;pre>&lt;code># iptables -S | grep tap7c7ae61e-05
-A quantum-openvswi-FORWARD -m physdev --physdev-out tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-sg-chain
-A quantum-openvswi-FORWARD -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-sg-chain
-A quantum-openvswi-INPUT -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-o7c7ae61e-0
-A quantum-openvswi-sg-chain -m physdev --physdev-out tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-i7c7ae61e-0
-A quantum-openvswi-sg-chain -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-o7c7ae61e-0
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>quantum-openvswi-sg-chain&lt;/code> is where &lt;code>neutron&lt;/code>-managed security
groups are realized. The &lt;code>quantum-openvswi-o7c7ae61e-0&lt;/code> chain
controls outbound traffic FROM the instance, and by default looks like
this:&lt;/p>
&lt;pre>&lt;code>-A quantum-openvswi-o7c7ae61e-0 -m mac ! --mac-source FA:16:3E:03:00:E7 -j DROP
-A quantum-openvswi-o7c7ae61e-0 -p udp -m udp --sport 68 --dport 67 -j RETURN
-A quantum-openvswi-o7c7ae61e-0 ! -s 10.1.0.2/32 -j DROP
-A quantum-openvswi-o7c7ae61e-0 -p udp -m udp --sport 67 --dport 68 -j DROP
-A quantum-openvswi-o7c7ae61e-0 -m state --state INVALID -j DROP
-A quantum-openvswi-o7c7ae61e-0 -m state --state RELATED,ESTABLISHED -j RETURN
-A quantum-openvswi-o7c7ae61e-0 -j RETURN
-A quantum-openvswi-o7c7ae61e-0 -j quantum-openvswi-sg-fallback
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>quantum-openvswi-i7c7ae61e-0&lt;/code> chain controls inbound traffic TO
the instance. After opening up port 22 in the default security group:&lt;/p>
&lt;pre>&lt;code># neutron security-group-rule-create --protocol tcp \
--port-range-min 22 --port-range-max 22 --direction ingress default
&lt;/code>&lt;/pre>
&lt;p>The rules look like this:&lt;/p>
&lt;pre>&lt;code>-A quantum-openvswi-i7c7ae61e-0 -m state --state INVALID -j DROP
-A quantum-openvswi-i7c7ae61e-0 -m state --state RELATED,ESTABLISHED -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -p icmp -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -p tcp -m tcp --dport 22 -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -p tcp -m tcp --dport 80 -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -s 10.1.0.3/32 -p udp -m udp --sport 67 --dport 68 -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -j quantum-openvswi-sg-fallback
&lt;/code>&lt;/pre>
&lt;p>A second interface attached to the bridge, &lt;code>qvb7c7ae61e-05&lt;/code>, attaches
the firewall bridge to the integration bridge, typically named
&lt;code>br-int&lt;/code>.&lt;/p>
&lt;h1 id="compute-host-integration-bridge-de">Compute host: integration bridge (D,E)&lt;/h1>
&lt;p>The integration bridge, &lt;code>br-int&lt;/code>, performs VLAN tagging and un-tagging
for traffic coming from and to your instances. At this moment,
&lt;code>br-int&lt;/code> looks something like this:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl show
Bridge br-int
Port &amp;quot;qvo7c7ae61e-05&amp;quot;
tag: 1
Interface &amp;quot;qvo7c7ae61e-05&amp;quot;
Port patch-tun
Interface patch-tun
type: patch
options: {peer=patch-int}
Port br-int
Interface br-int
type: internal
&lt;/code>&lt;/pre>
&lt;p>The interface &lt;code>qvo7c7ae61e-05&lt;/code> is the other end of &lt;code>qvb7c7ae61e-05&lt;/code>,
and carries traffic to and from the firewall bridge. The &lt;code>tag: 1&lt;/code> you
see in the above output integrates that this is an access port
attached to VLAN 1. Untagged outbound traffic from this instance will be
assigned VLAN ID 1, and inbound traffic with VLAN ID 1 will
stripped of it&amp;rsquo;s VLAN tag and sent out this port.&lt;/p>
&lt;p>Each network you create (with &lt;code>neutron net-create&lt;/code>) will be assigned a
different VLAN ID.&lt;/p>
&lt;p>The interface named &lt;code>patch-tun&lt;/code> connects the integration bridge to the
tunnel bridge, &lt;code>br-tun&lt;/code>.&lt;/p>
&lt;h1 id="compute-host-tunnel-bridge-fg">Compute host: tunnel bridge (F,G)&lt;/h1>
&lt;p>The tunnel bridge translates VLAN-tagged traffic from the
integration bridge into &lt;code>GRE&lt;/code> tunnels. The translation between VLAN
IDs and tunnel IDs is performed by OpenFlow rules installed on
&lt;code>br-tun&lt;/code>. Before creating any instances, the flow rules on the bridge
look like this:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=871.283s, table=0, n_packets=4, n_bytes=300, idle_age=862, priority=1 actions=drop
&lt;/code>&lt;/pre>
&lt;p>There is a single rule that causes the bridge to drop all traffic.
Afrer you boot an instance on this compute node, the rules are
modified to look something like:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=422.158s, table=0, n_packets=2, n_bytes=120, idle_age=55, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
cookie=0x0, duration=421.948s, table=0, n_packets=64, n_bytes=8337, idle_age=31, priority=3,tun_id=0x2,dl_dst=fa:16:3e:dd:c1:62 actions=mod_vlan_vid:1,NORMAL
cookie=0x0, duration=422.357s, table=0, n_packets=82, n_bytes=10443, idle_age=31, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
cookie=0x0, duration=1502.657s, table=0, n_packets=8, n_bytes=596, idle_age=423, priority=1 actions=drop
&lt;/code>&lt;/pre>
&lt;p>In general, these rules are responsible for mapping traffic between
VLAN ID 1, used by the integration bridge, and tunnel id 2, used by
the GRE tunnel.&lt;/p>
&lt;p>The first rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=422.158s, table=0, n_packets=2, n_bytes=120, idle_age=55, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches all multicast traffic (see &lt;a href="http://openvswitch.org/cgi-bin/ovsman.cgi?page=utilities%2Fovs-ofctl.8">ovs-ofctl(8)&lt;/a>)
on tunnel id 2 (&lt;code>tun_id=0x2&lt;/code>), tags the ethernet frame with VLAN ID
1 (&lt;code>actions=mod_vlan_vid:1&lt;/code>), and sends it out port 1. We can see
from &lt;code>ovs-ofctl show br-tun&lt;/code> that port 1 is &lt;code>patch-int&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl show br-tun
OFPT_FEATURES_REPLY (xid=0x2): dpid:0000068df4e44a49
n_tables:254, n_buffers:256
capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP
actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC SET_DL_DST SET_NW_SRC SET_NW_DST SET_NW_TOS SET_TP_SRC SET_TP_DST ENQUEUE
1(patch-int): addr:46:3d:59:17:df:62
config: 0
state: 0
speed: 0 Mbps now, 0 Mbps max
2(gre-2): addr:a2:5f:a1:92:29:02
config: 0
state: 0
speed: 0 Mbps now, 0 Mbps max
LOCAL(br-tun): addr:06:8d:f4:e4:4a:49
config: 0
state: 0
speed: 0 Mbps now, 0 Mbps max
OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0
&lt;/code>&lt;/pre>
&lt;p>The next rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=421.948s, table=0, n_packets=64, n_bytes=8337, idle_age=31, priority=3,tun_id=0x2,dl_dst=fa:16:3e:dd:c1:62 actions=mod_vlan_vid:1,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches traffic coming in on tunnel 2 (&lt;code>tun_id=0x2&lt;/code>) with an
ethernet destination of &lt;code>fa:16:3e:dd:c1:62&lt;/code>
(&lt;code>dl_dst=fa:16:3e:dd:c1:62&lt;/code>) and tags the ethernet frame with VLAN
ID 1 (&lt;code>actions=mod_vlan_vid:1&lt;/code>) before sending it out &lt;code>patch-int&lt;/code>.&lt;/p>
&lt;p>The following rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=422.357s, table=0, n_packets=82, n_bytes=10443, idle_age=31, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches traffic coming in on port 1 (&lt;code>in_port=1&lt;/code>) with VLAN ID 1
(&lt;code>dl_vlan=1&lt;/code>) and set the tunnel id to 2 (&lt;code>actions=set_tunnel:0x2&lt;/code>)
before sending it out the GRE tunnel.&lt;/p>
&lt;h1 id="network-host-tunnel-bridge-hi">Network host: tunnel bridge (H,I)&lt;/h1>
&lt;p>Traffic arrives on the network host via the GRE tunnel attached to
&lt;code>br-tun&lt;/code>. This bridge has a flow table very similar to &lt;code>br-tun&lt;/code> on
the compute host:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=1239.229s, table=0, n_packets=23, n_bytes=4246, idle_age=15, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
cookie=0x0, duration=524.477s, table=0, n_packets=15, n_bytes=3498, idle_age=10, priority=3,tun_id=0x2,dl_dst=fa:16:3e:83:69:cc actions=mod_vlan_vid:1,NORMAL
cookie=0x0, duration=1239.157s, table=0, n_packets=50, n_bytes=4565, idle_age=148, priority=3,tun_id=0x2,dl_dst=fa:16:3e:aa:99:3c actions=mod_vlan_vid:1,NORMAL
cookie=0x0, duration=1239.304s, table=0, n_packets=76, n_bytes=9419, idle_age=10, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
cookie=0x0, duration=1527.016s, table=0, n_packets=12, n_bytes=880, idle_age=527, priority=1 actions=drop
&lt;/code>&lt;/pre>
&lt;p>As on the compute host, the first rule maps multicast traffic on
tunnel ID 2 to VLAN 1.&lt;/p>
&lt;p>The second rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=524.477s, table=0, n_packets=15, n_bytes=3498, idle_age=10, priority=3,tun_id=0x2,dl_dst=fa:16:3e:83:69:cc actions=mod_vlan_vid:1,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches traffic on the tunnel destined for the DHCP server at
&lt;code>fa:16:3e:83:69:cc&lt;/code>. This is a &lt;code>dnsmasq&lt;/code> process running inside a
network namespace, the details of which we will examine shortly.&lt;/p>
&lt;p>The next rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=1239.157s, table=0, n_packets=50, n_bytes=4565, idle_age=148, priority=3,tun_id=0x2,dl_dst=fa:16:3e:aa:99:3c actions=mod_vlan_vid:1,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches traffic on tunnel ID 2 destined for the router at &lt;code>fa:16:3e:aa:99:3c&lt;/code>, which is an interface in another network namespace.&lt;/p>
&lt;p>The following rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=1239.304s, table=0, n_packets=76, n_bytes=9419, idle_age=10, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;simply maps outbound traffic on VLAN ID 1 to tunnel ID 2.&lt;/p>
&lt;h1 id="network-host-integration-bridge-jkm">Network host: integration bridge (J,K,M)&lt;/h1>
&lt;p>The integration bridge on the network controller serves to connect
instances to network services, such as routers and DHCP servers.&lt;/p>
&lt;pre>&lt;code># ovs-vsctl show
.
.
.
Bridge br-int
Port patch-tun
Interface patch-tun
type: patch
options: {peer=patch-int}
Port &amp;quot;tapf14c598d-98&amp;quot;
tag: 1
Interface &amp;quot;tapf14c598d-98&amp;quot;
Port br-int
Interface br-int
type: internal
Port &amp;quot;tapc2d7dd02-56&amp;quot;
tag: 1
Interface &amp;quot;tapc2d7dd02-56&amp;quot;
.
.
.
&lt;/code>&lt;/pre>
&lt;p>It connects to the tunnel bridge, &lt;code>br-tun&lt;/code>, via a patch interface,
&lt;code>patch-tun&lt;/code>.&lt;/p>
&lt;h1 id="network-host-dhcp-server-m">Network host: DHCP server (M)&lt;/h1>
&lt;p>Each network for which DHCP is enabled has a DHCP server running on
the network controller. The DHCP server is an instance of &lt;a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq&lt;/a>
running inside a &lt;em>network namespace&lt;/em>. A &lt;em>network namespace&lt;/em> is a
Linux kernel facility that allows groups of processes to have a
network stack (interfaces, routing tables, iptables rules) distinct
from that of the host.&lt;/p>
&lt;p>You can see a list of network namespace with the &lt;code>ip netns&lt;/code> command,
which in our configuration will look something like this:&lt;/p>
&lt;pre>&lt;code># ip netns
qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264
qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f
&lt;/code>&lt;/pre>
&lt;p>The first of these (&lt;code>qdhcp...&lt;/code>) is the DHCP server namespace for our private
subnet, while the second (&lt;code>qrouter...&lt;/code>) is the router.&lt;/p>
&lt;p>You can run a command inside a network namespace using the &lt;code>ip netns exec&lt;/code> command. For example, to see the interface configuration inside
the DHCP server namespace (&lt;code>lo&lt;/code> removed for brevity):&lt;/p>
&lt;pre>&lt;code># ip netns exec qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264 ip addr
71: tapf14c598d-98: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
link/ether fa:16:3e:10:2f:03 brd ff:ff:ff:ff:ff:ff
inet 10.1.0.3/24 brd 10.1.0.255 scope global ns-f14c598d-98
inet6 fe80::f816:3eff:fe10:2f03/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>Note the MAC address on interface &lt;code>tapf14c598d-98&lt;/code>; this matches the MAC address in the flow rule we saw on the tunnel bridge.&lt;/p>
&lt;p>You can find the &lt;code>dnsmasq&lt;/code> process associated with this namespace by
search the output of &lt;code>ps&lt;/code> for the id (the number after &lt;code>qdhcp-&lt;/code> in the
namespace name):&lt;/p>
&lt;pre>&lt;code># ps -fe | grep 88b1609c-68e0-49ca-a658-f1edff54a264
nobody 23195 1 0 Oct26 ? 00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=ns-f14c598d-98 --except-interface=lo --pid-file=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/pid --dhcp-hostsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/host --dhcp-optsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/opts --dhcp-script=/usr/bin/quantum-dhcp-agent-dnsmasq-lease-update --leasefile-ro --dhcp-range=tag0,10.1.0.0,static,120s --conf-file= --domain=openstacklocal
root 23196 23195 0 Oct26 ? 00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=ns-f14c598d-98 --except-interface=lo --pid-file=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/pid --dhcp-hostsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/host --dhcp-optsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/opts --dhcp-script=/usr/bin/quantum-dhcp-agent-dnsmasq-lease-update --leasefile-ro --dhcp-range=tag0,10.1.0.0,static,120s --conf-file= --domain=openstacklocal
&lt;/code>&lt;/pre>
&lt;h1 id="network-host-router-kl">Network host: Router (K,L)&lt;/h1>
&lt;p>A router is a network namespace with a set of routing tables
and iptables rules that performs the routing between subnets. Recall
that we saw two network namespaces in our configuration:&lt;/p>
&lt;pre>&lt;code># ip netns
qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264
qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f
&lt;/code>&lt;/pre>
&lt;p>Using the &lt;code>ip netns exec&lt;/code> command, we can inspect the interfaces
associated with the router (&lt;code>lo&lt;/code> removed for brevity):&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f ip addr
66: qg-d48b49e0-aa: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
link/ether fa:16:3e:5c:a2:ac brd ff:ff:ff:ff:ff:ff
inet 172.24.4.227/28 brd 172.24.4.239 scope global qg-d48b49e0-aa
inet 172.24.4.228/32 brd 172.24.4.228 scope global qg-d48b49e0-aa
inet6 fe80::f816:3eff:fe5c:a2ac/64 scope link
valid_lft forever preferred_lft forever
68: qr-c2d7dd02-56: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
link/ether fa:16:3e:ea:64:6e brd ff:ff:ff:ff:ff:ff
inet 10.1.0.1/24 brd 10.1.0.255 scope global qr-c2d7dd02-56
inet6 fe80::f816:3eff:feea:646e/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>The first interface, &lt;code>qg-d48b49e0-aa&lt;/code>, connects the router to the
gateway set by the &lt;code>router-gateway-set&lt;/code> command. The second
interface, &lt;code>qr-c2d7dd02-56&lt;/code>, is what connects the router to the
integration bridge.&lt;/p>
&lt;p>Looking at the routing tables inside the router, we see that there is
a default gateway pointing to the &lt;code>.1&lt;/code> address of our external
network, and the expected network routes for directly attached
networks:&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f ip route
172.24.4.224/28 dev qg-d48b49e0-aa proto kernel scope link src 172.24.4.227
10.1.0.0/24 dev qr-c2d7dd02-56 proto kernel scope link src 10.1.0.1
default via 172.24.4.225 dev qg-d48b49e0-aa
&lt;/code>&lt;/pre>
&lt;p>The netfilter &lt;code>nat&lt;/code> table inside the router namespace is responsible
for associating floating IP addresses with your instances. For
example, after associating the address &lt;code>172.24.4.228&lt;/code> with our
instance, the &lt;code>nat&lt;/code> table looks like this:&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f iptables -t nat -S
-P PREROUTING ACCEPT
-P POSTROUTING ACCEPT
-P OUTPUT ACCEPT
-N quantum-l3-agent-OUTPUT
-N quantum-l3-agent-POSTROUTING
-N quantum-l3-agent-PREROUTING
-N quantum-l3-agent-float-snat
-N quantum-l3-agent-snat
-N quantum-postrouting-bottom
-A PREROUTING -j quantum-l3-agent-PREROUTING
-A POSTROUTING -j quantum-l3-agent-POSTROUTING
-A POSTROUTING -j quantum-postrouting-bottom
-A OUTPUT -j quantum-l3-agent-OUTPUT
-A quantum-l3-agent-OUTPUT -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2
-A quantum-l3-agent-POSTROUTING ! -i qg-d48b49e0-aa ! -o qg-d48b49e0-aa -m conntrack ! --ctstate DNAT -j ACCEPT
-A quantum-l3-agent-PREROUTING -d 169.254.169.254/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 9697
-A quantum-l3-agent-PREROUTING -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2
-A quantum-l3-agent-float-snat -s 10.1.0.2/32 -j SNAT --to-source 172.24.4.228
-A quantum-l3-agent-snat -j quantum-l3-agent-float-snat
-A quantum-l3-agent-snat -s 10.1.0.0/24 -j SNAT --to-source 172.24.4.227
-A quantum-postrouting-bottom -j quantum-l3-agent-snat
&lt;/code>&lt;/pre>
&lt;p>There are &lt;code>SNAT&lt;/code> and &lt;code>DNAT&lt;/code> rules to map traffic between the floating
address, &lt;code>172.24.4.228&lt;/code>, and the private address &lt;code>10.1.0.2&lt;/code>:&lt;/p>
&lt;pre>&lt;code>-A quantum-l3-agent-OUTPUT -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2
-A quantum-l3-agent-PREROUTING -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2
-A quantum-l3-agent-float-snat -s 10.1.0.2/32 -j SNAT --to-source 172.24.4.228
&lt;/code>&lt;/pre>
&lt;p>When you associate a floating ip address with an instance, similar
rules will be created in this table.&lt;/p>
&lt;p>There is also an &lt;code>SNAT&lt;/code> rule that NATs all outbound traffic from our
private network to &lt;code>172.24.4.227&lt;/code>:&lt;/p>
&lt;pre>&lt;code>-A quantum-l3-agent-snat -s 10.1.0.0/24 -j SNAT --to-source 172.24.4.227
&lt;/code>&lt;/pre>
&lt;p>This permits instances to have outbound connectivity even without a
public ip address.&lt;/p>
&lt;h1 id="network-host-external-traffic-l">Network host: External traffic (L)&lt;/h1>
&lt;p>&amp;ldquo;External&amp;rdquo; traffic flows through &lt;code>br-ex&lt;/code> via the &lt;code>qg-d48b49e0-aa&lt;/code>
interface in the router name space.&lt;/p>
&lt;pre>&lt;code>Bridge br-ex
Port &amp;quot;qg-d48b49e0-aa&amp;quot;
Interface &amp;quot;qg-d48b49e0-aa&amp;quot;
Port br-ex
Interface br-ex
type: internal
&lt;/code>&lt;/pre>
&lt;p>What happens when traffic gets this far depends on your local
configuration.&lt;/p>
&lt;h2 id="nat-to-host-address">NAT to host address&lt;/h2>
&lt;p>If you assign the gateway address for your public network to &lt;code>br-ex&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ip addr add 172.24.4.225/28 dev br-ex
&lt;/code>&lt;/pre>
&lt;p>Then you can create forwarding and NAT rules that will cause
&amp;ldquo;external&amp;rdquo; traffic from your instances to get rewritten to your
network controller&amp;rsquo;s ip address and sent out on the network:&lt;/p>
&lt;pre>&lt;code># iptables -A FORWARD -d 172.24.4.224/28 -j ACCEPT
# iptables -A FORWARD -s 172.24.4.224/28 -j ACCEPT
# iptables -t nat -I POSTROUTING 1 -s 172.24.4.224/28 -j MASQUERADE
&lt;/code>&lt;/pre>
&lt;h2 id="direct-network-connection">Direct network connection&lt;/h2>
&lt;p>If you have an external router that will act as a gateway for your
public network, you can add an interface on that network to the
bridge. For example, assuming that &lt;code>eth2&lt;/code> was on the same network as
&lt;code>172.24.4.225&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-port br-ex eth2
&lt;/code>&lt;/pre></content></item><item><title>Moving to GitHub</title><link>https://blog.oddbit.com/post/2013-11-13-moving-to-github/</link><pubDate>Wed, 13 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-13-moving-to-github/</guid><description>This blog has been hosted on scriptogram for the past year or so. Unfortunately, while I like the publish-via-Dropbox mechanism, there have been enough problems recently that I&amp;rsquo;ve finally switched over to using GitHub Pages for hosting. I&amp;rsquo;ve been thinking about doing this for a while, but the things that finally pushed me to make the change were:
Sync problems that would prevent new posts from appearing (and that at least once caused posts to disappear).</description><content>&lt;p>This blog has been hosted on &lt;a href="http://scriptogr.am/">scriptogram&lt;/a> for the past year or so.
Unfortunately, while I like the publish-via-Dropbox mechanism, there
have been enough problems recently that I&amp;rsquo;ve finally switched over to
using &lt;a href="http://pages.github.com/">GitHub Pages&lt;/a> for hosting. I&amp;rsquo;ve been thinking about doing
this for a while, but the things that finally pushed me to make the
change were:&lt;/p>
&lt;ul>
&lt;li>Sync problems that would prevent new posts from appearing (and that
at least once caused posts to disappear).&lt;/li>
&lt;li>Lack of any response to bug reports by the site maintainers.&lt;/li>
&lt;/ul>
&lt;p>A benefit of the publish-via-Dropbox mechanism is, of course, that I
already had all the data and didn&amp;rsquo;t need to go through any sort of
export process.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="fixing-metadata">Fixing metadata&lt;/h2>
&lt;p>Like &lt;a href="http://scriptogr.am/">scriptogram&lt;/a>, &lt;a href="http://pages.github.com/">GitHub Pages&lt;/a> is also a Markdown-based
solution. GitHub uses &lt;a href="http://jekyllrb.com/">Jekyll&lt;/a> to render Markdown to HTML, which
requires some metadata at the beginning of each post. On
&lt;a href="http://scriptogr.am/">scriptogram&lt;/a> the file headers looked like this:&lt;/p>
&lt;pre>&lt;code>Title: A random collection of OpenStack Tools
Date: 2013-11-12
Tags: openstack
&lt;/code>&lt;/pre>
&lt;p>Whereas the corresponding header for GitHub would look like this:&lt;/p>
&lt;pre>&lt;code>---
layout: post
title: A random collection of OpenStack Tools
date: 2013-11-12
tags:
- openstack
---
&lt;/code>&lt;/pre>
&lt;p>I was able to generally automate this with the following script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
for post in &amp;quot;$@&amp;quot;; do
sed -i '
1,/^$/ {
1 i\---
1 i\layout: post
s/Title:/title:/
s/Date:/date:/
s/Tags:/tags:/
/^$/ i\---
}
' $post
done
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>tags:&lt;/code> header need further processing to transform them into a
&lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a> list. That means something like:&lt;/p>
&lt;pre>&lt;code>tags: foo,bar,baz
&lt;/code>&lt;/pre>
&lt;p>Would need to end up looking like:&lt;/p>
&lt;pre>&lt;code>tags:
- foo
- bar
- baz
&lt;/code>&lt;/pre>
&lt;p>While that&amp;rsquo;s not entirely accurate &amp;ndash; YAML supports multiple list
syntaxes and I could have just expressed that as &lt;code>[foo,bar,baz]&lt;/code> &amp;ndash; I
prefer this extended syntax and got there via the following &lt;code>awk&lt;/code>
script:&lt;/p>
&lt;pre>&lt;code>BEGIN {state=0}
state == 1 &amp;amp;&amp;amp; /^tags:/ {
tags=$2
next
}
state == 1 &amp;amp;&amp;amp; /^---$/ {
if (tags) {
split(tags, taglist, &amp;quot;,&amp;quot;)
print &amp;quot;tags:&amp;quot;
for (t in taglist)
print &amp;quot; -&amp;quot;, taglist[t]
}
state=2
}
state == 0 &amp;amp;&amp;amp; /^---$/ { state=1 }
{print}
&lt;/code>&lt;/pre>
&lt;p>(This would process a single post; I wrapped it in a shell script to
run it across all the posts.)&lt;/p>
&lt;h1 id="redirecting-legacy-links">Redirecting legacy links&lt;/h1>
&lt;p>In order to preserve links pointing at the old blog I needed to generate
a bunch of HTML redirect files. &lt;a href="http://scriptogr.am/">Scriptogram&lt;/a> posts had permalinks
of the form &lt;code>/post/&amp;lt;slug&amp;gt;&lt;/code>, where &lt;code>&amp;lt;slug&amp;gt;&lt;/code> was computed from the post
aliases: [&amp;quot;/2013/11/13/moving-to-github/&amp;quot;]
title. GitHub posts (with &lt;code>permalinks: pretty&lt;/code>) have the form
&lt;code>/&amp;lt;year&amp;gt;/&amp;lt;month&amp;gt;/&amp;lt;day&amp;gt;/&amp;lt;title&amp;gt;&lt;/code>, where &lt;code>&amp;lt;title&amp;gt;&lt;/code> comes from the
filename rather than the post metadata.&lt;/p>
&lt;p>I automated the generation of redirects with the following script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
for post in _posts/*; do
# read the title from the post metadata
title=$(grep '^title:' $post)
title=${title/title: /}
# convert the title from the metadata into the slug
# used by scriptogram
slug=${title,,}
slug=${slug// /-}
slug=${slug//[.,:?\/\'\&amp;quot;]/}
# parse the post filename into year, month, day, and title
# as used by github
post_name=${post/_posts\//}
post_date=${post_name:0:10}
post_title=${post_name:11}
post_title=${post_title:0:$(( ${#post_title} - 3))}
post_year=${post_date%%-*}
tmp=${post_date#*-}
post_month=${tmp%%-*}
post_day=${post_date##*-}
# the url at which the post is available on github
new_url=&amp;quot;/$post_year/$post_month/$post_day/$post_title/&amp;quot;
# generate the html redirect file
mkdir -p post/$slug
sed &amp;quot;s|URL|$new_url|g&amp;quot; redirect.html &amp;gt; post/$slug/index.html
done
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>redirect.html&lt;/code> looks like this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;link rel=&amp;quot;canonical&amp;quot; href=&amp;quot;URL&amp;quot;/&amp;gt;
&amp;lt;meta http-equiv=&amp;quot;content-type&amp;quot; content=&amp;quot;text/html; charset=utf-8&amp;quot; /&amp;gt;
&amp;lt;meta http-equiv=&amp;quot;refresh&amp;quot; content=&amp;quot;0;url=URL&amp;quot; /&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>So given a file &lt;code>_posts/2013-11-12-a-random-collection.md&lt;/code>, this would
result in a new file
&lt;code>post/a-random-collection-of-openstack-tools/index.html&lt;/code> with the
following content:&lt;/p>
&lt;pre>&lt;code>&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;link rel=&amp;quot;canonical&amp;quot; href=&amp;quot;/2013/11/12/a-random-collection/&amp;quot;/&amp;gt;
&amp;lt;meta http-equiv=&amp;quot;content-type&amp;quot; content=&amp;quot;text/html; charset=utf-8&amp;quot; /&amp;gt;
&amp;lt;meta http-equiv=&amp;quot;refresh&amp;quot; content=&amp;quot;0;url=https://blog.oddbit.com/2013/11/12/a-random-collection/&amp;quot; /&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>With this in place, a URL such as &lt;a href="http://blog.oddbit.com/post/a-random-collection-of-openstack-tools">http://blog.oddbit.com/post/a-random-collection-of-openstack-tools&lt;/a> goes to the right place.&lt;/p>
&lt;p>&lt;strong>Update&lt;/strong>: It turns out that it has been almost exactly a year since
I &lt;a href="http://blog.oddbit.com/2012/11/06/moving-from-blogger/">moved from Blogger to Scriptogram&lt;/a>.&lt;/p></content></item><item><title>A random collection of OpenStack Tools</title><link>https://blog.oddbit.com/post/2013-11-12-a-random-collection/</link><pubDate>Tue, 12 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-12-a-random-collection/</guid><description>I&amp;rsquo;ve been working with OpenStack a lot recently, and I&amp;rsquo;ve ended up with a small collection of utilities that make my life easier. On the odd chance that they&amp;rsquo;ll make your life easier, too, I thought I&amp;rsquo;d hilight them here.
Crux Crux is a tool for provisioning tenants, users, and roles in keystone. Instead of a sequence of keystone command, you can provision new tenants, users, and roles with a single comand.</description><content>&lt;p>I&amp;rsquo;ve been working with &lt;a href="http://openstack.org/">OpenStack&lt;/a> a lot recently, and I&amp;rsquo;ve ended up with a small collection of utilities that make my life easier. On the odd chance that they&amp;rsquo;ll make your life easier, too, I thought I&amp;rsquo;d hilight them here.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="crux">Crux&lt;/h2>
&lt;p>&lt;a href="http://github.com/larsks/crux">Crux&lt;/a> is a tool for provisioning tenants, users, and roles in keystone. Instead of a sequence of keystone command, you can provision new tenants, users, and roles with a single comand.&lt;/p>
&lt;p>For example, to create user &lt;code>demo&lt;/code> in the &lt;code>demo&lt;/code> tenant with password secret:&lt;/p>
&lt;pre>&lt;code># crux --user demo:demo::secret
2013-10-21 crux WARNING creating tenant demo
2013-10-21 crux WARNING creating user demo with password secret
&lt;/code>&lt;/pre>
&lt;p>Crux is in general idempotent; if we were to run the same command a second time we woudl see:&lt;/p>
&lt;pre>&lt;code># crux --user demo:demo::secret
2013-10-21 crux WARNING set password for user demo to secret
&lt;/code>&lt;/pre>
&lt;p>Crux can also take input from a configuration file, so you can quickly set up a complex set of tenants and users.&lt;/p>
&lt;h2 id="sqlcli">Sqlcli&lt;/h2>
&lt;p>&lt;a href="http://github.com/larsks/sqlcli">Sqlcli&lt;/a> uses &lt;a href="http://www.sqlalchemy.org/">SQLAlchemy&lt;/a> to run SQL queries against a variety of backends specified by SQL connection URLs. I wrote this to perform queries and simple maintenance against the various databases used by OpenStack. For example, you can get a list of networks from the Neutron database with a command like this:&lt;/p>
&lt;pre>&lt;code># sqlcli -f /etc/neutron/plugin.ini -i DATABASE/sql_connection \
'select name,cidr from subnets'
&lt;/code>&lt;/pre>
&lt;p>And get output like this:&lt;/p>
&lt;pre>&lt;code>public,172.24.4.224/28
net0-subnet0,10.0.0.0/24
&lt;/code>&lt;/pre>
&lt;p>You can add the &lt;code>--pretty&lt;/code> flag and get output like this:&lt;/p>
&lt;pre>&lt;code>+--------------+-----------------+
| name | cidr |
+--------------+-----------------+
| public | 172.24.4.224/28 |
| net0-subnet0 | 10.0.0.0/24 |
+--------------+-----------------+
&lt;/code>&lt;/pre>
&lt;h2 id="openstack-service">openstack-service&lt;/h2>
&lt;p>The &lt;a href="http://github.com/larsks/osctl">openstack-service&lt;/a> command is a convenience tool for managing OpenStack services. It lets you start/stop or query the status of groups of related services. For example, if you want to see the status of all your Cinder services, you can run:&lt;/p>
&lt;pre>&lt;code># openstack-service status cinder
&lt;/code>&lt;/pre>
&lt;p>And get:&lt;/p>
&lt;pre>&lt;code>openstack-cinder-api (pid 11644) is running...
openstack-cinder-scheduler (pid 11790) is running...
openstack-cinder-volume (pid 11712) is running...
&lt;/code>&lt;/pre>
&lt;p>You can stop all Cinder and Glance services like this:&lt;/p>
&lt;pre>&lt;code># openstack-service stop cinder glance
Stopping openstack-cinder-api: [ OK ]
Stopping openstack-cinder-scheduler: [ OK ]
Stopping openstack-cinder-volume: [ OK ]
Stopping openstack-glance-api: [ OK ]
Stopping openstack-glance-registry: [ OK ]
&lt;/code>&lt;/pre>
&lt;p>And start them back up again the same way:&lt;/p>
&lt;pre>&lt;code># openstack-service start cinder glance
Starting openstack-cinder-api: [ OK ]
Starting openstack-cinder-scheduler: [ OK ]
Starting openstack-cinder-volume: [ OK ]
Starting openstack-glance-api: [ OK ]
Starting openstack-glance-registry: [ OK ]
&lt;/code>&lt;/pre>
&lt;p>Without any additional arguments &lt;code>openstack-service stop&lt;/code> will stop all OpenStack services on the current host.&lt;/p></content></item><item><title>Why does the Neutron documentation recommend three interfaces?</title><link>https://blog.oddbit.com/post/2013-10-28-why-does-the-neutron/</link><pubDate>Mon, 28 Oct 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-10-28-why-does-the-neutron/</guid><description>The documentation for configuring Neutron recommends that a network controller has three physical interfaces:
Before you start, set up a machine to be a dedicated network node. Dedicated network nodes should have the following NICs: the management NIC (called MGMT_INTERFACE), the data NIC (called DATA_INTERFACE), and the external NIC (called EXTERNAL_INTERFACE).
People occasionally ask, &amp;ldquo;why three interfaces? What if I only have two?&amp;rdquo;, so I wanted to provide an extended answer that might help people understand what the interfaces are for and what trade-offs are involved in using fewer interfaces.</description><content>&lt;p>The &lt;a href="http://docs.openstack.org/havana/install-guide/install/yum/content/neutron-install.dedicated-network-node.html">documentation for configuring Neutron&lt;/a> recommends
that a network controller has three physical interfaces:&lt;/p>
&lt;blockquote>
&lt;p>Before you start, set up a machine to be a dedicated network node.
Dedicated network nodes should have the following NICs: the
management NIC (called MGMT_INTERFACE), the data NIC (called
DATA_INTERFACE), and the external NIC (called EXTERNAL_INTERFACE).&lt;/p>
&lt;/blockquote>
&lt;p>People occasionally ask, &amp;ldquo;why three interfaces? What if I only have
two?&amp;rdquo;, so I wanted to provide an extended answer that might help
people understand what the interfaces are for and what trade-offs are
involved in using fewer interfaces.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The &lt;code>MGMT_INTERFACE&lt;/code> is used for communication between nodes. This
can include traffic from services to the messaging server (&lt;code>qpid&lt;/code>,
&lt;code>rabbitmq&lt;/code>, etc), traffic between nova and neutron, connections to the
database, and other traffic used to manage your OpenStack environment.&lt;/p>
&lt;p>The &lt;code>DATA_INTERFACE&lt;/code> is used for instance traffic&amp;hellip;that is, traffic
generated by or inbound to instances running in your OpenStack
environment. If you are using GRE or VXLAN tunnels your tunnel
endpoints will be associated with this interface.&lt;/p>
&lt;p>The &lt;code>EXTERNAL_INTERFACE&lt;/code> is used to provide public access to your
instances. The network attached to this interface is generally open
to external traffic, and ip addresses are managed by the floating-ip
functionality in Neutron or Nova.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You want your &lt;code>MGMT_INTERFACE&lt;/code> seperate from your &lt;code>DATA_INTERFACE&lt;/code>
in order to avoid accidentally granting management access to your
OpenStack hosts to your tenants. A typical OpenStack environment
may not use authentication in all cases, and a tenant host with
access to the management network could intentionally or accidentally
cause problems.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You want your &lt;code>EXTERNAL_INTERFACE&lt;/code> separate from your
&lt;code>DATA_INTERFACE&lt;/code> because your network controller &lt;em>must&lt;/em> be acting as
a router between these two interfaces in order for the netfilter
&lt;code>PREROUTING&lt;/code> and &lt;code>POSTROUTING&lt;/code> rules to activate. These rules are
used to map floating ip addresses to internal addresses via &lt;code>SNAT&lt;/code>
and &lt;code>DNAT&lt;/code> rules, which only work packets traverse the &lt;code>FORWARD&lt;/code>
chain.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You want your &lt;code>MGMT_INTERFACE&lt;/code> separate from your
&lt;code>EXTERNAL_INTERFACE&lt;/code> because they have dramatically different access
requirements. Your &lt;code>MGMT_INTERFACE&lt;/code> should typically only be
available to other hosts in your OpenStack deployment, while your
&lt;code>EXTERNAL_INTERFACE&lt;/code> will generally require much broader access.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If you are deploying a proof-of-concept (POC) deployment to which you
are not actually providing public access, you can elect to not have an
&lt;code>EXTERNAL_INTERFACE&lt;/code>. Rather than adding this device to &lt;code>br-ex&lt;/code>, you
will set up outbound NAT rules so that &amp;ldquo;external&amp;rdquo; traffic from your
instances will masquerade using the primary ip address of your network
controller.&lt;/p></content></item><item><title>Automatic hostname entries for libvirt domains</title><link>https://blog.oddbit.com/post/2013-10-04-automatic-dns-entrie/</link><pubDate>Fri, 04 Oct 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-10-04-automatic-dns-entrie/</guid><description>Have you ever wished that you could use libvirt domain names as hostnames? So that you could do something like this:
$ virt-install -n anewhost ... $ ssh clouduser@anewhost Since this is something that would certainly make my life convenient, I put together a small script called virt-hosts that makes this possible. You can find virt-hosts in my virt-utils GitHub repository:
https://raw.github.com/larsks/virt-utils/master/virt-hosts Run by itself, with no options, virt-hosts will scan through your running domains for interfaces on the libvirt default network, look up those MAC addresses up in the corresponding default.</description><content>&lt;p>Have you ever wished that you could use &lt;code>libvirt&lt;/code> domain names as
hostnames? So that you could do something like this:&lt;/p>
&lt;pre>&lt;code>$ virt-install -n anewhost ...
$ ssh clouduser@anewhost
&lt;/code>&lt;/pre>
&lt;p>Since this is something that would certainly make my life convenient,
I put together a small script called &lt;a href="https://raw.github.com/larsks/virt-utils/master/virt-hosts">virt-hosts&lt;/a> that makes this
possible. You can find &lt;a href="https://raw.github.com/larsks/virt-utils/master/virt-hosts">virt-hosts&lt;/a> in my &lt;a href="https://raw.github.com/larsks/virt-utils/">virt-utils&lt;/a> GitHub
repository:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://raw.github.com/larsks/virt-utils/master/virt-hosts">https://raw.github.com/larsks/virt-utils/master/virt-hosts&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Run by itself, with no options, &lt;code>virt-hosts&lt;/code> will scan through your
running domains for interfaces on the libvirt &lt;code>default&lt;/code> network, look
up those MAC addresses up in the corresponding &lt;code>default.leases&lt;/code> file,
and then generate a hosts file on &lt;code>stdout&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code>$ virt-hosts
192.168.122.221 compute-tmp0-net0.default.virt compute-tmp0.default.virt
192.168.122.101 centos-0-net0.default.virt centos-0.default.virt
192.168.122.214 controller-tmp-net0.default.virt controller-tmp.default.virt
&lt;/code>&lt;/pre>
&lt;p>Each address will be assigned the name
&lt;code>&amp;lt;domain_name&amp;gt;-&amp;lt;interface_name&amp;gt;.&amp;lt;network_name&amp;gt;.virt&lt;/code>. The first
interface on the network will also be given the alias
&lt;code>&amp;lt;domain_name&amp;gt;.&amp;lt;network_name&amp;gt;.virt&lt;/code>, so a host with multiple
interfaces on the same network would look like this:&lt;/p>
&lt;pre>&lt;code>$ virt-hosts
192.168.122.221 host0-net0.default.virt host0.default.virt
192.168.122.110 host0-net1.default.virt
&lt;/code>&lt;/pre>
&lt;p>Of course, this is only half the solution: having generated a hosts
file we need to put it somewhere where your system can find it.&lt;/p>
&lt;h2 id="an-aside-incron">An aside: incron&lt;/h2>
&lt;p>Both of the following solutions rely on &lt;a href="http://inotify.aiken.cz/?section=incron&amp;amp;page=about&amp;amp;lang=en">incron&lt;/a>, a tool that uses
the Linux &lt;a href="http://en.wikipedia.org/wiki/Inotify">inotify&lt;/a> subsystem to trigger scripts in reaction to
events on file and directories. In this case, we&amp;rsquo;ll be using &lt;code>incron&lt;/code>
to monitor the dnsmasq &lt;code>default.leases&lt;/code> file and firing off a script
when it changes.&lt;/p>
&lt;p>You could accomplish the same thing using the &lt;code>inotifywait&lt;/code> program
from the &lt;a href="https://github.com/rvoicilas/inotify-tools/wiki">inotify-tools&lt;/a> package and a small wrapper script, or you
could hook up something to the libvirt events framework.&lt;/p>
&lt;h2 id="using-etchosts">Using /etc/hosts&lt;/h2>
&lt;p>If you want to update your &lt;code>/etc/hosts&lt;/code> file, you can place the
following into a script called &lt;code>update-virt-hosts&lt;/code> (somewhere in
root&amp;rsquo;s &lt;code>PATH&lt;/code>) and run that via &lt;a href="http://inotify.aiken.cz/?section=incron&amp;amp;page=about&amp;amp;lang=en">incron&lt;/a>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
sed -i '/^# BEGIN VIRT HOSTS/,/^# END VIRT HOSTS/ d' /etc/hosts
cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt;/etc/hosts
# BEGIN VIRT HOSTS
$(virt-hosts)
# END VIRT HOSTS
EOF
&lt;/code>&lt;/pre>
&lt;p>Make sure you have &lt;code>incron&lt;/code> installed, and add the following to
&lt;code>/etc/incron.d/virt-hosts&lt;/code>:&lt;/p>
&lt;pre>&lt;code>/var/lib/libvirt/dnsmasq/default.leases IN_MODIFY update-virt-hosts
&lt;/code>&lt;/pre>
&lt;p>This will cause &lt;code>incron&lt;/code> to run your &lt;code>update-virt-hosts&lt;/code> script
whenever it sees an &lt;code>IN_MODIFY&lt;/code> event on the &lt;code>default.leases&lt;/code> file.&lt;/p>
&lt;h2 id="using-networkmanager--dnsmasq">Using NetworkManager + dnsmasq&lt;/h2>
&lt;p>I am running NetworkManager with the &lt;code>dnsmasq&lt;/code> dns plugin. I created
the file &lt;code>/etc/NetworkManager/dnsmasq.d/virthosts&lt;/code> containing:&lt;/p>
&lt;pre>&lt;code>addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts
&lt;/code>&lt;/pre>
&lt;p>This will cause the &lt;code>dnsmasq&lt;/code> process started by &lt;code>NetworkManager&lt;/code> to
use that file as an additional hosts file. I then installed the
&lt;code>incron&lt;/code> package and dropped the following in
&lt;code>/etc/incron.d/virt-hosts&lt;/code>:&lt;/p>
&lt;pre>&lt;code>/var/lib/libvirt/dnsmasq/default.leases IN_MODIFY /usr/local/bin/virt-hosts -ur
&lt;/code>&lt;/pre>
&lt;p>This has &lt;code>incron&lt;/code> listen for changes to the &lt;code>default.leases&lt;/code> file, and
whenever it receives the &lt;code>IN_MODIFY&lt;/code> event it runs &lt;code>virt-hosts&lt;/code> with
the &lt;code>-u&lt;/code> (aka &lt;code>--update&lt;/code>) and &lt;code>-r&lt;/code> (aka &lt;code>--reload-dnsmasq&lt;/code>) flags.
Thef former causes &lt;code>virt-hosts&lt;/code> to send output to
&lt;code>/var/lib/libvirt/dnsmasq/default.addnhosts&lt;/code> instead of &lt;code>stdout&lt;/code>, and
the latter does a &lt;code>killall -HUP dnsmasq&lt;/code> after installing the new
hosts file.&lt;/p></content></item><item><title>Interrupts on the PiFace</title><link>https://blog.oddbit.com/post/2013-08-05-interrupts-on-the-pi/</link><pubDate>Mon, 05 Aug 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-08-05-interrupts-on-the-pi/</guid><description>I recently acquired both a Raspberry Pi and a PiFace IO board. I had a rough time finding examples of how to read the input ports via interrupts (rather than periodically polling for values), especially for the newer versions of the PiFace python libraries.
After a little research, here&amp;rsquo;s some simple code that will print out pin names as you press the input buttons. Button 3 will cause the code to exit:</description><content>&lt;p>I recently acquired both a &lt;a href="http://www.raspberrypi.org/">Raspberry Pi&lt;/a> and a &lt;a href="http://www.element14.com/community/docs/DOC-52857/l/piface-digital-for-raspberry-pi">PiFace&lt;/a> IO board.
I had a rough time finding examples of how to read the input ports via
interrupts (rather than periodically polling for values), especially
for the &lt;a href="https://github.com/piface">newer versions&lt;/a> of the PiFace python libraries.&lt;/p>
&lt;p>After a little research, &lt;a href="https://gist.github.com/larsks/6161684">here&amp;rsquo;s&lt;/a> some simple code that
will print out pin names as you press the input buttons. Button 3
will cause the code to exit:&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/python
import pifacecommon.core
import pifacecommon.interrupts
import os
import time
quit = False
def print_flag(event):
print 'You pressed button', event.pin_num, '.'
def stop_listening(event):
global quit
quit = True
pifacecommon.core.init()
# GPIOB is the input ports, including the four buttons.
port = pifacecommon.core.GPIOB
listener = pifacecommon.interrupts.PortEventListener(port)
# set up listeners for all buttons
listener.register(0, pifacecommon.interrupts.IODIR_ON, print_flag)
listener.register(1, pifacecommon.interrupts.IODIR_ON, print_flag)
listener.register(2, pifacecommon.interrupts.IODIR_ON, print_flag)
listener.register(3, pifacecommon.interrupts.IODIR_ON, stop_listening)
# Start listening for events. This spawns a new thread.
listener.activate()
# Hang around until someone presses button 3.
while not quit:
time.sleep(1)
print 'you pressed button 3 (quitting)'
listener.deactivate()
&lt;/code>&lt;/pre></content></item><item><title>Generating a memberOf attribute for posixGroups</title><link>https://blog.oddbit.com/post/2013-07-22-generating-a-membero/</link><pubDate>Mon, 22 Jul 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-07-22-generating-a-membero/</guid><description>This showed up on #openstack earlier today:
2013-07-22T13:56:10 &amp;lt;m0zes&amp;gt; hello, all. I am looking to setup keystone with an ldap backend. I need to filter users based on group membership, in this case a non-rfc2307 posixGroup. This means that memberOf doesn't show up, and that the memberUid in the group is not a dn. any thoughts on how to accomplish this? It turns out that this is a not uncommon question, so I spent some time today working out a solution using the dynlist overlay for OpenLDAP.</description><content>&lt;p>This showed up on &lt;a href="https://wiki.openstack.org/wiki/IRC">#openstack&lt;/a> earlier today:&lt;/p>
&lt;pre>&lt;code>2013-07-22T13:56:10 &amp;lt;m0zes&amp;gt; hello, all. I am looking to
setup keystone with an ldap backend. I need to filter
users based on group membership, in this case a
non-rfc2307 posixGroup. This means that memberOf doesn't
show up, and that the memberUid in the group is not a
dn. any thoughts on how to accomplish this?
&lt;/code>&lt;/pre>
&lt;p>It turns out that this is a not uncommon question, so I spent some
time today working out a solution using the &lt;a href="http://www.openldap.org/faq/data/cache/1209.html">dynlist&lt;/a> overlay for
&lt;a href="http://www.openldap.org/">OpenLDAP&lt;/a>.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The LDIF data presented in this article can be found &lt;a href="https://github.com/larsks/blog-openldap-dynlist">on github&lt;/a>.&lt;/p>
&lt;h2 id="assumptions">Assumptions&lt;/h2>
&lt;p>I&amp;rsquo;m assuming that you have a traditional &lt;code>posixGroup&lt;/code> that looks
something like this:&lt;/p>
&lt;pre>&lt;code>dn: cn=lars,ou=groups,dc=oddbit,dc=com
objectClass: posixGroup
cn: lars
gidNumber: 1000
memberUid: lars
&lt;/code>&lt;/pre>
&lt;p>That is, members are recorded in the &lt;code>memberUid&lt;/code> attribute which
corresponds to the &lt;code>uidNumber&lt;/code> attribute of a user object.&lt;/p>
&lt;h2 id="loading-the-dynlist-module">Loading the dynlist module&lt;/h2>
&lt;p>This solution makes use of the &lt;code>dynlist&lt;/code> dynamic overlay, so you&amp;rsquo;ll
first need to make sure that module is loaded. Most modern OpenLDAP
deployments make use of the new &lt;code>slapd.d&lt;/code> configuration directory,
which means you&amp;rsquo;ll modify your configuration by loading the following
LDIF file:&lt;/p>
&lt;pre>&lt;code>dn: cn=modules,cn=config
objectClass: olcModuleList
cn: modules
olcModuleLoad: dynlist
&lt;/code>&lt;/pre>
&lt;p>You would load this into your running instance with something like the
following:&lt;/p>
&lt;pre>&lt;code># ldapadd -Y EXTERNAL -H ldapi://%2fvar%2frun%2fldapi -f dynlist.ldif
&lt;/code>&lt;/pre>
&lt;p>This makes certain assumptions about how your permissions are
configured (in particular, it assumes that your server is configured
to permit administrative access to system UID 0 when accessing the
&lt;code>ldapi&lt;/code> socket).&lt;/p>
&lt;p>If you already have a &lt;code>cn=modules{0},cn=config&lt;/code> object, you&amp;rsquo;ll need to
modify instead using the following:&lt;/p>
&lt;pre>&lt;code>dn: cn=modules,cn=config
changetype: modify
add: olcModuleLoad
olcModuleLoad: dynlist
&lt;/code>&lt;/pre>
&lt;p>And use &lt;code>ldapmodify&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ldapmodify -Y EXTERNAL -H ldapi://%2fvar%2frun%2fldapi -f dynlist.ldif
&lt;/code>&lt;/pre>
&lt;h2 id="schema-modifications">Schema modifications&lt;/h2>
&lt;p>In an ideal world, we would be able to make our solution populate the
standard &lt;code>memberOf&lt;/code> attribute. Unfortunately, this is an
&amp;ldquo;operational&amp;rdquo; attribute in OpenLDAP, which means we can&amp;rsquo;t make it
available to a user class&amp;hellip;so, we&amp;rsquo;re going to define (a) a new
&lt;code>attributeType&lt;/code> that is largely identical to the &lt;code>memberOf&lt;/code> attribute,
and (b) a new auxiliary object class that allows the new attribute.&lt;/p>
&lt;pre>&lt;code>dn: cn=oddbit,cn=schema,cn=config
objectClass: olcSchemaConfig
cn: oddbit
olcAttributeTypes: ( 1.3.6.1.4.1.24441.1.1.1
NAME 'obMemberOf'
DESC 'Distinguished name of a group of which the object is a member'
EQUALITY distinguishedNameMatch
SYNTAX 1.3.6.1.4.1.1466.115.121.1.12 )
olcObjectClasses: ( 1.3.6.1.4.1.24441.2.1.1
NAME 'obPerson' DESC 'oddbit.com person'
AUXILIARY MAY ( obMemberOf ) )
&lt;/code>&lt;/pre>
&lt;p>This gives us the &lt;code>obMemberOf&lt;/code> attribute and the &lt;code>obPerson&lt;/code> object
class. &lt;strong>NOTE&lt;/strong>: the OIDs I&amp;rsquo;m using here are using my own
IANA-assigned OID prefix. You should replace &lt;code>1.3.6.1.4.1.24441&lt;/code> with
your own OID prefix. If you don&amp;rsquo;t have one (and you&amp;rsquo;re sure your
organization doesn&amp;rsquo;t already have one), you can &lt;a href="http://pen.iana.org/pen/PenApplication.page">register&lt;/a> for your
own.&lt;/p>
&lt;h2 id="defining-a-dynamic-list">Defining a dynamic list&lt;/h2>
&lt;p>We&amp;rsquo;re going to configure the &lt;code>dynlist&lt;/code> overlay so that when it sees an
&lt;code>obPerson&lt;/code> object, it will use the &lt;code>labeledURI&lt;/code> attribute of that
object to generate a list of &lt;code>obMemberOf&lt;/code> attributes containing the
distinguished names of the groups of which the user is a member.
We&amp;rsquo;ll load the following LDIF file into our server:&lt;/p>
&lt;pre>&lt;code>dn: olcOverlay=dynlist,olcDatabase={2}hdb,cn=config
objectClass: olcOverlayConfig
objectClass: olcDynamicList
olcOverlay: dynlist
olcDlAttrSet: obPerson labeledURI obMemberOf
&lt;/code>&lt;/pre>
&lt;p>Note that the distinguished name for this entry depends on the DN of
the database which you are configuring, so you&amp;rsquo;ll need to modify the
&lt;code>olcDatabase=&lt;/code> component in the DN.&lt;/p>
&lt;h2 id="setting-user-attributes">Setting user attributes&lt;/h2>
&lt;p>With the above configuration in place, we can now add the necessary
&lt;code>labeledURI&lt;/code> attribute to a user and see what happens. For our
purposes, this attribute needs to contain an LDAP URI that returns the
groups of which the user is a member. Assuming a user like this:&lt;/p>
&lt;pre>&lt;code>dn: cn=user1,ou=people,dc=oddbit,dc=com
objectClass: posixAccount
objectClass: inetOrgPerson
cn: user1
sn: testuser
uid: user1
uidNumber: 1001
gidNumber: 1001
homeDirectory: /home/user1
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ll need to add the following:&lt;/p>
&lt;pre>&lt;code>labeledURI: ldap:///ou=groups,dc=oddbit,dc=com??sub?(&amp;amp;(
objectclass=posixgroup)(memberuid=user1))
&lt;/code>&lt;/pre>
&lt;p>You could do this with the following LDIF file and &lt;code>ldapmodify&lt;/code>:&lt;/p>
&lt;pre>&lt;code>dn: cn=user1,ou=people,dc=oddbit,dc=com
changetype: modify
add: labeledURI
labeledURI: ldap:///ou=groups,dc=oddbit,dc=com??sub?(&amp;amp;(
objectclass=posixgroup)(memberuid=user1))
&lt;/code>&lt;/pre>
&lt;h2 id="testing-things-out">Testing things out&lt;/h2>
&lt;p>Assuming we have the following groups:&lt;/p>
&lt;pre>&lt;code>dn: cn=user1,ou=groups,dc=oddbit,dc=com
objectClass: posixGroup
cn: user1
gidNumber: 1001
memberUid: lars
memberUid: user1
dn: cn=staff,ou=groups,dc=oddbit,dc=com
objectClass: posixGroup
cn: staff
gidNumber: 2000
memberUid: user1
&lt;/code>&lt;/pre>
&lt;p>If we look up the &lt;code>user1&lt;/code> user:&lt;/p>
&lt;pre>&lt;code># ldapsearch -Y EXTERNAL -H ldapi://%2fvar%2frun%2fldapi -b \
ou=people,dc=oddbit,dc=com cn=user1
&lt;/code>&lt;/pre>
&lt;p>We should see &lt;code>obMemberOf&lt;/code> attributes in the result:&lt;/p>
&lt;pre>&lt;code>dn: cn=user1,ou=people,dc=oddbit,dc=com
cn: user1
sn: testuser
uid: user1
uidNumber: 1001
gidNumber: 1001
homeDirectory: /home/user1
labeledURI: ldap:///ou=groups,dc=oddbit,dc=com??sub?(&amp;amp;(objectclass=posixgroup)
(memberuid=user1))
objectClass: inetOrgPerson
objectClass: obPerson
objectClass: posixAccount
obmemberof: cn=user1,ou=groups,dc=oddbit,dc=com
obmemberof: cn=staff,ou=groups,dc=oddbit,dc=com
&lt;/code>&lt;/pre>
&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>Note that this solution requires searching through all of your group
entries every time you look up a user object. Given a sufficiently
large directory this may not be an optimal solution.&lt;/p></content></item><item><title>Split concatenated certificates with awk</title><link>https://blog.oddbit.com/post/2013-07-16-split-concatenated-c/</link><pubDate>Tue, 16 Jul 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-07-16-split-concatenated-c/</guid><description>This is a short script that takes a list of concatenated certificates as input (such as a collection of CA certificates) and produces a collection of numbered files, each containing a single certificate.
#!/bin/awk -f # This script expects a list of concatenated certificates on input and # produces a collection of individual numbered files each containing # a single certificate. BEGIN {incert=0} /-----BEGIN( TRUSTED)? CERTIFICATE-----/ { certno++ certfile=sprintf(&amp;quot;cert-%d.crt&amp;quot;, certno) incert=1 } /-----END( TRUSTED)?</description><content>&lt;p>&lt;a href="https://gist.github.com/larsks/6008833">This&lt;/a> is a short script that takes a list of concatenated
certificates as input (such as a collection of CA certificates) and
produces a collection of numbered files, each containing a single
certificate.&lt;/p>
&lt;pre>&lt;code>#!/bin/awk -f
# This script expects a list of concatenated certificates on input and
# produces a collection of individual numbered files each containing
# a single certificate.
BEGIN {incert=0}
/-----BEGIN( TRUSTED)? CERTIFICATE-----/ {
certno++
certfile=sprintf(&amp;quot;cert-%d.crt&amp;quot;, certno)
incert=1
}
/-----END( TRUSTED)? CERTIFICATE-----/ {
print &amp;gt;&amp;gt; certfile
incert=0
}
incert==1 { print &amp;gt;&amp;gt; certfile }
&lt;/code>&lt;/pre></content></item><item><title>Did Arch Linux eat your KVM?</title><link>https://blog.oddbit.com/post/2013-04-08-did-archlinux-eat-yo/</link><pubDate>Mon, 08 Apr 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-04-08-did-archlinux-eat-yo/</guid><description>A recent update to Arch Linux replaced the qemu-kvm package with an updated version of qemu. A side effect of this change is that the qemu-kvm binary is no longer available, and any libvirt guests on your system utilizing that binary will no longer operate. As is typical with Arch, there is no announcement about this incompatible change, and queries to #archlinux will be met with the knowledge, grace and decorum you would expect of that channel:</description><content>&lt;p>A recent update to &lt;a href="https://www.archlinux.org/">Arch Linux&lt;/a> replaced the &lt;code>qemu-kvm&lt;/code> package with
an updated version of &lt;code>qemu&lt;/code>. A side effect of this change is that
the &lt;code>qemu-kvm&lt;/code> binary is no longer available, and any &lt;code>libvirt&lt;/code> guests
on your system utilizing that binary will no longer operate. As is
typical with Arch, there is no announcement about this incompatible
change, and queries to &lt;code>#archlinux&lt;/code> will be met with the knowledge,
grace and decorum you would expect of that channel:&lt;/p>
&lt;pre>&lt;code>2013-04-08T18:00 &amp;lt; gtmanfred&amp;gt; USE --enable-kvm for fucks sake
2013-04-08T18:00 &amp;lt; gtmanfred&amp;gt; DO I HAVE TO SAY IT AGAIN?
&lt;/code>&lt;/pre>
&lt;p>The emulator binary is hardcoded into your domain in the &lt;code>&amp;lt;emulator&amp;gt;&lt;/code>
emulator, and typically looks something like this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;emulator&amp;gt;/usr/bin/qemu-kvm&amp;lt;/emulator&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>In order to get your guests working again after the upgrade you&amp;rsquo;ll
need to replace this path with an appropriate selection from one of
the other binaries provided by the &lt;code>qemu&lt;/code> package, which include
&lt;code>qemu-system-i386&lt;/code> and &lt;code>qemu-system-x86_64&lt;/code>. You&amp;rsquo;ll want to select
the one appropriate for your &lt;em>guest&lt;/em> architecture. You can do this
manually running &lt;code>virsh edit&lt;/code> for each affected guest, but if you have
more than a couple that rapidly becomes annoying.&lt;/p>
&lt;p>We can use &lt;a href="https://en.wikipedia.org/wiki/XSLT">XSLT&lt;/a> to write a transformation that will set the
&lt;code>&amp;lt;emulator&amp;gt;&lt;/code> to an appropriate value, and we can set things up to run
this automatically across all of our guests. The following stylesheet
will replace the &lt;code>&amp;lt;emulator&amp;gt;&lt;/code> tag with a path to an appropriate &lt;code>qemu&lt;/code> (by
extracting the &lt;code>arch&lt;/code> attribute of the &lt;code>domain/os/type&lt;/code> element:&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;xsl:stylesheet version=&amp;quot;1.0&amp;quot; xmlns:xsl=&amp;quot;http://www.w3.org/1999/XSL/Transform&amp;quot;&amp;gt;
&amp;lt;!-- copy all elements verbatim... --&amp;gt;
&amp;lt;xsl:template match=&amp;quot;@*|node()&amp;quot;&amp;gt;
&amp;lt;xsl:copy&amp;gt;
&amp;lt;xsl:apply-templates select=&amp;quot;@*|node()&amp;quot;/&amp;gt;
&amp;lt;/xsl:copy&amp;gt;
&amp;lt;/xsl:template&amp;gt;
&amp;lt;!-- ...except for the 'emulator' element. --&amp;gt;
&amp;lt;xsl:template match=&amp;quot;emulator&amp;quot;&amp;gt;
&amp;lt;emulator&amp;gt;/usr/bin/qemu-system-&amp;lt;xsl:value-of select=&amp;quot;/*/os/type/@arch&amp;quot;/&amp;gt;&amp;lt;/emulator&amp;gt;
&amp;lt;/xsl:template&amp;gt;
&amp;lt;/xsl:stylesheet&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re going to apply this to all of our (inactive) guests via the
&lt;code>virsh edit&lt;/code> subcommand. This command runs an editor (selected based
on your &lt;code>VISUAL&lt;/code> or &lt;code>EDITOR&lt;/code> environment variables) on your domain
XML. We need to create an &amp;ldquo;editor&amp;rdquo; that will apply the above
transformation to its input file. Something like this will work:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
tmpfile=$(mktemp &amp;quot;$1.patched.XXXXXX&amp;quot;)
xsltproc -o &amp;quot;$tmpfile&amp;quot; patch-emulator.xsl &amp;quot;$1&amp;quot;
mv &amp;quot;$tmpfile&amp;quot; &amp;quot;$1&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Assuming the above script has been saved as &amp;ldquo;patch-emulator.sh&amp;rdquo; (and
made executable), we can run this across all of our inactive guests
like this:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
VISUAL=./patch-emulator.sh
export VISUAL
virsh list --inactive --name | while read vm; do
[ &amp;quot;$vm&amp;quot; ] || continue
virsh edit $vm
done
&lt;/code>&lt;/pre></content></item><item><title>I2C on the Raspberry Pi</title><link>https://blog.oddbit.com/post/2013-03-12-i2c-on-the-raspberry/</link><pubDate>Tue, 12 Mar 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-03-12-i2c-on-the-raspberry/</guid><description>I&amp;rsquo;ve set up my Raspberry Pi to communicate with my Arduino via I2C. The Raspberry Pi is a 3.3v device and the Arduino is a 5v device. While in general this means that you need to use a level converter when connecting the two devices, you don&amp;rsquo;t need to use a level converter when connecting the Arduino to the Raspberry Pi via I2C.
The design of the I2C bus is such that the only device driving a voltage on the bus is the master (in this case, the Raspberry Pi), via pull-up resistors.</description><content>&lt;p>I&amp;rsquo;ve set up my &lt;a href="http://www.raspberrypi.org/">Raspberry Pi&lt;/a> to communicate with my &lt;a href="http://www.arduino.cc/">Arduino&lt;/a> via
&lt;a href="http://en.wikipedia.org/wiki/I%C2%B2C">I2C&lt;/a>. The Raspberry Pi is a 3.3v device and the Arduino is a 5v
device. While in general this means that you need to use a level
converter when connecting the two devices, &lt;strong>you don&amp;rsquo;t need to use a
level converter when connecting the Arduino to the Raspberry Pi via
I2C.&lt;/strong>&lt;/p>
&lt;p>The design of the I2C bus is such that the only device driving a
voltage on the bus is the master (in this case, the Raspberry Pi), via
pull-up resistors. So when &amp;ldquo;idle&amp;rdquo;, the bus is pulled to 3.3v volts by
the Pi, which is perfectly safe for the Arduino (and compatible with
it&amp;rsquo;s 5v signaling). To transmit data on the bus, a device brings the
bus low by connecting it to ground. In other words, slave devices
&lt;em>never&lt;/em> drive the bus high. This means that the Raspberry Pi will
never see a 5v signal from the Arduino&amp;hellip;unless, of course, you make a
mistake and accidentally &lt;code>digitalWrite&lt;/code> a &lt;code>HIGH&lt;/code> value on one of the
Arduino&amp;rsquo;s &lt;code>I2C&lt;/code> pins. So don&amp;rsquo;t do that.&lt;/p>
&lt;p>Note that the built-in pull-up resistors are &lt;em>only&lt;/em> available on the
Pi&amp;rsquo;s I2C pins (Pins 3 (&lt;code>SDA&lt;/code>) and 5 (&lt;code>SCL&lt;/code>), aka BCM &lt;code>GPIO0&lt;/code> and
&lt;code>GPIO1&lt;/code> on a Rev. 1 board, &lt;code>GPIO2&lt;/code> and &lt;code>GPIOP3&lt;/code> on a Rev. 2 board):&lt;/p>
&lt;p>&lt;img src="raspberry-pi-i2c-pins.jpg" alt="Raspberry Pi Pins">&lt;/p>
&lt;p>On the Arduino Uno, the &lt;code>I2C&lt;/code> pins are pins &lt;code>A4&lt;/code> (&lt;code>SDA&lt;/code>) and &lt;code>A5&lt;/code>
(&lt;code>SCL&lt;/code>):&lt;/p>
&lt;p>&lt;img src="arduino-i2c-pins.jpg" alt="Arduino Uno Pins">&lt;/p>
&lt;p>For information about other boards and about the Arduino I2C API, see
the documentation for the &lt;a href="http://arduino.cc/en/Reference/Wire">Wire library&lt;/a>.&lt;/p></content></item><item><title>Interrupt driven GPIO with Python</title><link>https://blog.oddbit.com/post/2013-03-08-interrupt-driven-gpi/</link><pubDate>Fri, 08 Mar 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-03-08-interrupt-driven-gpi/</guid><description>There are several Python libraries out there for interacting with the GPIO pins on a Raspberry Pi:
RPi.GPIO The WiringPi bindings for Python, and The Quick2Wire Python API (which depends on Python 3) All of them are reasonably easy to use, but the Quick2Wire API provides a uniquely useful feature: epoll-enabled GPIO interrupts. This makes it trivial to write code that efficiently waits for and responds to things like button presses.</description><content>&lt;p>There are several Python libraries out there for interacting with the
&lt;a href="https://en.wikipedia.org/wiki/General_Purpose_Input/Output">GPIO&lt;/a> pins on a Raspberry Pi:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://pypi.python.org/pypi/RPi.GPIO">RPi.GPIO&lt;/a>&lt;/li>
&lt;li>The &lt;a href="https://github.com/WiringPi/WiringPi-Python">WiringPi&lt;/a> bindings for Python, and&lt;/li>
&lt;li>The &lt;a href="https://github.com/quick2wire/quick2wire-python-api">Quick2Wire&lt;/a> Python API (which depends on Python 3)&lt;/li>
&lt;/ul>
&lt;p>All of them are reasonably easy to use, but the Quick2Wire API
provides a uniquely useful feature: &lt;code>epoll&lt;/code>-enabled GPIO interrupts.
This makes it trivial to write code that efficiently waits for and
responds to things like button presses.&lt;/p>
&lt;p>The following simple example waits for a button press attached to
&lt;code>GPIO1&lt;/code> (but refer to the chart in &lt;a href="https://projects.drogon.net/raspberry-pi/wiringpi/pins/">this document&lt;/a> to see
exactly what that means; this is pin 12 on a Raspberry Pi v2 board)
and lights an LED attached to &lt;code>GPIO0&lt;/code> when the button is pressed:&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env python3
import select
from quick2wire.gpio import pins, In, Out, Rising, Falling, Both
button1 = pins.pin(0, direction=In, interrupt=Both)
led = pins.pin(1, direction=Out)
with button1,led:
epoll = select.epoll()
epoll.register(button1, select.EPOLLIN|select.EPOLLET)
while True:
events = epoll.poll()
for fileno, event in events:
if fileno == button1.fileno():
print('BUTTON 1!', button1.value)
led.value = button1.value
&lt;/code>&lt;/pre>
&lt;p>There is also a &lt;code>Selector&lt;/code> class that makes the &lt;code>epoll&lt;/code> interface a
little easier to use. The following code is equivalent to the above
&lt;code>epoll&lt;/code> example:&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env python3
from quick2wire.gpio import pins, In, Out, Both
from quick2wire.selector import Selector
button1 = pins.pin(0, direction=In, interrupt=Both)
led = pins.pin(1, direction=Out)
with button1, led, Selector(1) as selector:
selector.add(button1)
while True:
selector.wait()
if selector.ready == button1:
print('BUTTON 1!', button1.value)
led.value = button1.value
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>selector&lt;/code> module includes a &lt;code>Timer&lt;/code> class that lets you add
one-shot or repeating timers to a &lt;code>Selector&lt;/code>. The following example
will light the LED for one second after the button is pressed, unless
the button is pressed again, in which case the LED will go out
immediately:&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env python3
from quick2wire.gpio import pins, In, Out, Both
from quick2wire.selector import Selector, Timer
button1 = pins.pin(0, direction=In, interrupt=Both)
led = pins.pin(1, direction=Out)
active = False
with button1, led, \
Selector(1) as selector, \
Timer(offset=2) as timer:
selector.add(button1)
selector.add(timer)
while True:
selector.wait()
if selector.ready == button1:
print('BUTTON 1!', button1.value, active)
if button1.value:
if active:
active = False
led.value = 0
timer.stop()
else:
active = True
led.value = 1
timer.start()
if selector.ready == timer:
if active:
active = False
led.value = 0
&lt;/code>&lt;/pre>
&lt;p>All of these examples rely on Python&amp;rsquo;s &lt;a href="http://docs.python.org/3/reference/compound_stmts.html#the-with-statement">with statement&lt;/a>. If you&amp;rsquo;re
unfamiliar with &lt;code>with&lt;/code>, you can find more information &lt;a href="http://docs.python.org/3/reference/compound_stmts.html#the-with-statement">here&lt;/a>.&lt;/p></content></item><item><title>Controlling a servo with your Arduino</title><link>https://blog.oddbit.com/post/2013-03-07-controlling-a-servo/</link><pubDate>Thu, 07 Mar 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-03-07-controlling-a-servo/</guid><description>I&amp;rsquo;ve recently started playing with an Arduino kit I purchased a year ago (and only just now got around to unboxing). I purchased the kit from SparkFun, and it includes a motley collection of resistors, LEDs, a motor, a servo, and more.
I was fiddling around with this exercise, which uses the SoftwareServo library to control a servo. Using this library, you just pass it an angle and the library takes care of everything else, e.</description><content>&lt;p>I&amp;rsquo;ve recently started playing with an &lt;a href="http://arduino.cc/">Arduino&lt;/a> kit I purchased a year
ago (and only just now got around to unboxing). I purchased the kit
from &lt;a href="https://www.sparkfun.com/">SparkFun&lt;/a>, and it includes a motley collection of resistors,
LEDs, a motor, a servo, and more.&lt;/p>
&lt;p>I was fiddling around with &lt;a href="http://oomlout.com/a/products/ardx/circ-04/">this exercise&lt;/a>, which uses the
&lt;code>SoftwareServo&lt;/code> library to control a servo. Using this library,
you just pass it an angle and the library takes care of everything
else, e.g. to rotate to 90 degrees you would do this:&lt;/p>
&lt;pre>&lt;code>myservo.write(90);
&lt;/code>&lt;/pre>
&lt;p>The exercise suggests trying to control the servo without using the
library:&lt;/p>
&lt;blockquote>
&lt;p>While it is easy to control a servo using the Arduino’s included
library sometimes it is fun to figure out how to program something
yourself. Try it. We’re controlling the pulse directly so you could
use this method to control servos on any of the Arduino’s 20
available pins (you need to highly optimize this code before doing
that).&lt;/p>
&lt;/blockquote>
&lt;p>It took me a few tries, and it looks as if the upper and lower limits
for the servo pulses given in that documentation may not be 100%
accurate. This is what I finally came with. As an added bonus, it
writes position information to the serial port:&lt;/p>
&lt;pre>&lt;code>int incomingByte = 0;
int servo0 = 600;
int servo180 = 2100;
int inc = 20;
int pos = servo0;
int servoPin = 9;
int pulseInterval=2000;
void setup() {
Serial.begin(9600); // opens serial port, sets data rate to 9600 bps
pinMode(servoPin, OUTPUT);
}
void loop() {
int i;
pos += inc;
if (pos &amp;gt; servo180) {
Serial.println(&amp;quot;REVERSE!&amp;quot;);
pos = servo180;
inc *= -1;
delay(500);
} else if (pos &amp;lt; servo0) {
Serial.println(&amp;quot;FORWARD!&amp;quot;);
pos = servo0;
inc *= -1;
delay(500);
}
Serial.print(&amp;quot;pos = &amp;quot;);
Serial.println(pos, DEC);
digitalWrite(servoPin, HIGH);
delayMicroseconds(pos);
digitalWrite(servoPin, LOW);
delay(20);
}
&lt;/code>&lt;/pre>
&lt;p>Under Linux or OS X, you could view the serial output using &lt;code>screen&lt;/code>
like this:&lt;/p>
&lt;pre>&lt;code>screen /dev/tty.usbmodemfd12441 9600
&lt;/code>&lt;/pre></content></item><item><title>A quote about XMLRPC</title><link>https://blog.oddbit.com/post/2013-02-25-puppet-xmlrpc-quote/</link><pubDate>Mon, 25 Feb 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-02-25-puppet-xmlrpc-quote/</guid><description>I&amp;rsquo;ve been reading up on Puppet 3 lately, and came across the following:
XMLRPC was the new hotness when development on Puppet started. Now, XMLRPC is that horrible thing with the XML and the angle brackets and the pain and sad.
(from http://somethingsinistral.net/blog/the-angry-guide-to-puppet-3/)
&amp;hellip;which also accurately sums up my feelings when I come across yet another piece of software where someone has decided that XML (or even JSON) is a good user-facing configuration syntax.</description><content>&lt;p>I&amp;rsquo;ve been reading up on Puppet 3 lately, and came across the
following:&lt;/p>
&lt;blockquote>
&lt;p>XMLRPC was the new hotness when development on Puppet started. Now,
XMLRPC is that horrible thing with the XML and the angle brackets and
the pain and sad.&lt;/p>
&lt;/blockquote>
&lt;p>(from &lt;a href="http://somethingsinistral.net/blog/the-angry-guide-to-puppet-3/">http://somethingsinistral.net/blog/the-angry-guide-to-puppet-3/&lt;/a>)&lt;/p>
&lt;p>&amp;hellip;which also accurately sums up my feelings when I come across yet
another piece of software where someone has decided that XML (or even
JSON) is a good user-facing configuration syntax.&lt;/p></content></item><item><title>A systemd unit for ucarp</title><link>https://blog.oddbit.com/post/2013-02-21-ucarp-unit-for-systemd/</link><pubDate>Thu, 21 Feb 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-02-21-ucarp-unit-for-systemd/</guid><description>In Fedora 17 there are still a number of services that either have not been ported over to systemd or that do not take full advantage of systemd. I&amp;rsquo;ve been investigating some IP failover solutions recently, including ucarp, which includes only a System-V style init script.
I&amp;rsquo;ve created a template service for ucarp that will let you start a specific virtual ip like this:
systemctl start ucarp@001 This will start ucarp using settings from /etc/ucarp/vip-001.</description><content>&lt;p>In Fedora 17 there are still a number of services that either have not
been ported over to &lt;code>systemd&lt;/code> or that do not take full advantage of
&lt;code>systemd&lt;/code>. I&amp;rsquo;ve been investigating some IP failover solutions
recently, including &lt;a href="http://www.pureftpd.org/project/ucarp">ucarp&lt;/a>, which includes only a System-V style
init script.&lt;/p>
&lt;p>I&amp;rsquo;ve created a &lt;a href="http://0pointer.de/blog/projects/instances.html">template service&lt;/a> for ucarp that will let
you start a specific virtual ip like this:&lt;/p>
&lt;pre>&lt;code>systemctl start ucarp@001
&lt;/code>&lt;/pre>
&lt;p>This will start ucarp using settings from &lt;code>/etc/ucarp/vip-001.conf&lt;/code>.
The unit file is &lt;a href="https://gist.github.com/larsks/5009872">on github&lt;/a> and embedded here for your
reading pleasure:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=UCARP virtual interface %I
After=network.target
[Service]
Type=simple
EnvironmentFile=-/etc/ucarp/vip-common.conf
EnvironmentFile=-/etc/ucarp/vip-%I.conf
ExecStart=/usr/sbin/ucarp -i $BIND_INTERFACE -p $PASSWORD -v %I -a $VIP_ADDRESS -s $SOURCE_ADDRESS $OPTIONS -u $UPSCRIPT -d $DOWNSCRIPT
KillMode=control-group
[Install]
WantedBy=multiuser.target
&lt;/code>&lt;/pre></content></item><item><title>Running dhcpcd under LXC</title><link>https://blog.oddbit.com/post/2013-02-01-dhcpcd-under-lxc/</link><pubDate>Fri, 01 Feb 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-02-01-dhcpcd-under-lxc/</guid><description>I&amp;rsquo;ve been working with Arch Linux recently, which uses dhcpcd as its default DHCP agent. If you try booting Arch inside an LXC container, you will find that dhcpcd is unable to configure your network interfaces. Running it by hand you will first see the following error:
# dhcpcd eth0 dhcpcd[492]: version 5.6.4 starting dhcpcd[492]: eth0: if_init: Read-only file system dhcpcd[492]: eth0: interface not found or invalid This happens because dhcpcd is trying to modify a sysctl value.</description><content>&lt;p>I&amp;rsquo;ve been working with &lt;a href="http://www.archlinux.org/">Arch Linux&lt;/a> recently, which uses &lt;a href="http://roy.marples.name/projects/dhcpcd/">dhcpcd&lt;/a>
as its default DHCP agent. If you try booting Arch inside an &lt;a href="http://lxc.sourceforge.net/">LXC&lt;/a>
container, you will find that &lt;code>dhcpcd&lt;/code> is unable to configure your
network interfaces. Running it by hand you will first see the
following error:&lt;/p>
&lt;pre>&lt;code># dhcpcd eth0
dhcpcd[492]: version 5.6.4 starting
dhcpcd[492]: eth0: if_init: Read-only file system
dhcpcd[492]: eth0: interface not found or invalid
&lt;/code>&lt;/pre>
&lt;p>This happens because &lt;code>dhcpcd&lt;/code> is trying to modify a sysctl value.
Running &lt;code>dhcpcd&lt;/code> under &lt;code>strace&lt;/code> we see:&lt;/p>
&lt;pre>&lt;code>open(&amp;quot;/proc/sys/net/ipv4/conf/eth0/promote_secondaries&amp;quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = -1 EROFS (Read-only file system)
&lt;/code>&lt;/pre>
&lt;p>This happens because &lt;code>/proc&lt;/code> is typically mounted read-only in a
container environment (to prevent the container from modifying things
that would potentially affect the host system).&lt;/p>
&lt;p>We can use a &amp;ldquo;bind mount&amp;rdquo; to solve this problem. A &amp;ldquo;bind mount&amp;rdquo;
allows you to mount part of a filesystem on another part of the
filesystem. In this case, we&amp;rsquo;re going to mask that value in &lt;code>/proc&lt;/code>
by bind mounting a file on top of it.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>First, we create the file we&amp;rsquo;ll use as a mask:&lt;/p>
&lt;pre>&lt;code> # echo 0 &amp;gt; /var/tmp/promote_secondaries
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Then we mount in on top of the &lt;code>/proc&lt;/code> entry:&lt;/p>
&lt;pre>&lt;code> # mount -o bind /var/tmp/promote_secondaries \
/proc/sys/net/ipv4/conf/eth0/promote_secondaries
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>And now that &lt;code>/proc&lt;/code> value is &amp;ldquo;writable&amp;rdquo; from the perspective of
&lt;code>dhcpcd&lt;/code>. If we try to run &lt;code>dhcpcd&lt;/code> now, we see:&lt;/p>
&lt;pre>&lt;code># dhcpcd eth0
dhcpcd[770]: version 5.6.4 starting
dhcpcd[770]: eth0: sending IPv6 Router Solicitation
dhcpcd[770]: eth0: rebinding lease of 192.168.117.53
dhcpcd[770]: eth0: acknowledged 192.168.117.53 from 192.168.117.1
dhcpcd[770]: eth0: checking for 192.168.117.53
dhcpcd[770]: eth0: sending IPv6 Router Solicitation
dhcpcd[770]: eth0: leased 192.168.117.53 for 3600 seconds
dhcpcd[770]: forked to background, child pid 796
&lt;/code>&lt;/pre>
&lt;p>If you are running &lt;code>dhcpcd&lt;/code> via the &lt;code>dhcpcd@.service&lt;/code> unit, then you
can automate this masking with the following service unit:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=Mask read-only /proc entries for %I.
RequiredBy=dhcpcd@%I
Before=dhcpcd@%I
[Service]
ExecStartPre=/bin/dd if=/proc/sys/net/ipv4/conf/%I/promote_secondaries \
of=/var/tmp/promote_secondaries_%I
ExecStart=/bin/mount -o bind /var/tmp/promote_secondaries_%I \
/proc/sys/net/ipv4/conf/%I/promote_secondaries
RemainAfterExit=yes
ExecStop=/bin/unmount /proc/sys/net/ipv4/conf/%I/promote_secondaries
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;p>If you see&amp;hellip;&lt;/p>
&lt;pre>&lt;code>/usr/lib/dhcpcd/dhcpcd-hooks/30-hostname: line 17: /proc/sys/kernel/hostname: Read-only file system
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;you may need to do something similar to mask the &lt;code>kernel.hostname&lt;/code>
entry in &lt;code>/proc&lt;/code>, although this will need to be done once rather than
per-interface. Alternatively, you can modify the hook script
responsible for setting the hostname
(&lt;code>/usr/lib/dhcpcd/dhcpcd-hooks/30-hostname&lt;/code>).&lt;/p></content></item><item><title>Cleaning up LXC cgroups</title><link>https://blog.oddbit.com/post/2013-01-28-lxc-cant-remove-cgroup/</link><pubDate>Mon, 28 Jan 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-01-28-lxc-cant-remove-cgroup/</guid><description>I spent some time today looking at systemd (44) under Fedora (17). When stopping an LXC container using lxc-stop, I would always encounter this problem:
# lxc-stop -n node0 lxc-start: Device or resource busy - failed to remove cgroup '/sys/fs/cgroup/systemd/node0 This prevents one from starting a new container with the same name:
# lxc-start -n node0 lxc-start: Device or resource busy - failed to remove previous cgroup '/sys/fs/cgroup/systemd/node0' lxc-start: failed to spawn 'node0' lxc-start: Device or resource busy - failed to remove cgroup '/sys/fs/cgroup/systemd/node0' You can correct the problem manually by removing all the child cgroups underneath /sys/fs/cgroup/systemd/&amp;lt;container&amp;gt;, like this:</description><content>&lt;p>I spent some time today looking at systemd (44) under Fedora (17).
When stopping an LXC container using &lt;code>lxc-stop&lt;/code>, I would always
encounter this problem:&lt;/p>
&lt;pre>&lt;code># lxc-stop -n node0
lxc-start: Device or resource busy - failed to remove cgroup '/sys/fs/cgroup/systemd/node0
&lt;/code>&lt;/pre>
&lt;p>This prevents one from starting a new container with the same name:&lt;/p>
&lt;pre>&lt;code># lxc-start -n node0
lxc-start: Device or resource busy - failed to remove previous cgroup '/sys/fs/cgroup/systemd/node0'
lxc-start: failed to spawn 'node0'
lxc-start: Device or resource busy - failed to remove cgroup '/sys/fs/cgroup/systemd/node0'
&lt;/code>&lt;/pre>
&lt;p>You can correct the problem manually by removing all the child cgroups
underneath &lt;code>/sys/fs/cgroup/systemd/&amp;lt;container&amp;gt;&lt;/code>, like this:&lt;/p>
&lt;pre>&lt;code># find /sys/fs/cgroup/systemd/node0/ -type d |
tac |
xargs rmdir
&lt;/code>&lt;/pre>
&lt;p>The call to &lt;code>tac&lt;/code> (which will output lines in reverse order) is
necessary because we need to start with the &amp;ldquo;deepest&amp;rdquo; directory and
work our way back up.&lt;/p>
&lt;p>This appears to be a version-specific problem. I do not see the same
behavior with &lt;code>systemd&lt;/code> 197 under Arch.&lt;/p></content></item><item><title>How do I LXC console?</title><link>https://blog.oddbit.com/post/2013-01-28-how-do-i-lxc-console/</link><pubDate>Mon, 28 Jan 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-01-28-how-do-i-lxc-console/</guid><description>It took me an unreasonably long time to boot an LXC container with working console access. For the record:
When you boot an LXC container, the console appears to be attached to a pts device. For example, when booting with the console attached to your current terminal:
# lxc-start -n node0 ... node0 login: root Last login: Mon Jan 28 16:35:19 on tty1 [root@node0 ~]# tty /dev/console [root@node0 ~]# ls -l /dev/console crw------- 1 root tty 136, 12 Jan 28 16:36 /dev/console This is also true when you attach to a container using lxc-console:</description><content>&lt;p>It took me an unreasonably long time to boot an LXC container with
working console access. For the record:&lt;/p>
&lt;p>When you boot an LXC container, the console appears to be attached to
a &lt;code>pts&lt;/code> device. For example, when booting with the console attached to
your current terminal:&lt;/p>
&lt;pre>&lt;code># lxc-start -n node0
...
node0 login: root
Last login: Mon Jan 28 16:35:19 on tty1
[root@node0 ~]# tty
/dev/console
[root@node0 ~]# ls -l /dev/console
crw------- 1 root tty 136, 12 Jan 28 16:36 /dev/console
&lt;/code>&lt;/pre>
&lt;p>This is also true when you attach to a container using &lt;code>lxc-console&lt;/code>:&lt;/p>
&lt;pre>&lt;code># lxc-start -n node0 -d
# lxc-console -n node0
Type &amp;lt;Ctrl+a q&amp;gt; to exit the console
node0 login: root
Last login: Mon Jan 28 16:36:00 on console
[root@node0 ~]# tty
/dev/tty1
[root@node0 ~]# ls -l /dev/tty1
crw------- 1 root tty 136, 6 Jan 28 16:37 /dev/tty1
&lt;/code>&lt;/pre>
&lt;p>In both cases, the devices have major number &lt;code>136&lt;/code>, which is the &lt;code>pts&lt;/code>
driver. This means that if your LXC configuration file has this:&lt;/p>
&lt;pre>&lt;code>lxc.cgroup.devices.deny = a
&lt;/code>&lt;/pre>
&lt;p>Then your LXC configuration file will also need:&lt;/p>
&lt;pre>&lt;code>lxc.cgroup.devices.allow = c 136:* rwm
&lt;/code>&lt;/pre></content></item><item><title>Systemd and the case of the missing network</title><link>https://blog.oddbit.com/post/2013-01-28-net-with-no-net/</link><pubDate>Mon, 28 Jan 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-01-28-net-with-no-net/</guid><description>I was intrigued by this post on socket activated containers with systemd. The basic premise is:
systemd opens a socket on the host and listens for connections. When a client connections, systemd spawns a new container. The host systemd passes the connected socket to the container systemd. Services in the container receive these sockets from the container systemd. This is a very neat idea, since it delegates all the socket listening to the host and only spins up container and service resources when necessary.</description><content>&lt;p>I was intrigued by &lt;a href="http://0pointer.de/blog/projects/socket-activated-containers.html">this post&lt;/a> on socket activated containers with &lt;code>systemd&lt;/code>. The basic premise is:&lt;/p>
&lt;ul>
&lt;li>&lt;code>systemd&lt;/code> opens a socket on the host and listens for connections.&lt;/li>
&lt;li>When a client connections, &lt;code>systemd&lt;/code> spawns a new container.&lt;/li>
&lt;li>The host &lt;code>systemd&lt;/code> passes the connected socket to the container
&lt;code>systemd&lt;/code>.&lt;/li>
&lt;li>Services in the container receive these sockets from the container
&lt;code>systemd&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>This is a very neat idea, since it delegates all the socket listening
to the host and only spins up container and service resources when
necessary.&lt;/p>
&lt;p>An interesting corollary to this is that the service container doesn&amp;rsquo;t
actually need any networking: since the &lt;em>host&lt;/em> is responsible for
opening the socket and listening for connections, and the container
receives an already connected socket, you can create containers that
have no network interfaces other than the loopback interface &lt;em>and
still connect to them remotely&lt;/em>.&lt;/p>
&lt;p>The example presented in &lt;a href="http://0pointer.de/blog/projects/socket-activated-containers.html">Lennarts article&lt;/a> will work just
fine if you change this:&lt;/p>
&lt;pre>&lt;code>ExecStart=/usr/bin/systemd-nspawn -jbD /srv/mycontainer 3
&lt;/code>&lt;/pre>
&lt;p>To this:&lt;/p>
&lt;pre>&lt;code>ExecStart=/usr/bin/systemd-nspawn --private-network -jbD /srv/mycontainer 3
&lt;/code>&lt;/pre>
&lt;p>After this change, if you connect to this container you&amp;rsquo;ll see:&lt;/p>
&lt;pre>&lt;code># ip addr
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>This opens up a variety of interesting possibilities for creating
&amp;ldquo;endpoint&amp;rdquo; containers that offer services over the network but are
able to limit the scope of a compromised service. Because
&lt;code>systemd-nspawn&lt;/code> has been designed as more of a convenice tool than a
full container solution, we&amp;rsquo;ll need to wait for &lt;code>libvirt&lt;/code> and &lt;code>lxc&lt;/code> to
introduce this socket-passing feature before it&amp;rsquo;s more than an
interesting idea.&lt;/p></content></item><item><title>A second look at Arch Linux</title><link>https://blog.oddbit.com/post/2013-01-25-archlinux-second-look/</link><pubDate>Fri, 25 Jan 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-01-25-archlinux-second-look/</guid><description>This is a followup to an earlier post about Arch Linux.
I&amp;rsquo;ve since spent a little more time working with Arch, and these are the things I like:
The base system is very small and has a very short boot time. I replaced Ubuntu on my old Eee PC with Arch and suddenly the boot time is reasonable (&amp;lt; 10 seconds to a text prompt, &amp;lt; 30 seconds to a full GUI login).</description><content>&lt;p>This is a followup to an &lt;a href="https://blog.oddbit.com/post/a-first-look-at-arch-linux">earlier post about Arch Linux&lt;/a>.&lt;/p>
&lt;p>I&amp;rsquo;ve since spent a little more time working with Arch, and these are
the things I like:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The base system is very small and has a very short boot time. I
replaced Ubuntu on my old &lt;a href="https://en.wikipedia.org/wiki/Asus_Eee_PC#Eee_900_series">Eee PC&lt;/a> with Arch and suddenly the boot
time is reasonable (&amp;lt; 10 seconds to a text prompt, &amp;lt; 30 seconds to a
full GUI login).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I feel better about the Arch installation process after seeing what
happened to Anaconda (the Fedora installer) in Fedora 18.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I have been pleasantly surprised at the speed with which bug reports
have been addressed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I really like the fact that Arch has completely converted over to
&lt;a href="http://www.freedesktop.org/wiki/Software/systemd">systemd&lt;/a>. There are no legacy init scripts, the base system
relies on the systemd journal for logging, etc.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I run a KVM-based virtualization environment at home, and I&amp;rsquo;ve
switched to Arch on the physical host because of its small footprint
(and because it tends to have very recent versions of the various
libvirt-related tools).&lt;/p>
&lt;p>Things that still bother me:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The &lt;a href="https://aur.archlinux.org/">Arch User Repository&lt;/a> is a train wreck. The fact that
there are no tools in the standard repositories for working with the
AUR is a good sign that nobody really trusts it, and despite
language to the contrary there does not appear to be much movement
from the AUR in to the Community repository.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Packages in general are not curated with same care as in Fedora. It
is much more common in Arch to find packages that either have broken
dependencies (e.g., they require a shared library that is not
actually installed) or they have unreasonably broad dependencies
(which can happen if a package includes both GUI and command line
tools, rather than splitting them into separate package). In
particular, my experience here highlights the value of (a) the
automatic dependency mechanisms invoked by &lt;code>rpmbuild&lt;/code> (finding
shared libraries, Python/Perl/Ruby modules required by scripts, etc)
and (b) the use of a tool such as &lt;code>mock&lt;/code> for building packages in a
&amp;ldquo;pristine&amp;rdquo; environment.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The average age on &lt;a href="https://wiki.archlinux.org/index.php/IRC_Channel">#archlinux&lt;/a> appears to be 13. I&amp;rsquo;m not sure
what&amp;rsquo;s up with that (does anyone maintain age demographics for Linux
distributions?). Do we really need &amp;ldquo;b00bs&amp;rdquo; in the &lt;code>/topic&lt;/code>?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Also, and this is completely unrelated to anything else, I am terribly
disappointed that the Arch forums are hosted at
&lt;a href="http://bbs.archlinux.org">http://bbs.archlinux.org&lt;/a> but there is no actual BBS
(&lt;a href="http://www.synchro.net/">e.g.&lt;/a>) running on
port 23. Because that would be cool.&lt;/p></content></item><item><title>Parsing Libvirt XML with xmllint</title><link>https://blog.oddbit.com/post/2012-12-21-parsing-libvirt-xml/</link><pubDate>Fri, 21 Dec 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-12-21-parsing-libvirt-xml/</guid><description>I&amp;rsquo;ve been playing around with the LXC support in libvirt recently, and I&amp;rsquo;m trying to use a model where each LXC instance is backed by a dedicated LVM volume. This means that the process of starting an instance is:
mount the instance root filesystem if necessary start the instance It&amp;rsquo;s annoying to have to do this by hand. I could simply add all the LXC filesystems to /etc/fstab, but this would mean and extra step when creating and deleting each instance.</description><content>&lt;p>I&amp;rsquo;ve been playing around with the &lt;a href="http://lxc.sourceforge.net/">LXC&lt;/a> support in libvirt recently,
and I&amp;rsquo;m trying to use a model where each LXC instance is backed by a
dedicated LVM volume. This means that the process of starting an
instance is:&lt;/p>
&lt;ul>
&lt;li>mount the instance root filesystem if necessary&lt;/li>
&lt;li>start the instance&lt;/li>
&lt;/ul>
&lt;p>It&amp;rsquo;s annoying to have to do this by hand. I could simply add all the
LXC filesystems to &lt;code>/etc/fstab&lt;/code>, but this would mean and extra step
when creating and deleting each instance.&lt;/p>
&lt;p>I thought about creating a wrapper script to handle the mounting (and
unmounting, perhaps) for me, but this leads to another problem: I need
a way to figure out which backing device corresponding to the virtual
instance I&amp;rsquo;m trying to boot.&lt;/p>
&lt;p>I could just adopt a strict naming scheme, so that for a given
instance &amp;ldquo;foo&amp;rdquo; the backing store would be
&lt;code>/dev/vg_something/domain-foo&lt;/code>, but that&amp;rsquo;s too easy! In order to make
life interesting:&lt;/p>
&lt;ul>
&lt;li>I&amp;rsquo;ve added some namespace-prefixed metadata to the instance XML
description, and&lt;/li>
&lt;li>Written a shell script to extract this data (and more) using
&lt;code>xmllint&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="metadata">Metadata&lt;/h2>
&lt;p>The Libvirt domain XML format allows for a general &lt;a href="http://libvirt.org/formatdomain.html#elementsMetadata">metadata
section&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>The &lt;code>metadata&lt;/code> node can be used by applications to store custom
metadata in the form of XML nodes/trees. Applications &lt;strong>must&lt;/strong> use
custom namespaces on their XML nodes/trees, with only one top-level
element per namespace (if the application needs structure, they
should have sub-elements to their namespace element).&lt;/p>
&lt;/blockquote>
&lt;p>This means that if you want to add metadata to a domain description,
you need to define a namespace and use a namespace prefix. For
example, if I want to add a &lt;code>device&lt;/code> element pointing at the backend
storage device for my domain, it might look like this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;metadata xmlns:ob=&amp;quot;http://oddbit.com/ns/libvirt/1&amp;quot;&amp;gt;
&amp;lt;ob:device&amp;gt;/dev/vg0/instance&amp;lt;/ob:device&amp;gt;
&amp;lt;/metadata&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="extracting-metadata-with-xmllint">Extracting metadata with xmllint&lt;/h2>
&lt;p>The &lt;code>xmllint&lt;/code> tool, part of &lt;a href="http://www.xmlsoft.org/">libxml&lt;/a>, can extract nodes from an XML
document using &lt;a href="https://en.wikipedia.org/wiki/XPath">xpath&lt;/a> expressions. Unfortunately, while &lt;code>xmllint&lt;/code>
does have namespace support, it&amp;rsquo;s not particularly convenient. Using
the &lt;code>--shell&lt;/code> mode there&amp;rsquo;s a helpful &lt;code>setns&lt;/code> command:&lt;/p>
&lt;pre>&lt;code>$ xmllint --shell domain.xml
/ &amp;gt; setns x=http://oddbit.com/ns/libvirt/1
/ &amp;gt; xpath //x:device
Object is a Node Set :
Set contains 1 nodes:
1 ELEMENT ob:device
&lt;/code>&lt;/pre>
&lt;p>But that&amp;rsquo;s not available from the command line. We can use the
&lt;code>namespace-uri()&lt;/code> and &lt;code>local-name()&lt;/code> xpath functions to get to the
same place, albeit more verbosely. An equivalent to the above shell
session would be:&lt;/p>
&lt;pre>&lt;code>$ xmllint --xpath '//*[namespace-uri()=&amp;quot;http://oddbit.com/ns/libvirt/1&amp;quot; and local-name()=&amp;quot;device&amp;quot;]'
&amp;lt;ob:device&amp;gt;/dev/vg0/instance&amp;lt;/ob:device&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>The following shell script looks through all inactive LXC domains and
figures out:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>If it should try to start them, using the &lt;code>&amp;lt;oddbit:autostart&amp;gt;&lt;/code> element,&lt;/p>
&lt;/li>
&lt;li>
&lt;p>What the backend storage is, using the &lt;code>&amp;lt;oddbit:device&amp;gt;&lt;/code> element,
and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Where to mount it, by looking for the libvirt &lt;code>&amp;lt;filesystem&amp;gt;&lt;/code> element
with a target of &lt;code>/&lt;/code>.&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
tmpfile=$(mktemp)
trap 'rm -f $tmpfile' EXIT
virsh -c lxc:/// list --name --inactive | while read domain; do
[ &amp;quot;$domain&amp;quot; ] || continue
virsh dumpxml $domain &amp;gt; $tmpfile
autostart=$(xmllint --xpath '//domain/metadata/*[namespace-uri()=&amp;quot;http://oddbit.com/ns/libvirt/1&amp;quot; and local-name()=&amp;quot;autostart&amp;quot;]/text()' $tmpfile)
[ &amp;quot;$autostart&amp;quot; = True ] || continue
device=$(xmllint --xpath '//domain/metadata/*[namespace-uri()=&amp;quot;http://oddbit.com/ns/libvirt/1&amp;quot; and local-name()=&amp;quot;device&amp;quot;]/text()' $tmpfile)
mount=$(xmllint --xpath 'string(//filesystem/target[@dir = &amp;quot;/&amp;quot;]/../source/@dir)' $tmpfile)
echo &amp;quot;$domain $autostart $device $mount&amp;quot;
done
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;m not actually planning on using this in practice. I&amp;rsquo;ll
probably go the naming scheme route. But this was fun to figure out.&lt;/p></content></item><item><title>Getting the IP address of a libvirt domain</title><link>https://blog.oddbit.com/post/2012-12-15-get-vm-ip/</link><pubDate>Sat, 15 Dec 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-12-15-get-vm-ip/</guid><description>If you are starting virtual machines via libvirt, and you have attached them to the default network, there is a very simple method you can use to determine the address assigned to your running instance:
Libvirt runs dnsmasq for the default network, and saves leases in a local file (/var/lib/libvirt/dnsmasq/default.leases under RHEL). You can get the MAC address assigned to a virtual machine by querying the domain XML description. Putting this together gets us something along the lines of:</description><content>&lt;p>If you are starting virtual machines via &lt;code>libvirt&lt;/code>, and you have
attached them to the &lt;code>default&lt;/code> network, there is a very simple method
you can use to determine the address assigned to your running
instance:&lt;/p>
&lt;ul>
&lt;li>Libvirt runs &lt;code>dnsmasq&lt;/code> for the &lt;code>default&lt;/code> network, and saves leases
in a local file (&lt;code>/var/lib/libvirt/dnsmasq/default.leases&lt;/code> under
RHEL).&lt;/li>
&lt;li>You can get the MAC address assigned to a virtual machine by
querying the domain XML description.&lt;/li>
&lt;/ul>
&lt;p>Putting this together gets us something along the lines of:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
# Get the MAC address of the first interface.
mac=$(virsh dumpxml $1 |
xml2 |
awk -F= '$1 == &amp;quot;/domain/devices/interface/mac/@address&amp;quot; {print $2; exit}')
# Get the ip address assigned to this MAC from dnsmasq
ip=$(awk -vmac=$mac '$2 == mac {print $3}' /var/lib/libvirt/dnsmasq/default.leases )
echo $ip
&lt;/code>&lt;/pre>
&lt;p>(&lt;a href="https://gist.github.com/4300055">gist&lt;/a>)&lt;/p>
&lt;p>This uses &lt;a href="http://ofb.net/~egnor/xml2/">xml2&lt;/a> to transform the XML description into something
more amendable to processing in a shell script. You could accomplish
much the same thing using some sort of XPath based tool. For example:&lt;/p>
&lt;pre>&lt;code>mac=$(virsh dumpxml $1 |
xmllint --xpath //interface'[1]/mac/@address' - |
sed 's/.*=&amp;quot;\([^&amp;quot;]*\)&amp;quot;/\1/'
)
&lt;/code>&lt;/pre>
&lt;p>&lt;code>xmllint&lt;/code> is part of &lt;a href="http://www.xmlsoft.org/">libxml2&lt;/a>.&lt;/p></content></item><item><title>A first look at Arch Linux</title><link>https://blog.oddbit.com/post/2012-12-10-archlinux/</link><pubDate>Mon, 10 Dec 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-12-10-archlinux/</guid><description>I decided to take a look at Arch Linux this evening. It&amp;rsquo;s an interesting idea, but has a long way to go:
The installer configured the wrong root= command line into my syslinux configuration, resulting in a system that wouldn&amp;rsquo;t boot.
Update: As far as I can tell, the syslinux-install_update command doesn&amp;rsquo;t actually make any attempt to configure syslinux.cfg at all.
I tried to install libvirt and lxc, but there are unresolved library dependencies&amp;hellip;the virsh command apparently requires libX11.</description><content>&lt;p>I decided to take a look at &lt;a href="https://www.archlinux.org/">Arch Linux&lt;/a> this evening. It&amp;rsquo;s an
interesting idea, but has a long way to go:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The installer configured the wrong &lt;code>root=&lt;/code> command line into my
&lt;code>syslinux&lt;/code> configuration, resulting in a system that wouldn&amp;rsquo;t boot.&lt;/p>
&lt;p>&lt;strong>Update&lt;/strong>: As far as I can tell, the &lt;code>syslinux-install_update&lt;/code>
command doesn&amp;rsquo;t actually make any attempt to configure
&lt;code>syslinux.cfg&lt;/code> at all.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I tried to install &lt;code>libvirt&lt;/code> and &lt;code>lxc&lt;/code>, but there are unresolved
library dependencies&amp;hellip;the &lt;code>virsh&lt;/code> command apparently requires
&lt;code>libX11.so.6&lt;/code>, but the package is missing the appropriate
dependencies to pull in the necessary packages automatically.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I tried to install &lt;code>puppet&lt;/code>, but there is no &lt;code>puppet&lt;/code> package in
Arch Linux. One apparently exists in the &lt;a href="https://aur.archlinux.org/">Arch User
Repository&lt;/a>, but there are no packages in Arch Linux that will
let you install packages from the AUR.&lt;/p>
&lt;p>Arch also seems to be missing &lt;code>chef&lt;/code>, so there go the two biggest
names in configuration management.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There&amp;rsquo;s no way to report bugs without first registering with the bug
tracking system&amp;hellip;and there&amp;rsquo;s no federated login (e.g., with Google,
Facebook, Twitter, OpenID, etc), so this would be Yet Another
Account to remember.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>So, back to Fedora, I guess.&lt;/p></content></item><item><title>Service discovery in the cloud using Avahi</title><link>https://blog.oddbit.com/post/2012-11-27-avahi-service-discovery/</link><pubDate>Tue, 27 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-27-avahi-service-discovery/</guid><description>I&amp;rsquo;m been writing a provisioning tool for OpenStack recently, and I&amp;rsquo;ve put together a demo configuration that installs a simple cluster consisting of three backend nodes and a front-end http proxy. I needed a way for the backend servers to discover the ip address of the frontend server. Since in my target environment everything would be on the same layer-2 network segment, service discovery with multicast DNS (mDNS) seemed like the way to go.</description><content>&lt;p>I&amp;rsquo;m been writing &lt;a href="http://github.com/larsks/drifter">a provisioning tool&lt;/a> for OpenStack
recently, and I&amp;rsquo;ve put together a demo configuration that installs a
simple cluster consisting of three backend nodes and a front-end http
proxy. I needed a way for the backend servers to discover the ip
address of the frontend server. Since in my target environment
everything would be on the same layer-2 network segment, service
discovery with multicast DNS (mDNS) seemed like the way to go.&lt;/p>
&lt;p>&lt;a href="http://avahi.org/">Avahi&lt;/a> is the canonical mDNS implementation for Linux, and it comes
with command-line tools for interacting with the Avahi server. There
is also a DBUS interface and appropriate Python bindings for taking
advantage of it.&lt;/p>
&lt;p>It is relatively simple to publish a service with Avahi; you can
simply drop an XML file into &lt;code>/etc/avahi/services&lt;/code> and you&amp;rsquo;re done.
Discovering services on the client is a little bit more complicated.
Doing it right would involve a chunk of code that interacts with DBUS
in an event-driven environment with lots of callbacks. It seemed like
a big hammer for my little problem, so I ended up parsing the output
of &lt;code>avahi-browse&lt;/code> instead. This turns out to be a little tricker than
you might think, since:&lt;/p>
&lt;ul>
&lt;li>The master node might come up &lt;em>after&lt;/em> the backend nodes, so we need
to wait for it to publish the service registration.&lt;/li>
&lt;li>The registration process on the backends might run before the local
&lt;code>avahi-daemon&lt;/code> has started.&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>-p&lt;/code> flag to &lt;code>avahi-browse&lt;/code> produces parsable output, like this:&lt;/p>
&lt;pre>&lt;code>$ avahi-browse -p _http._tcp
+;eth0;IPv4;master-cluster-lars;Web Site;local
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-r&lt;/code> flag performs name-&amp;gt;address resolution:&lt;/p>
&lt;pre>&lt;code>$ avahi-browse -rp _http._tcp
+;eth0;IPv4;master-cluster-lars;Web Site;local
=;eth0;IPv4;master-cluster-lars;Web Site;local;master.local;172.16.10.56;80;
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-t&lt;/code> flag asks &lt;code>avahi-browse&lt;/code> to terminate after receiving &amp;ldquo;all&amp;rdquo;
entries, and &lt;code>-f&lt;/code> causes &lt;code>avahi-browse&lt;/code> to retry a connection to
&lt;code>avahi-daemon&lt;/code> rather than failing if the daemon is not available. Pu
tall together, we get:&lt;/p>
&lt;pre>&lt;code>$ avahi-browse -rptf _http._tcp
&lt;/code>&lt;/pre>
&lt;p>We can use &lt;code>awk&lt;/code> to get the information we need, and &lt;code>timeout&lt;/code> from
GNU coreutils to take care of the situation I encountered in which
&lt;code>avahi-browse&lt;/code> would never exit.&lt;/p>
&lt;p>The final solution look something like this:&lt;/p>
&lt;pre>&lt;code>master_name=&amp;quot;master-cluster-lars&amp;quot;
while ! [ &amp;quot;$master_ip&amp;quot; ] ; do
master_ip=$(timeout 5 avahi-browse -rptf _http._tcp |
awk -F';' -vmaster_name=&amp;quot;$master_name&amp;quot; '
$1 == &amp;quot;=&amp;quot; &amp;amp;&amp;amp; $4 == master_name {print $8}
')
done
&lt;/code>&lt;/pre>
&lt;p>The value of &lt;code>$master_ip&lt;/code> is then used to &lt;code>POST&lt;/code> a notification to the
master server with &lt;code>curl&lt;/code>:&lt;/p>
&lt;pre>&lt;code>curl --silent -X POST \
&amp;quot;http://$master_ip/proxy/backend/$(facter macaddress)&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The master service is running &lt;a href="http://github.com/larsks/dynproxy-http">dynproxy&lt;/a>, which is responsible for
maintaining a list of backend servers that can be queried by other
tools (such as Apache).&lt;/p></content></item><item><title>Using Oracle JDK under CentOS</title><link>https://blog.oddbit.com/post/2012-11-26-using-oracle-jdk/</link><pubDate>Mon, 26 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-26-using-oracle-jdk/</guid><description>I needed to replace the native OpenJDK based Java VM with the Oracle Java distribution on one of our CentOS servers. In order to do it cleanly I wanted to set up the alternatives system to handle it, but it took a while to figure out the exact syntax.
For the record (and because I will probably forget):
alternatives --install /usr/bin/java java /usr/java/latest/bin/java 2000 \ --slave /usr/bin/keytool keytool /usr/java/latest/bin/keytool \ --slave /usr/bin/rmiregistry rmiregistry /usr/java/latest/bin/rmiregistry</description><content>&lt;p>I needed to replace the native OpenJDK based Java VM with the Oracle
Java distribution on one of our CentOS servers. In order to do it
cleanly I wanted to set up the &lt;code>alternatives&lt;/code> system to handle it, but
it took a while to figure out the exact syntax.&lt;/p>
&lt;p>For the record (and because I will probably forget):&lt;/p>
&lt;pre>&lt;code>alternatives --install /usr/bin/java java /usr/java/latest/bin/java 2000 \
--slave /usr/bin/keytool keytool /usr/java/latest/bin/keytool \
--slave /usr/bin/rmiregistry rmiregistry /usr/java/latest/bin/rmiregistry
&lt;/code>&lt;/pre></content></item><item><title>Document classification with POPFile</title><link>https://blog.oddbit.com/post/2012-11-08-popfile-document-classificatio/</link><pubDate>Thu, 08 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-08-popfile-document-classificatio/</guid><description>I recently embarked upon a quest to categorize a year&amp;rsquo;s worth of trouble tickets (around 15000 documents total). We wanted to see what sort of things are generating the most work for our helpdesk staff so that we can identify areas in which improvements would have the biggest impact. One of my colleagues took a first pass at the data by manually categorizing the tickets based on their subject. This resulted in some useful data, but in the end just over 40% of the tickets are still uncategorized.</description><content>&lt;p>I recently embarked upon a quest to categorize a year&amp;rsquo;s worth of
trouble tickets (around 15000 documents total). We wanted to see what
sort of things are generating the most work for our helpdesk staff so
that we can identify areas in which improvements would have the
biggest impact. One of my colleagues took a first pass at the data by
manually categorizing the tickets based on their subject. This
resulted in some useful data, but in the end just over 40% of the
tickets are still uncategorized.&lt;/p>
&lt;p>I was convinced that we could do better than that by taking into
account the actual content of the trouble tickets. This seemed like a
good task for a &lt;a href="https://en.wikipedia.org/wiki/Bayesian_spam_filtering">Bayesian filter&lt;/a> &amp;ndash; a tool that uses the
statistical probability of words to categorize documents, and is most
commonly used to differentiate &amp;ldquo;spam&amp;rdquo; from &amp;ldquo;non-spam&amp;rdquo; messages in
email. Because of this common use case, many of the tools out there
are built explicitly to make binary (spam/not-spam) determinations,
while for my purposes I needed something that was capable for sorting
documents into multiple categories.&lt;/p>
&lt;p>I finally stumbled across &lt;a href="http://getpopfile.org/">POPFile&lt;/a>, a tool that does almost exactly
what I want. Out of the box, POPFile is designed to act as a proxy
between you and a POP mailbox, categorizing messages as your mail
client retrieves them from a server. While this is tremendously
convenient for use categorizing email, it would be a sub-optimal
interface for categorizing a collection of existing documents.&lt;/p>
&lt;p>Fortunately, POPFile offers an &lt;a href="http://getpopfile.org/docs/popfilemodules:xmlrpc#popfile_xml-rpc_api">XML-RPC API&lt;/a> that allows programmatic
interaction with the classification engine. Usage is relatively
simple; first you acquire a connection to the XML-RPC API and
establish a session key:&lt;/p>
&lt;pre>&lt;code>popfile = ServerProxy(&amp;quot;http://localhost:8081&amp;quot;)
api = popfile.POPFile.API
session = api.get_session_key('admin', '')
&lt;/code>&lt;/pre>
&lt;p>And then for each document, perform whatever transformations you wish
to make (I&amp;rsquo;m building a minimal mail header) and then pass it to the
&lt;code>handle_message()&lt;/code> method:&lt;/p>
&lt;pre>&lt;code>with tempfile.NamedTemporaryFile() as fd:
fd.write('Subject: %s [%s]\n' % (subject, id))
fd.write('Message-ID: &amp;lt;%s@localhost&amp;gt;\n' % id)
fd.write('\n')
fd.write('\n'.join(text))
fd.flush()
# Pass file to POPFile service.
bucket = api.handle_message(session, fd.name, '/dev/null')
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>handle_message()&lt;/code> call takes three parameters:&lt;/p>
&lt;ul>
&lt;li>The session key,&lt;/li>
&lt;li>A path to the input file,&lt;/li>
&lt;li>A path to the output file (POPFile returns the message with header
modifications)&lt;/li>
&lt;/ul>
&lt;p>In this example, I&amp;rsquo;m passing &lt;code>/dev/null&lt;/code> as the third parameter
because I don&amp;rsquo;t care about the data returned from POPFile.&lt;/p>
&lt;p>Initially, POPFile will not perform any categorization of documents.
After manually categorizing just a few documents, two things happen:&lt;/p>
&lt;ul>
&lt;li>POPFile will start using any &lt;a href="http://getpopfile.org/docs/glossary:amagnet">magnets&lt;/a> you have defined, which are
keyword rules that automatically assign documents to a given
category.&lt;/li>
&lt;li>For documents that do not match any magnet rules, POPFile will
attempt to categorize them using the Bayesian inference engine.&lt;/li>
&lt;/ul>
&lt;p>POPFile provides a web interface for interacting with the
classification engine. In particular, this is where you go to
manually classify documents, which further enhances the accuracy of
the Bayesian filters. I got bored after manually categorizing on the
order of 300 or 400 tickets and just fed the rest of the collection
into the filter. I suspect the accuracy of the system is somewhere
between 70% and 80% (based on POPFiles&amp;rsquo;s estimates of accuracy while I
was manually categorizing documents).&lt;/p>
&lt;p>For more information:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://getpopfile.org/">POPFile&lt;/a> website&lt;/li>
&lt;/ul></content></item><item><title>Converting HTML to Markdown</title><link>https://blog.oddbit.com/post/2012-11-06-convert-html-to-markdown/</link><pubDate>Tue, 06 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-06-convert-html-to-markdown/</guid><description>In order to import posts from Blogger into Scriptogr.am I needed to convert all the HTML formatting into Markdown. Thankfully there are a number of tools out there that can help with this task.
MarkdownRules. This is an online service build around Markdownify. It&amp;rsquo;s a slick site with a nice API, but the backend wasn&amp;rsquo;t able to correctly render &amp;lt;pre&amp;gt; blocks. Since I&amp;rsquo;m often writing about code, my posts are filled with things like embedded XML and #include &amp;lt;stdio.</description><content>&lt;p>In order to import posts from &lt;a href="http://blogger.com/">Blogger&lt;/a> into &lt;a href="http://scriptogr.am/">Scriptogr.am&lt;/a> I needed to convert all the HTML formatting into Markdown. Thankfully there are a number of tools out there that can help with this task.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://markdownrules.com/">MarkdownRules&lt;/a>. This is an online service build around
&lt;a href="http://milianw.de/projects/markdownify/">Markdownify&lt;/a>. It&amp;rsquo;s a slick site with a nice API, but the backend
wasn&amp;rsquo;t able to correctly render &lt;code>&amp;lt;pre&amp;gt;&lt;/code> blocks. Since I&amp;rsquo;m often
writing about code, my posts are filled with things like embedded
XML and &lt;code>#include &amp;lt;stdio.h&amp;gt;&lt;/code>, so this was a problem.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://johnmacfarlane.net/pandoc/">Pandoc&lt;/a>. This is a general purpose tool that can convert between
a variety of markup formats. Unfortunately, it &lt;em>also&lt;/em> had similar
problems with &lt;code>&amp;lt;pre&amp;gt;&lt;/code> blocks.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/aaronsw/html2text">html2text&lt;/a>. This a Python tool that converts HTML to Markdown.
It seems to do a better job at handling the &lt;code>&amp;lt;pre&amp;gt;&lt;/code> blocks, although
it doesn&amp;rsquo;t always get the indent level correct when the &lt;code>&amp;lt;pre&amp;gt;&lt;/code>
blocks are embedded in lists.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I ultimately ended up using &lt;a href="https://github.com/aaronsw/html2text">html2text&lt;/a>, combined with a &lt;a href="https://gist.github.com/4022537">simple
script&lt;/a> to read the &lt;a href="http://www.dataliberation.org/takeout-products/blogger">export from Blogger&lt;/a> and feed each document to
the converter.&lt;/p></content></item><item><title>Relocating from Blogger</title><link>https://blog.oddbit.com/post/2012-11-06-moving-from-blogger/</link><pubDate>Tue, 06 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-06-moving-from-blogger/</guid><description>I&amp;rsquo;m in the process of porting over content from Blogger. This may lead to odd formatting or broken links here and there. If you spot something, please let me know.
If you came here from Google and found a broken link, try starting at the archive and see if you can spot what you were looking for.</description><content>&lt;p>I&amp;rsquo;m in the process of porting over content from Blogger. This may
lead to odd formatting or broken links here and there. If you spot
something, please &lt;a href="http://blog.oddbit.com/about">let me know&lt;/a>.&lt;/p>
&lt;p>If you came here from Google and found a broken link, try starting at
the &lt;a href="http://blog.oddbit.com/archive">archive&lt;/a> and see if you can spot what you were looking for.&lt;/p></content></item><item><title>Posting to Scriptogr.am using the API</title><link>https://blog.oddbit.com/post/2012-11-05-posting-via-api/</link><pubDate>Mon, 05 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-05-posting-via-api/</guid><description>Scriptogr.am has a very simple api that allows one to POST and DELETE articles. POSTing an article will place it in the appropriate Dropbox directory and make it available on your blog all in one step.
Here is how you could use this API via Curl:
curl \ -d app_key=$APP_KEY \ -d user_id=$USER_ID \ -d name=&amp;quot;${title:-$1}&amp;quot; \ --data-urlencode text@$tmpfile \ \ http://scriptogr.am/api/article/post/ This assumes that you&amp;rsquo;ve registered for an application key and that you have configured the value into $APP_KEY and your Scriptogr.</description><content>&lt;p>Scriptogr.am has a &lt;a href="http://scriptogr.am/dashboard/#api_documentation">very simple api&lt;/a> that allows one to &lt;code>POST&lt;/code> and
&lt;code>DELETE&lt;/code> articles. &lt;code>POST&lt;/code>ing an article will place it in the
appropriate Dropbox directory and make it available on your blog all
in one step.&lt;/p>
&lt;p>Here is how you could use this API via Curl:&lt;/p>
&lt;pre>&lt;code>curl \
-d app_key=$APP_KEY \
-d user_id=$USER_ID \
-d name=&amp;quot;${title:-$1}&amp;quot; \
--data-urlencode text@$tmpfile \
\
http://scriptogr.am/api/article/post/
&lt;/code>&lt;/pre>
&lt;p>This assumes that you&amp;rsquo;ve registered for an application key and that
you have configured the value into &lt;code>$APP_KEY&lt;/code> and your Scriptogr.am
user id into &lt;code>$USER_ID&lt;/code>.&lt;/p>
&lt;p>The &lt;code>name&lt;/code> attribute is both the title of your article &lt;em>and also&lt;/em> the
filename (normalized and with a &lt;code>.md&lt;/code> extension). Your article &lt;em>must
not&lt;/em> contain the metadata values that are allowed when you&amp;rsquo;re editing
files directly in the Dropbox directory.&lt;/p></content></item><item><title>Private /tmp directories in Fedora</title><link>https://blog.oddbit.com/post/2012-11-05-fedora-private-tmp/</link><pubDate>Mon, 05 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-05-fedora-private-tmp/</guid><description>I ran into an odd problem the other day: I was testing out some configuration changes for a web application by dropping files into /tmp and pointing the application configuration at the appropriate directory. Everything worked out great when testing it by hand&amp;hellip;but when starting up the httpd service, the application behaved as if it was unable to find any of the files in /tmp.
My first assumption was that had simply missed something obvious like file permissions or that I had a typo in my configuration, but after repeated checks and lots of testing it was obvious that something else was going on.</description><content>&lt;p>I ran into an odd problem the other day: I was testing out some
configuration changes for a web application by dropping files into
&lt;code>/tmp&lt;/code> and pointing the application configuration at the appropriate
directory. Everything worked out great when testing it by hand&amp;hellip;but
when starting up the &lt;code>httpd&lt;/code> service, the application behaved as if it
was unable to find any of the files in &lt;code>/tmp&lt;/code>.&lt;/p>
&lt;p>My first assumption was that had simply missed something obvious like
file permissions or that I had a typo in my configuration, but after
repeated checks and lots of testing it was obvious that something else
was going on.&lt;/p>
&lt;p>Grasping at straws I took a close look at the &lt;code>systemd&lt;/code> service file
for &lt;code>httpd&lt;/code>, which looks like this:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=The Apache HTTP Server (prefork MPM)
After=syslog.target network.target remote-fs.target nss-lookup.target
[Service]
Type=forking
PIDFile=/var/run/httpd/httpd.pid
EnvironmentFile=/etc/sysconfig/httpd
ExecStart=/usr/sbin/httpd $OPTIONS -k start
ExecReload=/usr/sbin/httpd $OPTIONS -t
ExecReload=/bin/kill -HUP $MAINPID
ExecStop=/usr/sbin/httpd $OPTIONS -k stop
PrivateTmp=true
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;p>Browsing throught file the following line caught my eye:&lt;/p>
&lt;pre>&lt;code>PrivateTmp=true
&lt;/code>&lt;/pre>
&lt;p>If you know about per-process namespaces in Linux, you&amp;rsquo;re probably
saying &amp;ldquo;Ah-ha!&amp;rdquo;. If you &lt;em>don&amp;rsquo;t&lt;/em> know about per-process namespaces in
Linux&amp;hellip;you should, because this is the foundation for all sorts of
things including Linux Containers (&lt;a href="http://lxc.sourceforge.net/">LXC&lt;/a>). Here&amp;rsquo;s some good
introductory reading:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://lxr.free-electrons.com/source/Documentation/unshare.txt">http://lxr.free-electrons.com/source/Documentation/unshare.txt&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.debian-administration.org/article/628/Per-Process_Namespaces">http://www.debian-administration.org/article/628/Per-Process_Namespaces&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://glandium.org/blog/?p=217">http://glandium.org/blog/?p=217&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>In short, with this configuration in place, the service gets it&amp;rsquo;s very
own version of &lt;code>/tmp&lt;/code> not shared with any other process. While the
files I placed in &lt;code>/tmp&lt;/code> were visible in &lt;em>my&lt;/em> process, they didn&amp;rsquo;t
exist from the point of view of Apache.&lt;/p>
&lt;p>The fix in my case was to place the files somewhere other than &lt;code>/tmp&lt;/code>.
One could also disable the &lt;code>PrivateTmp&lt;/code> setting, but it&amp;rsquo;s generally
turned on for reasons of security.&lt;/p>
&lt;p>The &lt;code>PrivateTmp&lt;/code> option is documented in &lt;a href="https://docs.fedoraproject.org/en-US/Fedora/17/html/Release_Notes/sect-Release_Notes-Changes_for_Sysadmin.html">Changes in Fedora for System
Administrators&lt;/a>, and Dan Walsh discusses it briefly on
&lt;a href="http://danwalsh.livejournal.com/51459.html">his blog&lt;/a>.&lt;/p></content></item><item><title>Automatic configuration of Windows instances in OpenStack, part 1</title><link>https://blog.oddbit.com/post/2012-11-04-openstack-windows-config-part1/</link><pubDate>Sun, 04 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-04-openstack-windows-config-part1/</guid><description>This is the first of two articles in which I discuss my work in getting some Windows instances up and running in our OpenStack environment. This article is primarily about problems I encountered along the way.
Motivations Like many organizations, we have a mix of Linux and Windows in our environment. Some folks in my group felt that it would be nice to let our Windows admins take advantage of OpenStack for prototyping and sandboxing in the same ways our Linux admins can use it.</description><content>&lt;p>This is the first of two articles in which I discuss my work in
getting some Windows instances up and running in our &lt;a href="http://www.openstack.org/">OpenStack&lt;/a>
environment. This article is primarily about problems I encountered
along the way.&lt;/p>
&lt;h2 id="motivations">Motivations&lt;/h2>
&lt;p>Like many organizations, we have a mix of Linux and Windows in our
environment. Some folks in my group felt that it would be nice to let
our Windows admins take advantage of OpenStack for prototyping and
sandboxing in the same ways our Linux admins can use it.&lt;/p>
&lt;p>While it is trivial to get Linux instances running in
OpenStack (there are downloadable images from several distributions that
will magically configure themselves on first boot), getting Windows
systems set up is a little trickier. There are no pre-configured
images to download, and it looks as if there aren&amp;rsquo;t that many people
trying to run Windows under OpenStack right now so there is a lot less
common experience to reference.&lt;/p>
&lt;h2 id="like-the-cool-kids-do-it">Like the cool kids do it&lt;/h2>
&lt;p>My first approach to this situation was to set up our Windows
instances to act just like our Linux instances:&lt;/p>
&lt;ul>
&lt;li>Install &lt;a href="http://cygwin.com/">Cygwin&lt;/a>.&lt;/li>
&lt;li>Run an SSH server.&lt;/li>
&lt;li>Have the system pull down an SSH public key on first boot and use
this for administrative access.&lt;/li>
&lt;/ul>
&lt;p>This worked reasonably well, but many people felt that this wasn&amp;rsquo;t a
great solution because it wouldn&amp;rsquo;t feel natural to a typical Windows
administrator. It also required a full Cygwin install to drive
things, which isn&amp;rsquo;t terrible but still feels like a pretty big hammer.&lt;/p>
&lt;p>As an alternative, we decided we needed some way to either (a) allow
the user to pass a password into the instance environment, or (b)
provide some way for the instance to communicate a generated password
back to the user.&lt;/p>
&lt;h2 id="how-about-user-data">How about user-data?&lt;/h2>
&lt;p>One of my colleagues suggested that we could allow people to pass an
administrative password into the environment via the &lt;code>user-data&lt;/code>
attribute available from the &lt;a href="http://docs.openstack.org/trunk/openstack-compute/admin/content/metadata-service.html">metadata service&lt;/a>. While this sounds
like a reasonable idea at first, it has one major flaw: data from the
metadata service is available to anyone on the system who is able to
retrieve a URL. This would make it trivial for anyone on the instance
to retrieve the administrator password.&lt;/p>
&lt;h2 id="how-about-adminpass">How about adminPass?&lt;/h2>
&lt;p>When you boot an instance using the nova command line tools&amp;hellip;&lt;/p>
&lt;pre>&lt;code>nova boot ...
&lt;/code>&lt;/pre>
&lt;p>You get back a chunk of metadata, including an &lt;code>adminPass&lt;/code> key, which
is a password randomly generated by OpenStack and availble during the
instance provisioning process:&lt;/p>
&lt;pre>&lt;code>+------------------------+--------------------------------------+
| Property | Value |
+------------------------+--------------------------------------+
...
| adminPass | RBiWrSNYqK5R |
...
+------------------------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>This would be an ideal solution, if only I were able to figure out how
OpenStack made this value available to the instance. After asking
around on &lt;a href="http://wiki.openstack.org/UsingIRC">#openstack&lt;/a> it turns
out that not many people were even aware this feature exists, so
information was hard to come by. I ran across some &lt;a href="http://docs.openstack.org/trunk/openstack-compute/admin/content/hypervisor-configuration-basics.html">documentation&lt;/a>
that mentioned the &lt;code>libvirt_inject_password&lt;/code> option in &lt;code>nova.conf&lt;/code>
with the following description:&lt;/p>
&lt;blockquote>
&lt;p>(BoolOpt) Inject the admin password at boot time, without an agent.&lt;/p>
&lt;/blockquote>
&lt;p>&amp;hellip;but that still didn&amp;rsquo;t actually explain how it worked, so I went
diving through the code. The &lt;code>libvirt_inject_password&lt;/code> option appears
in only a single file, &lt;code>nova/virt/libvirt/connection.py&lt;/code>, so I knew
where to start. This led me to the &lt;code>_create_image&lt;/code> method, which
grabs the &lt;code>admin_pass&lt;/code> generated by OpenStack:&lt;/p>
&lt;pre>&lt;code>if FLAGS.libvirt_inject_password:
admin_pass = instance.get('admin_pass')
else:
admin_pass = None
&lt;/code>&lt;/pre>
&lt;p>And then passes it to the &lt;code>inject_data&lt;/code> method:&lt;/p>
&lt;pre>&lt;code>disk.inject_data(injection_path,
key, net, metadata, admin_pass, files,
partition=target_partition,
use_cow=FLAGS.use_cow_images,
config_drive=config_drive)
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>inject_data&lt;/code> method comes from &lt;code>nova/virt/disk/api.py&lt;/code>, which is
where things get interesting: it turns out that the injection
mechanism works by:&lt;/p>
&lt;ul>
&lt;li>Mounting the root filesystem,&lt;/li>
&lt;li>Copying out &lt;code>/etc/passwd&lt;/code> and &lt;code>/etc/shadow&lt;/code>,&lt;/li>
&lt;li>Modifying them, and&lt;/li>
&lt;li>Copying them back.&lt;/li>
&lt;/ul>
&lt;p>Like this:&lt;/p>
&lt;pre>&lt;code>passwd_path = _join_and_check_path_within_fs(fs, 'etc', 'passwd')
shadow_path = _join_and_check_path_within_fs(fs, 'etc', 'shadow')
utils.execute('cp', passwd_path, tmp_passwd, run_as_root=True)
utils.execute('cp', shadow_path, tmp_shadow, run_as_root=True)
_set_passwd(admin_user, admin_passwd, tmp_passwd, tmp_shadow)
utils.execute('cp', tmp_passwd, passwd_path, run_as_root=True)
os.unlink(tmp_passwd)
utils.execute('cp', tmp_shadow, shadow_path, run_as_root=True)
os.unlink(tmp_shadow)
&lt;/code>&lt;/pre>
&lt;p>Do you see a problem here, given that I&amp;rsquo;m working with a Windows
instance? First, it&amp;rsquo;s possible that the host will be unable to mount
the NTFS filesystem, and secondly, there are no &lt;code>passwd&lt;/code> or &lt;code>shadow&lt;/code>
files of any use on the target.&lt;/p>
&lt;p>You can pass &lt;code>--config-drive=True&lt;/code> to &lt;code>nova boot&lt;/code> and it will use a
configuration drive (a whole-disk FAT filesystem) for configuration
data (and make this available as a block device when the system
boots), but this fails, hard: most of the code treats this as being
identical to the original root filesystem, so it still tries to
perform the modifications to &lt;code>/etc/passwd&lt;/code> and &lt;code>/etc/shadow&lt;/code> which, of
course, don&amp;rsquo;t exist.&lt;/p>
&lt;p>I whipped some quick
&lt;a href="https://github.com/seas-computing/nova/commits/lars/admin_pass">patches&lt;/a>
that would write the configuration data (such as &lt;code>admin_pass&lt;/code>) to
simple files at the root of the configuration drive&amp;hellip;but then I ran
into a new problem:&lt;/p>
&lt;p>Windows doesn&amp;rsquo;t know how to deal with whole-disk filesystems (nor,
apparently, do many &lt;a href="http://serverfault.com/questions/444446/mounting-whole-disk-filesystems-in-windows-2008/444448#comment481758_444448">windows
admins&lt;/a>).
In the absence of a partition map, Windows assumes that the device is
empty.&lt;/p>
&lt;p>Oops. At this point it was obvious I was treading on ground best left
undisturbed.&lt;/p></content></item><item><title>Generating random passwords in PowerShell</title><link>https://blog.oddbit.com/post/2012-11-04-powershell-random-passwords/</link><pubDate>Sun, 04 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-04-powershell-random-passwords/</guid><description>I was looking for PowerShell solutions for generating a random password (in order to set the Administrator password on a Windows instance provisioned in OpenStack), and found several solutions using the GeneratePassword method of System.Web.Security.Membership (documentation here), along the lines of this:
Function New-RandomComplexPassword ($length=8) { $Assembly = Add-Type -AssemblyName System.Web $password = [System.Web.Security.Membership]::GeneratePassword($length,2) return $password } While this works, I was unhappy with the generated passwords: they were difficult to type or transcribe because they make heavy use of punctuation.</description><content>&lt;p>I was looking for PowerShell solutions for generating a random password (in
order to set the Administrator password on a Windows instance provisioned in
&lt;a href="http://www.openstack.org/">OpenStack&lt;/a>), and found several solutions using the GeneratePassword method
of &lt;code>System.Web.Security.Membership&lt;/code> (documentation &lt;a href="http://msdn.microsoft.com/en-us/library/system.web.security.membership.generatepassword.aspx">here&lt;/a>),
along the lines of &lt;a href="https://gist.github.com/4011878">this&lt;/a>:&lt;/p>
&lt;pre>&lt;code>Function New-RandomComplexPassword ($length=8)
{
$Assembly = Add-Type -AssemblyName System.Web
$password = [System.Web.Security.Membership]::GeneratePassword($length,2)
return $password
}
&lt;/code>&lt;/pre>
&lt;p>While this works, I was unhappy with the generated passwords: they
were difficult to type or transcribe because they make heavy use of
punctuation. For example:&lt;/p>
&lt;ul>
&lt;li>&lt;code>(O;RK_wx(IcD;&amp;lt;V&lt;/code>&lt;/li>
&lt;li>&lt;code>+3N)lkU5r)nHiL#&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>These looks more like line noise (remember that? No? Get off my
lawn&amp;hellip;) than anything else and feel very unnatural to type.&lt;/p>
&lt;p>I was looking for longer strings consisting primarily of letters and
digits. Thanks to Hey, Scripting Guy I learned about the Get-Random
and ForEach-Object methods (and the % alias for the latter), and ended
up with &lt;a href="https://gist.github.com/4011916">the following&lt;/a>:&lt;/p>
&lt;pre>&lt;code># Generate a random password
# Usage: random-password &amp;lt;length&amp;gt;
Function random-password ($length = 15)
{
$punc = 46..46
$digits = 48..57
$letters = 65..90 + 97..122
# Thanks to
# https://blogs.technet.com/b/heyscriptingguy/archive/2012/01/07/use-pow
$password = get-random -count $length `
-input ($punc + $digits + $letters) |
% -begin { $aa = $null } `
-process {$aa += [char]$_} `
-end {$aa}
return $password
}
&lt;/code>&lt;/pre>
&lt;p>This generates strings of letters and digits (and &amp;ldquo;.&amp;rdquo;) that look something like:&lt;/p>
&lt;ul>
&lt;li>&lt;code>2JQ0bW7VMqcm4UB&lt;/code>&lt;/li>
&lt;li>&lt;code>V4DObnQl0vJX1wC&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;m a lot happier with this.&lt;/p></content></item><item><title>Waiting for networking using PowerShell</title><link>https://blog.oddbit.com/post/2012-11-04-powershell-wait-for-networking/</link><pubDate>Sun, 04 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-04-powershell-wait-for-networking/</guid><description>I&amp;rsquo;ve recently been exploring the world of Windows scripting, and I ran into a small problem: I was running a script at system startup, and the script was running before the network interface (which was using DHCP) was configured.
There are a number of common solutions proposed to this problem:
Just wait for some period of time.
This can work but it&amp;rsquo;s ugly, and because it doesn&amp;rsquo;t actually verify the network state it can result in things breaking if some problem prevents Windows from pulling a valid DHCP lease.</description><content>&lt;p>I&amp;rsquo;ve recently been exploring the world of Windows scripting, and I ran
into a small problem: I was running a script at system startup, and
the script was running before the network interface (which was using
DHCP) was configured.&lt;/p>
&lt;p>There are a number of common solutions proposed to this problem:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Just wait for some period of time.&lt;/p>
&lt;p>This can work but it&amp;rsquo;s ugly, and because it doesn&amp;rsquo;t actually
verify the network state it can result in things breaking if some
problem prevents Windows from pulling a valid DHCP lease.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Use ping to verify the availability of some remote host.&lt;/p>
&lt;p>This works reasonably well if you have a known endpoint you can
test, but it&amp;rsquo;s hard to make a generic solution using this method.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>What I really wanted to do was to have my script wait until a default
gateway appeared on the system (which would indicate that Windows had
successfully acquired a DHCP lease and had configured the interface).&lt;/p>
&lt;p>My first attempts involved traditional batch scripts (&lt;code>.bat&lt;/code>) running
some variant of &lt;code>route print&lt;/code> and searching the output. This can
work, but it&amp;rsquo;s ugly, and I was certain there must be a better way. I
spent some time learning about accessing network configuration
information using PowerShell, and I came up with &lt;a href="https://gist.github.com/4011808">the following
code&lt;/a>:&lt;/p>
&lt;pre>&lt;code># Wait for a DHCP-enabled interface to develop
# a default gateway.
#
# usage: wait-for-network [ &amp;lt;tries&amp;gt; ]
function wait-for-network ($tries) {
while (1) {
# Get a list of DHCP-enabled interfaces that have a
# non-$null DefaultIPGateway property.
$x = gwmi -class Win32_NetworkAdapterConfiguration `
-filter DHCPEnabled=TRUE |
where { $_.DefaultIPGateway -ne $null }
# If there is (at least) one available, exit the loop.
if ( ($x | measure).count -gt 0 ) {
break
}
# If $tries &amp;gt; 0 and we have tried $tries times without
# success, throw an exception.
if ( $tries -gt 0 -and $try++ -ge $tries ) {
throw &amp;quot;Network unavaiable after $try tries.&amp;quot;
}
# Wait one second.
start-sleep -s 1
}
}
&lt;/code>&lt;/pre>
&lt;p>This uses various sort of filtering to get a list of DHCP-enabled
interfaces that have a default gateway (the &lt;code>DefaultIPGateway&lt;/code>
attribute). It will poll the state of things once/second up to &lt;code>$tries&lt;/code>
times, and if nothing is available it will ultimately throw an
exception.&lt;/p></content></item><item><title>Growing a filesystem on a virtual disk</title><link>https://blog.oddbit.com/post/2012-10-24-resizing-virtual-disk/</link><pubDate>Wed, 24 Oct 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-10-24-resizing-virtual-disk/</guid><description>Occasionally we will deploy a virtual instance into our KVM infrastructure and realize after the fact that we need more local disk space available. This is the process we use to expand the disk image. This process assumes the following:
You&amp;rsquo;re using legacy disk partitions. The process for LVM is similar and I will describe that in another post (it&amp;rsquo;s generally identical except for an additional pvresize thrown in and lvextend in place of resize2fs).</description><content>&lt;p>Occasionally we will deploy a virtual instance into our KVM
infrastructure and realize after the fact that we need more local disk
space available. This is the process we use to expand the disk image.
This process assumes the following:&lt;/p>
&lt;ul>
&lt;li>You&amp;rsquo;re using legacy disk partitions. The process for LVM is similar
and I will describe that in another post (it&amp;rsquo;s generally identical
except for an additional &lt;code>pvresize&lt;/code> thrown in and &lt;code>lvextend&lt;/code> in
place of &lt;code>resize2fs&lt;/code>).&lt;/li>
&lt;li>The partition you need to resize is the last partition on the disk.&lt;/li>
&lt;/ul>
&lt;p>This process will work with either a &lt;code>qcow2&lt;/code> or &lt;code>raw&lt;/code> disk image. For
&lt;code>raw&lt;/code> images you can also run &lt;code>fdisk&lt;/code> on the host, potentially saving
yourself a reboot, but that&amp;rsquo;s less convenient for &lt;code>qcow2&lt;/code> format
images.&lt;/p>
&lt;hr>
&lt;p>We start with a 5.5G root filesystem with 4.4G free:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda2 5.5G 864M 4.4G 17% /
&lt;/code>&lt;/pre>
&lt;p>We need to shut down the system to grow the underlying image:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# poweroff
&lt;/code>&lt;/pre>
&lt;p>On the host, we use the &lt;code>qemu-img resize&lt;/code> command to grow the image.
First we need the path to the underlying disk image:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system dumpxml lars-test-0 | grep file
&amp;lt;disk type='file' device='disk'&amp;gt;
&amp;lt;source file='/var/lib/libvirt/images/lars-test-0-1.img'/&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And now we increase the image size by 10G:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ sudo qemu-img resize /var/lib/libvirt/images/lars-test-0.img +10G
Image resized.
&lt;/code>&lt;/pre>
&lt;p>Now reboot the instance:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system start lars-test-0
&lt;/code>&lt;/pre>
&lt;p>And login in on the console:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system console lars-test-0
Connected to domain lars-test-0
Escape character is ^]
Fedora release 17 (Beefy Miracle)
Kernel 3.6.2-4.fc17.x86_64 on an x86_64 (ttyS0)
localhost login: root
Password:
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re going to use &lt;code>fdisk&lt;/code> to modify the partition layout. Run
&lt;code>fdisk&lt;/code> on the system disk:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# fdisk /dev/vda
Welcome to fdisk (util-linux 2.21.2).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.
&lt;/code>&lt;/pre>
&lt;p>Print out the existing partition table and verify that you really are
going to be modifying the final partition:&lt;/p>
&lt;pre>&lt;code>Command (m for help): p
Disk /dev/vda: 19.3 GB, 19327352832 bytes
16 heads, 63 sectors/track, 37449 cylinders, total 37748736 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00007d9f
Device Boot Start End Blocks Id System
/dev/vda1 * 2048 1026047 512000 83 Linux
/dev/vda2 1026048 5154815 2064384 82 Linux swap / Solaris
/dev/vda3 5154816 16777215 5811200 83 Linux
&lt;/code>&lt;/pre>
&lt;p>Delete and recreate the final partition, in this case &lt;code>/dev/vda3&lt;/code>&amp;hellip;&lt;/p>
&lt;pre>&lt;code>Command (m for help): d
Partition number (1-4): 3
Partition 3 is deleted
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and create a new partition, accepting all the defaults. This will
create a new partition starting in the same place and extending to the
end of the disk:&lt;/p>
&lt;pre>&lt;code>Command (m for help): n
Partition type:
p primary (2 primary, 0 extended, 2 free)
e extended
Select (default p): p
Partition number (1-4, default 3): 3
First sector (5154816-37748735, default 5154816):
Using default value 5154816
Last sector, +sectors or +size{K,M,G} (5154816-37748735, default 37748735):
Using default value 37748735
Partition 3 of type Linux and of size 15.6 GiB is set
&lt;/code>&lt;/pre>
&lt;p>You can print out the new partition table to see that indeed
&lt;code>/dev/vda3&lt;/code> is now larger:&lt;/p>
&lt;pre>&lt;code>Command (m for help): p
Disk /dev/vda: 19.3 GB, 19327352832 bytes
16 heads, 63 sectors/track, 37449 cylinders, total 37748736 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00007d9f
Device Boot Start End Blocks Id System
/dev/vda1 * 2048 1026047 512000 83 Linux
/dev/vda2 1026048 5154815 2064384 82 Linux swap / Solaris
/dev/vda3 5154816 37748735 16296960 83 Linux
&lt;/code>&lt;/pre>
&lt;p>Write the changes to disk:&lt;/p>
&lt;pre>&lt;code>Command (m for help): w
The partition table has been altered!
Calling ioctl() to re-read partition table.
WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>Note the warning!&lt;/strong> The kernel has cached a copy of the old
partition table. We need to reboot the system before our changes are
visible! So we reboot the system:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# reboot
&lt;/code>&lt;/pre>
&lt;p>And log back in. Run &lt;code>df&lt;/code> to see the current size of the root
filesystem:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda3 5.5G 864M 4.4G 17% /
&lt;/code>&lt;/pre>
&lt;p>Now run &lt;code>resize2fs&lt;/code> to resize the root filesystem so that it expands
to fill our extended &lt;code>/dev/vda3&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# resize2fs /dev/vda3
resize2fs 1.42.3 (14-May-2012)
Filesystem at /dev/vda3 is mounted on /; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vda3 is now 4074240 blocks long.
&lt;/code>&lt;/pre>
&lt;p>Run &lt;code>df&lt;/code> again to see that we now have additional space available:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda3 16G 867M 14G 6% /
[root@localhost ~]#
&lt;/code>&lt;/pre></content></item><item><title>Parsing XML with Awk</title><link>https://blog.oddbit.com/post/2012-09-10-awk-parsing-xml/</link><pubDate>Mon, 10 Sep 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-09-10-awk-parsing-xml/</guid><description>Recently, changes from the xmlgawk project have been integrated into GNU awk, and xmlgawk has been renamed to gawkextlib. With both a recent (post-4.0.70) gawk and gawkextlib built and installed correctly, you can write simple XML parsing scripts using gawk.
For example, let&amp;rsquo;s say you would like to generate a list of disk image files associated with a KVM virtual instance. You can use the virsh dumpxml command to get output like the following:</description><content>&lt;p>Recently, changes from the &lt;a href="http://gawkextlib.sourceforge.net/">xmlgawk&lt;/a> project have been integrated into
&lt;a href="https://www.gnu.org/software/gawk/">GNU awk&lt;/a>, and xmlgawk has been renamed to &lt;a href="http://gawkextlib.sourceforge.net/">gawkextlib&lt;/a>. With both a
recent (post-4.0.70) gawk and gawkextlib built and installed
correctly, you can write simple XML parsing scripts using gawk.&lt;/p>
&lt;p>For example, let&amp;rsquo;s say you would like to generate a list of disk image
files associated with a KVM virtual instance. You can use the &lt;code>virsh dumpxml&lt;/code> command to get output like the following:&lt;/p>
&lt;pre>&lt;code>&amp;lt;devices&amp;gt;
&amp;lt;emulator&amp;gt;/usr/bin/qemu-kvm&amp;lt;/emulator&amp;gt;
&amp;lt;disk type='file' device='disk'&amp;gt;
&amp;lt;driver name='qemu' type='qcow2'/&amp;gt;
&amp;lt;source file='/var/lib/libvirt/images/client.qcow2'/&amp;gt;
&amp;lt;target dev='sda' bus='ide'/&amp;gt;
&amp;lt;alias name='ide0-0-0'/&amp;gt;
&amp;lt;address type='drive' controller='0' bus='0' unit='0'/&amp;gt;
&amp;lt;/disk&amp;gt;
...
&amp;lt;/devices&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>You could then write code similar to &lt;a href="https://gist.github.com/4012705">the
following&lt;/a> to extract the relevant
information:&lt;/p>
&lt;pre>&lt;code>@load &amp;quot;xml&amp;quot;
XMLSTARTELEM == &amp;quot;disk&amp;quot; {
in_disk=1
disk_file=&amp;quot;&amp;quot;
disk_target=&amp;quot;&amp;quot;
}
in_disk == 1 &amp;amp;&amp;amp; XMLSTARTELEM == &amp;quot;source&amp;quot; {
disk_file=XMLATTR[&amp;quot;file&amp;quot;]
}
in_disk == 1 &amp;amp;&amp;amp; XMLSTARTELEM == &amp;quot;target&amp;quot; {
disk_target=XMLATTR[&amp;quot;dev&amp;quot;]
}
XMLENDELEM == &amp;quot;disk&amp;quot; {
in_disk=0
print disk_target, disk_file
}
&lt;/code>&lt;/pre>
&lt;p>Given the sample input above, the script will produce the following
output:&lt;/p>
&lt;pre>&lt;code>sda /var/lib/libvirt/images/client.qcow2
&lt;/code>&lt;/pre>
&lt;p>The xml extension for gawk populates a number of variables that
can be used in your scripts:&lt;/p>
&lt;ul>
&lt;li>&lt;code>XMLSTARTELEM&lt;/code> marks the start of a new element (and is set to the
name of that element).&lt;/li>
&lt;li>&lt;code>XMLATTR&lt;/code> is available when &lt;code>XMLSTARTELEM&lt;/code> is set and contains the
element attributes.&lt;/li>
&lt;li>&lt;code>XMLENDELEM&lt;/code> marks the end of an element (and is set to the name of
the element).&lt;/li>
&lt;/ul>
&lt;p>There are other variables available, but with this basic set is
becomes easy to extract information from XML documents.&lt;/p></content></item><item><title>Markdown in your Email</title><link>https://blog.oddbit.com/post/2012-08-09-markdown-email/</link><pubDate>Thu, 09 Aug 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-08-09-markdown-email/</guid><description>I really like Markdown, a minimal markup language designed to be readable as plain text that can be rendered into structurally valid HTML. Markdown is already used on sites such as GitHub and all the StackExchange sites.
I use Markdown often enough that it&amp;rsquo;s become ingrained in my fingers, to the point that I&amp;rsquo;ve started unconsciously using Markdown syntax in my email. This isn&amp;rsquo;t particularly useful by itself, although it means that I can take a message and render it to something pretty if I decide it needs to go somewhere other than my sent mail folder.</description><content>&lt;p>I really like &lt;a href="http://daringfireball.net/projects/markdown/">Markdown&lt;/a>, a minimal markup language designed to be readable as plain text that can be rendered into structurally valid HTML. Markdown is already used on sites such as &lt;a href="http://github.com/">GitHub&lt;/a> and all the &lt;a href="http://stackexchange.com/sites">StackExchange&lt;/a> sites.&lt;/p>
&lt;p>I use Markdown often enough that it&amp;rsquo;s become ingrained in my fingers, to the point that I&amp;rsquo;ve started unconsciously using Markdown syntax in my email. This isn&amp;rsquo;t particularly useful by itself, although it means that I can take a message and render it to something pretty if I decide it needs to go somewhere other than my sent mail folder.&lt;/p>
&lt;p>I thought it would be fun to implement something that would actually render Markdown syntax in my outbound email and render it into an HTML attachment. I spent a little time last night putting together a &lt;a href="https://github.com/larsks/mutt-utils/blob/master/markdownmail.py">small Python script&lt;/a> that does exactly that:&lt;/p>
&lt;ul>
&lt;li>It looks for a leading &lt;code>&amp;lt;!-- markdown --&amp;gt;&lt;/code> in a message body.&lt;/li>
&lt;li>It renders the markdown to HTML.&lt;/li>
&lt;li>It transforms the message into a &lt;code>multipart/mixed&lt;/code> message.&lt;/li>
&lt;li>It attaches the rendered content as &lt;code>text/html&lt;/code>.&lt;/li>
&lt;li>It attaches the original message as &lt;code>text/plain&lt;/code>.&lt;/li>
&lt;li>It attaches any signature that was found in the original message as &lt;code>text/plain&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>I have Mutt configured to pass outbound email through this filter by setting the &lt;code>sendmail&lt;/code> variable&amp;hellip;&lt;/p>
&lt;pre>&lt;code>set sendmail=&amp;quot;~/.mutt/bin/sendmail&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;to point to a small shell script that passes everything off to &lt;a href="http://www.procmail.org/">procmail&lt;/a>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
exec procmail -m msmtp_args=&amp;quot;$*&amp;quot; $HOME/.procmail/rc.sent
&lt;/code>&lt;/pre>
&lt;p>And then &lt;code>procmail&lt;/code> filters outbound messages before sending them via &lt;code>msmtp&lt;/code>:&lt;/p>
&lt;pre>&lt;code># Render markdown email to HTML
:0f
| $HOME/.mutt/bin/markdownmail
# Send via msmtp
:0w
| msmtp $msmtp_args
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s not especially robust but it seems to work so far.&lt;/p></content></item><item><title>Chasing OpenStack idle connection timeouts</title><link>https://blog.oddbit.com/post/2012-07-30-openstack-idle-connection-time/</link><pubDate>Mon, 30 Jul 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-07-30-openstack-idle-connection-time/</guid><description>The original problem I&amp;rsquo;ve recently spent some time working on an OpenStack deployment. I ran into a problem in which the compute service would frequently stop communicating with the AMQP message broker (qpidd).
In order to gather some data on the problem, I ran the following simple test:
Wait n minutes Run nova boot ... to create an instance Wait a minute and see if the new instance becomes ACTIVE If it works, delete the instance, set n = 2n and repeat This demonstrated that communication was failing after about an hour, which correlates rather nicely with the idle connection timeout on the firewall.</description><content>&lt;h2 id="the-original-problem">The original problem&lt;/h2>
&lt;p>I&amp;rsquo;ve recently spent some time working on an OpenStack deployment. I ran into a
problem in which the &lt;a href="http://docs.openstack.org/trunk/openstack-compute/starter/content/Compute_Worker_nova-compute_-d1e232.html">compute service&lt;/a> would frequently stop communicating
with the &lt;a href="http://www.amqp.org/">AMQP&lt;/a> message broker (&lt;code>qpidd&lt;/code>).&lt;/p>
&lt;p>In order to gather some data on the problem, I ran the following simple test:&lt;/p>
&lt;ul>
&lt;li>Wait &lt;code>n&lt;/code> minutes&lt;/li>
&lt;li>Run &lt;code>nova boot ...&lt;/code> to create an instance&lt;/li>
&lt;li>Wait a minute and see if the new instance becomes &lt;code>ACTIVE&lt;/code>&lt;/li>
&lt;li>If it works, delete the instance, set &lt;code>n&lt;/code> = &lt;code>2n&lt;/code> and repeat&lt;/li>
&lt;/ul>
&lt;p>This demonstrated that communication was failing after about an hour, which
correlates rather nicely with the idle connection timeout on the firewall.&lt;/p>
&lt;p>I wanted to continue working with our OpenStack environment while testing
different solutions to this problem, so I put an additional interface on the
controller (the system running the AMQ message broker, &lt;code>qpidd&lt;/code>, as well as
&lt;code>nova-api&lt;/code>, &lt;code>nova-scheduler&lt;/code>, etc) that was on the same network as our
&lt;code>nova-compute&lt;/code> hosts. This would allow the compute service to communicate with
the message broker without traversing the firewall infrastructure.&lt;/p>
&lt;p>As a workaround it worked fine, but it introduced a &lt;em>new&lt;/em> problem that sent us
down a bit of a rabbit hole.&lt;/p>
&lt;h2 id="the-new-problem">The new problem&lt;/h2>
&lt;p>With the compute hosts talking happily to the controller, I started looking at
the connection timeout settings in the firewall. As a first step I cranked the
default connection timeout up to two hours and repeated our earlier test&amp;hellip;only
to find that connections were now failing in a matter of minutes!&lt;/p>
&lt;p>So, what happened?&lt;/p>
&lt;p>By adding an interface on a shared network, I created an asymmetric route
between the two hosts &amp;ndash; that is, the network path taking by packets from the
compute host to the controller was different from the network path taken by
packets in the other direction.&lt;/p>
&lt;p>In the most common configuration, Linux (and other operating systems) only have
a single routing decision to make:&lt;/p>
&lt;ul>
&lt;li>Am I communicating with a host on a directly attached network?&lt;/li>
&lt;/ul>
&lt;p>If the answer is &amp;ldquo;yes&amp;rdquo;, a packet will be routed directly to the destination
host, otherwise it will be routed via the default gateway (and transit the
campus routing/firewall infrastructure).&lt;/p>
&lt;p>On the compute host, with its single interface, the decision is simple. Since
the canonical address of the controller is not on the same network, packets
will be routed via the default gateway. On the controller, the situation is
different. While the packet came in on the canonical interface, the kernel will
realize that the request comes from a host on a network to which there is a
more specific route than the default gateway: the new network interface on the
same network as the compute host. This means that reply packets will be routed
directly.&lt;/p>
&lt;p>Asymmetric routing is not, by itself, a problem. However, throw in a stateful
firewall and you now have a recipe for dropped connections. The firewall
appliances in use at my office maintain a table of established TCP connections.
This is used to reduce the processing necessary for packets associated with
established connections. From the &lt;a href="http://www.cisco.com/en/US/docs/security/asdm/6_2/user/guide/protect.html#wp1291963">Cisco documentation&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>By default, all traffic that goes through the security appliance is inspected
using the Adaptive Security Algorithm and is either allowed through or
dropped based on the security policy. The security appliance maximizes the
firewall performance by checking the state of each packet (is this a new
connection or an established connection?) and assigning it to either the
session management path (a new connection SYN packet), the fast path (an
established connection), or the control plane path (advanced inspection).&lt;/p>
&lt;/blockquote>
&lt;p>In order for two systems to successfully established a TCP connection, they
must complete a &lt;a href="http://en.wikipedia.org/wiki/Transmission_Control_Protocol#Connection_establishment">three-way handshake&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>The initiating systems sends a &lt;code>SYN&lt;/code> packet.&lt;/li>
&lt;li>The receiving system sends &lt;code>SYN-ACK&lt;/code> packet.&lt;/li>
&lt;li>The initiating system sends an &lt;code>ACK&lt;/code> packet.&lt;/li>
&lt;/ul>
&lt;p>The routing structure introduced by our interface change meant that while the
initial &lt;code>SYN&lt;/code> packet was traversing the firewall, the subsequent &lt;code>SYN-ACK&lt;/code>
packet was being routed directly, which means that from the point of view of
the firewall the connection was never successfully established&amp;hellip;and after 20
seconds (the default &amp;ldquo;embryonic connection timeout&amp;rdquo;) the connection gets
dropped by the firewall. The diagram below illustrates exactly what was
happening:&lt;/p>
&lt;p>&lt;img src="asymmetric-routing.png" alt="assymetric routing">&lt;/p>
&lt;p>There are various ways of correcting this situation:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You could use the advanced &lt;a href="http://www.policyrouting.org/PolicyRoutingBook/ONLINE/TOC.html">policy routing&lt;/a> features available in Linux
to set up a routing policy that would route replies out the same interface
on which a packet was received, thus returning to a more typical symmetric
routing model.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You could use the &lt;a href="http://www.cisco.com/en/US/docs/security/asdm/6_2/user/guide/protect.html#wp1291963">tcp state bypass&lt;/a> feature available in the Cisco
firewall appliance to exempt AMQ packets from the normal TCP state
processing.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;m not going to look at either of these solution in detail, since this whole
issue was secondary to the initial idle connection timeout problem, which has a
different set of solutions.&lt;/p>
&lt;h2 id="dealing-with-connection-timeouts">Dealing with connection timeouts&lt;/h2>
&lt;p>Returning to the original problem, what am I to do about the idle connection
timeouts?&lt;/p>
&lt;h3 id="disable-idle-connection-timeouts-on-the-firewall">Disable idle connection timeouts on the firewall&lt;/h3>
&lt;p>Once can disable idle connection timeouts on the firewall, either globally &amp;ndash;
which would be a bad idea &amp;ndash; or for certain traffic classes. For example, &amp;ldquo;all
traffic to or from TCP port 5672&amp;rdquo;. This can be done by adding a rule to the
default global policy:&lt;/p>
&lt;pre>&lt;code>class-map amq
description Disable connection timeouts for AMQ connections (for OpenStack)
match port tcp eq 5672
policy-map global_policy
class amq
set connection random-sequence-number disable
set connection timeout embryonic 0:00:20 half-closed 0:00:20 tcp 0:00:00
&lt;/code>&lt;/pre>
&lt;p>While this works fine, it makes successful deployment of OpenStack dependent on
a specific firewall configuration.&lt;/p>
&lt;h3 id="enable-linux-kernel-keepalive-support-for-tcp-connections">Enable Linux kernel keepalive support for TCP connections&lt;/h3>
&lt;p>The Linux kernel supports a &lt;a href="http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/">keepalive&lt;/a> feature intended to deal with this
exact situation. After a connection has been idle for a certain amount of time
(&lt;code>net.ipv4.tcp_keepalive_time&lt;/code>), the kernel will send zero-length packets every
&lt;code>net.ipv4.tcp_keepalive_intvl&lt;/code> seconds in order to keep the connection active.
The kernel defaults to an initial interval of 7200 seconds (aka two hours),
which is longer than the default idle connection timeout on our Cisco
firewalls, but this value is easily tuneable via the
&lt;code>net.ipv4.tcp_keepalive_time&lt;/code> sysctl value.&lt;/p>
&lt;p>This sounds like a great solution, until you pay close attention to the &lt;code>tcp&lt;/code>
man page (or the &lt;code>HOWTO&lt;/code> document):&lt;/p>
&lt;blockquote>
&lt;p>Keep-alives are only sent when the &lt;code>SO_KEEPALIVE&lt;/code> socket option is enabled.&lt;/p>
&lt;/blockquote>
&lt;p>If your application doesn&amp;rsquo;t already set &lt;code>SO_KEEPALIVE&lt;/code> (or give you an option
for doing do), you&amp;rsquo;re mostly out of luck. While it would certainly be possible
to modify either the OpenStack source or the QPID source to set the appropriate
option on AMQ sockets, I don&amp;rsquo;t really want to put myself in the position of
having to maintain this sort of one-off patch.&lt;/p>
&lt;p>But all is not lost! It is possible to override functions in dynamic
executables using a mechanism called &lt;a href="http://www.jayconrod.com/cgi/view_post.py?23">function interposition&lt;/a>. Create a
library that implements the function you want to override, and then preload it
when running an application via the &lt;code>LD_PRELOAD&lt;/code> environment variable (or
&lt;code>/etc/ld.so.preload&lt;/code>, if you want it to affect everything).&lt;/p>
&lt;p>It can be tricky to correctly implement function interposition, so I&amp;rsquo;m
fortunate that the &lt;a href="http://libkeepalive.sourceforge.net">libkeepalive&lt;/a> project has already taken care of this. By
installing &lt;code>libkeepalive&lt;/code> and adding &lt;code>libkeepalive.so&lt;/code> to &lt;code>/etc/ld.so.preload&lt;/code>,
it is possible to have the &lt;code>SO_KEEPALIVE&lt;/code> option set by default on all sockets.
&lt;code>libkeepalive&lt;/code> implements a wrapper to the &lt;code>socket&lt;/code> system call that calls
&lt;code>setsockopt&lt;/code> with the &lt;code>SO_KEEPALIVE&lt;/code> option for all TCP sockets.&lt;/p>
&lt;p>Here&amp;rsquo;s what setting up a listening socket with [netcat][] looks like before
installing &lt;code>libkeepalive&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ strace -e trace=setsockopt nc -l 8080
setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
&lt;/code>&lt;/pre>
&lt;p>And here&amp;rsquo;s what things look like after adding &lt;code>libkeepalive.so&lt;/code> to
&lt;code>/etc/ld.so.preload&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ strace -e trace=setsockopt nc -l 8080
setsockopt(3, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0
setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
&lt;/code>&lt;/pre>
&lt;h3 id="enable-application-level-keepalive">Enable application level keepalive&lt;/h3>
&lt;p>Many applications implement their own keepalive mechanism. For example,
&lt;a href="http://openssh.org/">OpenSSH&lt;/a> provides the &lt;a href="http://dan.hersam.com/2007/03/05/how-to-avoid-ssh-timeouts/">ClientAliveInterval&lt;/a> configuration setting to
control how often keepalive packets are sent by the server to a connected
client. When this option is available it&amp;rsquo;s probably the best choice, since it
has been designed with the particular application in mind.&lt;/p>
&lt;p>OpenStack in theory provides the &lt;a href="http://docs.openstack.org/essex/openstack-compute/admin/content/configuration-qpid.html">qpid_heartbeat&lt;/a> setting, which is meant
to provide a heartbeat for AMQ connections to the &lt;code>qpidd&lt;/code> process. According to
the documentation, the default behavior of QPID clients in the OpenStack
framework is to send heartbeat packets every five seconds.&lt;/p>
&lt;p>When first testing this option it was obvious that things weren&amp;rsquo;t working as
documented. Querying the connection table on the firewall would often should
connections with more than five seconds of idle time:&lt;/p>
&lt;pre>&lt;code>% show conn lport 5672
[...]
TCP ...:630 10.243.16.151:39881 ...:621 openstack-dev-2:5672 idle 0:34:02 Bytes 5218662 FLAGS - UBOI
[...]
&lt;/code>&lt;/pre>
&lt;p>And of course if the &lt;code>qpid_heartbeat&lt;/code> option had been working correctly I would
not have experienced the idle connection timeout issue in the first place.&lt;/p>
&lt;p>A &lt;a href="https://lists.launchpad.net/openstack/msg15191.html">post to the OpenStack mailing list&lt;/a> led to the source of the problem: a
typo in the &lt;code>impl_qpid&lt;/code> Python module:&lt;/p>
&lt;pre>&lt;code>diff --git a/nova/rpc/impl_qpid.py b/nova/rpc/impl_qpid.py
index 289f21b..e19079e 100644
--- a/nova/rpc/impl_qpid.py
+++ b/nova/rpc/impl_qpid.py
@@ -317,7 +317,7 @@ class Connection(object):
FLAGS.qpid_reconnect_interval_min)
if FLAGS.qpid_reconnect_interval:
self.connection.reconnect_interval = FLAGS.qpid_reconnect_interval
- self.connection.hearbeat = FLAGS.qpid_heartbeat
+ self.connection.heartbeat = FLAGS.qpid_heartbeat
self.connection.protocol = FLAGS.qpid_protocol
self.connection.tcp_nodelay = FLAGS.qpid_tcp_nodelay
&lt;/code>&lt;/pre>
&lt;p>If it&amp;rsquo;s not obvious, &lt;code>heartbeat&lt;/code> was mispelled &lt;code>hearbeat&lt;/code> in the above block of
code. Putting this change into production has completely resolved the idle
connection timeout problem.&lt;/p></content></item><item><title>Git fetch, tags, remotes, and more</title><link>https://blog.oddbit.com/post/2012-07-27-git-fetch-tags-et-al/</link><pubDate>Fri, 27 Jul 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-07-27-git-fetch-tags-et-al/</guid><description>I’ve been playing around with Git, Puppet, and GPG verification of our Puppet configuration repository, and these are some random facts about Git that have come to light as part of the process.
If you want to pull both changes and new tags from a remote repository, you can do this:
$ git fetch $ git fetch --tags Or you can do this:
$ git fetch --tags $ git fetch What’s the difference?</description><content>&lt;p>I’ve been playing around with Git, Puppet, and GPG verification of our
Puppet configuration repository, and these are some random facts about
Git that have come to light as part of the process.&lt;/p>
&lt;p>If you want to pull both changes &lt;em>and&lt;/em> new tags from a remote
repository, you can do this:&lt;/p>
&lt;pre>&lt;code>$ git fetch
$ git fetch --tags
&lt;/code>&lt;/pre>
&lt;p>Or you can do this:&lt;/p>
&lt;pre>&lt;code>$ git fetch --tags
$ git fetch
&lt;/code>&lt;/pre>
&lt;p>What’s the difference? &lt;code>git fetch&lt;/code> will leave &lt;code>FETCH_HEAD&lt;/code> pointing at
the remote &lt;code>HEAD&lt;/code>, whereas &lt;code>git fetch --tags&lt;/code> will leave &lt;code>FETCH_HEAD&lt;/code>
pointing at the most recent tag.&lt;/p>
&lt;p>You can also do:&lt;/p>
&lt;pre>&lt;code>$ git remote update
&lt;/code>&lt;/pre>
&lt;p>Which unlike &lt;code>git fetch&lt;/code> will pull down any new tags…but unlike
&lt;code>git fetch --tags&lt;/code> will not update tags that already exist in the local
repository (&lt;code>git remote update&lt;/code> also sets &lt;code>FETCH_HEAD&lt;/code> to the remote
&lt;code>HEAD&lt;/code>).&lt;/p></content></item><item><title>Capturing Envoy Data</title><link>https://blog.oddbit.com/post/2012-02-22-capturing-envoy-data/</link><pubDate>Wed, 22 Feb 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-02-22-capturing-envoy-data/</guid><description>Pursuant to my last post, I&amp;rsquo;ve written a simple man-in-the-middle proxy to intercept communication between the Envoy and the Enphase servers. The code is available here.
What it does As I detailed in my previous post, the Envoy sends data to Enphase via http POST requests. The proxy intercepts these requests, extracts the XML data from the request, and writes it to a local file (by default in /var/spool/envoy). It then forwards the request on to Enphase, and returns the reply to your Envoy.</description><content>&lt;p>Pursuant to my &lt;a href="http://blog.oddbit.com/2012/02/13/enphase-envoy-xml-data-format/">last post&lt;/a>, I&amp;rsquo;ve written a simple man-in-the-middle proxy to intercept communication between the Envoy and the Enphase servers. The code is available &lt;a href="https://github.com/larsks/envoy-tools">here&lt;/a>.&lt;/p>
&lt;h2 id="what-it-does">What it does&lt;/h2>
&lt;p>As I detailed in my previous post, the Envoy sends data to Enphase via http &lt;code>POST&lt;/code> requests. The proxy intercepts these requests, extracts the XML data from the request, and writes it to a local file (by default in &lt;code>/var/spool/envoy&lt;/code>). It then forwards the request on to Enphase, and returns the reply to your Envoy.&lt;/p>
&lt;p>In addition to extracting the XML data, the proxy also logs the complete contents (headers and message content) of the request and the reply to files.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>Out of the box, your Envoy configures itself automatically using DHCP. Possibly, you&amp;rsquo;ve configured it statically. In either case, it will typically be configured to connect via your default gateway &amp;ndash; generally, your home router, cable modem, etc. In order to intercept the communication between the Envoy and Enphase, we insert another server between the Envoy and your network gateway. In the foollowing diagram, the dotted line represents the original communication path, while the solid lines represent the new communication path:&lt;/p>
&lt;p>&lt;img src="envoy-capture.png" alt="envoy communication">&lt;/p>
&lt;p>The intermediate system &amp;ndash; which we&amp;rsquo;ll call the interceptor &amp;ndash; will use a few tricks to redirect traffic destined for Enphase to the local proxy (which will log the data locally and then forward it on to Enphase).&lt;/p>
&lt;h2 id="assumptions">Assumptions&lt;/h2>
&lt;p>For the purposes of this article, we&amp;rsquo;ll assume that your Envoy is at address 192.168.1.100, and the address of the interceptor is 192.168.1.200.&lt;/p>
&lt;p>I&amp;rsquo;m assuming that your interceptor is running Linux. It may be possible to accomplish the same thing with other tools, but I&amp;rsquo;m relying on the Linux netfilter subsystem (aka &amp;ldquo;iptables&amp;rdquo;) to perform certain key tasks.&lt;/p>
&lt;h2 id="configuring-the-envoy">Configuring the Envoy&lt;/h2>
&lt;p>You will need to congfigure the Envoy to use your interceptor host as its default gateway.&lt;/p>
&lt;ol>
&lt;li>Go to the &lt;a href="http://192.168.1.100/admin/lib/network_display?locale=en">network connectivity&lt;/a> page on your Envoy.&lt;/li>
&lt;li>If it&amp;rsquo;s checked, uncheck the &amp;ldquo;Use DHCP&amp;rdquo; setting and select the &amp;ldquo;Updating DHCP setting&amp;rdquo; button.&lt;/li>
&lt;li>Set the &amp;ldquo;Gateway IP&amp;rdquo; field to the address of your interceptor (192.168.1.200 in this example).&lt;/li>
&lt;li>Select the &amp;ldquo;Update Interface 0&amp;rdquo; button.&lt;/li>
&lt;/ol>
&lt;h2 id="configuring-the-interceptor">Configuring the interceptor&lt;/h2>
&lt;h3 id="redirecting-requests">Redirecting requests&lt;/h3>
&lt;p>We need to configure the interceptor to redirect requests to the Enphase servers to a local application. We&amp;rsquo;ll add the following firewall rule:&lt;/p>
&lt;pre>&lt;code>iptables -t nat -A PREROUTING -s 192.168.1.100 -p tcp \
--dport 443 -j REDIRECT --to-ports 4430
&lt;/code>&lt;/pre>
&lt;p>This rule matches https (port 443) requests from your Envoy (192.168.1.100) and redirects them to port 4430 on the interceptor.&lt;br>
Note that this rule will be lost if you reboot your system. Making firewall rules persistent is beyond the scope of this article; consult the documentation for your distribution of choice.&lt;/p>
&lt;h3 id="handling-ssl">Handling SSL&lt;/h3>
&lt;p>My simple Python proxy doesn&amp;rsquo;t speak SSL, so we need to create a plain http request from the https request. Normally this would be difficult, but Enphase has made our life easier by not checking the validity of the SSL certificate. We&amp;rsquo;re going to use &lt;a href="http://www.stunnel.org/">stunnel&lt;/a> as an https-to-http proxy. Create a file called /etc/stunnel/envoy-ssl.conf with the following contents:&lt;/p>
&lt;pre>&lt;code>[https_in]
accept = 4430
cert = /etc/pki/tls/certs/localhost.crt
connect = 127.0.0.1:8080
&lt;/code>&lt;/pre>
&lt;p>Run stunnel with this configuration:&lt;/p>
&lt;pre>&lt;code>stunnel /etc/stunnel/envoy-ssl.conf
&lt;/code>&lt;/pre>
&lt;p>This assumes you have an SSL certificate in /etc/pki/tls/certs/localhost.crt. You will probably need to generate one, which again is left as an exercise to the reader.&lt;/p>
&lt;h3 id="installing-bottle">Installing bottle&lt;/h3>
&lt;p>The proxy relies on the &lt;a href="http://bottlepy.org/">bottle&lt;/a> Python web framework, which is probably not installed on your system. The easiest way to get things going is to install a Python &amp;ldquo;virtual environment&amp;rdquo; with the appropriate modules. Create a new virtual environment:&lt;/p>
&lt;pre>&lt;code>virtualenv ~/env/envoy
&lt;/code>&lt;/pre>
&lt;p>And install bottle:&lt;/p>
&lt;pre>&lt;code>~/env/envoy/bin/pip install bottle
&lt;/code>&lt;/pre>
&lt;h3 id="creating-directories">Creating directories&lt;/h3>
&lt;p>By default the proxy will write data to /var/spool/envoy. You&amp;rsquo;ll need to make sure this directory exists and is writable by whatever account you&amp;rsquo;re using to run the proxy.&lt;/p>
&lt;h2 id="running-the-proxy">Running the proxy&lt;/h2>
&lt;p>Now that you&amp;rsquo;ve got all the prerequisites in place, you should be able to start the proxy by running:&lt;/p>
&lt;pre>&lt;code>~/env/envoy/bin/python proxy.py
&lt;/code>&lt;/pre>
&lt;p>You should see something like this:&lt;/p>
&lt;pre>&lt;code>Bottle server starting up (using WSGIRefServer())...
Listening on http://127.0.0.1:8080/
Hit Ctrl-C to quit.
&lt;/code>&lt;/pre>
&lt;p>Assuming that everything else went as planned, sometime within the next five minutes you should see the proxy service a request from your Envoy:&lt;/p>
&lt;pre>&lt;code>localhost.localdomain - - [22/Feb/2012 09:03:57] &amp;quot;POST /emu_reports/
performance_report?webcomm_version=3.0 HTTP/1.1&amp;quot; 200 103
&lt;/code>&lt;/pre>
&lt;p>From this request you will end up with three files in /var/spool/envoy:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>2012-02-22T09:03:01-0j1FFs.xml&lt;/code>&lt;br>
This is the XML data from the Envoy and is probably the most interesting file.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>2012-02-22T09:03:01-ZMxw6b.request&lt;/code>&lt;br>
This is the raw request from the Envoy.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>2012-02-22T09:03:02-NB4DbR.response&lt;/code>&lt;br>
This is the response from the Enphase servers.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If you find some bugs, please let me know by creating a new issue &lt;a href="https://github.com/larsks/envoy-tools/issues">here&lt;/a>. Note that this is only for bugs in the code; if you need basic networking tutorials and so forth the Google has lots of help for you.&lt;/p></content></item><item><title>Enphase Envoy XML Data Format</title><link>https://blog.oddbit.com/post/2012-02-13-enphase-envoy-xml-data-format/</link><pubDate>Mon, 13 Feb 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-02-13-enphase-envoy-xml-data-format/</guid><description>We recently installed a (photovoltaic) solar array on our house. The system uses Enphase microinverters, and includes a monitoring device called the &amp;ldquo;Envoy&amp;rdquo;. The Envoy collects data from the microinverters and sends it back to Enphase. Enphase performs monitoring services for the array and also provides access to the data collected by the Envoy product.
I&amp;rsquo;m interested in getting direct access to the data provided by the Envoy. In pursuit of that goal, I set up a man-in-the-middle proxy server on my home network to intercept the communication from the Envoy to the Enphase servers.</description><content>&lt;p>We recently installed a (photovoltaic) solar array on our house. The system uses &lt;a href="http://enphase.com/">Enphase&lt;/a> microinverters, and includes a monitoring device called the &amp;ldquo;&lt;a href="http://enphase.com/products/envoy/">Envoy&lt;/a>&amp;rdquo;. The Envoy collects data from the microinverters and sends it back to Enphase. Enphase performs monitoring services for the array and also provides access to the data collected by the Envoy product.&lt;/p>
&lt;p>I&amp;rsquo;m interested in getting direct access to the data provided by the Envoy. In pursuit of that goal, I set up a man-in-the-middle proxy server on my home network to intercept the communication from the Envoy to the Enphase servers. I&amp;rsquo;m documenting the results of my exploration here in case somebody else finds the information useful.&lt;/p>
&lt;p>The Envoy sends deflate-encoded XML data to the Enphase servers. You can decompress the data using Python&amp;rsquo;s &lt;code>zlib&lt;/code> module like this:&lt;/p>
&lt;pre>&lt;code>import zlib
xml_data = zlib.decompress(deflate_compressed_data)
&lt;/code>&lt;/pre>
&lt;p>The request is an http &lt;code>POST&lt;/code> to &lt;a href="https://reports.enphaseenergy.com/emu_reports/performance_report?webcomm_version=3.0">https://reports.enphaseenergy.com/emu_reports/performance_report?webcomm_version=3.0&lt;/a>. The request headers look like this:&lt;/p>
&lt;pre>&lt;code>POST /emu_reports/performance_report?webcomm_version=3.0 HTTP/1.1
Accept: */*
Connection: close
Content-Type: application/x-deflate
Content-Length: 729
Host: reports.enphaseenergy.com:443
&lt;/code>&lt;/pre>
&lt;p>The request body &amp;ndash; after inflating it &amp;ndash; looks something like this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version='1.0' encoding='utf-8'?&amp;gt;
&amp;lt;perf_report report_timestamp='1329134202'&amp;gt;
&amp;lt;envoy ip_addr='192.168.1.166' mac_addr='00:11:22:33:44:55'
timezone='US/Eastern' part_num='800-00024-r04'
sw_version='R3.0.0 (9720d7)' serial_num='123456115374' /&amp;gt;
&amp;lt;event correlation_id='161' event_state='1' id='221'
event_code='10009' eqid='123456102635' serial_num='123456102635'
event_date='1329134118' /&amp;gt;
&amp;lt;event correlation_id='194' event_state='1' id='222'
event_code='101' eqid='123456103685.1' serial_num='123456103685'
event_date='1329134119' /&amp;gt;
&amp;lt;event correlation_id='196' event_state='1' id='223'
event_code='101' eqid='123456105331.1' serial_num='123456105331'
event_date='1329134120' /&amp;gt;
&amp;lt;event correlation_id='202' event_state='1' id='224'
event_code='101' eqid='123456103294.1' serial_num='123456103294'
event_date='1329134130' /&amp;gt;
&amp;lt;event correlation_id='206' event_state='1' id='225'
event_code='101' eqid='123456105321.1' serial_num='123456105321'
event_date='1329134139' /&amp;gt;
&amp;lt;event event_state='2' id='226' event_code='101'
eqid='123456104335.1' serial_num='123456104335'
event_date='1329134152' /&amp;gt;
&amp;lt;event event_state='0' id='227' event_code='509'
eqid='123456104335' serial_num='123456104335'
event_date='1329134152' /&amp;gt;
&amp;lt;event correlation_id='213' event_state='1' id='228'
event_code='101' eqid='123456105296.1' serial_num='123456105296'
event_date='1329134153' /&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456105041' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456105041.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456104335' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456104335.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456104246' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456104246.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456104224' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456104224.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456102776' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456102776.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456103271' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456103271.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456105190' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456105190.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2097152' part_num='800-00103-r05'
observed_flags='0' eqid='123456105321' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='0'
observed_flags='0' eqid='123456105321.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456105178' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456105178.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456104988' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456104988.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2097152' part_num='800-00103-r05'
observed_flags='0' eqid='123456103294' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='0'
observed_flags='0' eqid='123456103294.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456105346' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456105346.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456105297' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456105297.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456104896' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456104896.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2097152' part_num='800-00103-r05'
observed_flags='0' eqid='123456105331' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='0'
observed_flags='0' eqid='123456105331.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456103546' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456103546.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2097152' part_num='800-00103-r05'
observed_flags='0' eqid='123456103685' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='0'
observed_flags='0' eqid='123456103685.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2359296' part_num='800-00103-r05'
observed_flags='0' eqid='123456103215' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='16'
observed_flags='0' eqid='123456103215.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2097152' part_num='800-00103-r05'
observed_flags='0' eqid='123456102635' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='0'
observed_flags='0' eqid='123456102635.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;device image_bits='131072' device_type='1' admin_state='1'
condition_flags='2097152' part_num='800-00103-r05'
observed_flags='0' eqid='123456105296' control_bits='0'&amp;gt;
&amp;lt;channel channel_type='1' condition_flags='0'
observed_flags='0' eqid='123456105296.1' control_bits='0' /&amp;gt;
&amp;lt;/device&amp;gt;
&amp;lt;/perf_report&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The response headers look like this:&lt;/p>
&lt;pre>&lt;code>HTTP/1.1 200 OK
Date: Mon, 13 Feb 2012 11:56:48 GMT
Server: Apache
X-Powered-By: Phusion Passenger (mod_rails/mod_rack) 3.0.11
ETag: &amp;quot;95bc2eaddfac613540f469946f28af4b&amp;quot;
X-Runtime: 297
Cache-Control: private, max-age=0, must-revalidate
Content-Length: 142
Status: 200
Cache-Control: max-age=31536000
Expires: Tue, 12 Feb 2013 11:56:48 GMT
Connection: close
Content-Type: application/x-deflate; charset=utf-8
&lt;/code>&lt;/pre>
&lt;p>And the response body &amp;ndash; after inflating it &amp;ndash; looks something like this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;perf_report_response status=&amp;quot;success&amp;quot;&amp;gt;
&amp;lt;events_processed&amp;gt;
&amp;lt;event id=&amp;quot;221&amp;quot;/&amp;gt;
&amp;lt;event id=&amp;quot;222&amp;quot;/&amp;gt;
&amp;lt;event id=&amp;quot;223&amp;quot;/&amp;gt;
&amp;lt;event id=&amp;quot;224&amp;quot;/&amp;gt;
&amp;lt;event id=&amp;quot;225&amp;quot;/&amp;gt;
&amp;lt;event id=&amp;quot;226&amp;quot;/&amp;gt;
&amp;lt;event id=&amp;quot;227&amp;quot;/&amp;gt;
&amp;lt;event id=&amp;quot;228&amp;quot;/&amp;gt;
&amp;lt;/events_processed&amp;gt;
&amp;lt;/perf_report_response&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>My goal is to set up a simple proxy that in addition to passing the information along to Enphase will also collect it locally. I&amp;rsquo;ll update the blog if I make progress on that front.&lt;/p></content></item><item><title>Rate limiting made simple</title><link>https://blog.oddbit.com/post/2011-12-26-simple-rate-limiting/</link><pubDate>Mon, 26 Dec 2011 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2011-12-26-simple-rate-limiting/</guid><description>I use CrashPlan as a backup service. It works and is very simple to set up, but has limited options for controlling bandwidth. In fact, if you&amp;rsquo;re running it on a headless system (e.g., a fileserver of some sort), your options are effectively &amp;ldquo;too slow&amp;rdquo; and &amp;ldquo;CONSUME EVERYTHING&amp;rdquo;. There is an open request to add time-based limitations to the application itself, but for now I&amp;rsquo;ve solved this using a very simple traffic shaping configuration.</description><content>&lt;p>I use &lt;a href="http://www.crashplan.com/">CrashPlan&lt;/a> as a backup service. It works and is very simple to set
up, but has limited options for controlling bandwidth. In fact, if you&amp;rsquo;re
running it on a headless system (e.g., a fileserver of some sort), your options
are effectively &amp;ldquo;too slow&amp;rdquo; and &amp;ldquo;CONSUME EVERYTHING&amp;rdquo;. There is an &lt;a href="https://crashplan.zendesk.com/entries/446273-throttle-bandwidth-by-hours?page=1#post_20799486">open
request&lt;/a> to add time-based limitations to the application itself, but for
now I&amp;rsquo;ve solved this using a very simple traffic shaping configuration.
Because the learning curve for &amp;ldquo;tc&amp;rdquo; and friends is surprisingly high, I&amp;rsquo;m
putting &lt;a href="https://gist.github.com/larsks/4014881">my script&lt;/a> here in the hopes
that other people might find it useful, and so that I can find it when I need
to do this again someday.&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
# The network device used for backups
dev=p10p1
# The remove address of the CrashPlanserver
crashplan_addr=50.93.246.1
# The port
crashplan_port=443
# The rate limit. See tc(8) for acceptable syntax.
crashplan_limit=2mbit
if [ &amp;quot;$1&amp;quot; = &amp;quot;enable&amp;quot; ]; then
#
# This creates and activates the traffic shaper
# configuration.
#
logger -s -t ratelimit -p user.notice &amp;quot;enabling rate limits&amp;quot;
tc qdisc del dev $dev root &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
tc qdisc add dev $dev root handle 1: htb
tc class add dev $dev parent 1: classid 1:10 htb rate $crashplan_limit
tc filter add dev $dev parent 1: prio 0 protocol ip handle 10 fw flowid 1:10
iptables -t mangle -A OUTPUT -d $crashplan_addr -p tcp --dport $crashplan_port -j MARK --set-mark 10
elif [ &amp;quot;$1&amp;quot; = &amp;quot;disable&amp;quot; ]; then
#
# This removes the traffic shaper
# configuration.
#
logger -s -t ratelimit -p user.notice &amp;quot;disabling rate limits&amp;quot;
tc qdisc del dev $dev root &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
iptables -t mangle -D OUTPUT -d $crashplan_addr -p tcp --dport $crashplan_port -j MARK --set-mark 10
elif [ &amp;quot;$1&amp;quot; = &amp;quot;show&amp;quot; ]; then
#
# Shows the current traffic shaper configuration.
#
tc qdisc show dev $dev
tc class show dev $dev
tc filter show dev $dev
iptables -t mangle -vnL OUTPUT
fi
&lt;/code>&lt;/pre></content></item><item><title>Puppet, scope, and inheritance</title><link>https://blog.oddbit.com/post/2011-08-16-puppet-scope-and-inheritance/</link><pubDate>Tue, 16 Aug 2011 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2011-08-16-puppet-scope-and-inheritance/</guid><description>I note this here because it wasn&amp;rsquo;t apparent to me from the Puppet documentation.
If you have a Puppet class like this:
class foo { File { ensure =&amp;gt; file, mode =&amp;gt; 600, } } And you use it like this:
class bar { include foo file { '/tmp/myfile': } } Then /tmp/myfile will not be created. But if instead you do this:
class bar inherits foo { file { '/tmp/myfile': } } It will be created with mode 0600.</description><content>&lt;p>I note this here because it wasn&amp;rsquo;t apparent to me from the Puppet documentation.&lt;/p>
&lt;p>If you have a Puppet class like this:&lt;/p>
&lt;pre>&lt;code>class foo {
File { ensure =&amp;gt; file,
mode =&amp;gt; 600,
}
}
&lt;/code>&lt;/pre>
&lt;p>And you use it like this:&lt;/p>
&lt;pre>&lt;code>class bar {
include foo
file { '/tmp/myfile': }
}
&lt;/code>&lt;/pre>
&lt;p>Then /tmp/myfile will not be created. But if instead you do this:&lt;/p>
&lt;pre>&lt;code>class bar inherits foo {
file { '/tmp/myfile': }
}
&lt;/code>&lt;/pre>
&lt;p>It will be created with mode 0600. In other words, if you use inherits then definitions in the parent class are available in the scope of your subclass. If you include, then definitions in he included class are &amp;ldquo;below&amp;rdquo; the scope of the including class.&lt;/p></content></item><item><title>Fixing RPM with evil magic</title><link>https://blog.oddbit.com/post/2011-07-26-fixing-rpm-with-evil-magic/</link><pubDate>Tue, 26 Jul 2011 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2011-07-26-fixing-rpm-with-evil-magic/</guid><description>Fixing rpmsign with evil magic At my office we are developing a deployment mechanism for RPM packages. The general workflow looks like this:
You build a source rpm on your own machine. You sign the rpm with your GPG key. You submit the source RPM to our buildserver. The buildserver validates your signature and then builds the package. The buildserver signs the package using a master signing key. The last step in that sequence represents a problem, because the rpmsign command will always, always prompt for a password and read the response from /dev/tty.</description><content>&lt;h1 id="fixing-rpmsign-with-evil-magic">Fixing rpmsign with evil magic&lt;/h1>
&lt;p>At my office we are developing a deployment mechanism for RPM packages. The
general workflow looks like this:&lt;/p>
&lt;ul>
&lt;li>You build a source rpm on your own machine.&lt;/li>
&lt;li>You sign the rpm with your GPG key.&lt;/li>
&lt;li>You submit the source RPM to our buildserver.&lt;/li>
&lt;li>The buildserver validates your signature and then builds the package.&lt;/li>
&lt;li>The buildserver signs the package using a master signing key.&lt;/li>
&lt;/ul>
&lt;p>The last step in that sequence represents a problem, because the &lt;code>rpmsign&lt;/code>
command will always, always prompt for a password and read the response from
&lt;code>/dev/tty&lt;/code>. This means that (a) you can&amp;rsquo;t easily provide the password on stdin,
and (b) you can&amp;rsquo;t fix the problem using a passwordless key.&lt;/p>
&lt;p>Other people have &lt;a href="http://jrmonk-techzine.blogspot.com/2010/06/how-to-sign-rpm-files-in-batch-mode.html">solved this problem using expect&lt;/a>, but I&amp;rsquo;ve opted for
another solution which in some ways seems cleaner and in others seems like a
terrible idea: function interposition using &lt;code>LD_PRELOAD&lt;/code>.&lt;/p>
&lt;p>The &lt;code>rpmsign&lt;/code> command prompts for (and reads) a password using the &lt;code>getpass()&lt;/code>
function call. If you look at the &lt;code>getpass(3)&lt;/code> man page, you&amp;rsquo;ll see that the
function is defined like this:&lt;/p>
&lt;pre>&lt;code>#include &amp;lt;unistd.h&amp;gt;
char *getpass( const char *prompt);
&lt;/code>&lt;/pre>
&lt;p>So we start with the following short block of C code:&lt;/p>
&lt;pre>&lt;code>#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
char *getpass( const char *prompt) {
printf(&amp;quot;I ATE YOUR PASSPHRASE.n&amp;quot;);
return &amp;quot;&amp;quot;;
}
&lt;/code>&lt;/pre>
&lt;p>This &amp;ndash; when properly loaded &amp;ndash; will replace the standard C library &lt;code>getpass()&lt;/code> function with our own version, which simply returns an empty string. This of course means we&amp;rsquo;ll be using a passwordless key, but you could obviously have our replacement function return an actual password instead of an empty string. I would argue that by doing so you would not substantially increase the security of your solution.&lt;/p>
&lt;p>Next we create a shared library:&lt;/p>
&lt;pre>&lt;code>$ cc -fPIC -g -c -o getpass.o getpass.c
$ ld -shared -o getpass.so getpass.o
&lt;/code>&lt;/pre>
&lt;p>And now we perform our magic:&lt;/p>
&lt;pre>&lt;code>$ LD_PRELOAD=$(pwd)/getpass.so rpmsign --addsign some.src.rpm
I ATE YOUR PASSPHRASE.
Pass phrase is good.
&lt;/code>&lt;/pre>
&lt;p>And &lt;em>voila&lt;/em>! A solution for operating &lt;code>rpmsign&lt;/code> in batch mode.&lt;/p></content></item><item><title>Installing CrashPlan under FreeBSD 8</title><link>https://blog.oddbit.com/post/2011-05-22-installing-crashplan-under-fre/</link><pubDate>Sun, 22 May 2011 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2011-05-22-installing-crashplan-under-fre/</guid><description>This articles describes how I got CrashPlan running on my FreeBSD 8(-STABLE) system. These instructions by Kim Scarborough were my starting point, but as these were for FreeBSD 7 there were some additional steps necessary to get things working.
Install Java I had originally thought that it might be possible to run the CrashPlan client &amp;ldquo;natively&amp;rdquo; under FreeBSD. CrashPlan is a Java application, so this seemed like a possible solution. Unfortunately, Java under FreeBSD 8 seems to be a lost cause.</description><content>&lt;p>This articles describes how I got &lt;a href="http://crashplan.com/">CrashPlan&lt;/a> running on my FreeBSD 8(-STABLE) system. &lt;a href="http://kim.scarborough.chicago.il.us/do/nerd/tips/crashplan">These instructions&lt;/a> by Kim Scarborough were my starting point, but as these were for FreeBSD 7 there were some additional steps necessary to get things working.&lt;/p>
&lt;h1 id="install-java">Install Java&lt;/h1>
&lt;p>I had originally thought that it might be possible to run the CrashPlan client &amp;ldquo;natively&amp;rdquo; under FreeBSD. CrashPlan is a Java application, so this seemed like a possible solution. Unfortunately, Java under FreeBSD 8 seems to be a lost cause. I finally gave up and just installed Java under Linux.&lt;/p>
&lt;h2 id="set-up-your-linux-compatability-environment">Set up your Linux compatability environment&lt;/h2>
&lt;p>The simplest way to do this is to follow the instructions in the &lt;a href="http://www.freebsd.org/doc/handbook/linuxemu-lbc-install.html">FreeBSD Handbook&lt;/a>. This will get you a Fedora 10 based Linux userspace, which should be more than sufficient. I&amp;rsquo;m using a CentOS 5.6 userspace, but for what we&amp;rsquo;re doing it shouldn&amp;rsquo;t matter, modulo some minor differences in paths.&lt;/p>
&lt;p>Note that Linux software running in this environment will have a modified view of your filesystem. In particular, /etc will map to /compat/linux/etc, and ZFS filesystems with non-default mountpoints seem to behave oddly (they are accessible, but not necessarily visible before you access them). This may require some workarounds in CrashPlan, depending on what you&amp;rsquo;re trying to back up.&lt;/p>
&lt;h2 id="install-java-jre">Install Java JRE&lt;/h2>
&lt;p>I installed a compatible Java environment from the CentOS package repository:&lt;/p>
&lt;pre>&lt;code># chroot /compat/linux bash
bash-3.2# yum install java-1.6.0-openjdk
bash-3.2# exit
&lt;/code>&lt;/pre>
&lt;h1 id="install-crashplan">Install CrashPlan&lt;/h1>
&lt;h2 id="install-the-crashplan-software">Install the CrashPlan software&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Download &lt;a href="http://www.crashplan.com/consumer/download.html?os=Linux">CrashPlan for Linux&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Unpack the archive (named something like CrashPlan_3.0.3_Linux.tgz)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Change to the CrashPlan-install directory.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run the following commands:&lt;/p>
&lt;h1 id="export-pathcompatlinuxusrlibjvmjre-160-openjdkbinpath">export PATH=/compat/linux/usr/lib/jvm/jre-1.6.0-openjdk/bin:$PATH&lt;/h1>
&lt;h1 id="compatlinuxbinbash-installsh">/compat/linux/bin/bash install.sh&lt;/h1>
&lt;/li>
&lt;li>
&lt;p>Install CrashPlan into /usr/local. When prompted for where to locate init scripts (&amp;ldquo;What directory contains your SYSV init scripts?&amp;rdquo; and &amp;ldquo;What directory contains your runlevel init links?&amp;rdquo;), enter /tmp (because the installed init scripts aren&amp;rsquo;t ideal for your FreeBSD environment &amp;ndash; we&amp;rsquo;ll install our own later on).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="fix-java">Fix Java&lt;/h2>
&lt;p>The Linux runtime provided by the FreeBSD Linux compatability layer does not include all of the features of recent Linux kernels. In particular, it is missing the epoll* syscalls, which will cause Java to die with a &lt;em>Function not implemented&lt;/em> error. The workaround for this is documented in the &lt;a href="http://wiki.freebsd.org/linux-kernel">linux-kernel&lt;/a> page on the &lt;a href="http://wiki.freebsd.org/">FreeBSD wiki&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>If you run an application in the linux java which wants to use the linux epoll functions (you should see &amp;ldquo;not implemented&amp;rdquo; messages in dmesg), you can start java with the argument -Djava.nio.channels.spi.SelectorProvider=sun.nio.ch.PollSelectorProvider&lt;/p>
&lt;/blockquote>
&lt;h2 id="install-an-rc-script">Install an rc script&lt;/h2>
&lt;p>Place the following script into /usr/local/etc/rc.d/crashplan:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
#
# PROVIDE: crashplan
# REQUIRE: NETWORKING
# KEYWORD: shutdown
. /etc/rc.subr
name=&amp;quot;crashplan&amp;quot;
rcvar=`set_rcvar`
start_cmd=crashplan_start
stop_cmd=crashplan_stop
crashplan_start () {
/compat/linux/bin/bash /usr/local/crashplan/bin/CrashPlanEngine start
}
crashplan_stop () {
/compat/linux/bin/bash /usr/local/crashplan/bin/CrashPlanEngine stop
}
load_rc_config $name
run_rc_command &amp;quot;$1&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And then add:&lt;/p>
&lt;pre>&lt;code>crashplan_enable=&amp;quot;YES&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>To /etc/rc.conf (or /etc/rc.conf.local).&lt;/p>
&lt;h2 id="start-crashplan">Start CrashPlan&lt;/h2>
&lt;p>Run:&lt;/p>
&lt;pre>&lt;code>/usr/local/etc/rc.d/crashplan start
&lt;/code>&lt;/pre>
&lt;p>Wait a moment, then run:&lt;/p>
&lt;pre>&lt;code>/compat/linux/bin/bash /usr/local/crashplan/bin/CrashPlanEngine status
&lt;/code>&lt;/pre>
&lt;p>This should verify that CrashPlan is running.&lt;/p>
&lt;h1 id="connect-crashplan-client">Connect CrashPlan client&lt;/h1>
&lt;p>Follow the instructions provided by CrashPlan for &lt;a href="http://stgsupport.crashplan.com/doku.php/how_to/configure_a_headless_client">connecting to a headless CrashPlan desktop&lt;/a>.&lt;/p></content></item><item><title>Signing data with ssh-agent</title><link>https://blog.oddbit.com/post/2011-05-09-signing-data-with-ssh-agent/</link><pubDate>Mon, 09 May 2011 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2011-05-09-signing-data-with-ssh-agent/</guid><description>This is follow-up to my previous post, Converting OpenSSH public keys.
OpenSSH allows one to use an agent that acts as a proxy to your private key. When using an agent &amp;ndash; particularly with agent forwarding enabled &amp;ndash; this allows you to authenticate to a remote host without having to (a) repeatedly type in your password or (b) expose an unencrypted private key to remote systems.
If one is temtped to use SSH keys as authentication credentials outside of ssh, one would ideally be able to take advantage of the ssh agent for these same reasons.</description><content>&lt;p>This is follow-up to my previous post, &lt;a href="http://blog.oddbit.com/2011/05/08/converting-openssh-public-keys/">Converting OpenSSH public keys&lt;/a>.&lt;/p>
&lt;p>OpenSSH allows one to use an &lt;em>agent&lt;/em> that acts as a proxy to your private key. When using an agent &amp;ndash; particularly with agent forwarding enabled &amp;ndash; this allows you to authenticate to a remote host without having to (a) repeatedly type in your password or (b) expose an unencrypted private key to remote systems.&lt;/p>
&lt;p>If one is temtped to use SSH keys as authentication credentials outside of ssh, one would ideally be able to take advantage of the ssh agent for these same reasons.&lt;/p>
&lt;p>This article discusses what is required to programmatically interact with the agent and with the OpenSSL libraries for signing data and verifying signatures.&lt;/p>
&lt;h4 id="signing-data-with-ssh-agent">Signing data with ssh-agent&lt;/h4>
&lt;p>The SSH agent does not provide clients with direct access to an unencrypted private key. Rather, it will accept data from the client and return the signature of the SHA1 hash of the data.&lt;/p>
&lt;p>The agent communicates over a unix socket using the &lt;a href="http://www.openbsd.org/cgi-bin/cvsweb/src/usr.bin/ssh/PROTOCOL.agent?rev=HEAD;content-type=text%2Fplain">ssh agent protocol&lt;/a> defined in &lt;a href="http://www.openbsd.org/cgi-bin/cvsweb/src/usr.bin/ssh/authfd.h?rev=HEAD;content-type=text%2Fplain">authfd.h&lt;/a>. The Python &lt;a href="http://www.lag.net/paramiko/">Paramiko&lt;/a> libary (a pure-python implementation of ssh) includes support for interacting with an ssh agent.&lt;/p>
&lt;p>Signing data is very simple:&lt;/p>
&lt;pre>&lt;code>import hashlib
import paramiko.agent
data = 'something to sign'
data_sha1 = hashlib.sha1(data).digest()
a = paramiko.agent.Agent()
key = a.keys[0]
d = key.sign_ssh_data(None, data_sha1)
&lt;/code>&lt;/pre>
&lt;p>Internally, the agent computes the SHA1 digest for the data, signs this using the selected key, and returns a &lt;em>signature_blob&lt;/em> that varies depending on the key type in use. For an RSA signature, the result format is a series of (length, data) pairs, where the length is encoded as a four-byte unsigned integer. The response contains the following elements:&lt;/p>
&lt;ol>
&lt;li>algorithm name (ssh-rsa)&lt;/li>
&lt;li>rsa signature&lt;/li>
&lt;/ol>
&lt;p>For example, after signing some data using a 1024-bit private key, the value returned from sign_ssh_data looked like this:&lt;/p>
&lt;pre>&lt;code>0000000: 0000 0007 7373 682d 7273 6100 0000 8027 ....ssh-rsa....'
0000010: 953c 771c 5ee4 f4b0 9849 c061 0ac2 2adb .&amp;lt;w.^....I.a..*.
0000020: b53d 2bcb a545 8dbb d582 05e5 a916 6490 .=+..E........d.
0000030: 1b67 3210 9bfc c74d d0ad 5011 394b a3fe .g2....M..P.9K..
0000040: 96e2 910b bbfd 19cd 73e5 6720 503a 95e1 ........s.g P:..
0000050: 5b8b 63c4 14a3 ec3d bf57 846e f0b4 e66c [.c....=.W.n...l
0000060: ce5d 6327 6055 b4e2 3c14 c13f 8303 4b1a .]c'`U..&amp;lt;..?..K.
0000070: 7ce3 9f33 9e7c 7ca4 a97b 506d fa0b a39e |..3.||..{Pm....
0000080: cb53 befc d725 9cd1 a8af 6042 5ac8 01 .S...%....`BZ..
&lt;/code>&lt;/pre>
&lt;p>The first four bytes (0000 0007) are the length of the algorithm name (ssh-rsa). The next field is the length of the signature (0000 0080, or 128 bytes), followed by the signature data. This means we can extract the signature data like this:&lt;/p>
&lt;pre>&lt;code>parts = []
while d:
len = struct.unpack('&amp;gt;I', d[:4])[0]
bits = d[4:len+4]
parts.append(bits)
d = d[len+4:]
sig = parts[1]
open('signature', 'w').write(sig)
&lt;/code>&lt;/pre>
&lt;h4 id="signing-the-data-with-openssl">Signing the data with OpenSSL&lt;/h4>
&lt;h5 id="using-m2crypto">Using M2Crypto&lt;/h5>
&lt;p>You can accomplish the same thing using the &lt;a href="http://sandbox.rulemaker.net/ngps/m2/">M2Crypto&lt;/a> library for Python like this:&lt;/p>
&lt;pre>&lt;code>import hashlib
import M2Crypto.RSA
data = 'something to sign'
data_sha1 = hashlib.sha1(data).digest()
key = M2Crypto.RSA.load_key('testkey')
sig = key.sign(data_sha1)
open('signature', 'w').write(sig)
&lt;/code>&lt;/pre>
&lt;p>This assumes that testkey is the private key file corresponding to the first key loaded into your agent in the previous example.&lt;/p>
&lt;h5 id="using-the-command-line">Using the command line&lt;/h5>
&lt;p>You can also generate an equivalent signature using the OpenSSL command line tools:&lt;/p>
&lt;pre>&lt;code>echo -n 'something to sign' |
openssl sha1 -binary |
openssl pkeyutl -sign -inkey testkey -pkeyopt digest:sha1 &amp;gt; signature
&lt;/code>&lt;/pre>
&lt;p>Note that including -pkeyopt digest:sha1 is necessary to get a signature block that is compatible with the one returned by the ssh agent. The pkeyutl man page has this to say:&lt;/p>
&lt;blockquote>
&lt;p>In PKCS#1 padding if the message digest is not set then the supplied data is signed or verified directly instead of using a DigestInfo structure. If a digest is set then the a DigestInfo structure is used and its the length must correspond to the digest type.&lt;/p>
&lt;/blockquote>
&lt;h4 id="veryfying-the-data">Veryfying the data&lt;/h4>
&lt;p>You can verify the signature using the corresponding public key.&lt;/p>
&lt;h5 id="using-m2crypto-1">Using M2Crypto&lt;/h5>
&lt;p>This uses the &lt;a href="http://sandbox.rulemaker.net/ngps/m2/">M2Crypto&lt;/a> module to verify the signature computed in the previous step:&lt;/p>
&lt;pre>&lt;code>import hashlib
import M2Crypto.RSA
# let's pretend that you've read my previous blog post and have
# created an &amp;quot;sshkey&amp;quot; module for reading the ssh public key format.
import sshkey
data = 'something to sign'
data_sha1 = hashlib.sha1(data).digest()
# read the signature generated in the previous step
sig = open('signature').read()
e,n = sshkey.load_rsa_pub_key('testkey.pub')
key = M2Crypto.RSA.new_pub_key((
M2Crypto.m2.bn_to_mpi(M2Crypto.m2.hex_to_bn(hex(e)[2:])),
M2Crypto.m2.bn_to_mpi(M2Crypto.m2.hex_to_bn(hex(n)[2:])),
))
if key.verify(data_sha1, sig):
print 'Verified!'
else:
print 'Failed!'
&lt;/code>&lt;/pre>
&lt;p>If you have converted the ssh public key into a standard format, you could do this instead:&lt;/p>
&lt;pre>&lt;code>import hashlib
import M2Crypto.RSA
data = 'something to sign'
data_sha1 = hashlib.sha1(data).digest()
# read the signature generated in the previous step
sig = open('signature').read()
key = M2Crypto.RSA.load_pub_key('testkey.pubssl')
if key.verify(data_sha1, sig):
print 'Verified!'
else:
print 'Failed!'
&lt;/code>&lt;/pre>
&lt;h5 id="using-openssl">Using OpenSSL&lt;/h5>
&lt;p>We can do the same thing on the command line, but we&amp;rsquo;ll first need to convert the ssh public key into a format useful to OpenSSL. This is easy if you have the private key handy&amp;hellip;which we do:&lt;/p>
&lt;pre>&lt;code>openssl rsa -in testkey -pubout &amp;gt; testkey.pubssl
&lt;/code>&lt;/pre>
&lt;p>And now we can verify the signature:&lt;/p>
&lt;pre>&lt;code>echo 'something to sign' |
openssl sha1 -binary |
openssl pkeyutl -verify -sigfile signature \
-pubin -inkey testkey.pubssl -pkeyopt digest:sha1
&lt;/code>&lt;/pre></content></item><item><title>Converting OpenSSH public keys</title><link>https://blog.oddbit.com/post/2011-05-08-converting-openssh-public-keys/</link><pubDate>Sun, 08 May 2011 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2011-05-08-converting-openssh-public-keys/</guid><description>I&amp;rsquo;ve posted a followup to this article that discusses ssh-agent.
For reasons best left to another post, I wanted to convert an SSH public key into a PKCS#1 PEM-encoded public key. That is, I wanted to go from this:
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD7EZn/BzP26AWk/Ts2ymjpTXuXRiEWIWn HFTilOTcuJ/P1HfOwiy4RHC1rv59Yh/E6jbTx623+OGySJWh1IS3dAEaHhcGKnJaikrBn3c cdoNVkAAuL/YD7FMG1Z0SjtcZS6MoO8Lb9pkq6R+Ok6JQjwCEsB+OaVwP9RnVA+HSYeyCVE 0KakLCbBJcD1U2aHP4+IH4OaXhZacpb9Ueja6NNfGrv558xTgfZ+fLdJ7cpg6wU8UZnVM1B JiUW5KFasc+2IuZR0+g/oJXaYwvW2T6XsMgipetCEtQoMAJ4zmugzHSQuFRYHw/7S6PUI2U 03glFmULvEV+qIxsVFT1ng3pj lars@tiamat.house To this:
-----BEGIN RSA PUBLIC KEY----- MIIBCgKCAQEA+xGZ/wcz9ugFpP07Nspo6U17l0YhFiFpxxU4pTk3Lifz9R3zsIsu ERwta7+fWIfxOo208ett/jhskiVodSEt3QBGh4XBipyWopKwZ93HHaDVZAALi/2A +xTBtWdEo7XGUujKDvC2/aZKukfjpOiUI8AhLAfjmlcD/UZ1QPh0mHsglRNCmpCw mwSXA9VNmhz+PiB+Dml4WWnKW/VHo2ujTXxq7+efMU4H2fny3Se3KYOsFPFGZ1TN QSYlFuShWrHPtiLmUdPoP6CV2mML1tk+l7DIIqXrQhLUKDACeM5roMx0kLhUWB8P +0uj1CNlNN4JRZlC7xFfqiMbFRU9Z4N6YwIDAQAB -----END RSA PUBLIC KEY----- If you have a recent version of OpenSSH (where recent means 5.</description><content>&lt;blockquote>
&lt;p>I&amp;rsquo;ve posted a &lt;a href="http://blog.oddbit.com/2011/05/09/signing-data-with-ssh-agent/">followup&lt;/a> to this article that discusses ssh-agent.&lt;/p>
&lt;/blockquote>
&lt;p>For reasons best left to another post, I wanted to convert an SSH public key into a PKCS#1 PEM-encoded public key. That is, I wanted to go from this:&lt;/p>
&lt;pre>&lt;code>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD7EZn/BzP26AWk/Ts2ymjpTXuXRiEWIWn
HFTilOTcuJ/P1HfOwiy4RHC1rv59Yh/E6jbTx623+OGySJWh1IS3dAEaHhcGKnJaikrBn3c
cdoNVkAAuL/YD7FMG1Z0SjtcZS6MoO8Lb9pkq6R+Ok6JQjwCEsB+OaVwP9RnVA+HSYeyCVE
0KakLCbBJcD1U2aHP4+IH4OaXhZacpb9Ueja6NNfGrv558xTgfZ+fLdJ7cpg6wU8UZnVM1B
JiUW5KFasc+2IuZR0+g/oJXaYwvW2T6XsMgipetCEtQoMAJ4zmugzHSQuFRYHw/7S6PUI2U
03glFmULvEV+qIxsVFT1ng3pj lars@tiamat.house
&lt;/code>&lt;/pre>
&lt;p>To this:&lt;/p>
&lt;pre>&lt;code>-----BEGIN RSA PUBLIC KEY-----
MIIBCgKCAQEA+xGZ/wcz9ugFpP07Nspo6U17l0YhFiFpxxU4pTk3Lifz9R3zsIsu
ERwta7+fWIfxOo208ett/jhskiVodSEt3QBGh4XBipyWopKwZ93HHaDVZAALi/2A
+xTBtWdEo7XGUujKDvC2/aZKukfjpOiUI8AhLAfjmlcD/UZ1QPh0mHsglRNCmpCw
mwSXA9VNmhz+PiB+Dml4WWnKW/VHo2ujTXxq7+efMU4H2fny3Se3KYOsFPFGZ1TN
QSYlFuShWrHPtiLmUdPoP6CV2mML1tk+l7DIIqXrQhLUKDACeM5roMx0kLhUWB8P
+0uj1CNlNN4JRZlC7xFfqiMbFRU9Z4N6YwIDAQAB
-----END RSA PUBLIC KEY-----
&lt;/code>&lt;/pre>
&lt;p>If you have a recent version of OpenSSH (where recent means 5.6 or later), you can just do this:&lt;/p>
&lt;pre>&lt;code>ssh-keygen -f key.pub -e -m pem
&lt;/code>&lt;/pre>
&lt;p>If you don&amp;rsquo;t have that, read on.&lt;/p>
&lt;h1 id="openssh-public-key-format">OpenSSH Public Key Format&lt;/h1>
&lt;p>The OpenSSH public key format is fully documented &lt;a href="http://tools.ietf.org/html/rfc4253#section-6.6">RFC 4253&lt;/a>. Briefly, an OpenSSH public key consists of three fields:&lt;/p>
&lt;ul>
&lt;li>The key type&lt;/li>
&lt;li>A chunk of PEM-encoded data&lt;/li>
&lt;li>A comment&lt;/li>
&lt;/ul>
&lt;p>What, you may ask, is PEM encoding? &lt;a href="http://en.wikipedia.org/wiki/Base64#Privacy-enhanced_mail">Privacy Enhanced Mail&lt;/a> (PEM) is a specific type of &lt;a href="http://en.wikipedia.org/wiki/Base64">Base64&lt;/a> encoding&amp;hellip;which is to say it is a way of representing binary data using only printable ASCII characters.&lt;/p>
&lt;p>For an ssh-rsa key, the PEM-encoded data is a series of (length, data) pairs. The length is encoded as four octets (in big-endian order). The values encoded are:&lt;/p>
&lt;ol>
&lt;li>algorithm name (one of (ssh-rsa, ssh-dsa)). This duplicates the key type in the first field of the public key.&lt;/li>
&lt;li>RSA exponent&lt;/li>
&lt;li>RSA modulus&lt;/li>
&lt;/ol>
&lt;p>For more information on how RSA works and what the exponent and modulus are used for, read the Wikipedia article on &lt;a href="http://en.wikipedia.org/wiki/RSA">RSA&lt;/a>.&lt;/p>
&lt;p>We can read this in with the following Python code:&lt;/p>
&lt;pre>&lt;code>import sys
import base64
import struct
# get the second field from the public key file.
keydata = base64.b64decode(
open('key.pub').read().split(None)[1])
parts = []
while keydata:
# read the length of the data
dlen = struct.unpack('&amp;gt;I', keydata[:4])[0]
# read in &amp;lt;length&amp;gt; bytes
data, keydata = keydata[4:dlen+4], keydata[4+dlen:]
parts.append(data)
&lt;/code>&lt;/pre>
&lt;p>This leaves us with an array that, for an RSA key, will look like:&lt;/p>
&lt;pre>&lt;code>['ssh-rsa', '...bytes in exponent...', '...bytes in modulus...']
&lt;/code>&lt;/pre>
&lt;p>We need to convert the character buffers currently holding &lt;em>e&lt;/em> (the exponent) and &lt;em>n&lt;/em> (the modulus) into numeric types. There may be better ways to do this, but this works:&lt;/p>
&lt;pre>&lt;code>e_val = eval('0x' + ''.join(['%02X' % struct.unpack('B', x)[0] for x in
parts[1]]))
n_val = eval('0x' + ''.join(['%02X' % struct.unpack('B', x)[0] for x in
parts[2]]))
&lt;/code>&lt;/pre>
&lt;p>We now have the RSA public key. The next step is to produce the appropriate output format.&lt;/p>
&lt;h1 id="pkcs1-public-key-format">PKCS#1 Public Key Format&lt;/h1>
&lt;p>Our target format is a PEM-encoded PKCS#1 public key.&lt;/p>
&lt;p>PKCS#1 is &amp;ldquo;the first of a family of standards called Public-Key Cryptography Standards (PKCS), published by RSA Laboratories.&amp;rdquo; (&lt;a href="http://en.wikipedia.org/wiki/PKCS1">Wikipedia&lt;/a>). You can identify a PKCS#1 PEM-encoded public key by the markers used to delimit the base64 encoded data:&lt;/p>
&lt;pre>&lt;code>-----BEGIN RSA PUBLIC KEY-----
...
-----END RSA PUBLIC KEY-----
&lt;/code>&lt;/pre>
&lt;p>This is different from an x.509 public key, which looks like this:&lt;/p>
&lt;pre>&lt;code>-----BEGIN PUBLIC KEY-----
...
-----END PUBLIC KEY-----
&lt;/code>&lt;/pre>
&lt;p>The x.509 format may be used to store keys generated using algorithms other than RSA.&lt;/p>
&lt;p>The data in a PKCS#1 key is encoded using DER, which is a set of rules for serializing ASN.1 data. For more information see:&lt;/p>
&lt;ul>
&lt;li>The WikiPedia entry for &lt;a href="http://en.wikipedia.org/wiki/Distinguished_Encoding_Rules">Distinguished Encoding Rules&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://luca.ntop.org/Teaching/Appunti/asn1.html">A Layman&amp;rsquo;s Guide to a Subset of ASN.1, BER, and DER&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Basically, ASN.1 is a standard for describing abstract data types, and DER is a set of rules for transforming an ASN.1 data type into a series of octets.&lt;/p>
&lt;p>According to the &lt;a href="ftp://ftp.rsasecurity.com/pub/pkcs/pkcs-1/pkcs-1v2-1.asn">ASN module for PKCS#1&lt;/a>, a PKCS#1 public key looks like this:&lt;/p>
&lt;pre>&lt;code>RSAPublicKey ::= SEQUENCE {
modulus INTEGER, -- n
publicExponent INTEGER -- e
}
&lt;/code>&lt;/pre>
&lt;p>We can generate this from our key data using Python&amp;rsquo;s &lt;a href="http://pyasn1.sourceforge.net/">PyASN1&lt;/a> module:&lt;/p>
&lt;pre>&lt;code>from pyasn1.type import univ
pkcs1_seq = univ.Sequence()
pkcs1_seq.setComponentByPosition(0, univ.Integer(n_val))
pkcs1_seq.setComponentByPosition(1, univ.Integer(e_val))
&lt;/code>&lt;/pre>
&lt;h1 id="generating-the-output">Generating the output&lt;/h1>
&lt;p>Now that we have the DER encoded key, generating the output is easy:&lt;/p>
&lt;pre>&lt;code>from pyasn1.codec.der import encoder as der_encoder
print '-----BEGIN RSA PUBLIC KEY-----'
print base64.encodestring(der_encoder.encode(pkcs1_seq))
print '-----END RSA PUBLIC KEY-----'
&lt;/code>&lt;/pre>
&lt;h1 id="appendix-openssh-private-key-format">Appendix: OpenSSH private key format&lt;/h1>
&lt;p>Whereas the OpenSSH public key format is effectively &amp;ldquo;proprietary&amp;rdquo; (that is, the format is used only by OpenSSH), the private key is already stored as a PKCS#1 private key. This means that the private key can be manipulated using the OpenSSL command line tools.&lt;/p>
&lt;p>The clever folks among you may be wondering if, assuming we have the private key available, we could have skipped this whole exercise and simply extracted the public key in the correct format using the openssl command. We can come very close&amp;hellip;the following demonstrates how to extract the public key from the private key using openssl:&lt;/p>
&lt;pre>&lt;code>$ openssl rsa -in key -pubout
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA+xGZ/wcz9ugFpP07Nspo
6U17l0YhFiFpxxU4pTk3Lifz9R3zsIsuERwta7+fWIfxOo208ett/jhskiVodSEt
3QBGh4XBipyWopKwZ93HHaDVZAALi/2A+xTBtWdEo7XGUujKDvC2/aZKukfjpOiU
I8AhLAfjmlcD/UZ1QPh0mHsglRNCmpCwmwSXA9VNmhz+PiB+Dml4WWnKW/VHo2uj
TXxq7+efMU4H2fny3Se3KYOsFPFGZ1TNQSYlFuShWrHPtiLmUdPoP6CV2mML1tk+
l7DIIqXrQhLUKDACeM5roMx0kLhUWB8P+0uj1CNlNN4JRZlC7xFfqiMbFRU9Z4N6
YwIDAQAB
-----END PUBLIC KEY-----
&lt;/code>&lt;/pre>
&lt;p>So close! But this is in x.509 format, and even though the OpenSSL library supports PKCS#1 encoding, there is no mechanism to make the command line tools cough up this format.&lt;/p>
&lt;p>Additionally, I am trying for a solution that does not require the private key to be available, which means that in any case I will still have to parse the OpenSSH public key format.&lt;/p></content></item><item><title>Python ctypes module</title><link>https://blog.oddbit.com/post/2010-08-10-python-ctypes-module/</link><pubDate>Tue, 10 Aug 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-08-10-python-ctypes-module/</guid><description>I just learned about the Python ctypes module, which is a Python module for interfacing with C code. Among other things, ctypes lets you call arbitrary functions in shared libraries. This is, from my perspective, some very cool magic. I thought I would provide a short example here, since it took me a little time to get everything working smoothly.
For this example, we&amp;rsquo;ll write a wrapper for the standard statvfs(2) function:</description><content>&lt;p>I just learned about the Python &lt;code>ctypes&lt;/code> module, which is a Python module for interfacing with C code. Among other things, &lt;code>ctypes&lt;/code> lets you call arbitrary functions in shared libraries. This is, from my perspective, some very cool magic. I thought I would provide a short example here, since it took me a little time to get everything working smoothly.&lt;/p>
&lt;p>For this example, we&amp;rsquo;ll write a wrapper for the standard &lt;code>statvfs(2)&lt;/code> function:&lt;/p>
&lt;pre>&lt;code>SYNOPSIS
#include &amp;lt;sys/statvfs.h&amp;gt;
int statvfs(const char *path, struct statvfs *buf);
int fstatvfs(int fd, struct statvfs *buf);
DESCRIPTION
The function statvfs() returns information about a mounted file
system. path is the pathname of any file within the mounted file
system. buf is a pointer to a statvfs structure defined
approximately as follows:
&lt;/code>&lt;/pre>
&lt;p>Note the wording there: &amp;ldquo;&amp;hellip;defined &lt;em>approximately&lt;/em> as follows.&amp;rdquo; Our first job is finding out exactly what the &lt;code>statvfs&lt;/code> structure looks like. We can use gcc to show us the contents of the appropriate #include file:&lt;/p>
&lt;pre>&lt;code>echo '#include &amp;lt;sys/statvfs.h&amp;gt;' | gcc -E | less
&lt;/code>&lt;/pre>
&lt;p>Browsing through the results, we find the the following definition:&lt;/p>
&lt;pre>&lt;code>struct statvfs
{
unsigned long int f_bsize;
unsigned long int f_frsize;
__fsblkcnt_t f_blocks;
__fsblkcnt_t f_bfree;
__fsblkcnt_t f_bavail;
__fsfilcnt_t f_files;
__fsfilcnt_t f_ffree;
__fsfilcnt_t f_favail;
unsigned long int f_fsid;
unsigned long int f_flag;
unsigned long int f_namemax;
int __f_spare[6];
};
&lt;/code>&lt;/pre>
&lt;p>We need to investigate further to determine what &lt;code>__fsblkcnt_t&lt;/code> and &lt;code>__fsfilcnt_t&lt;/code> really mean. There are a number of ways to do this. Here&amp;rsquo;s what I did:&lt;/p>
&lt;pre>&lt;code>$ cd /usr/include
$ ctags -R
$ ex
Entering Ex mode. Type &amp;quot;visual&amp;quot; to go to Normal mode.
:tag __fsblkcnt_t
&amp;quot;bits/types.h&amp;quot; [readonly] 197L, 7601C
:p
__STD_TYPE __FSBLKCNT_T_TYPE __fsblkcnt_t;
:tag __FSBLKCNT_T_TYPE
&amp;quot;bits/typesizes.h&amp;quot; [readonly] 66L, 2538C
:p
#define __FSBLKCNT_T_TYPE __ULONGWORD_TYPE
:tag __ULONGWORD_TYPE
&amp;quot;bits/types.h&amp;quot; [readonly] 197L, 7601C
:p
#define __ULONGWORD_TYPE unsigned long int
&lt;/code>&lt;/pre>
&lt;p>Repeat this for &lt;code>__fsfilcnt_t&lt;/code> and we find that they are both unsigned long int.&lt;/p>
&lt;p>This means that we need to create a &lt;code>ctypes.Structure&lt;/code> object like the following:&lt;/p>
&lt;pre>&lt;code>from ctypes import *
class struct_statvfs (Structure):
_fields_ = [
('f_bsize', c_ulong),
('f_frsize', c_ulong),
('f_blocks', c_ulong),
('f_bfree', c_ulong),
('f_bavail', c_ulong),
('f_files', c_ulong),
('f_ffree', c_ulong),
('f_favail', c_ulong),
('f_fsid', c_ulong),
('f_flag', c_ulong),
('f_namemax', c_ulong),
('__f_spare', c_int * 6),
]
&lt;/code>&lt;/pre>
&lt;p>Failure to create the correct structure (e.g., if you&amp;rsquo;re missing fields) can result in a number of weird errors, including segfaults and warnings from gcc about memory corruption.&lt;/p>
&lt;p>Now that we have the appropriate structure defined, we need to load up the appropriate shared library:&lt;/p>
&lt;pre>&lt;code>libc = CDLL('libc.so.6')
&lt;/code>&lt;/pre>
&lt;p>And then tell ctypes about the function arguments expected by statvfs():&lt;/p>
&lt;pre>&lt;code>libc.statvfs.argtypes = [c_char_p, POINTER(struct_statvfs)]
&lt;/code>&lt;/pre>
&lt;p>With all this in place, we can now call the function:&lt;/p>
&lt;pre>&lt;code>s = struct_statvfs()
res = libc.statvfs('/etc', byref(s))
for k in s._fields_:
print '%20s: %s' % (k[0], getattr(s, k[0]))
&lt;/code>&lt;/pre>
&lt;p>We use &lt;code>byref(s)&lt;/code> because &lt;code>statvfs()&lt;/code> expects a pointer to a structure. This outputs the following on my local system:&lt;/p>
&lt;pre>&lt;code> f_bsize: 4096
f_frsize: 4096
f_blocks: 10079070
f_bfree: 5043632
f_bavail: 4941270
f_files: 2564096
f_ffree: 2419876
f_favail: 2419876
f_fsid: 18446744071962486827
f_flag: 4096
f_namemax: 255
__f_spare: &amp;lt;__main__.c_int_Array_6 object at 0x7f718fb6b3b0&amp;gt;
&lt;/code>&lt;/pre></content></item><item><title>Importing vCard contacts into an LG 420G</title><link>https://blog.oddbit.com/post/2010-08-06-importing-vcard-contacts-into-/</link><pubDate>Fri, 06 Aug 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-08-06-importing-vcard-contacts-into-/</guid><description>Alix recently acquired an LG 420G from TracFone. She was interested in getting all of her contacts onto the phone, which at first seemed like a simple task &amp;ndash; transfer a vCard (.vcf) file to the phone via Bluetooth, and the phone would import all the contacts. This turned out to be a great idea in theory, but in practice there was a fatal flaw &amp;ndash; while the phone did indeed import the contacts, it only imported names and the occasional note or email address.</description><content>&lt;p>Alix recently acquired an LG 420G from &lt;a href="http://www.tracfone.com/">TracFone&lt;/a>. She was interested in getting all of her contacts onto the phone, which at first seemed like a simple task &amp;ndash; transfer a vCard (.vcf) file to the phone via Bluetooth, and the phone would import all the contacts. This turned out to be a great idea in theory, but in practice there was a fatal flaw &amp;ndash; while the phone did indeed import the contacts, it only imported names and the occasional note or email address. There were no phone numbers.&lt;/p>
&lt;p>Thus began the long investigation to find out exactly what the phone expected the contacts to look like.&lt;/p>
&lt;h1 id="eol-format">EOL format&lt;/h1>
&lt;p>The LG 420G needs DOS end-of-line (CRLF), otherwise it won&amp;rsquo;t recognize more than a single contact in your file. There are a number of ways to convert EOL style in a text document; I used the unix2dos tool.&lt;/p>
&lt;h1 id="phone-number-labels">Phone number labels&lt;/h1>
&lt;p>In our source vCard files, telephone numbers were typically listed with one qualifier, like this:&lt;/p>
&lt;pre>&lt;code>TEL;WORK:555-555-5555
TEL;HOME:555-555-5555
TEL;CELL:555-555-5555
&lt;/code>&lt;/pre>
&lt;p>I created new contact entry on the phone and sent it back to my computer, and found that the telephone numbers were labelled with two qualifiers (and a character set identifier), like this:&lt;/p>
&lt;pre>&lt;code>TEL;HOME;CELL;CHARSET=UTF-8:1111111111
TEL;WORK;VOICE;CHARSET=UTF-8:2222222222
TEL;HOME;VOICE;CHARSET=UTF-8:3333333333
&lt;/code>&lt;/pre>
&lt;p>So, the first thing we needed to do was to transform the phone number entries in our original list into the format expected by the 420G. I extracted a list of all the unique phone number labels, like this:&lt;/p>
&lt;pre>&lt;code>grep TEL contacts.vcf | cut -f1 -d: | sort -u
&lt;/code>&lt;/pre>
&lt;p>Which resulted in this list:&lt;/p>
&lt;pre>&lt;code>TEL;CELL
TEL;HOME
TEL;HOME;PREF
TEL;PAGER
TEL;PREF
TEL;PREF;CELL
TEL;PREF;FAX
TEL;WORK
TEL;WORK;PREF
&lt;/code>&lt;/pre>
&lt;p>I used this to generate the following sed script:&lt;/p>
&lt;pre>&lt;code>s/TEL;HOME;PREF/TEL;HOME;VOICE/
t
s/TEL;HOME/TEL;HOME;VOICE/
t
s/TEL;CELL/TEL;HOME;CELL/
t
s/TEL;WORK;PREF/TEL;WORK;VOICE/
t
s/TEL;PREF;CELL/TEL;WORK;CELL/
t
s/TEL;PREF;FAX/TEL;WORK;FAX/
t
s/TEL;WORK/TEL;WORK;VOICE/
t
s/TEL;PREF/TEL;WORK;VOICE/
t
s/TEL;PAGER/TEL;WORK;PAGER/
t
&lt;/code>&lt;/pre>
&lt;p>Running this over all the contacts gives us the following list of distinct labels:&lt;/p>
&lt;pre>&lt;code>TEL;HOME;CELL
TEL;HOME;VOICE
TEL;WORK;CELL
TEL;WORK;FAX
TEL;WORK;PAGER
TEL;WORK;VOICE
&lt;/code>&lt;/pre>
&lt;h1 id="no-dashes-in-phone-numbers-really-lg">No dashes in phone numbers (really, LG?)&lt;/h1>
&lt;p>It turns out that the 420G does not accept anything other than digits in phone numbers. So, after processing the labels with the above sed script we fix up the data with the following awk script:&lt;/p>
&lt;pre>&lt;code># Set input and output field separators to &amp;quot;:&amp;quot;.
BEGIN {
FS=&amp;quot;:&amp;quot;
OFS=&amp;quot;:&amp;quot;
}
# Remove anything not a digit from the phone number.
/TEL;/ {
gsub(&amp;quot;[^0-9]&amp;quot;, &amp;quot;&amp;quot;, $2)
}
{
print
}
&lt;/code>&lt;/pre>
&lt;h1 id="and-were-off">And we&amp;rsquo;re off!&lt;/h1>
&lt;p>With these transformations in place, the LG 420G was able to successfully import the contacts. I automated the whole process with the following Makefile:&lt;/p>
&lt;pre>&lt;code>SED = gsed
AWK = awk
FILTERS = \
fix-tel-labels.sed \
remove-non-digits.awk
all: all-contacts-filtered.vcf
all-contacts-filtered.vcf: all-contacts-orig.vcf $(FILTERS)
$(SED) -f fix-tel-labels.sed $&amp;lt; | \
$(AWK) -f remove-non-digits.awk | \
unix2dos &amp;gt; $@
&lt;/code>&lt;/pre></content></item><item><title>Patch to gPXE dhcp command</title><link>https://blog.oddbit.com/post/2010-07-22-patch-to-gpxe-dhcp-command/</link><pubDate>Thu, 22 Jul 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-07-22-patch-to-gpxe-dhcp-command/</guid><description>Update: This patch has been accepted into gPXE.
I just released a patch to gPXE that modifies the dhcp command so that it can iterate over multiple interfaces. The stock dhcp command only accepts a single interface as an argument, which can be a problem if you are trying to boot on a machine with multiple interfaces. The builtin autoboot commands attempts to resolve this, but is only useful if you expect to receive appropriate boot parameters from your dhcp server.</description><content>&lt;p>&lt;strong>Update&lt;/strong>: This patch has been &lt;a href="http://git.etherboot.org/?p=gpxe.git;a=commit;h=fa91c2c3269554df855107a24afec9a1149fee8f">accepted&lt;/a> into gPXE.&lt;/p>
&lt;p>I just released a &lt;a href="http://gist.github.com/486907">patch&lt;/a> to &lt;a href="http://etherboot.org/wiki/index.php">gPXE&lt;/a> that modifies the dhcp command so that it can iterate over multiple interfaces. The stock dhcp command only accepts a single interface as an argument, which can be a problem if you are trying to boot on a machine with multiple interfaces. The builtin autoboot commands attempts to resolve this, but is only useful if you expect to receive appropriate boot parameters from your dhcp server.&lt;/p>
&lt;p>My patch extends the dhcp command in the following ways:&lt;/p>
&lt;ol>
&lt;li>It allows the &amp;ldquo;dhcp&amp;rdquo; command to accept a list of interfaces and to try them in order until it succeeds, e.g.:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>gPXE&amp;gt; dhcp net0 net1 net2
&lt;/code>&lt;/pre>
&lt;p>In order to preserve the original syntax of the command, this will fail on an unknown interface name:&lt;/p>
&lt;pre>&lt;code>gPXE&amp;gt; dhcp foo net0
No such interface: foo
gPXE&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The &amp;ldquo;-c&amp;rdquo; flag allows it to continue:&lt;/p>
&lt;pre>&lt;code>gPXE&amp;gt; dhcp -c foo net0
No such interface: foo
DHCP (net0 xx:xx:xx:xx:xx:xx).... ok
gPXE&amp;gt;
&lt;/code>&lt;/pre>
&lt;ol start="2">
&lt;li>If given the single parameter &amp;ldquo;any&amp;rdquo; as an interface name, iterate over all known interfaces in a manner similar to autoboot():&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>gPXE&amp;gt; dhcp any
DHCP (net0 xx:xx:xx:xx:xx:xx)........ Connection timed out (...)
Could not configure net0: Connection timed out (...)
DHCP (net1 xx:xx:xx:xx:xx:xx).... ok
gPXE&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>I think this manages to preserve the syntax of the existing &amp;ldquo;dhcp&amp;rdquo; command while making the magic of autoboot available to gpxe scripts.&lt;/p></content></item><item><title>Kerberos authenticated queries to Active Directory</title><link>https://blog.oddbit.com/post/2010-06-29-linux-kerberos-ad/</link><pubDate>Tue, 29 Jun 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-06-29-linux-kerberos-ad/</guid><description>There are many guides out there to help you configure your Linux system as an LDAP and Kerberos client to an Active Directory server. Most of these guides solve the problem of authentication by embedding a username and password into a configuration file somewhere on your system. While this works, it presents some problems:
If you use a common account for authentication from all of your Linux systems, a compromise on one system means updating the configuration of all of your systems.</description><content>&lt;p>There are many guides out there to help you configure your Linux system as an LDAP and Kerberos client to an Active Directory server. Most of these guides solve the problem of authentication by embedding a username and password into a configuration file somewhere on your system. While this works, it presents some problems:&lt;/p>
&lt;ul>
&lt;li>If you use a common account for authentication from all of your Linux systems, a compromise on one system means updating the configuration of all of your systems.&lt;/li>
&lt;li>If you don&amp;rsquo;t want to use a common account, you need to provision a new account for each computer&amp;hellip;&lt;/li>
&lt;li>&amp;hellip;which is silly, because if you join the system to Active Directory there is already a computer object associated with the system that can be used for authentication.&lt;/li>
&lt;/ul>
&lt;p>This document describes how to configure a Linux system such that queries
generated by &lt;a href="http://www.padl.com/OSS/nss_ldap.html">nss_ldap&lt;/a> will use either the current user&amp;rsquo;s Kerberos
credentials, or, for the root user, credentials stored in a Kerberos
credentials cache.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Your Linux system must have a valid &lt;code>keytab&lt;/code> file.&lt;/p>
&lt;p>A &lt;code>keytab&lt;/code> is a file containing pairs of Kerberos principals and encrypted keys.&lt;/p>
&lt;p>Joining Active Directory using Samba&amp;rsquo;s &lt;code>net ads join&lt;/code> will create the
necessary keytab. It is also possible to create the keytab on your Windows
domain controller and install it on your Linux systems. Instructions for
doing this are beyond the scope of this document.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Host objects in Active Directory must have a &lt;code>userPrincipalName&lt;/code> attribute.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;pre>&lt;code>$ ldapsearch cn=dottiness userPrincipalName
dn: CN=DOTTINESS,CN=Computers,dc=example,dc=com
userPrincipalName: host/dottiness.example.com@EXAMPLE.COM
&lt;/code>&lt;/pre>
&lt;p>Active Directory &lt;em>does not&lt;/em> automatically create a &lt;code>userPrincipalName&lt;/code> when a new host object is provisioned. You will either need to provide this value manually or develop an automated process that will populate this field when provisioning new host objects.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Kerberos credentials have a maximum usable lifetime. The cached credentials
used for root queries by &lt;code>nss_ldap&lt;/code> must be refreshed periodically in order to
function.&lt;/p>
&lt;p>You will need to install a crontab (e.g., in &lt;code>/etc/cron.d&lt;/code>) that looks something
like this:&lt;/p>
&lt;pre>&lt;code>PATH=/bin:/usr/bin:/usr/kerberos/bin
@reboot root kinit -k -c /var/run/ldap_cc &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
@hourly root kinit -k -c /var/run/ldap_cc &amp;gt; /dev/null 2&amp;gt;&amp;amp;1
&lt;/code>&lt;/pre>
&lt;p>This periodically reauthenticates to your domain controller used the cached
principal in the system keytab (&lt;code>/etc/krb5.keytab&lt;/code>) and caches the credentials in
&lt;code>/var/run/ldap_cc&lt;/code>.&lt;/p>
&lt;p>You will need something similar to the following in &lt;code>/etc/ldap.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code># This is your domain controller.
uri ldap://dc1.example.com
base dc=example,dc=com
scope one
referrals no
timelimit 120
bind_timelimit 120
idle_timelimit 3600
ldap_version 3
# Authenticate using SASL for user and root queries.
use_sasl on
rootuse_sasl on
# Use SASL's gssapi (Kerberos) mechanism.
sasl_mech gssapi
# Use these cached credentials for root.
krb5_ccname /var/run/ldap_cc
nss_base_group ou=groups,dc=example,dc=com
nss_base_passwd ou=people,dc=example,dc=com
nss_initgroups_ignoreusers root,ldap,named,avahi,haldaemon,dbus,radvd,tomcat,radiusd,news,mailman,nscd,gdm,polkituser
# These are common mappings for working with Active Directory.
nss_map_attribute uid sAMAccountName
nss_map_attribute uniqueMember member
nss_map_objectclass posixAccount user
nss_map_objectclass posixGroup group
nss_map_objectclass shadowAccount user
pam_login_attribute sAMAccountName
pam_member_attribute member
pam_password ad
pam_password_prohibit_message Please visit http://password.example.com to change your password.
pam_filter objectclass=User
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>use_sasl on&lt;/code> directive configures &lt;code>nss_ldap&lt;/code> to use the Kerberos credentials
for the current user when looking up user/group/etc information. The
&lt;code>rootuse_sasl on&lt;/code> attribute does the same thing for processes running as &lt;code>root&lt;/code>.&lt;/p>
&lt;p>Note that this configuration sets scope &lt;code>one&lt;/code>, which means that &lt;code>nss_ldap&lt;/code> &lt;em>will
not&lt;/em> recurse down a directory tree. This is a performance optimization, not a
requirement.&lt;/p>
&lt;h2 id="as-an-unprivileged-user">As an unprivileged user&lt;/h2>
&lt;p>Before acquiring Kerberos credentials:&lt;/p>
&lt;pre>&lt;code>$ getent passwd lars
(times out)
&lt;/code>&lt;/pre>
&lt;p>Authenticate to Kerberos:&lt;/p>
&lt;pre>&lt;code>$ kinit
Password for lars@EXAMPLE.COM:
&lt;/code>&lt;/pre>
&lt;p>With valid credentials:&lt;/p>
&lt;pre>&lt;code>$ getent passwd lars
lars:*:500:500:lars:\\emc00.example.com\staff\l\lars\windows:
&lt;/code>&lt;/pre>
&lt;h2 id="as-root">As root&lt;/h2>
&lt;p>Before acquiring Kerberos credentials:&lt;/p>
&lt;pre>&lt;code># getent passwd lars
(times out)
&lt;/code>&lt;/pre>
&lt;p>Update credentials cache from system keytab:&lt;/p>
&lt;pre>&lt;code># kinit -k
&lt;/code>&lt;/pre>
&lt;p>With valid credentials:&lt;/p>
&lt;pre>&lt;code># getent passwd lars
lars:*:500:500:lars:\\emc00.example.com\staff\l\lars\windows:
&lt;/code>&lt;/pre>
&lt;p>This configuration makes the operation of &lt;code>nss_ldap&lt;/code> dependent on valid Kerberos
credentials. If a user remains logged in after her Kerberos credentials have
expired, she will experience degraded behavior, since many name lookup
operations will timeout. Similarly, local system accounts that do not have
valid Kerberos credentials will experience similar behavior (and will thus only
be able to see local users and groups).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted --></content></item><item><title>Pushing a Git repository to Subversion</title><link>https://blog.oddbit.com/post/2010-05-11-pushing-git-repository-to-subv/</link><pubDate>Tue, 11 May 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-05-11-pushing-git-repository-to-subv/</guid><description>I recently set up a git repository server (using gitosis and gitweb). Among the required features of the system was the ability to publish the git repository to a read-only Subversion repository. This sounds simple in principle but in practice proved to be a bit tricky.
Git makes an excellent Subversion client. You can use the git svn &amp;hellip; series of commands to pull a remote Subversion repository into a local git working tree and then have all the local advantages of git forcing the central code repository to change version control software.</description><content>&lt;p>I recently set up a git repository server (using &lt;a href="http://scie.nti.st/2007/11/14/hosting-git-repositories-the-easy-and-secure-way">gitosis&lt;/a> and &lt;a href="https://git.wiki.kernel.org/index.php/Gitweb">gitweb&lt;/a>). Among the required features of the system was the ability to publish the git repository to a read-only Subversion repository. This sounds simple in principle but in practice proved to be a bit tricky.&lt;/p>
&lt;p>Git makes an excellent Subversion client. You can use the git svn &amp;hellip; series of commands to pull a remote Subversion repository into a local git working tree and then have all the local advantages of git forcing the central code repository to change version control software. An important aspect of this model is that:&lt;/p>
&lt;ul>
&lt;li>The Subversion repository is the primary source of the code, and&lt;/li>
&lt;li>You populate your local git repository by pulling from the remote Subversion repository.&lt;/li>
&lt;/ul>
&lt;p>It is possible to push a git change history into an empty Subversion repository. Most instructions for importing a git repository look something like this, and involve replaying your git change history on top of the Subversion change history:&lt;/p>
&lt;ul>
&lt;li>svn mkdir $REPO/{trunk, tags, branches}&lt;/li>
&lt;li>git svn init -s $REPO&lt;/li>
&lt;li>git svn fetch&lt;/li>
&lt;li>git rebase trunk&lt;/li>
&lt;li>git svn dcommit&lt;/li>
&lt;/ul>
&lt;p>This works, and is fine as long as there are no other clones of your git repository out there. The mechanism outlined here has a fatal flaw: it modifies the change history of the &lt;em>master&lt;/em> branch. If you were working in a clone of a remote git repository and you were to run git status after the above steps, you would see something like:&lt;/p>
&lt;pre>&lt;code># On branch master
# Your branch and 'origin/master' have diverged,
# and have 3 and 2 different commit(s) each, respectively.
&lt;/code>&lt;/pre>
&lt;p>If you were then to try to push this to the remote repository, you would get an error:&lt;/p>
&lt;pre>&lt;code>$ git push
To .../myrepo:
! [rejected] master -&amp;gt; master (non-fast forward)
error: failed to push some refs to '.../myrepo'
&lt;/code>&lt;/pre>
&lt;p>In cases where the git change history is shared with other git repositories, we need a solution that does not modify the &lt;em>master&lt;/em> branch. We can get this my modifying the procedure slightly.&lt;/p>
&lt;p>The initial sequence is still the same:&lt;/p>
&lt;ul>
&lt;li>svn mkdir $REPO/{trunk, tags, branches}&lt;/li>
&lt;li>git svn init -s $REPO&lt;/li>
&lt;li>git svn fetch&lt;/li>
&lt;/ul>
&lt;p>But instead of rebasing onto the &lt;em>master&lt;/em> branch, we create a local branch for managing the synchronization:&lt;/p>
&lt;ul>
&lt;li>git checkout -b svnsync&lt;/li>
&lt;li>git rebase trunk&lt;/li>
&lt;li>git svn dcommit&lt;/li>
&lt;/ul>
&lt;p>At this point we have changed the history of the &lt;em>svnsync&lt;/em> branch and we have left the &lt;em>master&lt;/em> branch untouched. Subsequent updates look like this:&lt;/p>
&lt;ul>
&lt;li>git checkout master&lt;/li>
&lt;li>git pull&lt;/li>
&lt;li>git checkout svnsync&lt;/li>
&lt;li>git rebase master&lt;/li>
&lt;li>git rebase trunk&lt;/li>
&lt;li>git svn dcommit&lt;/li>
&lt;/ul>
&lt;p>This gives us what we want: we can publish our git repository to a Subversion repository while maintaining the shared change history among our existing git clones.&lt;/p></content></item><item><title>LDAP redundancy through proxy servers</title><link>https://blog.oddbit.com/post/2010-02-24-ldap-redundancy-through-proxy-/</link><pubDate>Wed, 24 Feb 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-02-24-ldap-redundancy-through-proxy-/</guid><description>Problem 1: Failover The problem Many applications only allow you to configure a single LDAP server. This can lead to unnecessary service outages if your directory service infrastructure is highly available (e.g., you are running Active Directory) and your application cannot take advantage of this fact.
A solution We can provide a level of redundancy by passing the LDAP connections through a load balancing proxy. While this makes the proxy a single point of failure, it is (a) a very simple tool and thus less prone to complex failure modes, (b) running on the same host as the web application, and (c) is completely under our control.</description><content>&lt;h1 id="problem-1-failover">Problem 1: Failover&lt;/h1>
&lt;h2 id="the-problem">The problem&lt;/h2>
&lt;p>Many applications only allow you to configure a single LDAP server. This can lead to unnecessary service outages if your directory service infrastructure is highly available (e.g., you are running Active Directory) and your application cannot take advantage of this fact.&lt;/p>
&lt;h2 id="a-solution">A solution&lt;/h2>
&lt;p>We can provide a level of redundancy by passing the LDAP connections through a load balancing proxy. While this makes the proxy a single point of failure, it is (a) a very simple tool and thus less prone to complex failure modes, (b) running on the same host as the web application, and (c) is completely under our control.&lt;/p>
&lt;p>For this example, I will use &lt;a href="http://www.inlab.de/balance.html">Balance&lt;/a>, a simple TCP load balancer from &lt;a href="http://www.inlab.de/">Inlab Software GmbH&lt;/a>. There are packages available for most major Linux distributions, including &lt;a href="http://fedoraproject.org/">Fedora&lt;/a> and &lt;a href="http://www.centos.org/">CentOS&lt;/a>.&lt;/p>
&lt;p>Balance is configured completely on the command line. To provide round-robin access to a suite of three directory servers running LDAP over SSL, you might use the following command line:&lt;/p>
&lt;pre>&lt;code>balance -b 127.0.0.1 636 10.1.1.1 10.1.1.2
&lt;/code>&lt;/pre>
&lt;p>Using balance&amp;rsquo;s terminology, this creates one &lt;em>group&lt;/em> of two &lt;em>channels&lt;/em>. Balance will round-robin among the channels in this group. Note that here and in subsequent examples we are binding the proxy to the loopback interface so that it is only available to applications running on the same host.&lt;/p>
&lt;p>If you would prefer to preferentially send all the requests to the first server, and only use the second server if the first is unavailable, you could use a configuration like this:&lt;/p>
&lt;pre>&lt;code>balance -b 127.0.0.1 636 10.1.1.1 \! 10.1.1.2
&lt;/code>&lt;/pre>
&lt;p>While you can run balance from a standard init (/etc/rc.d/&amp;hellip;) script, I prefer to use a service manager such as &lt;a href="http://smarden.org/runit/">runit&lt;/a> which takes care of restarting the service if it should exit unexpectedly. You could achieve the same thing in a slightly less flexible fashion by putting your balance command line in /etc/inittab. In either case you need to add the -f option to the command line, which causes balance to stay in the foreground.&lt;/p>
&lt;h1 id="problem-2-debugging-ldap-over-ssl">Problem 2: Debugging LDAP over SSL&lt;/h1>
&lt;h2 id="the-problem-1">The problem&lt;/h2>
&lt;p>It is convenient to use a packet tracer such as &lt;a href="http://www.wireshark.org/">Wireshark&lt;/a> to debug LDAP protocol errors. This is often more informative than the debugging information that will be available to you on the client side, and may be more useful than server side debugging in many cases, even supposing that you have administrative access to the directory servers.&lt;/p>
&lt;h2 id="a-solution-1">A solution&lt;/h2>
&lt;p>You can use &lt;a href="http://www.stunnel.org/">Stunnel&lt;/a>, a general purpose SSL proxy tool, to intercept unencrypted client connections on the local machine and then forward them over an SSL channel to a remote server. This makes the unencrypted LDAP traffic available on the loopback interface while still ensuring that it is encrypted on the wire.&lt;/p>
&lt;p>Stunnel can operate both as an SSL server and as an SSL client. In this case, we will be running it in client mode, connecting to a remote SSL server (or to the proxy configured in our previous example). Stunnel is configured by means of a simple INI-style configuration file. To achieve the goals of this example we might put the following configuration in a file (say, stunnel.conf):&lt;/p>
&lt;pre>&lt;code>[ldap]
accept = 127.0.0.1:389
client = yes
connect = localhost:636
&lt;/code>&lt;/pre>
&lt;p>We would run stunnel like this:&lt;/p>
&lt;pre>&lt;code>stunnel /path/to/stunnel.conf
&lt;/code>&lt;/pre>
&lt;p>Again, I would run this under the control of a service supervisor. To keep stunnel in the foreground we would add the following to the global section of the configuration file (i.e., before the &lt;code>[ldap]&lt;/code> section marker):&lt;/p>
&lt;pre>&lt;code>foreground = yes
&lt;/code>&lt;/pre>
&lt;p>With both of these solutions in place, we have achieved the following:&lt;/p>
&lt;ul>
&lt;li>High availability.&lt;/li>
&lt;/ul>
&lt;p>Our application will transparently make use of multiple directory servers. If a server fails, our application will continue to operate.&lt;/p>
&lt;ul>
&lt;li>Security&lt;/li>
&lt;/ul>
&lt;p>Our traffic is encrypted on the wire, regardless of whether the application has support for LDAP over SSL.&lt;/p>
&lt;ul>
&lt;li>Visibility&lt;/li>
&lt;/ul>
&lt;p>We are free to examine unencrypted traffic with a packet sniffer running on the local host.&lt;/p></content></item><item><title>Apache virtual host statistics</title><link>https://blog.oddbit.com/post/2010-02-19-apache-virtual-host-statistics/</link><pubDate>Fri, 19 Feb 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-02-19-apache-virtual-host-statistics/</guid><description>As part of a project I&amp;rsquo;m working on I wanted to get a rough idea of the activity of the Apache virtual hosts on the system. I wasn&amp;rsquo;t able to find exactly what I wanted, so I refreshed my memory of curses to bring you vhoststats.
This tools reads an Apache log file (with support for arbitrary formats) and generates a dynamic bar chart showing the activity (in number of requests and bytes transferred) of hosts on the system.</description><content>&lt;p>As part of a project I&amp;rsquo;m working on I wanted to get a rough idea of the activity of the Apache virtual hosts on the system. I wasn&amp;rsquo;t able to find exactly what I wanted, so I refreshed my memory of curses to bring you &lt;em>vhoststats&lt;/em>.&lt;/p>
&lt;p>This tools reads an Apache log file (with support for arbitrary formats) and generates a dynamic bar chart showing the activity (in number of requests and bytes transferred) of hosts on the system. The output might look something like this (but with colors):&lt;/p>
&lt;pre>&lt;code>[2010/02/19 20:21:32] Hosts: 7 [Displayed: 7] Requests: 104
host1.companyA.com [R:1 ] #
[B:3 ]
devel.internal [R:1 ] #
[B:208 ]
host2.companyA.com [R:1 ] #
[B:4499 ]
A-truncated-host-nam [R:10 ] ############
[B:65380 ] #
host1.companyB.com [R:21 ] ##########################
[B:166715 ] ####
www.google.com [R:32 ] #################################
[B:1566614 ] ####################################
&lt;/code>&lt;/pre>
&lt;p>The tool keeps running totals over a five minute window, but you can change the window size on the command line. You can tail your active access log to see live results, or for a more exciting display you can just pipe in an existing log.&lt;/p>
&lt;p>It&amp;rsquo;s not &lt;a href="http://code.google.com/p/logstalgia/">pong&lt;/a>, but I&amp;rsquo;ve found it useful.&lt;/p>
&lt;p>You can download the code from the &lt;a href="http://github.com/larsks/vhoststats/">project page&lt;/a> on GitHub.&lt;/p></content></item><item><title>Merging directories with OpenLDAP's Meta backend</title><link>https://blog.oddbit.com/post/2010-02-16-merging-directories-with-openl/</link><pubDate>Tue, 16 Feb 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-02-16-merging-directories-with-openl/</guid><description>This document provides an example of using OpenLDAP&amp;rsquo;s meta backend to provide a unified view of two distinct LDAP directory trees. I was frustrated by the lack of simple examples available when I went looking for information on this topic, so this is my attempt to make life easier for the next person looking to do the same thing.
The particular use case that motiviated my interest in this topic was the need to configure web applications to (a) authenticate against an existing Active Directory server while (b) also allowing new accounts to be provisioned quickly and without granting any access in the AD environment.</description><content>&lt;p>This document provides an example of using OpenLDAP&amp;rsquo;s meta backend to provide a unified view of two distinct LDAP directory trees. I was frustrated by the lack of simple examples available when I went looking for information on this topic, so this is my attempt to make life easier for the next person looking to do the same thing.&lt;/p>
&lt;p>The particular use case that motiviated my interest in this topic was the need to configure web applications to (a) authenticate against an existing Active Directory server while (b) also allowing new accounts to be provisioned quickly and without granting any access in the AD environment. A complicating factor is that the group managing the AD server(s) was not the group implementing the web applications.&lt;/p>
&lt;h1 id="assumptions">Assumptions&lt;/h1>
&lt;p>I&amp;rsquo;m making several assumptions while writing this document:&lt;/p>
&lt;ul>
&lt;li>You have root access on your system and are able to modify files in /etc/openldap and elsewhere on the filesystem.&lt;/li>
&lt;li>You are somewhat familiar with LDAP.&lt;/li>
&lt;li>You are somewhat familiar with OpenLDAP.&lt;/li>
&lt;/ul>
&lt;h1 id="set-up-backend-directories">Set up backend directories&lt;/h1>
&lt;h2 id="configure-slapd">Configure slapd&lt;/h2>
&lt;p>We&amp;rsquo;ll first create two &amp;ldquo;backend&amp;rdquo; LDAP directories. These will represent the directories you&amp;rsquo;re trying to merge. For the purposes of this example we&amp;rsquo;ll use the ldif backend, which stores data in LDIF format on the filesystem. This is great for testing (it&amp;rsquo;s simple and easy to understand), but not so great for performance.&lt;/p>
&lt;p>We define one backend like this in /etc/openldap/slapd-be-1.conf:&lt;/p>
&lt;pre>&lt;code>database ldif
suffix &amp;quot;ou=backend1&amp;quot;
directory &amp;quot;/var/lib/ldap/backend1&amp;quot;
rootdn &amp;quot;cn=ldif-admin,ou=backend1&amp;quot;
rootpw &amp;quot;LDIF&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And a second backend like this in /etc/openldap/slapd-be-2.conf:&lt;/p>
&lt;pre>&lt;code>database ldif
suffix &amp;quot;ou=backend2&amp;quot;
directory &amp;quot;/var/lib/ldap/backend2&amp;quot;
rootdn &amp;quot;cn=ldif-admin,ou=backend2&amp;quot;
rootpw &amp;quot;LDIF&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Now, we need to load these configs into the main slapd configuration file. Open slapd.conf, and look for the following comment:&lt;/p>
&lt;pre>&lt;code>#######################################################################
# ldbm and/or bdb database definitions
#######################################################################
&lt;/code>&lt;/pre>
&lt;p>Remove anything below this comment and then add the following lines:&lt;/p>
&lt;pre>&lt;code>include /etc/openldap/slapd-be-1.conf
include /etc/openldap/slapd-be-2.conf
&lt;/code>&lt;/pre>
&lt;h2 id="start-up-slapd">Start up slapd&lt;/h2>
&lt;p>Start up your LDAP service:&lt;/p>
&lt;pre>&lt;code># slapd -f slapd.conf -h ldap://localhost/
&lt;/code>&lt;/pre>
&lt;p>And check to make sure it&amp;rsquo;s running:&lt;/p>
&lt;pre>&lt;code># ps -fe | grep slapd
root 15087 1 0 22:48 ? 00:00:00 slapd -f slapd.conf -h ldap://localhost/
&lt;/code>&lt;/pre>
&lt;h2 id="populate-backends-with-sample-data">Populate backends with sample data&lt;/h2>
&lt;p>We need to populate the directories with something to query.&lt;/p>
&lt;p>Put this in backend1.ldif:&lt;/p>
&lt;pre>&lt;code>dn: ou=backend1
objectClass: top
objectClass: organizationalUnit
ou: backend1
dn: ou=people,ou=backend1
objectClass: top
objectClass: organizationalUnit
ou: people
dn: cn=user1,ou=people,ou=backend1
objectClass: inetOrgPerson
cn: user1
givenName: user1
sn: Somebodyson
mail: user1@example.com
&lt;/code>&lt;/pre>
&lt;p>And this in backend2.ldif:&lt;/p>
&lt;pre>&lt;code>dn: ou=backend2
objectClass: top
objectClass: organizationalUnit
ou: backend2
dn: ou=people,ou=backend2
objectClass: top
objectClass: organizationalUnit
ou: people
dn: cn=user2,ou=people,ou=backend2
objectClass: inetOrgPerson
cn: user2
givenName: user2
sn: Somebodyson
mail: user2@example.com
&lt;/code>&lt;/pre>
&lt;p>And then load the data into the backends:&lt;/p>
&lt;pre>&lt;code>ldapadd -x -H ldap://localhost -D cn=ldif-admin,ou=backend1 \
-w LDIF -f backend1.ldif
&lt;/code>&lt;/pre>
&lt;p>And:&lt;/p>
&lt;pre>&lt;code>ldapadd -x -H ldap://localhost -D cn=ldif-admin,ou=backend2 \
-w LDIF -f backend2.ldif
&lt;/code>&lt;/pre>
&lt;p>You can verify that the data loaded correctly by issuing a query to the backends. E.g.:&lt;/p>
&lt;pre>&lt;code>ldapsearch -x -H ldap://localhost -b ou=backend1 -LLL
&lt;/code>&lt;/pre>
&lt;p>This should give you something that looks very much like the contents of backend1.ldif. You can do the same thing for backend2.&lt;/p>
&lt;h1 id="set-up-meta-database">Set up meta database&lt;/h1>
&lt;p>We&amp;rsquo;re now going to configure OpenLDAP&amp;rsquo;s meta backend to merge the two directory trees. Complete documentation for the meta backend can be found in the &lt;a href="http://www.openldap.org/software/man.cgi?query=slapd-meta&amp;amp;apropos=0&amp;amp;sektion=0&amp;amp;manpath=OpenLDAP+2.4-Release&amp;amp;format=html">slapd-meta man page&lt;/a>.&lt;/p>
&lt;p>Put the following into a file called slapd-frontend.conf (we&amp;rsquo;ll discuss the details in moment):&lt;/p>
&lt;pre>&lt;code>database meta
suffix &amp;quot;dc=example,dc=com&amp;quot;
uri &amp;quot;ldap://localhost/ou=backend1,dc=example,dc=com&amp;quot;
suffixmassage &amp;quot;ou=backend1,dc=example,dc=com&amp;quot; &amp;quot;ou=backend1&amp;quot;
uri &amp;quot;ldap://localhost/ou=backend2,dc=example,dc=com&amp;quot;
suffixmassage &amp;quot;ou=backend2,dc=example,dc=com&amp;quot; &amp;quot;ou=backend2&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And then add to slapd.conf:&lt;/p>
&lt;pre>&lt;code>include /etc/openldap/slapd-frontend.conf
&lt;/code>&lt;/pre>
&lt;p>Restart slapd. Let&amp;rsquo;s do a quick search to see exactly what we&amp;rsquo;ve accomplished:&lt;/p>
&lt;pre>&lt;code>$ ldapsearch -x -H 'ldap://localhost/' \
-b dc=example,dc=com objectclass=inetOrgPerson -LLL
dn: cn=user1,ou=people,ou=backend1,dc=example,dc=com
objectClass: inetOrgPerson
cn: user1
givenName: user1
sn: Somebodyson
mail: user1@example.com
dn: cn=user2,ou=people,ou=backend2,dc=example,dc=com
objectClass: inetOrgPerson
cn: user2
givenName: user2
sn: Somebodyson
mail: user2@example.com
&lt;/code>&lt;/pre>
&lt;p>As you can see from the output above, a single query is now returning results from both backends, merged into the dc=example,dc=com hierarchy.&lt;/p>
&lt;h2 id="a-closer-look">A closer look&lt;/h2>
&lt;p>Let&amp;rsquo;s take a closer look at the meta backend configuration.&lt;/p>
&lt;pre>&lt;code>database meta
suffix &amp;quot;dc=example,dc=com&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The database statement begins a new database definition. The suffix statement identifies the namespace that will be served by this particular database.&lt;/p>
&lt;p>Here is the proxy for backend1 (the entry for backend2 is virtually identical):&lt;/p>
&lt;pre>&lt;code>uri &amp;quot;ldap://localhost/ou=backend1,dc=example,dc=com&amp;quot;
suffixmassage &amp;quot;ou=backend1,dc=example,dc=com&amp;quot; &amp;quot;ou=backend1&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The uri statement defines the host (and port) serving the target directory tree. The full syntax of the uri statement is described in the &lt;a href="http://www.openldap.org/software/man.cgi?query=slapd-meta&amp;amp;apropos=0&amp;amp;sektion=0&amp;amp;manpath=OpenLDAP+2.4-Release&amp;amp;format=html">slapd-meta man page&lt;/a>; what we have here is a very simple example. The &lt;em>naming context&lt;/em> of the URI must fall within the namespace defined in the suffix statement at the beginning of the database definition.&lt;/p>
&lt;p>The suffixmassage statement performs simple rewriting on distinguished names. It directs &lt;em>slapd&lt;/em> to replace ou=backend1,dc=example,dc=com with ou=backend1 when communicating with the backend directory (and vice-versa).&lt;/p>
&lt;p>You can perform simple rewriting of attribute and object classes with the map statement. For example, if backend1 used a sAMAccountName attribute and our application was expecting a uid attribute, we could add this after the suffixmassage statement:&lt;/p>
&lt;pre>&lt;code>map attribute uid sAMAccountName
&lt;/code>&lt;/pre>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>The sample configuration files, data, and code referenced in this post are available online in &lt;a href="http://github.com/larsks/OpenLDAP-Metadirectory-Example">a github repository&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="http://github.com/larsks/OpenLDAP-Metadirectory-Example">http://github.com/larsks/OpenLDAP-Metadirectory-Example&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>I hope you&amp;rsquo;ve found this post useful, or at least informative. If you have any comments or questions regarding this post, please log them as issues on GitHub. This will make them easier for me to track.&lt;/p></content></item><item><title>Filtering Blogger feeds</title><link>https://blog.oddbit.com/post/2010-02-10-filtering-blogger-feeds/</link><pubDate>Wed, 10 Feb 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-02-10-filtering-blogger-feeds/</guid><description>After encountering a number of problems trying to filter Blogger feeds by tag (using services like Feedrinse and Yahoo Pipes), I&amp;rsquo;ve finally put together a solution that works:
Shadow the feed with Feedburner. Enable the Convert Format Burner, and convert your feed to RSS 2.0. Use Yahoo Pipes to filter the feed (because Feedrinse seems to be broken). This let me create a feed that excluded all my posts containing the fbpost tag, thus allowing me to avoid yet another postgasm in Facebook when adding new import URL to notes.</description><content>&lt;p>After encountering a number of problems trying to filter Blogger feeds by tag (using services like &lt;a href="http://feedrinse.com/">Feedrinse&lt;/a> and Yahoo &lt;a href="http://pipes.yahoo.com/">Pipes&lt;/a>), I&amp;rsquo;ve finally put together a solution that works:&lt;/p>
&lt;ul>
&lt;li>Shadow the feed with &lt;a href="http://feedburner.com/">Feedburner&lt;/a>.&lt;/li>
&lt;li>Enable the &lt;em>Convert Format Burner&lt;/em>, and convert your feed to RSS 2.0.&lt;/li>
&lt;li>Use Yahoo &lt;a href="http://pipes.yahoo.com/">Pipes&lt;/a> to filter the feed (because Feedrinse seems to be broken).&lt;/li>
&lt;/ul>
&lt;p>This let me create a feed that excluded all my posts containing the &lt;em>fbpost&lt;/em> tag, thus allowing me to avoid yet another postgasm in Facebook when adding new import URL to notes.&lt;/p>
&lt;p>While fiddling with this I came across &lt;a href="http://www.tothepc.com/archives/10-tools-to-combine-mix-blend-multiple-rss-feeds/">this article&lt;/a> that discusses a number of tools (some no longer available) for processing RSS feeds.&lt;/p></content></item><item><title>Funny usage message</title><link>https://blog.oddbit.com/post/2010-02-08-funny-usage-message/</link><pubDate>Mon, 08 Feb 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-02-08-funny-usage-message/</guid><description>I was poking around in a command shell on my Droid to see what was available. While it&amp;rsquo;s a pretty restricted environment, there&amp;rsquo;s a number of commands available in /system/bin, including dexopt.
Apparently dexopt isn&amp;rsquo;t something I&amp;rsquo;m supposed to poke at:
$ dexopt Usage: don't use this Hah.</description><content>&lt;p>I was poking around in a command shell on my Droid to see what was available. While it&amp;rsquo;s a pretty restricted environment, there&amp;rsquo;s a number of commands available in /system/bin, including dexopt.&lt;/p>
&lt;p>Apparently dexopt isn&amp;rsquo;t something I&amp;rsquo;m supposed to poke at:&lt;/p>
&lt;pre>&lt;code>$ dexopt
Usage: don't use this
&lt;/code>&lt;/pre>
&lt;p>Hah.&lt;/p></content></item><item><title>MBTA realtime XML feed</title><link>https://blog.oddbit.com/post/2010-02-07-mbta-realtime-xml-feed/</link><pubDate>Sun, 07 Feb 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-02-07-mbta-realtime-xml-feed/</guid><description>The MBTA has a trial web service interface that provides access to realtime location information for select MBTA buses, as well as access to route information, arrival prediction, and other features. More information can be found here:
http://www.eot.state.ma.us/developers/realtime/
The service is provided by NextBus, which specializes in real-time location information for public transit organizations. The API (sorry, PDF) is very simple and does not require any sort of advance registration.</description><content>&lt;p>The &lt;a href="http://mbta.com/">MBTA&lt;/a> has a trial web service interface that provides access to realtime location information for select MBTA buses, as well as access to route information, arrival prediction, and other features. More information can be found here:&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="http://www.eot.state.ma.us/developers/realtime/">http://www.eot.state.ma.us/developers/realtime/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>The service is provided by &lt;a href="http://www.nextbus.com/">NextBus&lt;/a>, which specializes in real-time location information for public transit organizations. The &lt;a href="http://www.eot.state.ma.us/developers/downloads/MBTA_XML_Feed_Trial_Docs_13Nov09.pdf">API&lt;/a> (sorry, PDF) is very simple and does not require any sort of advance registration.&lt;/p>
&lt;p>At the moment, the service only provides coverage for a small number of routes (39, 111, 114, 116, 117). I hope they expand the coverage of this service in the near future!&lt;/p></content></item><item><title>Blocking VNC with iptables</title><link>https://blog.oddbit.com/post/2010-02-04-vnc-blockingrst/</link><pubDate>Thu, 04 Feb 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-02-04-vnc-blockingrst/</guid><description>VNC clients use the RFB protocol to provide virtual display capabilities. The RFB protocol, as implemented by most clients, provides very poor authentication options. While passwords are not actually sent &amp;ldquo;in the clear&amp;rdquo;, it is possible to brute force them based on information available on the wire. The RFB 3.x protocol limits passwords to a maximum of eight characters, so the potential key space is relatively small.
It&amp;rsquo;s possible to securely connect to a remote VNC server by tunneling your connection using ssh port forwarding (or setting up some sort of SSL proxy).</description><content>&lt;p>VNC clients use the &lt;a href="http://www.realvnc.com/docs/rfbproto.pdf">RFB protocol&lt;/a> to provide virtual display capabilities. The RFB protocol, as implemented by most clients, provides very poor authentication options. While passwords are not actually sent &amp;ldquo;in the clear&amp;rdquo;, it is possible to brute force them based on information available on the wire. The RFB 3.x protocol limits passwords to a maximum of eight characters, so the potential key space is relatively small.&lt;/p>
&lt;p>It&amp;rsquo;s possible to securely connect to a remote VNC server by tunneling your connection using ssh port forwarding (or setting up some sort of SSL proxy). However, while this ameliorates the password problem, it still leaves a VNC server running that, depending on the local system configuration, may accept connections from all over the world. This leaves open the possibility that someone could brute force the password and gain access to the systsem. The problem is exacerbated if a user is running a passwordless VNC session.&lt;/p>
&lt;p>My colleague and I looked into the options for blocking VNC connections using layer 7 packet classification. This means identifying the protocol in use by inspecting packet payloads, rather than relying exclusively on port numbers (this prevents clever or malicious users from circumventing the restrictions by running a service on a non-standard port). Unfortunately, the actual &lt;a href="http://l7-filter.sourceforge.net/">l7 netfilter module&lt;/a> is not available in CentOS (or Fedora). But wait, all is not lost!&lt;/p>
&lt;p>First, a brief digression into the RFB protocol used by VNC. After completing a standard TCP handshake, the client and server engage in a RFB handshake. The server first sents the string &amp;ldquo;RFB &amp;quot; followed by the RFB protocol version supported by the server. The client responds with a similar message.&lt;/p>
&lt;p>The initial handshake packet from the server:&lt;/p>
&lt;pre>&lt;code>0000 00 00 0c 07 ac 34 00 21 86 14 e8 aa 08 00 45 00 .....4.!......E.
0010 00 40 e8 b7 40 00 40 06 b6 51 8c f7 34 e0 62 76 .@..@.@..Q..4.bv
0020 77 61 17 0d da ad ae 06 16 3f 22 48 92 cc 80 18 wa.......?&amp;quot;H....
0030 00 5b 9b e1 00 00 01 01 08 0a e8 b1 fe 88 24 f1 .[............$.
0040 e3 56 52 46 42 20 30 30 33 2e 30 30 38 0a .VRFB 003.008.
&lt;/code>&lt;/pre>
&lt;p>And the response from the client:&lt;/p>
&lt;pre>&lt;code>0000 00 21 86 14 e8 aa 00 1a 30 4d 0c 00 08 00 45 40 .!......0M....E@
0010 00 40 e7 15 40 00 34 06 c3 b3 62 76 77 61 8c f7 .@..@.4...bvwa..
0020 34 e0 da ad 17 0d 22 48 92 cc ae 06 16 4b 80 18 4.....&amp;quot;H.....K..
0030 ff ff 20 56 00 00 01 01 08 0a 24 f1 e3 57 e8 b1 .. V......$..W..
0040 fe 88 52 46 42 20 30 30 33 2e 30 30 38 0a ..RFB 003.008.
&lt;/code>&lt;/pre>
&lt;p>Ergo: if we can match the string &amp;ldquo;RFB &amp;quot; at the beginning of the TCP payload on inbound packets, we have a reliable way of blocking VNC packets ergardless of port.&lt;/p>
&lt;p>Looking through the iptables man page, we find:&lt;/p>
&lt;pre>&lt;code>u32
U32 tests whether quantities of up to 4 bytes extracted from
a packet have specified values. The specification of what to
extract is general enough to find data at given offsets from
tcp headers or payloads.
&lt;/code>&lt;/pre>
&lt;p>This looks especially appropriate, since our target match is exactly four bytes. Unfortunately, the syntax of the u32 module is a little baroque:&lt;/p>
&lt;pre>&lt;code>Example:
match IP packets with total length &amp;gt;= 256
The IP header contains a total length field in bytes 2-3.
--u32 &amp;quot;0 &amp;amp; 0xFFFF = 0x100:0xFFFF&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Fortunately, the internet is our friend:&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="http://www.stearns.org/doc/iptables-u32.v0.1.7.html">http://www.stearns.org/doc/iptables-u32.v0.1.7.html&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>This document provides a number of recipes designed for use with u32 module, including one that matches content at the beginning of the TCP payload. This gives us, ultimately:&lt;/p>
&lt;pre>&lt;code>iptables -A INPUT -p tcp \
-m connbytes --connbytes 0:1024 \
--connbytes-dir both --connbytes-mode bytes \
-m state --state ESTABLISHED \
-m u32 --u32 &amp;quot;0&amp;gt;&amp;gt;22&amp;amp;0x3C@ 12&amp;gt;&amp;gt;26&amp;amp;0x3C@ 0=0x52464220&amp;quot; \
-j REJECT --reject-with tcp-reset
&lt;/code>&lt;/pre>
&lt;p>This means:&lt;/p>
&lt;ul>
&lt;li>Match tcp packets only (-p tcp)&lt;/li>
&lt;li>Match only during the first 1024 bytes of the connection (-m connbytes &amp;ndash;connbytes 0:1024 &amp;ndash;connbytes-dir both &amp;ndash;connbytes-mode bytes)&lt;/li>
&lt;li>Match only ESTABLISHED connections (-m state &amp;ndash;state ESTABLISHED)&lt;/li>
&lt;li>Match bytes &amp;ldquo;0x52464240&amp;rdquo; (&amp;ldquo;RFB &amp;ldquo;) at the beginning of the TCP payload (-m u32 &amp;ndash;u32 &amp;ldquo;0&amp;raquo;22&amp;amp;0x3C@ 12&amp;raquo;26&amp;amp;0x3C@ 0=0x52464220&amp;rdquo;)&lt;/li>
&lt;li>Upon a match, force-close the connection with a RST packet. (-j REJECT &amp;ndash;reject-with tcp-reset)&lt;/li>
&lt;/ul>
&lt;p>With this rule in place, all unenrypted VNC connections will be forcefully disconnected by the server.&lt;/p>
&lt;p>Our original plan had been to try redirecting VNC traffic so that we could display a big &amp;ldquo;DON&amp;rsquo;T DO THAT&amp;rdquo; message, but this isn&amp;rsquo;t possible &amp;ndash; by the time we match the client payload, the connection has already been established and is not amendable to redirection.&lt;/p>
&lt;h1 id="update">Update&lt;/h1>
&lt;p>We modified this rule to use the iptables string module to make the match more specific to further reduce the chances of false positives. The rule now looks like this:&lt;/p>
&lt;pre>&lt;code>iptables -A INPUT -p tcp \
-m connbytes --connbytes 0:1024 \
--connbytes-dir both --connbytes-mode bytes \
-m state --state ESTABLISHED \
-m u32 --u32 &amp;quot;0&amp;gt;&amp;gt;22&amp;amp;0x3C@ 12&amp;gt;&amp;gt;26&amp;amp;0x3C@ 0=0x52464220&amp;quot; \
-m string --algo kmp --string &amp;quot;RFB 003.&amp;quot; --to 130 \
-j REJECT --reject-with tcp-reset
&lt;/code>&lt;/pre>
&lt;p>We thought about using the string module exclusively, but unlike the u32 module it is not possible to anchor the string match to the beginning of the TCP payload (since the ip and tcp headers may both be variable length).&lt;/p></content></item><item><title>NFS and the 16-group limit</title><link>https://blog.oddbit.com/post/2010-02-02-nfs-and-16-group-limit/</link><pubDate>Tue, 02 Feb 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-02-02-nfs-and-16-group-limit/</guid><description>I learned something new today: it appears that the underlying authorization mechanism used by NFS limits your group membership to 16 groups. From http://bit.ly/cBhU8N:
NFS is built on ONC RPC (Sun RPC). NFS depends on RPC for authentication and identification of users. Most NFS deployments use an RPC authentication flavor called AUTH_SYS (originally called AUTH_UNIX, but renamed to AUTH_SYS).
AUTH_SYS sends 3 important things:
A 32 bit numeric user identifier (what you&amp;rsquo;d see in the UNIX /etc/passwd file) A 32 bit primary numeric group identifier (ditto) A variable length list of up to 16 32-bit numeric supplemental group identifiers (what&amp;rsquo;d you see in the /etc/group file) We ran into this today while diagnosing a weird permissions issue.</description><content>&lt;p>I learned something new today: it appears that the underlying authorization mechanism used by NFS limits your group membership to 16 groups. From &lt;a href="http://bit.ly/cBhU8N">http://bit.ly/cBhU8N&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>NFS is built on ONC RPC (Sun RPC). NFS depends on RPC for authentication and identification of users. Most NFS deployments use an RPC authentication flavor called AUTH_SYS (originally called AUTH_UNIX, but renamed to AUTH_SYS).&lt;/p>
&lt;p>AUTH_SYS sends 3 important things:&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>A 32 bit numeric user identifier (what you&amp;rsquo;d see in the UNIX /etc/passwd file)&lt;/li>
&lt;li>A 32 bit primary numeric group identifier (ditto)&lt;/li>
&lt;li>A variable length list of up to 16 32-bit numeric supplemental group identifiers (what&amp;rsquo;d you see in the /etc/group file)&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;/blockquote>
&lt;p>We ran into this today while diagnosing a weird permissions issue. Who knew?&lt;/p></content></item><item><title>Cleaning up Subversion with Git</title><link>https://blog.oddbit.com/post/2010-01-29-cleaning-up-subversion-with-gi/</link><pubDate>Fri, 29 Jan 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-01-29-cleaning-up-subversion-with-gi/</guid><description>Overview At my office, we have a crufty Subversion repository (dating back to early 2006) that contains a jumble of unrelated projects. We would like to split this single repository up into a number of smaller repositories, each following the recommended trunk/tags/branches repository organization.
What we want to do is move a project from a path that looks like this:
.../projects/some-project-name To a new repository using the recommended Subversion repository layout, like this:</description><content>&lt;h1 id="overview">Overview&lt;/h1>
&lt;p>At my office, we have a crufty &lt;a href="http://subversion.tigris.org/">Subversion&lt;/a> repository (dating back to early 2006) that contains a jumble of unrelated projects. We would like to split this single repository up into a number of smaller repositories, each following the recommended trunk/tags/branches repository organization.&lt;/p>
&lt;p>What we want to do is move a project from a path that looks like this:&lt;/p>
&lt;pre>&lt;code>.../projects/some-project-name
&lt;/code>&lt;/pre>
&lt;p>To a new repository using the recommended Subversion repository layout, like this:&lt;/p>
&lt;pre>&lt;code>.../some-project-name/trunk
&lt;/code>&lt;/pre>
&lt;p>Our lives are complicated by the fact that there has been a lot of mobility (renames/moves) of projects within the repository.&lt;/p>
&lt;h1 id="setup">Setup&lt;/h1>
&lt;p>We&amp;rsquo;ll set up a test environment that will demonstrate the problem and our solution.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create the empty repositories:&lt;/p>
&lt;p>set -x
rm -rf work &amp;amp;&amp;amp; mkdir work
cd work
WORKDIR=$(pwd)
mkdir repos&lt;/p>
&lt;h1 id="create-source-repository">create source repository&lt;/h1>
&lt;p>svnadmin create repos/src&lt;/p>
&lt;h1 id="create-destination-reposiory">create destination reposiory&lt;/h1>
&lt;p>svnadmin create repos/dst&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create our desired repository structure in the destination repository:&lt;/p>
&lt;p>svn mkdir -m &amp;lsquo;create trunk&amp;rsquo; file://$WORKDIR/repos/dst/trunk
svn mkdir -m &amp;lsquo;create branches&amp;rsquo; file://$WORKDIR/repos/dst/branches
svn mkdir -m &amp;lsquo;create tags&amp;rsquo; file://$WORKDIR/repos/dst/tags&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create a simple revision history:&lt;/p>
&lt;p>svn co file://$WORKDIR/repos/src src
(
cd src&lt;/p>
&lt;h1 id="create-our-initial-set-of-projects">Create our initial set of projects.&lt;/h1>
&lt;p>mkdir projects
mkdir projects/{project1,project2}
touch projects/project1/{file11,file12}
touch projects/project2/{file21,file22}
svn add *
svn ci -m &amp;lsquo;initial commit&amp;rsquo;&lt;/p>
&lt;h1 id="relocate-a-file-between-projects">Relocate a file between projects.&lt;/h1>
&lt;p>svn mv projects/project1/file11 projects/project2/
svn ci -m &amp;lsquo;moved file11&amp;rsquo;&lt;/p>
&lt;h1 id="rename-a-project">Rename a project.&lt;/h1>
&lt;p>svn mv projects/project2 projects/project3
svn update
svn ci -m &amp;lsquo;renamed project2 to project3&amp;rsquo;
)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We can see the structure of the source repository like this:&lt;/p>
&lt;p>echo &amp;ldquo;Contents of source reposiory:&amp;rdquo;
svn ls -R file://$WORKDIR/repos/src&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Your output should look something like this:&lt;/p>
&lt;pre>&lt;code>projects/
projects/project1/
projects/project1/file12
projects/project3/
projects/project3/file11
projects/project3/file21
projects/project3/file22
&lt;/code>&lt;/pre>
&lt;p>In this example, we&amp;rsquo;ll try to import &lt;em>project3&lt;/em> into a new repository.&lt;/p>
&lt;h1 id="using-subversion">Using Subversion&lt;/h1>
&lt;p>With Subversion, it&amp;rsquo;s easy to extract a single project from the repository:&lt;/p>
&lt;pre>&lt;code>svn co file://$WORKDIR/repos/src/projects/project3
&lt;/code>&lt;/pre>
&lt;p>This gives us a directory called project3 containing the contents of the project. Unfortunately, there are no tools that will allow us to take this working copy and move it into another repository.&lt;/p>
&lt;p>Subversion includes a tool called svnadmin that allows on to perform a number of operations on a Subversion repository, but it requires access to the filesystem instance of the repository (it will not work over the network). This is a substantial limitation if you are working with a repository that is maintained by someone else, but we have the necessary access to our repository.&lt;/p>
&lt;p>The svnadmin command includes a dump operation that serializes a repository &amp;ndash; and its entire revision history &amp;ndash; into a text stream that can be loaded into another repository with a corresponding load operation. We don&amp;rsquo;t want the entire repository, so we&amp;rsquo;ll make use of the svndumpfilter command which, as you might expect, can apply certain filters to the output of svnadmin dump.&lt;/p>
&lt;p>We might try something like this:&lt;/p>
&lt;pre>&lt;code>svnadmin dump repos/src |
svndumpfilter include projects/project3/ |
svnadmin load repos/dst
&lt;/code>&lt;/pre>
&lt;p>Unforunately, this will fail with an error along the lines of:&lt;/p>
&lt;pre>&lt;code>svndumpfilter: Invalid copy source path '/projects/project2'
svnadmin: Can't write to stream: Broken pipe
&lt;/code>&lt;/pre>
&lt;p>And if you were to look at the destination repository, you would find projec3 entirely absent:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;Contents of destination repository (after dump/filter/load):&amp;quot;
svn ls -R file://$WORKDIR/repos/dst
&lt;/code>&lt;/pre>
&lt;p>And even if it worked we would still have to muck about in the destination repository to create our desired repository layout.&lt;/p>
&lt;h1 id="using-git">Using Git&lt;/h1>
&lt;p>&lt;a href="http://git-scm.com/">Git&lt;/a> is another version control system, similar in some ways to &lt;a href="http://subversion.tigris.org/">Subversion&lt;/a> but designed for distributed operation. If you&amp;rsquo;re not familiar with git there is lots of documentation available online.&lt;/p>
&lt;p>We&amp;rsquo;ll start by checking out &lt;em>project3&lt;/em> from the Subversion repository:&lt;/p>
&lt;pre>&lt;code>rm -rf project3
git svn clone file://$WORKDIR/repos/src/projects/project3
cd project3
&lt;/code>&lt;/pre>
&lt;p>Because we&amp;rsquo;re going to import this code into a new repository we need to erase all references to the source repository:&lt;/p>
&lt;pre>&lt;code>git branch -rD git-svn
git config --remove-section svn-remote.svn
rm -rf .git/svn
&lt;/code>&lt;/pre>
&lt;p>And now we associate this git repository with the destination Subversion repository:&lt;/p>
&lt;pre>&lt;code>git svn init -s file://$WORKDIR/repos/dst
git svn fetch
&lt;/code>&lt;/pre>
&lt;p>We now apply the revision history to the trunk of the destination repository and commit the changes:&lt;/p>
&lt;pre>&lt;code>git rebase trunk
git svn dcommit
&lt;/code>&lt;/pre>
&lt;p>After all of this, we have exactly what we want &amp;ndash; our project hosted in a new repository with our desired layout. The following commands show the contents of the repository:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;Contents of destination repository (after git):&amp;quot;
svn ls -R file://$WORKDIR/repos/dst
&lt;/code>&lt;/pre>
&lt;p>And produce output like this:&lt;/p>
&lt;pre>&lt;code>branches/
tags/
trunk/
trunk/file11
trunk/file21
trunk/file22
&lt;/code>&lt;/pre>
&lt;p>And the revision history of the project is available in the destination repository:&lt;/p>
&lt;pre>&lt;code>echo &amp;quot;Revision history in destination repository:&amp;quot;
svn log file://$WORKDIR/repos/dst
&lt;/code>&lt;/pre>
&lt;p>The output will look something like:&lt;/p>
&lt;pre>&lt;code>Revision history in destination repository:
+ svn log file:///home/lars/projects/svn-to-svn-via-git/work/repos/dst
------------------------------------------------------------------------
r7 | lars | 2009-06-03 14:46:02 -0400 (Wed, 03 Jun 2009) | 1 line
renamed project2 to project3
------------------------------------------------------------------------
r6 | lars | 2009-06-03 14:46:02 -0400 (Wed, 03 Jun 2009) | 1 line
initial commit
------------------------------------------------------------------------
r5 | (no author) | 2009-06-03 14:45:55 -0400 (Wed, 03 Jun 2009) | 1 line
This is an empty revision for padding.
------------------------------------------------------------------------
r4 | (no author) | 2009-06-03 14:45:53 -0400 (Wed, 03 Jun 2009) | 1 line
This is an empty revision for padding.
------------------------------------------------------------------------
r3 | lars | 2009-06-03 14:45:52 -0400 (Wed, 03 Jun 2009) | 1 line
create tags
------------------------------------------------------------------------
r2 | lars | 2009-06-03 14:45:52 -0400 (Wed, 03 Jun 2009) | 1 line
create branches
------------------------------------------------------------------------
r1 | lars | 2009-06-03 14:45:52 -0400 (Wed, 03 Jun 2009) | 1 line
create trunk
------------------------------------------------------------------------
&lt;/code>&lt;/pre></content></item><item><title>Linux UPnP Gateway</title><link>https://blog.oddbit.com/post/2010-01-29-linux-upnp-gateway/</link><pubDate>Fri, 29 Jan 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-01-29-linux-upnp-gateway/</guid><description>Like many other folks out there, I have several computers in my house connected to the outside world via a Linux box acting as a NAT gateway. I often want to use application such as BitTorrent and Freenet, which require that a number of ports be forwarded from my external connection to the particular computer on which I happen to be working. It turns out there&amp;rsquo;s a protocol for this, called UPnP.</description><content>&lt;p>Like many other folks out there, I have several computers in my house connected to the outside world via a Linux box acting as a NAT gateway. I often want to use application such as BitTorrent and Freenet, which require that a number of ports be forwarded from my external connection to the particular computer on which I happen to be working. It turns out there&amp;rsquo;s a protocol for this, called &lt;a href="http://en.wikipedia.org/wiki/Universal_Plug_and_Play">UPnP&lt;/a>. From Wikipedia:&lt;/p>
&lt;blockquote>
&lt;p>Universal Plug and Play (UPnP) is a set of networking protocols
promulgated by the UPnP Forum. The goals of UPnP are to allow
devices to connect seamlessly and to simplify the implementation of
networks in the home (data sharing, communications, and
entertainment) and in corporate environments for simplified
installation of computer components.&lt;/p>
&lt;/blockquote>
&lt;p>The practical use of UPnP, from my perspective, is that it allows a device or application &lt;em>inside&lt;/em> the network to request specific ports to be forwarded on the gateway. This means that what used to be a manual process &amp;ndash; adding the necessary forwarding rules to my iptables configuration &amp;ndash; is now performed automatically, and only when necessary.&lt;/p>
&lt;p>The &lt;a href="http://linux-igd.sourceforge.net/">Linux UPnP Internet Gateway Device&lt;/a> project implements a Linux UPnP service. You can download the source from the project web page.&lt;/p>
&lt;p>Using the gateway service is really simple:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Start upnpd:&lt;/p>
&lt;pre>&lt;code> # /etc/init.d/upnpd
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Start your application. You will see messages like the following in syslog (if you are logging DEBUG level messages):&lt;/p>
&lt;pre>&lt;code> Aug 6 20:10:12 arcadia upnpd[19816]: Failure in
GateDeviceDeletePortMapping: DeletePortMap: Proto:UDP Port:57875
Aug 6 20:10:12 arcadia upnpd[19816]: AddPortMap: DevUDN:
uuid:75802409-bccb-40e7-8e6c-fa095ecce13e ServiceID: urn:upnp-org:serviceId:WANIPConn1
RemoteHost: (null) Prot: UDP ExtPort: 57875 Int: 192.168.1.118.57875
Aug 6 20:10:12 arcadia upnpd[19816]: Failure in
GateDeviceDeletePortMapping: DeletePortMap: Proto:UDP Port:11657
Aug 6 20:10:12 arcadia upnpd[19816]: AddPortMap: DevUDN:
uuid:75802409-bccb-40e7-8e6c-fa095ecce13e ServiceID: urn:upnp-org:serviceId:WANIPConn1
RemoteHost: (null) Prot: UDP ExtPort: 11657 Int: 192.168.1.118.11657
&lt;/code>&lt;/pre>
&lt;p>For each forwarding requested by the client, upnpd first attempts to remove the mapping and then creates a new rule. Exactly how upnp implements these rules on your system is controlled by &lt;code>/etc/upnpd.conf&lt;/code> &amp;ndash; if you want to use something other than &lt;em>iptables&lt;/em>, or use custom chains, this is where you would make your changes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Look at your firewall rules. Upnpd modifies the &lt;em>FORWARD&lt;/em> chain in the &lt;em>filter&lt;/em> table and the &lt;em>PREROUTING&lt;/em> chain in the &lt;em>nat&lt;/em> table. You can change this behavior by editing &lt;code>/etc/upnpd.conf&lt;/code>.&lt;/p>
&lt;p>To see forwarding rules:&lt;/p>
&lt;pre>&lt;code> # iptables -nL FORWARD
&lt;/code>&lt;/pre>
&lt;p>The rules might look something like this:&lt;/p>
&lt;pre>&lt;code> Chain FORWARD (policy DROP)
target prot opt source destination
ACCEPT udp -- 0.0.0.0/0 192.168.1.118 udp dpt:57875
ACCEPT udp -- 0.0.0.0/0 192.168.1.118 udp dpt:11657
&lt;/code>&lt;/pre>
&lt;p>To see prerouting rules:&lt;/p>
&lt;pre>&lt;code> # iptables -t nat -vnL PREROUTING
&lt;/code>&lt;/pre>
&lt;p>The rules might look something like this:&lt;/p>
&lt;pre>&lt;code> Chain PREROUTING (policy ACCEPT)
target prot opt source destination
DNAT udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:11657 to:192.168.1.118:11657
DNAT udp -- 0.0.0.0/0 0.0.0.0/0 udp dpt:57875 to:192.168.1.118:57875
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Upnpd will delete the mappings when they expire. The expiration time may be set by the client, or, if the client specifies no expiration, than by the &amp;ldquo;duration&amp;rdquo; configuration item in /etc/upnpd.conf.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h1 id="configuration-file">Configuration file&lt;/h1>
&lt;p>The upnpd configuration file (&lt;code>/etc/upnpd.conf&lt;/code>) allows you to change various aspects of upnpd&amp;rsquo;s behavior. Of particular interest:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>insert_forward_rules&lt;/code>&lt;br>
Default: &lt;code>yes&lt;/code>&lt;/p>
&lt;p>Whether or not upnpd needs to create entries in the &lt;code>FORWARD&lt;/code> chain of the &lt;code>filter&lt;/code> table. If your &lt;code>FORWARD&lt;/code> chain has a policy of &lt;code>DROP&lt;/code> you need set to yes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>forward_chain_name&lt;/code>&lt;br>
Default: &lt;code>FORWARD&lt;/code>&lt;/p>
&lt;p>Normally, upnpd creates entries in the &lt;code>FORWARD&lt;/code> chain. If you have a more advanced firewall setup this may not be the appropriate place to make changes. If you enter a custom name here, you will need to create the corresponding chain:&lt;/p>
&lt;pre>&lt;code> iptables -N my-forward-chain
&lt;/code>&lt;/pre>
&lt;p>You will also need to call this chain from the &lt;em>FORWARD&lt;/em> chain:&lt;/p>
&lt;pre>&lt;code> iptables -A FORWARD -j my-forward-chain
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>prerouting_chain_name&lt;/code>&lt;br>
Default: &lt;code>PREROUTING&lt;/code>&lt;/p>
&lt;p>Like &lt;code>forward&lt;/code>chain&lt;code>name&lt;/code>, but for entries in the &lt;code>nat&lt;/code> table.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="security-considerations">Security considerations&lt;/h1>
&lt;p>Consider the following, from the &lt;a href="http://linux-igd.sourceforge.net/documentation.php">Linux IGD documentation&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>UPnP version 1.0, on which this program is based, is inherently flawed&amp;hellip;what appears to have happened is that in Microsoft&amp;rsquo;s first UPnP implementation they weren&amp;rsquo;t concerned with security &amp;hellip;. Simply all they wanted was connectivity&amp;hellip;. The UPnP server, by itself, does no security checking. If it recieves a UPnP request to add a portmapping for some ip address inside the firewall, it just does it. This program will attempt to verify the source ip contained in the UPnP request against the source ip of the actual packet, but as always, these can be forged. The UPnP server makes no attempt to verify this connection with the caller, and therefore it just assumes that whoever asked is the person really wanting it.&lt;/p>
&lt;/blockquote>
&lt;p>In other words, in the battle between security and convenience, UPnP is weighs in heavily on the convenience side. You will have to decide whether this meets your particular requirements.&lt;/p></content></item><item><title>Retrieving Blogger posts by post id</title><link>https://blog.oddbit.com/post/2010-01-29-retrieving-blogger-posts-by-po/</link><pubDate>Fri, 29 Jan 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-01-29-retrieving-blogger-posts-by-po/</guid><description>I spent some time recently trying to figure out, using Google&amp;rsquo;s gdata API, how to retrieve a post from a Blogger blog if I know corresponding post id. As far as I can tell there is no obvious way of doing this, at least not using the gdata.blogger.client api, but after much nashing of teeth I came up with the following solution.
Given client, a gdata.blogger.client instance, and blog, a gdata.</description><content>&lt;p>I spent some time recently trying to figure out, using Google&amp;rsquo;s &lt;a href="http://code.google.com/apis/gdata/docs/2.0/basics.html">gdata&lt;/a> API, how to retrieve a post from a &lt;a href="http://www.blogger.com/">Blogger&lt;/a> blog if I know corresponding post id. As far as I can tell there is no obvious way of doing this, at least not using the gdata.blogger.client api, but after much nashing of teeth I came up with the following solution.&lt;/p>
&lt;p>Given client, a &lt;a href="http://gdata-python-client.googlecode.com/svn/trunk/pydocs/gdata.blogger.client.html">gdata.blogger.client&lt;/a> instance, and blog, a &lt;a href="http://gdata-python-client.googlecode.com/svn/trunk/pydocs/gdata.blogger.data.html">gdata.blogger.data.Blog&lt;/a> instance, the following code will return a &lt;a href="http://gdata-python-client.googlecode.com/svn/trunk/pydocs/gdata.blogger.data.html">gdata.blogger.data.BlogPost&lt;/a> instance:&lt;/p>
&lt;pre>&lt;code>post = client.get_feed(blog.get_post_link().href
+ '/%s' % post_id,
auth_token=client.auth_token,
desired_class=gdata.blogger.data.BlogPost)
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m not sure if this is the canonical solution or not, but it appears to work for me.&lt;/p></content></item><item><title>Fring: How not to handle registration</title><link>https://blog.oddbit.com/post/2010-01-24-fring-how-not-to-handle-regist/</link><pubDate>Sun, 24 Jan 2010 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2010-01-24-fring-how-not-to-handle-regist/</guid><description>I thought I&amp;rsquo;d give Fring a try after seeing some favorable reviews on other sites. If you haven&amp;rsquo;t previously heard of Fring, the following blurb from their website might be helpful:
Using your handset&amp;rsquo;s internet connection, you can interact with friends on all your favourite social networks including Skype, MSN Messenger, Google Talk, ICQ, SIP, Twitter, Yahoo! and AIM. You can listen to music with your Last.fm friends, check out what each other are up to on Facebook, receive alerts of new Google Mail and tailor make your very own fring by adding more cool experiences from fringAdd-ons</description><content>&lt;p>I thought I&amp;rsquo;d give Fring a try after seeing some favorable reviews on other sites. If you haven&amp;rsquo;t previously heard of Fring, the following blurb from their website might be helpful:&lt;/p>
&lt;blockquote>
&lt;p>Using your handset&amp;rsquo;s internet connection, you can interact with friends on all your favourite social networks including Skype, MSN Messenger, Google Talk, ICQ, SIP, Twitter, Yahoo! and AIM. You can listen to music with your Last.fm friends, check out what each other are up to on Facebook, receive alerts of new Google Mail and tailor make your very own fring by adding more cool experiences from fringAdd-ons&lt;/p>
&lt;/blockquote>
&lt;p>Sounds great, right? To use any of these services, you need a Fring account. When you first launch the Fring application on your Android phone, it will prompt you to either provide your existing credentials or set up a new account.&lt;/p>
&lt;p>I haven&amp;rsquo;t previously used Fring so I needed to set up a new acount. I selected a username, alias, and password, provided my email, and then clicked on &amp;ldquo;Register&amp;rdquo;, and Fring said:&lt;/p>
&lt;blockquote>
&lt;p>Password contains invalid characters.&lt;/p>
&lt;/blockquote>
&lt;p>What, that&amp;rsquo;s it? How about a few hints? Or better yet, how about you tell me what your stupid password requirements are before you complain about them? Seriously, guys, have you hired monkeys to handle your authentication mechanisms? Or have you simply decided to ignore decades of best practices concerning password selection? I have some suggestions for you:&lt;/p>
&lt;ul>
&lt;li>If you have password character class limitations, state them up front.&lt;/li>
&lt;li>Don&amp;rsquo;t have character class limitations. Let me type whatever the heck I want.&lt;/li>
&lt;li>Don&amp;rsquo;t set weird arbitrary limits on maximum password lengths. Storage is not that expensive.&lt;/li>
&lt;li>Adjust complexity requirements with the length of my password. A 20 character password is secure even if it only contains letters.&lt;/li>
&lt;/ul>
&lt;p>Fring is the current winner of the &amp;ldquo;shortest lifetime on my phone&amp;rdquo; award, and because it was so bad it doesn&amp;rsquo;t even get a QR code. Bad app. Not for you.&lt;/p>
&lt;p>&lt;strong>Edit&lt;/strong>: Removed punctuation from my password. Now Fring tells me, &amp;ldquo;Password too long.&amp;rdquo; Ha!&lt;/p></content></item></channel></rss>