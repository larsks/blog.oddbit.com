<!doctype html><html lang=en><head><title>Creating a VXLAN overlay network with Open vSwitch :: blog.oddbit.com</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="In this post, we&amp;rsquo;ll walk through the process of getting virtual machines on two different hosts to communicate over an overlay network created using the support for VXLAN in Open vSwitch (or OVS).
The test environment For this post, I&amp;rsquo;ll be working with two systems:
node0.ovs.virt at address 192.168.122.107 node1.ovs.virt at address 192.168.122.174 These hosts are running CentOS 8, although once we get past the package installs the instructions will be similar for other distributions."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://blog.oddbit.com/post/2021-04-17-vm-ovs-vxlan/><link rel=stylesheet href=https://blog.oddbit.com/styles.css><link rel=stylesheet href=https://blog.oddbit.com/style.css><link rel="shortcut icon" href=https://blog.oddbit.com/img/theme-colors/orange.png><link rel=apple-touch-icon href=https://blog.oddbit.com/img/theme-colors/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Creating a VXLAN overlay network with Open vSwitch"><meta property="og:description" content="In this post, we&amp;rsquo;ll walk through the process of getting virtual machines on two different hosts to communicate over an overlay network created using the support for VXLAN in Open vSwitch (or OVS).
The test environment For this post, I&amp;rsquo;ll be working with two systems:
node0.ovs.virt at address 192.168.122.107 node1.ovs.virt at address 192.168.122.174 These hosts are running CentOS 8, although once we get past the package installs the instructions will be similar for other distributions."><meta property="og:url" content="https://blog.oddbit.com/post/2021-04-17-vm-ovs-vxlan/"><meta property="og:site_name" content="blog.oddbit.com"><meta property="og:image" content="https://blog.oddbit.com/img/favicon/orange.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:section" content="tech"><meta property="article:published_time" content="2021-04-17 00:00:00 +0000 UTC"></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>the odd bit blog</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/>/</a></li><li><a href=https://oddbit.com>/about</a></li><li><a href=/posts>/posts</a></li><li><a href=/tags>/tags</a></li><li><a href=/archive>/archive</a></li><li><a href=/rss.xml>/feed</a></li><li><a href=https://github.com/larsks>→Github</a></li><li><a href=https://hachyderm.io/@larsks>→Mastodon</a></li><li><a href=https://twitter.com/larsks>→Twitter</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/>/</a></li><li><a href=https://oddbit.com>/about</a></li><li><a href=/posts>/posts</a></li><li><a href=/tags>/tags</a></li><li><a href=/archive>/archive</a></li><li><a href=/rss.xml>/feed</a></li><li><ul class=menu><li class=menu__trigger>&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=https://github.com/larsks>→Github</a></li><li><a href=https://hachyderm.io/@larsks>→Mastodon</a></li><li><a href=https://twitter.com/larsks>→Twitter</a></li></ul></li></ul></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://blog.oddbit.com/post/2021-04-17-vm-ovs-vxlan/>Creating a VXLAN overlay network with Open vSwitch</a></h1><div class=post-meta><time class=post-date>2021-04-17 ::
[Updated :: 2023-02-16]</time></div><span class=post-tags>#<a href=https://blog.oddbit.com/tags/virtualization/>virtualization</a>&nbsp;
#<a href=https://blog.oddbit.com/tags/networking/>networking</a>&nbsp;
#<a href=https://blog.oddbit.com/tags/openvswitch/>openvswitch</a>&nbsp;
#<a href=https://blog.oddbit.com/tags/vxlan/>vxlan</a>&nbsp;</span><div class=post-content><div><p>In this post, we&rsquo;ll walk through the process of getting virtual
machines on two different hosts to communicate over an overlay network
created using the support for VXLAN in <a href=https://www.openvswitch.org/>Open vSwitch</a> (or OVS).</p><h2 id=the-test-environment>The test environment<a href=#the-test-environment class=hanchor arialabel=Anchor>&#8983;</a></h2><p>For this post, I&rsquo;ll be working with two systems:</p><ul><li><code>node0.ovs.virt</code> at address 192.168.122.107</li><li><code>node1.ovs.virt</code> at address 192.168.122.174</li></ul><p>These hosts are running CentOS 8, although once we get past the
package installs the instructions will be similar for other
distributions.</p><p>While reading through this post, remember that unless otherwise
specified we&rsquo;re going to be running the indicated commands on <em>both</em>
hosts.</p><h2 id=install-packages>Install packages<a href=#install-packages class=hanchor arialabel=Anchor>&#8983;</a></h2><p>Before we can get started configuring things we&rsquo;ll need to install OVS
and <a href=https://libvirt.org/>libvirt</a>. While <code>libvirt</code> is included with the base CentOS
distribution, for OVS we&rsquo;ll need to add both the <a href=https://fedoraproject.org/wiki/EPEL>EPEL</a> repository
as well as a recent CentOS <a href=https://www.openstack.org/>OpenStack</a> repository (OVS is included
in the CentOS OpenStack repositories because it is required by
OpenStack&rsquo;s networking service):</p><pre tabindex=0><code>yum -y install epel-release centos-release-openstack-victoria
</code></pre><p>With these additional repositories enabled we can now install the
required packages:</p><pre tabindex=0><code>yum -y install \
  libguestfs-tools-c \
  libvirt \
  libvirt-daemon-kvm \
  openvswitch2.15 \
  tcpdump \
  virt-install
</code></pre><h2 id=enable-services>Enable services<a href=#enable-services class=hanchor arialabel=Anchor>&#8983;</a></h2><p>We need to start both the <code>libvirtd</code> and <code>openvswitch</code> services:</p><pre tabindex=0><code>systemctl enable --now openvswitch libvirtd
</code></pre><p>This command will (a) mark the services to start automatically when
the system boots and (b) immediately start the service.</p><h2 id=configure-libvirt>Configure libvirt<a href=#configure-libvirt class=hanchor arialabel=Anchor>&#8983;</a></h2><p>When <code>libvirt</code> is first installed it doesn&rsquo;t have any configured
storage pools. Let&rsquo;s create one in the default location,
<code>/var/lib/libvirt/images</code>:</p><pre tabindex=0><code>virsh pool-define-as default --type dir --target /var/lib/libvirt/images
</code></pre><p>We need to mark the pool active, and we might as well configure it to
activate automatically next time the system boots:</p><pre tabindex=0><code>virsh pool-start default
virsh pool-autostart default
</code></pre><h2 id=configure-open-vswitch>Configure Open vSwitch<a href=#configure-open-vswitch class=hanchor arialabel=Anchor>&#8983;</a></h2><h3 id=create-the-bridge>Create the bridge<a href=#create-the-bridge class=hanchor arialabel=Anchor>&#8983;</a></h3><p>With all the prerequisites out of the way we can finally start working
with Open vSwitch. Our first task is to create the OVS bridge that
will host our VXLAN tunnels. To create a bridge named <code>br0</code>, we run:</p><pre tabindex=0><code>ovs-vsctl add-br br0
</code></pre><p>We can inspect the OVS configuration by running <code>ovs-vsctl show</code>,
which should output something like:</p><pre tabindex=0><code>cc1e7217-e393-4e21-97c1-92324d47946d
    Bridge br0
        Port br0
            Interface br0
                type: internal
    ovs_version: &#34;2.15.1&#34;
</code></pre><p>Let&rsquo;s not forget to mark the interface &ldquo;up&rdquo;:</p><pre tabindex=0><code>ip link set br0 up
</code></pre><h3 id=create-the-vxlan-tunnels>Create the VXLAN tunnels<a href=#create-the-vxlan-tunnels class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Up until this point we&rsquo;ve been running identical commands on both
<code>node0</code> and <code>node1</code>. In order to create our VXLAN tunnels, we need to
provide a remote endpoint for the VXLAN connection, which is going to
be &ldquo;the other host&rdquo;. On <code>node0</code>, we run:</p><pre tabindex=0><code>ovs-vsctl add-port br0 vx_node1 -- set interface vx_node1 \
  type=vxlan options:remote_ip=192.168.122.174
</code></pre><p>This creates a VXLAN interface named <code>vx_node1</code> (named that way
because the remote endpoint is <code>node1</code>). The OVS configuration now
looks like:</p><pre tabindex=0><code>cc1e7217-e393-4e21-97c1-92324d47946d
    Bridge br0
        Port vx_node1
            Interface vx_node1
                type: vxlan
                options: {remote_ip=&#34;192.168.122.174&#34;}
        Port br0
            Interface br0
                type: internal
    ovs_version: &#34;2.15.1&#34;
</code></pre><p>On <code>node1</code> we will run:</p><pre tabindex=0><code>ovs-vsctl add-port br0 vx_node0 -- set interface vx_node0 \
  type=vxlan options:remote_ip=192.168.122.107
</code></pre><p>Which results in:</p><pre tabindex=0><code>58451994-e0d1-4bf1-8f91-7253ddf4c016
    Bridge br0
        Port br0
            Interface br0
                type: internal
        Port vx_node0
            Interface vx_node0
                type: vxlan
                options: {remote_ip=&#34;192.168.122.107&#34;}
    ovs_version: &#34;2.15.1&#34;
</code></pre><p>At this point, we have a functional overlay network: anything attached
to <code>br0</code> on either system will appear to share the same layer 2
network. Let&rsquo;s take advantage of this to connect a pair of virtual
machines.</p><h2 id=create-virtual-machines>Create virtual machines<a href=#create-virtual-machines class=hanchor arialabel=Anchor>&#8983;</a></h2><h3 id=download-a-base-image>Download a base image<a href=#download-a-base-image class=hanchor arialabel=Anchor>&#8983;</a></h3><p>We&rsquo;ll need a base image for our virtual machines. I&rsquo;m going to use the
CentOS 8 Stream image, which we can download to our storage directory
like this:</p><pre tabindex=0><code>curl -L -o /var/lib/libvirt/images/centos-8-stream.qcow2 \
  https://cloud.centos.org/centos/8-stream/x86_64/images/CentOS-Stream-GenericCloud-8-20210210.0.x86_64.qcow2
</code></pre><p>We need to make sure <code>libvirt</code> is aware of the new image:</p><pre tabindex=0><code>virsh pool-refresh default
</code></pre><p>Lastly, we&rsquo;ll want to set a root password on the image so that we can
log in to our virtual machines:</p><pre tabindex=0><code>virt-customize -a /var/lib/libvirt/images/centos-8-stream.qcow2 \
  --root-password password:secret
</code></pre><h3 id=create-the-virtual-machine>Create the virtual machine<a href=#create-the-virtual-machine class=hanchor arialabel=Anchor>&#8983;</a></h3><p>We&rsquo;re going to create a pair of virtual machines (one on each host).
We&rsquo;ll be creating each vm with two network interfaces:</p><ul><li>One will be attached to the libvirt <code>default</code> network; this will
allow us to <code>ssh</code> into the vm in order to configure things.</li><li>The second will be attached to the OVS bridge</li></ul><p>To create a virtual machine on <code>node0</code> named <code>vm0.0</code>, run the
following command:</p><pre tabindex=0><code>virt-install \
  -r 3000 \
  --network network=default \
  --network bridge=br0,virtualport.type=openvswitch \
  --os-variant centos8 \
  --disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
  --import \
  --noautoconsole \
  -n vm0.0
</code></pre><p>The most interesting option in the above command line is probably the
one used to create the virtual disk:</p><pre tabindex=0><code>--disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
</code></pre><p>This creates a 10GB &ldquo;<a href=https://en.wikipedia.org/wiki/Copy-on-write>copy-on-write</a>&rdquo; disk that uses
<code>centos-8-stream.qcow2</code> as a backing store. That means that reads will
generally come from the <code>centos-8-stream.qcow2</code> image, but writes will
be stored in the new image. This makes it easy for us to quickly
create multiple virtual machines from the same base image.</p><p>On <code>node1</code> we would run a similar command, although here we&rsquo;re naming
the virtual machine <code>vm1.0</code>:</p><pre tabindex=0><code>virt-install \
  -r 3000 \
  --network network=default \
  --network bridge=br0,virtualport.type=openvswitch \
  --os-variant centos8 \
  --disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
  --import \
  --noautoconsole \
  -n vm1.0
</code></pre><h3 id=configure-networking-for-vm00>Configure networking for vm0.0<a href=#configure-networking-for-vm00 class=hanchor arialabel=Anchor>&#8983;</a></h3><p>On <code>node0</code>, get the address of the new virtual machine on the default
network using the <code>virsh domifaddr</code> command:</p><pre tabindex=0><code>[root@node0 ~]# virsh domifaddr vm0.0
 Name       MAC address          Protocol     Address
-------------------------------------------------------------------------------
 vnet2      52:54:00:21:6e:4f    ipv4         192.168.124.83/24
</code></pre><p>Connect to the vm using <code>ssh</code>:</p><pre tabindex=0><code>[root@node0 ~]# ssh 192.168.124.83
root@192.168.124.83&#39;s password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Sat Apr 17 14:08:17 2021 from 192.168.124.1
[root@localhost ~]#
</code></pre><p>(Recall that the <code>root</code> password is <code>secret</code>.)</p><p>Configure interface <code>eth1</code> with an address. For this post, we&rsquo;ll use
the <code>10.0.0.0/24</code> range for our overlay network. To assign this vm the
address <code>10.0.0.10</code>, we can run:</p><pre tabindex=0><code>ip addr add 10.0.0.10/24 dev eth1
ip link set eth1 up
</code></pre><h3 id=configure-networking-for-vm10>Configure networking for vm1.0<a href=#configure-networking-for-vm10 class=hanchor arialabel=Anchor>&#8983;</a></h3><p>We need to repeat the process for <code>vm1.0</code> on <code>node1</code>:</p><pre tabindex=0><code>[root@node1 ~]# virsh domifaddr vm1.0
 Name       MAC address          Protocol     Address
-------------------------------------------------------------------------------
 vnet0      52:54:00:e9:6e:43    ipv4         192.168.124.69/24
</code></pre><p>Connect to the vm using <code>ssh</code>:</p><pre tabindex=0><code>[root@node0 ~]# ssh 192.168.124.69
root@192.168.124.69&#39;s password:
Activate the web console with: systemctl enable --now cockpit.socket

Last login: Sat Apr 17 14:08:17 2021 from 192.168.124.1
[root@localhost ~]#
</code></pre><p>We&rsquo;ll use address 10.0.0.11 for this system:</p><pre tabindex=0><code>ip addr add 10.0.0.11/24 dev eth1
ip link set eth1 up
</code></pre><h3 id=verify-connectivity>Verify connectivity<a href=#verify-connectivity class=hanchor arialabel=Anchor>&#8983;</a></h3><p>At this point, our setup is complete. On <code>vm0.0</code>, we can connect to
<code>vm1.1</code> over the overlay network. For example, we can ping the remote
host:</p><pre tabindex=0><code>[root@localhost ~]# ping -c2 10.0.0.11
PING 10.0.0.11 (10.0.0.11) 56(84) bytes of data.
64 bytes from 10.0.0.11: icmp_seq=1 ttl=64 time=1.79 ms
64 bytes from 10.0.0.11: icmp_seq=2 ttl=64 time=0.719 ms

--- 10.0.0.11 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 0.719/1.252/1.785/0.533 ms
</code></pre><p>Or connect to it using <code>ssh</code>:</p><pre tabindex=0><code>[root@localhost ~]# ssh 10.0.0.11 uptime
root@10.0.0.11&#39;s password:
 14:21:33 up  1:18,  1 user,  load average: 0.00, 0.00, 0.00
</code></pre><p>Using <code>tcpdump</code>, we can verify that these connections are going over
the overlay network. Let&rsquo;s watch for VXLAN traffic on <code>node1</code> by
running the following command (VXLAN is a UDP protocol running on port
4789)</p><pre tabindex=0><code>tcpdump -i eth0 -n port 4789
</code></pre><p>When we run <code>ping -c2 10.0.0.11</code> on <code>vm0.0</code>, we see the following:</p><pre tabindex=0><code>14:23:50.312574 IP 192.168.122.107.52595 &gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &gt; 10.0.0.11: ICMP echo request, id 4915, seq 1, length 64
14:23:50.314896 IP 192.168.122.174.59510 &gt; 192.168.122.107.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.11 &gt; 10.0.0.10: ICMP echo reply, id 4915, seq 1, length 64
14:23:51.314080 IP 192.168.122.107.52595 &gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &gt; 10.0.0.11: ICMP echo request, id 4915, seq 2, length 64
14:23:51.314259 IP 192.168.122.174.59510 &gt; 192.168.122.107.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.11 &gt; 10.0.0.10: ICMP echo reply, id 4915, seq 2, length 64
</code></pre><p>In the output above, we see that each packet in the transaction
results in two lines of output from <code>tcpdump</code>:</p><pre tabindex=0><code>14:23:50.312574 IP 192.168.122.107.52595 &gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &gt; 10.0.0.11: ICMP echo request, id 4915, seq 1, length 64
</code></pre><p>The first line shows the contents of the VXLAN packet, while the
second lines shows the data that was encapsulated in the VXLAN packet.</p><h2 id=thats-all-folks>That&rsquo;s all folks<a href=#thats-all-folks class=hanchor arialabel=Anchor>&#8983;</a></h2><p>We&rsquo;ve achieved our goal: we have two virtual machines on two different
hosts communicating over a VXLAN overlay network. If you were to do
this &ldquo;for real&rdquo;, you would probably want to make a number of changes:
for example, the network configuration we&rsquo;ve applied in many cases
will not persist across a reboot; handling persistent network
configuration is still very distribution dependent, so I&rsquo;ve left it
out of this post.</p></div></div><script src=https://utteranc.es/client.js repo=larsks/blog.oddbit.com issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></article></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>Lars Kellogg-Stedman</span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script>
<script src=/js/mermaid.min.js></script>
<script>mermaid.initialize({startOnLoad:!0})</script></div></body></html>