<!doctype html><html lang=en><head><title>Quantum in Too Much Detail :: blog.oddbit.com</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content=" I originally posted this article on the RDO website.
The players This document describes the architecture that results from a particular OpenStack configuration, specifically:
Quantum networking using GRE tunnels; A dedicated network controller; A single instance running on a compute host Much of the document will be relevant to other configurations, but details will vary based on your choice of layer 2 connectivity, number of running instances, and so forth.
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/><script async src="https://www.googletagmanager.com/gtag/js?id=G-G1FYT93ENG"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G1FYT93ENG")}</script><link rel=stylesheet href=https://blog.oddbit.com/styles.css><link rel=stylesheet href=https://blog.oddbit.com/style.css><link rel="shortcut icon" href=https://blog.oddbit.com/img/theme-colors/orange.png><link rel=apple-touch-icon href=https://blog.oddbit.com/img/theme-colors/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="Quantum in Too Much Detail"><meta property="og:description" content=" I originally posted this article on the RDO website.
The players This document describes the architecture that results from a particular OpenStack configuration, specifically:
Quantum networking using GRE tunnels; A dedicated network controller; A single instance running on a compute host Much of the document will be relevant to other configurations, but details will vary based on your choice of layer 2 connectivity, number of running instances, and so forth.
"><meta property="og:url" content="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/"><meta property="og:site_name" content="blog.oddbit.com"><meta property="og:image" content="https://blog.oddbit.com/img/favicon/orange.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:section" content="tech"><meta property="article:published_time" content="2013-11-14 00:00:00 +0000 UTC"></head><body class=orange><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>the odd bit blog</div></a></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/>/</a></li><li><a href=https://oddbit.com>/about</a></li><li><a href=/posts>/posts</a></li><li><a href=/tags>/tags</a></li><li><a href=/archive>/archive</a></li><li><a href=/rss.xml>/feed</a></li><li><a href=https://github.com/larsks>→Github</a></li><li><a href=https://hachyderm.io/@larsks>→Mastodon</a></li><li><a href=https://twitter.com/larsks>→Twitter</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/>/</a></li><li><a href=https://oddbit.com>/about</a></li><li><a href=/posts>/posts</a></li><li><a href=/tags>/tags</a></li><li><a href=/archive>/archive</a></li><li><a href=/rss.xml>/feed</a></li><li><ul class=menu><li class=menu__trigger>&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=https://github.com/larsks>→Github</a></li><li><a href=https://hachyderm.io/@larsks>→Mastodon</a></li><li><a href=https://twitter.com/larsks>→Twitter</a></li></ul></li></ul></li></ul></nav></header><div class=content><article class=post><h1 class=post-title><a href=https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/>Quantum in Too Much Detail</a></h1><div class=post-meta><time class=post-date>2013-11-14 ::
[Updated :: 2023-02-16]</time></div><span class=post-tags>#<a href=https://blog.oddbit.com/tags/openstack/>openstack</a>&nbsp;
#<a href=https://blog.oddbit.com/tags/quantum/>quantum</a>&nbsp;
#<a href=https://blog.oddbit.com/tags/networking/>networking</a>&nbsp;</span><div class=post-content><div><blockquote><p>I originally posted this article on
the <a href=http://openstack.redhat.com/Networking_in_too_much_detail>RDO</a>
website.</p></blockquote><h1 id=the-players>The players<a href=#the-players class=hanchor arialabel=Anchor>&#8983;</a></h1><p>This document describes the architecture that results from a
particular OpenStack configuration, specifically:</p><ul><li>Quantum networking using GRE tunnels;</li><li>A dedicated network controller;</li><li>A single instance running on a compute host</li></ul><p>Much of the document will be relevant to other configurations, but
details will vary based on your choice of layer 2 connectivity, number
of running instances, and so forth.</p><p>The examples in this document were generated on a system with Quantum
networking but will generally match what you see under Neutron as
well, if you replace <code>quantum</code> by <code>neutron</code> in names. The OVS flow
rules under Neutron are somewhat more complex and I will cover those
in another post.</p><h1 id=the-lay-of-the-land>The lay of the land<a href=#the-lay-of-the-land class=hanchor arialabel=Anchor>&#8983;</a></h1><p>This is a simplified architecture diagram of network connectivity in a
quantum/neutron managed world:</p><figure class=left><img src=quantum-gre.svg></figure><p>Section names in this document include
parenthetical references to the nodes on the map relevant to that
particular section.</p><h1 id=compute-host-instance-networking-abc>Compute host: instance networking (A,B,C)<a href=#compute-host-instance-networking-abc class=hanchor arialabel=Anchor>&#8983;</a></h1><p>An outbound packet starts on <code>eth0</code> of the virtual instance, which is
connected to a <code>tap</code> device on the host, <code>tap7c7ae61e-05</code>. This <code>tap</code>
device is attached to a Linux bridge device, <code>qbr7c7ae61e-05</code>. What is
this bridge device for? From the <a href=http://docs.openstack.org/network-admin/admin/content/under_the_hood_openvswitch.html>OpenStack Networking Administration
Guide</a>:</p><blockquote><p>Ideally, the TAP device vnet0 would be connected directly to the
integration bridge, br-int. Unfortunately, this isn&rsquo;t possible because
of how OpenStack security groups are currently implemented. OpenStack
uses iptables rules on the TAP devices such as vnet0 to implement
security groups, and Open vSwitch is not compatible with iptables
rules that are applied directly on TAP devices that are connected to
an Open vSwitch port.</p></blockquote><p>Because this bridge device exists primarily to support firewall rules,
I&rsquo;m going to refer to it as the &ldquo;firewall bridge&rdquo;.</p><p>If you examine the firewall rules on your compute host, you will find
that there are several rules associated with this <code>tap</code> device:</p><pre><code># iptables -S | grep tap7c7ae61e-05
-A quantum-openvswi-FORWARD -m physdev --physdev-out tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-sg-chain 
-A quantum-openvswi-FORWARD -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-sg-chain 
-A quantum-openvswi-INPUT -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-o7c7ae61e-0 
-A quantum-openvswi-sg-chain -m physdev --physdev-out tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-i7c7ae61e-0 
-A quantum-openvswi-sg-chain -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-o7c7ae61e-0 
</code></pre><p>The <code>quantum-openvswi-sg-chain</code> is where <code>neutron</code>-managed security
groups are realized. The <code>quantum-openvswi-o7c7ae61e-0</code> chain
controls outbound traffic FROM the instance, and by default looks like
this:</p><pre><code>-A quantum-openvswi-o7c7ae61e-0 -m mac ! --mac-source FA:16:3E:03:00:E7 -j DROP 
-A quantum-openvswi-o7c7ae61e-0 -p udp -m udp --sport 68 --dport 67 -j RETURN 
-A quantum-openvswi-o7c7ae61e-0 ! -s 10.1.0.2/32 -j DROP 
-A quantum-openvswi-o7c7ae61e-0 -p udp -m udp --sport 67 --dport 68 -j DROP 
-A quantum-openvswi-o7c7ae61e-0 -m state --state INVALID -j DROP 
-A quantum-openvswi-o7c7ae61e-0 -m state --state RELATED,ESTABLISHED -j RETURN 
-A quantum-openvswi-o7c7ae61e-0 -j RETURN 
-A quantum-openvswi-o7c7ae61e-0 -j quantum-openvswi-sg-fallback 
</code></pre><p>The <code>quantum-openvswi-i7c7ae61e-0</code> chain controls inbound traffic TO
the instance. After opening up port 22 in the default security group:</p><pre><code># neutron security-group-rule-create --protocol tcp \
  --port-range-min 22 --port-range-max 22 --direction ingress default
</code></pre><p>The rules look like this:</p><pre><code>-A quantum-openvswi-i7c7ae61e-0 -m state --state INVALID -j DROP 
-A quantum-openvswi-i7c7ae61e-0 -m state --state RELATED,ESTABLISHED -j RETURN 
-A quantum-openvswi-i7c7ae61e-0 -p icmp -j RETURN 
-A quantum-openvswi-i7c7ae61e-0 -p tcp -m tcp --dport 22 -j RETURN 
-A quantum-openvswi-i7c7ae61e-0 -p tcp -m tcp --dport 80 -j RETURN 
-A quantum-openvswi-i7c7ae61e-0 -s 10.1.0.3/32 -p udp -m udp --sport 67 --dport 68 -j RETURN 
-A quantum-openvswi-i7c7ae61e-0 -j quantum-openvswi-sg-fallback 
</code></pre><p>A second interface attached to the bridge, <code>qvb7c7ae61e-05</code>, attaches
the firewall bridge to the integration bridge, typically named
<code>br-int</code>.</p><h1 id=compute-host-integration-bridge-de>Compute host: integration bridge (D,E)<a href=#compute-host-integration-bridge-de class=hanchor arialabel=Anchor>&#8983;</a></h1><p>The integration bridge, <code>br-int</code>, performs VLAN tagging and un-tagging
for traffic coming from and to your instances. At this moment,
<code>br-int</code> looks something like this:</p><pre><code># ovs-vsctl show
Bridge br-int
    Port &quot;qvo7c7ae61e-05&quot;
        tag: 1
        Interface &quot;qvo7c7ae61e-05&quot;
    Port patch-tun
        Interface patch-tun
            type: patch
            options: {peer=patch-int}
    Port br-int
        Interface br-int
            type: internal
</code></pre><p>The interface <code>qvo7c7ae61e-05</code> is the other end of <code>qvb7c7ae61e-05</code>,
and carries traffic to and from the firewall bridge. The <code>tag: 1</code> you
see in the above output integrates that this is an access port
attached to VLAN 1. Untagged outbound traffic from this instance will be
assigned VLAN ID 1, and inbound traffic with VLAN ID 1 will
stripped of it&rsquo;s VLAN tag and sent out this port.</p><p>Each network you create (with <code>neutron net-create</code>) will be assigned a
different VLAN ID.</p><p>The interface named <code>patch-tun</code> connects the integration bridge to the
tunnel bridge, <code>br-tun</code>.</p><h1 id=compute-host-tunnel-bridge-fg>Compute host: tunnel bridge (F,G)<a href=#compute-host-tunnel-bridge-fg class=hanchor arialabel=Anchor>&#8983;</a></h1><p>The tunnel bridge translates VLAN-tagged traffic from the
integration bridge into <code>GRE</code> tunnels. The translation between VLAN
IDs and tunnel IDs is performed by OpenFlow rules installed on
<code>br-tun</code>. Before creating any instances, the flow rules on the bridge
look like this:</p><pre><code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
 cookie=0x0, duration=871.283s, table=0, n_packets=4, n_bytes=300, idle_age=862, priority=1 actions=drop
</code></pre><p>There is a single rule that causes the bridge to drop all traffic.
Afrer you boot an instance on this compute node, the rules are
modified to look something like:</p><pre><code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
 cookie=0x0, duration=422.158s, table=0, n_packets=2, n_bytes=120, idle_age=55, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
 cookie=0x0, duration=421.948s, table=0, n_packets=64, n_bytes=8337, idle_age=31, priority=3,tun_id=0x2,dl_dst=fa:16:3e:dd:c1:62 actions=mod_vlan_vid:1,NORMAL
 cookie=0x0, duration=422.357s, table=0, n_packets=82, n_bytes=10443, idle_age=31, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
 cookie=0x0, duration=1502.657s, table=0, n_packets=8, n_bytes=596, idle_age=423, priority=1 actions=drop
</code></pre><p>In general, these rules are responsible for mapping traffic between
VLAN ID 1, used by the integration bridge, and tunnel id 2, used by
the GRE tunnel.</p><p>The first rule&mldr;</p><pre><code> cookie=0x0, duration=422.158s, table=0, n_packets=2, n_bytes=120, idle_age=55, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
</code></pre><p>&mldr;matches all multicast traffic (see <a href="http://openvswitch.org/cgi-bin/ovsman.cgi?page=utilities%2Fovs-ofctl.8">ovs-ofctl(8)</a>)
on tunnel id 2 (<code>tun_id=0x2</code>), tags the ethernet frame with VLAN ID
1 (<code>actions=mod_vlan_vid:1</code>), and sends it out port 1. We can see
from <code>ovs-ofctl show br-tun</code> that port 1 is <code>patch-int</code>:</p><pre><code># ovs-ofctl show br-tun
OFPT_FEATURES_REPLY (xid=0x2): dpid:0000068df4e44a49
n_tables:254, n_buffers:256
capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP
actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC SET_DL_DST SET_NW_SRC SET_NW_DST SET_NW_TOS SET_TP_SRC SET_TP_DST ENQUEUE
 1(patch-int): addr:46:3d:59:17:df:62
     config:     0
     state:      0
     speed: 0 Mbps now, 0 Mbps max
 2(gre-2): addr:a2:5f:a1:92:29:02
     config:     0
     state:      0
     speed: 0 Mbps now, 0 Mbps max
 LOCAL(br-tun): addr:06:8d:f4:e4:4a:49
     config:     0
     state:      0
     speed: 0 Mbps now, 0 Mbps max
OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0
</code></pre><p>The next rule&mldr;</p><pre><code> cookie=0x0, duration=421.948s, table=0, n_packets=64, n_bytes=8337, idle_age=31, priority=3,tun_id=0x2,dl_dst=fa:16:3e:dd:c1:62 actions=mod_vlan_vid:1,NORMAL
</code></pre><p>&mldr;matches traffic coming in on tunnel 2 (<code>tun_id=0x2</code>) with an
ethernet destination of <code>fa:16:3e:dd:c1:62</code>
(<code>dl_dst=fa:16:3e:dd:c1:62</code>) and tags the ethernet frame with VLAN
ID 1 (<code>actions=mod_vlan_vid:1</code>) before sending it out <code>patch-int</code>.</p><p>The following rule&mldr;</p><pre><code> cookie=0x0, duration=422.357s, table=0, n_packets=82, n_bytes=10443, idle_age=31, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
</code></pre><p>&mldr;matches traffic coming in on port 1 (<code>in_port=1</code>) with VLAN ID 1
(<code>dl_vlan=1</code>) and set the tunnel id to 2 (<code>actions=set_tunnel:0x2</code>)
before sending it out the GRE tunnel.</p><h1 id=network-host-tunnel-bridge-hi>Network host: tunnel bridge (H,I)<a href=#network-host-tunnel-bridge-hi class=hanchor arialabel=Anchor>&#8983;</a></h1><p>Traffic arrives on the network host via the GRE tunnel attached to
<code>br-tun</code>. This bridge has a flow table very similar to <code>br-tun</code> on
the compute host:</p><pre><code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
 cookie=0x0, duration=1239.229s, table=0, n_packets=23, n_bytes=4246, idle_age=15, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
 cookie=0x0, duration=524.477s, table=0, n_packets=15, n_bytes=3498, idle_age=10, priority=3,tun_id=0x2,dl_dst=fa:16:3e:83:69:cc actions=mod_vlan_vid:1,NORMAL
 cookie=0x0, duration=1239.157s, table=0, n_packets=50, n_bytes=4565, idle_age=148, priority=3,tun_id=0x2,dl_dst=fa:16:3e:aa:99:3c actions=mod_vlan_vid:1,NORMAL
 cookie=0x0, duration=1239.304s, table=0, n_packets=76, n_bytes=9419, idle_age=10, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
 cookie=0x0, duration=1527.016s, table=0, n_packets=12, n_bytes=880, idle_age=527, priority=1 actions=drop
</code></pre><p>As on the compute host, the first rule maps multicast traffic on
tunnel ID 2 to VLAN 1.</p><p>The second rule&mldr;</p><pre><code> cookie=0x0, duration=524.477s, table=0, n_packets=15, n_bytes=3498, idle_age=10, priority=3,tun_id=0x2,dl_dst=fa:16:3e:83:69:cc actions=mod_vlan_vid:1,NORMAL
</code></pre><p>&mldr;matches traffic on the tunnel destined for the DHCP server at
<code>fa:16:3e:83:69:cc</code>. This is a <code>dnsmasq</code> process running inside a
network namespace, the details of which we will examine shortly.</p><p>The next rule&mldr;</p><pre><code> cookie=0x0, duration=1239.157s, table=0, n_packets=50, n_bytes=4565, idle_age=148, priority=3,tun_id=0x2,dl_dst=fa:16:3e:aa:99:3c actions=mod_vlan_vid:1,NORMAL
</code></pre><p>&mldr;matches traffic on tunnel ID 2 destined for the router at <code>fa:16:3e:aa:99:3c</code>, which is an interface in another network namespace.</p><p>The following rule&mldr;</p><pre><code> cookie=0x0, duration=1239.304s, table=0, n_packets=76, n_bytes=9419, idle_age=10, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
</code></pre><p>&mldr;simply maps outbound traffic on VLAN ID 1 to tunnel ID 2.</p><h1 id=network-host-integration-bridge-jkm>Network host: integration bridge (J,K,M)<a href=#network-host-integration-bridge-jkm class=hanchor arialabel=Anchor>&#8983;</a></h1><p>The integration bridge on the network controller serves to connect
instances to network services, such as routers and DHCP servers.</p><pre><code># ovs-vsctl show
.
.
.
Bridge br-int
    Port patch-tun
        Interface patch-tun
            type: patch
            options: {peer=patch-int}
    Port &quot;tapf14c598d-98&quot;
        tag: 1
        Interface &quot;tapf14c598d-98&quot;
    Port br-int
        Interface br-int
            type: internal
    Port &quot;tapc2d7dd02-56&quot;
        tag: 1
        Interface &quot;tapc2d7dd02-56&quot;
.
.
.
</code></pre><p>It connects to the tunnel bridge, <code>br-tun</code>, via a patch interface,
<code>patch-tun</code>.</p><h1 id=network-host-dhcp-server-m>Network host: DHCP server (M)<a href=#network-host-dhcp-server-m class=hanchor arialabel=Anchor>&#8983;</a></h1><p>Each network for which DHCP is enabled has a DHCP server running on
the network controller. The DHCP server is an instance of <a href=http://www.thekelleys.org.uk/dnsmasq/doc.html>dnsmasq</a>
running inside a <em>network namespace</em>. A <em>network namespace</em> is a
Linux kernel facility that allows groups of processes to have a
network stack (interfaces, routing tables, iptables rules) distinct
from that of the host.</p><p>You can see a list of network namespace with the <code>ip netns</code> command,
which in our configuration will look something like this:</p><pre><code># ip netns
qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264
qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f
</code></pre><p>The first of these (<code>qdhcp...</code>) is the DHCP server namespace for our private
subnet, while the second (<code>qrouter...</code>) is the router.</p><p>You can run a command inside a network namespace using the <code>ip netns exec</code> command. For example, to see the interface configuration inside
the DHCP server namespace (<code>lo</code> removed for brevity):</p><pre><code># ip netns exec qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264 ip addr
71: tapf14c598d-98: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether fa:16:3e:10:2f:03 brd ff:ff:ff:ff:ff:ff
    inet 10.1.0.3/24 brd 10.1.0.255 scope global ns-f14c598d-98
    inet6 fe80::f816:3eff:fe10:2f03/64 scope link 
       valid_lft forever preferred_lft forever
</code></pre><p>Note the MAC address on interface <code>tapf14c598d-98</code>; this matches the MAC address in the flow rule we saw on the tunnel bridge.</p><p>You can find the <code>dnsmasq</code> process associated with this namespace by
search the output of <code>ps</code> for the id (the number after <code>qdhcp-</code> in the
namespace name):</p><pre><code># ps -fe | grep 88b1609c-68e0-49ca-a658-f1edff54a264
nobody   23195     1  0 Oct26 ?        00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=ns-f14c598d-98 --except-interface=lo --pid-file=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/pid --dhcp-hostsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/host --dhcp-optsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/opts --dhcp-script=/usr/bin/quantum-dhcp-agent-dnsmasq-lease-update --leasefile-ro --dhcp-range=tag0,10.1.0.0,static,120s --conf-file= --domain=openstacklocal
root     23196 23195  0 Oct26 ?        00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=ns-f14c598d-98 --except-interface=lo --pid-file=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/pid --dhcp-hostsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/host --dhcp-optsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/opts --dhcp-script=/usr/bin/quantum-dhcp-agent-dnsmasq-lease-update --leasefile-ro --dhcp-range=tag0,10.1.0.0,static,120s --conf-file= --domain=openstacklocal
</code></pre><h1 id=network-host-router-kl>Network host: Router (K,L)<a href=#network-host-router-kl class=hanchor arialabel=Anchor>&#8983;</a></h1><p>A router is a network namespace with a set of routing tables
and iptables rules that performs the routing between subnets. Recall
that we saw two network namespaces in our configuration:</p><pre><code># ip netns
qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264
qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f
</code></pre><p>Using the <code>ip netns exec</code> command, we can inspect the interfaces
associated with the router (<code>lo</code> removed for brevity):</p><pre><code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f ip addr
66: qg-d48b49e0-aa: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether fa:16:3e:5c:a2:ac brd ff:ff:ff:ff:ff:ff
    inet 172.24.4.227/28 brd 172.24.4.239 scope global qg-d48b49e0-aa
    inet 172.24.4.228/32 brd 172.24.4.228 scope global qg-d48b49e0-aa
    inet6 fe80::f816:3eff:fe5c:a2ac/64 scope link 
       valid_lft forever preferred_lft forever
68: qr-c2d7dd02-56: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether fa:16:3e:ea:64:6e brd ff:ff:ff:ff:ff:ff
    inet 10.1.0.1/24 brd 10.1.0.255 scope global qr-c2d7dd02-56
    inet6 fe80::f816:3eff:feea:646e/64 scope link 
       valid_lft forever preferred_lft forever
</code></pre><p>The first interface, <code>qg-d48b49e0-aa</code>, connects the router to the
gateway set by the <code>router-gateway-set</code> command. The second
interface, <code>qr-c2d7dd02-56</code>, is what connects the router to the
integration bridge.</p><p>Looking at the routing tables inside the router, we see that there is
a default gateway pointing to the <code>.1</code> address of our external
network, and the expected network routes for directly attached
networks:</p><pre><code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f ip route
172.24.4.224/28 dev qg-d48b49e0-aa  proto kernel  scope link  src 172.24.4.227 
10.1.0.0/24 dev qr-c2d7dd02-56  proto kernel  scope link  src 10.1.0.1 
default via 172.24.4.225 dev qg-d48b49e0-aa 
</code></pre><p>The netfilter <code>nat</code> table inside the router namespace is responsible
for associating floating IP addresses with your instances. For
example, after associating the address <code>172.24.4.228</code> with our
instance, the <code>nat</code> table looks like this:</p><pre><code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f iptables -t nat -S
-P PREROUTING ACCEPT
-P POSTROUTING ACCEPT
-P OUTPUT ACCEPT
-N quantum-l3-agent-OUTPUT
-N quantum-l3-agent-POSTROUTING
-N quantum-l3-agent-PREROUTING
-N quantum-l3-agent-float-snat
-N quantum-l3-agent-snat
-N quantum-postrouting-bottom
-A PREROUTING -j quantum-l3-agent-PREROUTING 
-A POSTROUTING -j quantum-l3-agent-POSTROUTING 
-A POSTROUTING -j quantum-postrouting-bottom 
-A OUTPUT -j quantum-l3-agent-OUTPUT 
-A quantum-l3-agent-OUTPUT -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2 
-A quantum-l3-agent-POSTROUTING ! -i qg-d48b49e0-aa ! -o qg-d48b49e0-aa -m conntrack ! --ctstate DNAT -j ACCEPT 
-A quantum-l3-agent-PREROUTING -d 169.254.169.254/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 9697 
-A quantum-l3-agent-PREROUTING -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2 
-A quantum-l3-agent-float-snat -s 10.1.0.2/32 -j SNAT --to-source 172.24.4.228 
-A quantum-l3-agent-snat -j quantum-l3-agent-float-snat 
-A quantum-l3-agent-snat -s 10.1.0.0/24 -j SNAT --to-source 172.24.4.227 
-A quantum-postrouting-bottom -j quantum-l3-agent-snat 
</code></pre><p>There are <code>SNAT</code> and <code>DNAT</code> rules to map traffic between the floating
address, <code>172.24.4.228</code>, and the private address <code>10.1.0.2</code>:</p><pre><code>-A quantum-l3-agent-OUTPUT -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2 
-A quantum-l3-agent-PREROUTING -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2 
-A quantum-l3-agent-float-snat -s 10.1.0.2/32 -j SNAT --to-source 172.24.4.228 
</code></pre><p>When you associate a floating ip address with an instance, similar
rules will be created in this table.</p><p>There is also an <code>SNAT</code> rule that NATs all outbound traffic from our
private network to <code>172.24.4.227</code>:</p><pre><code>-A quantum-l3-agent-snat -s 10.1.0.0/24 -j SNAT --to-source 172.24.4.227 
</code></pre><p>This permits instances to have outbound connectivity even without a
public ip address.</p><h1 id=network-host-external-traffic-l>Network host: External traffic (L)<a href=#network-host-external-traffic-l class=hanchor arialabel=Anchor>&#8983;</a></h1><p>&ldquo;External&rdquo; traffic flows through <code>br-ex</code> via the <code>qg-d48b49e0-aa</code>
interface in the router name space.</p><pre><code>Bridge br-ex
    Port &quot;qg-d48b49e0-aa&quot;
        Interface &quot;qg-d48b49e0-aa&quot;
    Port br-ex
        Interface br-ex
            type: internal
</code></pre><p>What happens when traffic gets this far depends on your local
configuration.</p><h2 id=nat-to-host-address>NAT to host address<a href=#nat-to-host-address class=hanchor arialabel=Anchor>&#8983;</a></h2><p>If you assign the gateway address for your public network to <code>br-ex</code>:</p><pre><code># ip addr add 172.24.4.225/28 dev br-ex
</code></pre><p>Then you can create forwarding and NAT rules that will cause
&ldquo;external&rdquo; traffic from your instances to get rewritten to your
network controller&rsquo;s ip address and sent out on the network:</p><pre><code># iptables -A FORWARD -d 172.24.4.224/28 -j ACCEPT 
# iptables -A FORWARD -s 172.24.4.224/28 -j ACCEPT 
# iptables -t nat -I POSTROUTING 1 -s 172.24.4.224/28 -j MASQUERADE
</code></pre><h2 id=direct-network-connection>Direct network connection<a href=#direct-network-connection class=hanchor arialabel=Anchor>&#8983;</a></h2><p>If you have an external router that will act as a gateway for your
public network, you can add an interface on that network to the
bridge. For example, assuming that <code>eth2</code> was on the same network as
<code>172.24.4.225</code>:</p><pre><code># ovs-vsctl add-port br-ex eth2
</code></pre></div></div><script src=https://utteranc.es/client.js repo=larsks/blog.oddbit.com issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></article></div><footer class=footer><div class=footer__inner><div class="copyright copyright--user"><span>Lars Kellogg-Stedman</span>
<span>:: <a href=https://github.com/panr/hugo-theme-terminal target=_blank>Theme</a> made by <a href=https://github.com/panr target=_blank>panr</a></span></div></div></footer><script type=text/javascript src=/bundle.min.js></script><script src=/js/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0})</script></div></body></html>