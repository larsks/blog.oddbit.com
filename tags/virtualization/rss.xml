<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>virtualization on blog.oddbit.com</title><link>https://blog.oddbit.com/tags/virtualization/</link><description>Recent content in virtualization on blog.oddbit.com</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Lars Kellogg-Stedman</copyright><lastBuildDate>Sat, 17 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.oddbit.com/tags/virtualization/rss.xml" rel="self" type="application/rss+xml"/><item><title>Creating a VXLAN overlay network with Open vSwitch</title><link>https://blog.oddbit.com/post/2021-04-17-vm-ovs-vxlan/</link><pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-04-17-vm-ovs-vxlan/</guid><description>In this post, we&amp;rsquo;ll walk through the process of getting virtual machines on two different hosts to communicate over an overlay network created using the support for VXLAN in Open vSwitch (or OVS).
The test environment For this post, I&amp;rsquo;ll be working with two systems:
node0.ovs.virt at address 192.168.122.107 node1.ovs.virt at address 192.168.122.174 These hosts are running CentOS 8, although once we get past the package installs the instructions will be similar for other distributions.</description><content>&lt;p>In this post, we&amp;rsquo;ll walk through the process of getting virtual
machines on two different hosts to communicate over an overlay network
created using the support for VXLAN in &lt;a href="https://www.openvswitch.org/">Open vSwitch&lt;/a> (or OVS).&lt;/p>
&lt;h2 id="the-test-environment">The test environment&lt;/h2>
&lt;p>For this post, I&amp;rsquo;ll be working with two systems:&lt;/p>
&lt;ul>
&lt;li>&lt;code>node0.ovs.virt&lt;/code> at address 192.168.122.107&lt;/li>
&lt;li>&lt;code>node1.ovs.virt&lt;/code> at address 192.168.122.174&lt;/li>
&lt;/ul>
&lt;p>These hosts are running CentOS 8, although once we get past the
package installs the instructions will be similar for other
distributions.&lt;/p>
&lt;p>While reading through this post, remember that unless otherwise
specified we&amp;rsquo;re going to be running the indicated commands on &lt;em>both&lt;/em>
hosts.&lt;/p>
&lt;h2 id="install-packages">Install packages&lt;/h2>
&lt;p>Before we can get started configuring things we&amp;rsquo;ll need to install OVS
and &lt;a href="https://libvirt.org/">libvirt&lt;/a>. While &lt;code>libvirt&lt;/code> is included with the base CentOS
distribution, for OVS we&amp;rsquo;ll need to add both the &lt;a href="https://fedoraproject.org/wiki/EPEL">EPEL&lt;/a> repository
as well as a recent CentOS &lt;a href="https://www.openstack.org/">OpenStack&lt;/a> repository (OVS is included
in the CentOS OpenStack repositories because it is required by
OpenStack&amp;rsquo;s networking service):&lt;/p>
&lt;pre tabindex="0">&lt;code>yum -y install epel-release centos-release-openstack-victoria
&lt;/code>&lt;/pre>&lt;p>With these additional repositories enabled we can now install the
required packages:&lt;/p>
&lt;pre tabindex="0">&lt;code>yum -y install \
libguestfs-tools-c \
libvirt \
libvirt-daemon-kvm \
openvswitch2.15 \
tcpdump \
virt-install
&lt;/code>&lt;/pre>&lt;h2 id="enable-services">Enable services&lt;/h2>
&lt;p>We need to start both the &lt;code>libvirtd&lt;/code> and &lt;code>openvswitch&lt;/code> services:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl enable --now openvswitch libvirtd
&lt;/code>&lt;/pre>&lt;p>This command will (a) mark the services to start automatically when
the system boots and (b) immediately start the service.&lt;/p>
&lt;h2 id="configure-libvirt">Configure libvirt&lt;/h2>
&lt;p>When &lt;code>libvirt&lt;/code> is first installed it doesn&amp;rsquo;t have any configured
storage pools. Let&amp;rsquo;s create one in the default location,
&lt;code>/var/lib/libvirt/images&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>virsh pool-define-as default --type dir --target /var/lib/libvirt/images
&lt;/code>&lt;/pre>&lt;p>We need to mark the pool active, and we might as well configure it to
activate automatically next time the system boots:&lt;/p>
&lt;pre tabindex="0">&lt;code>virsh pool-start default
virsh pool-autostart default
&lt;/code>&lt;/pre>&lt;h2 id="configure-open-vswitch">Configure Open vSwitch&lt;/h2>
&lt;h3 id="create-the-bridge">Create the bridge&lt;/h3>
&lt;p>With all the prerequisites out of the way we can finally start working
with Open vSwitch. Our first task is to create the OVS bridge that
will host our VXLAN tunnels. To create a bridge named &lt;code>br0&lt;/code>, we run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-br br0
&lt;/code>&lt;/pre>&lt;p>We can inspect the OVS configuration by running &lt;code>ovs-vsctl show&lt;/code>,
which should output something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>cc1e7217-e393-4e21-97c1-92324d47946d
Bridge br0
Port br0
Interface br0
type: internal
ovs_version: &amp;#34;2.15.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s not forget to mark the interface &amp;ldquo;up&amp;rdquo;:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip link set br0 up
&lt;/code>&lt;/pre>&lt;h3 id="create-the-vxlan-tunnels">Create the VXLAN tunnels&lt;/h3>
&lt;p>Up until this point we&amp;rsquo;ve been running identical commands on both
&lt;code>node0&lt;/code> and &lt;code>node1&lt;/code>. In order to create our VXLAN tunnels, we need to
provide a remote endpoint for the VXLAN connection, which is going to
be &amp;ldquo;the other host&amp;rdquo;. On &lt;code>node0&lt;/code>, we run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br0 vx_node1 -- set interface vx_node1 \
type=vxlan options:remote_ip=192.168.122.174
&lt;/code>&lt;/pre>&lt;p>This creates a VXLAN interface named &lt;code>vx_node1&lt;/code> (named that way
because the remote endpoint is &lt;code>node1&lt;/code>). The OVS configuration now
looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>cc1e7217-e393-4e21-97c1-92324d47946d
Bridge br0
Port vx_node1
Interface vx_node1
type: vxlan
options: {remote_ip=&amp;#34;192.168.122.174&amp;#34;}
Port br0
Interface br0
type: internal
ovs_version: &amp;#34;2.15.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>On &lt;code>node1&lt;/code> we will run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br0 vx_node0 -- set interface vx_node0 \
type=vxlan options:remote_ip=192.168.122.107
&lt;/code>&lt;/pre>&lt;p>Which results in:&lt;/p>
&lt;pre tabindex="0">&lt;code>58451994-e0d1-4bf1-8f91-7253ddf4c016
Bridge br0
Port br0
Interface br0
type: internal
Port vx_node0
Interface vx_node0
type: vxlan
options: {remote_ip=&amp;#34;192.168.122.107&amp;#34;}
ovs_version: &amp;#34;2.15.1&amp;#34;
&lt;/code>&lt;/pre>&lt;p>At this point, we have a functional overlay network: anything attached
to &lt;code>br0&lt;/code> on either system will appear to share the same layer 2
network. Let&amp;rsquo;s take advantage of this to connect a pair of virtual
machines.&lt;/p>
&lt;h2 id="create-virtual-machines">Create virtual machines&lt;/h2>
&lt;h3 id="download-a-base-image">Download a base image&lt;/h3>
&lt;p>We&amp;rsquo;ll need a base image for our virtual machines. I&amp;rsquo;m going to use the
CentOS 8 Stream image, which we can download to our storage directory
like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -L -o /var/lib/libvirt/images/centos-8-stream.qcow2 \
https://cloud.centos.org/centos/8-stream/x86_64/images/CentOS-Stream-GenericCloud-8-20210210.0.x86_64.qcow2
&lt;/code>&lt;/pre>&lt;p>We need to make sure &lt;code>libvirt&lt;/code> is aware of the new image:&lt;/p>
&lt;pre tabindex="0">&lt;code>virsh pool-refresh default
&lt;/code>&lt;/pre>&lt;p>Lastly, we&amp;rsquo;ll want to set a root password on the image so that we can
log in to our virtual machines:&lt;/p>
&lt;pre tabindex="0">&lt;code>virt-customize -a /var/lib/libvirt/images/centos-8-stream.qcow2 \
--root-password password:secret
&lt;/code>&lt;/pre>&lt;h3 id="create-the-virtual-machine">Create the virtual machine&lt;/h3>
&lt;p>We&amp;rsquo;re going to create a pair of virtual machines (one on each host).
We&amp;rsquo;ll be creating each vm with two network interfaces:&lt;/p>
&lt;ul>
&lt;li>One will be attached to the libvirt &lt;code>default&lt;/code> network; this will
allow us to &lt;code>ssh&lt;/code> into the vm in order to configure things.&lt;/li>
&lt;li>The second will be attached to the OVS bridge&lt;/li>
&lt;/ul>
&lt;p>To create a virtual machine on &lt;code>node0&lt;/code> named &lt;code>vm0.0&lt;/code>, run the
following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>virt-install \
-r 3000 \
--network network=default \
--network bridge=br0,virtualport.type=openvswitch \
--os-variant centos8 \
--disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
--import \
--noautoconsole \
-n vm0.0
&lt;/code>&lt;/pre>&lt;p>The most interesting option in the above command line is probably the
one used to create the virtual disk:&lt;/p>
&lt;pre tabindex="0">&lt;code>--disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
&lt;/code>&lt;/pre>&lt;p>This creates a 10GB &amp;ldquo;&lt;a href="https://en.wikipedia.org/wiki/Copy-on-write">copy-on-write&lt;/a>&amp;rdquo; disk that uses
&lt;code>centos-8-stream.qcow2&lt;/code> as a backing store. That means that reads will
generally come from the &lt;code>centos-8-stream.qcow2&lt;/code> image, but writes will
be stored in the new image. This makes it easy for us to quickly
create multiple virtual machines from the same base image.&lt;/p>
&lt;p>On &lt;code>node1&lt;/code> we would run a similar command, although here we&amp;rsquo;re naming
the virtual machine &lt;code>vm1.0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>virt-install \
-r 3000 \
--network network=default \
--network bridge=br0,virtualport.type=openvswitch \
--os-variant centos8 \
--disk pool=default,size=10,backing_store=centos-8-stream.qcow2,backing_format=qcow2 \
--import \
--noautoconsole \
-n vm1.0
&lt;/code>&lt;/pre>&lt;h3 id="configure-networking-for-vm00">Configure networking for vm0.0&lt;/h3>
&lt;p>On &lt;code>node0&lt;/code>, get the address of the new virtual machine on the default
network using the &lt;code>virsh domifaddr&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node0 ~]# virsh domifaddr vm0.0
Name MAC address Protocol Address
-------------------------------------------------------------------------------
vnet2 52:54:00:21:6e:4f ipv4 192.168.124.83/24
&lt;/code>&lt;/pre>&lt;p>Connect to the vm using &lt;code>ssh&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node0 ~]# ssh 192.168.124.83
root@192.168.124.83&amp;#39;s password:
Activate the web console with: systemctl enable --now cockpit.socket
Last login: Sat Apr 17 14:08:17 2021 from 192.168.124.1
[root@localhost ~]#
&lt;/code>&lt;/pre>&lt;p>(Recall that the &lt;code>root&lt;/code> password is &lt;code>secret&lt;/code>.)&lt;/p>
&lt;p>Configure interface &lt;code>eth1&lt;/code> with an address. For this post, we&amp;rsquo;ll use
the &lt;code>10.0.0.0/24&lt;/code> range for our overlay network. To assign this vm the
address &lt;code>10.0.0.10&lt;/code>, we can run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip addr add 10.0.0.10/24 dev eth1
ip link set eth1 up
&lt;/code>&lt;/pre>&lt;h3 id="configure-networking-for-vm10">Configure networking for vm1.0&lt;/h3>
&lt;p>We need to repeat the process for &lt;code>vm1.0&lt;/code> on &lt;code>node1&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node1 ~]# virsh domifaddr vm1.0
Name MAC address Protocol Address
-------------------------------------------------------------------------------
vnet0 52:54:00:e9:6e:43 ipv4 192.168.124.69/24
&lt;/code>&lt;/pre>&lt;p>Connect to the vm using &lt;code>ssh&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node0 ~]# ssh 192.168.124.69
root@192.168.124.69&amp;#39;s password:
Activate the web console with: systemctl enable --now cockpit.socket
Last login: Sat Apr 17 14:08:17 2021 from 192.168.124.1
[root@localhost ~]#
&lt;/code>&lt;/pre>&lt;p>We&amp;rsquo;ll use address 10.0.0.11 for this system:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip addr add 10.0.0.11/24 dev eth1
ip link set eth1 up
&lt;/code>&lt;/pre>&lt;h3 id="verify-connectivity">Verify connectivity&lt;/h3>
&lt;p>At this point, our setup is complete. On &lt;code>vm0.0&lt;/code>, we can connect to
&lt;code>vm1.1&lt;/code> over the overlay network. For example, we can ping the remote
host:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@localhost ~]# ping -c2 10.0.0.11
PING 10.0.0.11 (10.0.0.11) 56(84) bytes of data.
64 bytes from 10.0.0.11: icmp_seq=1 ttl=64 time=1.79 ms
64 bytes from 10.0.0.11: icmp_seq=2 ttl=64 time=0.719 ms
--- 10.0.0.11 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 0.719/1.252/1.785/0.533 ms
&lt;/code>&lt;/pre>&lt;p>Or connect to it using &lt;code>ssh&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@localhost ~]# ssh 10.0.0.11 uptime
root@10.0.0.11&amp;#39;s password:
14:21:33 up 1:18, 1 user, load average: 0.00, 0.00, 0.00
&lt;/code>&lt;/pre>&lt;p>Using &lt;code>tcpdump&lt;/code>, we can verify that these connections are going over
the overlay network. Let&amp;rsquo;s watch for VXLAN traffic on &lt;code>node1&lt;/code> by
running the following command (VXLAN is a UDP protocol running on port
4789)&lt;/p>
&lt;pre tabindex="0">&lt;code>tcpdump -i eth0 -n port 4789
&lt;/code>&lt;/pre>&lt;p>When we run &lt;code>ping -c2 10.0.0.11&lt;/code> on &lt;code>vm0.0&lt;/code>, we see the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>14:23:50.312574 IP 192.168.122.107.52595 &amp;gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &amp;gt; 10.0.0.11: ICMP echo request, id 4915, seq 1, length 64
14:23:50.314896 IP 192.168.122.174.59510 &amp;gt; 192.168.122.107.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.11 &amp;gt; 10.0.0.10: ICMP echo reply, id 4915, seq 1, length 64
14:23:51.314080 IP 192.168.122.107.52595 &amp;gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &amp;gt; 10.0.0.11: ICMP echo request, id 4915, seq 2, length 64
14:23:51.314259 IP 192.168.122.174.59510 &amp;gt; 192.168.122.107.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.11 &amp;gt; 10.0.0.10: ICMP echo reply, id 4915, seq 2, length 64
&lt;/code>&lt;/pre>&lt;p>In the output above, we see that each packet in the transaction
results in two lines of output from &lt;code>tcpdump&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>14:23:50.312574 IP 192.168.122.107.52595 &amp;gt; 192.168.122.174.vxlan: VXLAN, flags [I] (0x08), vni 0
IP 10.0.0.10 &amp;gt; 10.0.0.11: ICMP echo request, id 4915, seq 1, length 64
&lt;/code>&lt;/pre>&lt;p>The first line shows the contents of the VXLAN packet, while the
second lines shows the data that was encapsulated in the VXLAN packet.&lt;/p>
&lt;h2 id="thats-all-folks">That&amp;rsquo;s all folks&lt;/h2>
&lt;p>We&amp;rsquo;ve achieved our goal: we have two virtual machines on two different
hosts communicating over a VXLAN overlay network. If you were to do
this &amp;ldquo;for real&amp;rdquo;, you would probably want to make a number of changes:
for example, the network configuration we&amp;rsquo;ve applied in many cases
will not persist across a reboot; handling persistent network
configuration is still very distribution dependent, so I&amp;rsquo;ve left it
out of this post.&lt;/p></content></item><item><title>OpenShift and CNV: MAC address management in CNV 2.4</title><link>https://blog.oddbit.com/post/2020-08-10-mac-address-management-in-cnv/</link><pubDate>Mon, 10 Aug 2020 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2020-08-10-mac-address-management-in-cnv/</guid><description>This is part of a series of posts about my experience working with OpenShift and CNV. In this post, I&amp;rsquo;ll look at how the recently released CNV 2.4 resolves some issues in managing virtual machines that are attached directly to local layer 2 networks
In an earlier post, I discussed some issues around the management of virtual machine MAC addresses in CNV 2.3: in particular, that virtual machines are assigned a random MAC address not just at creation time but every time they boot.</description><content>&lt;p>This is part of a &lt;a href="https://blog.oddbit.com/tag/openshift-and-cnv">series of posts&lt;/a> about my experience working with
&lt;a href="https://www.openshift.com/">OpenShift&lt;/a> and &lt;a href="https://www.redhat.com/en/topics/containers/what-is-container-native-virtualization">CNV&lt;/a>. In this post, I&amp;rsquo;ll look at how the
recently released CNV 2.4 resolves some issues in managing virtual
machines that are attached directly to local layer 2 networks&lt;/p>
&lt;p>In &lt;a href="https://blog.oddbit.com/post/2020-07-30-openshift-and-cnv-part-2-expos/">an earlier post&lt;/a>, I discussed some issues around the
management of virtual machine MAC addresses in CNV 2.3: in particular,
that virtual machines are assigned a random MAC address not just at
creation time but every time they boot. CNV 2.4 (re-)introduces &lt;a href="https://docs.openshift.com/container-platform/4.5/virt/virtual_machines/vm_networking/virt-using-mac-address-pool-for-vms.html">MAC
address pools&lt;/a> to alleviate these issues. The high level description
reads:&lt;/p>
&lt;blockquote>
&lt;p>The KubeMacPool component provides a MAC address pool service for
virtual machine NICs in designated namespaces.&lt;/p>
&lt;/blockquote>
&lt;p>In more specific terms, that means that if you enable MAC address
pools on a namespace, when you create create virtual machine network
interfaces they will receive a MAC address from the pool. This is
associated with the &lt;code>VirtualMachine&lt;/code> resource, &lt;strong>not&lt;/strong> the
&lt;code>VirtualMachineInstance&lt;/code> resource, which means that the MAC address
will persist across reboots.&lt;/p>
&lt;p>This solves one of the major pain points of using CNV-managed virtual
machines attached to host networks.&lt;/p>
&lt;p>To enable MAC address pools for a given namespace, set the
&lt;code>mutatevirtualmachines.kubemacpool.io&lt;/code> label to &lt;code>allocate&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>oc label namespace &amp;lt;namespace&amp;gt; mutatevirtualmachines.kubemacpool.io=allocate
&lt;/code>&lt;/pre></content></item><item><title>Automatic hostname entries for libvirt domains</title><link>https://blog.oddbit.com/post/2013-10-04-automatic-dns-entrie/</link><pubDate>Fri, 04 Oct 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-10-04-automatic-dns-entrie/</guid><description>Have you ever wished that you could use libvirt domain names as hostnames? So that you could do something like this:
$ virt-install -n anewhost ... $ ssh clouduser@anewhost Since this is something that would certainly make my life convenient, I put together a small script called virt-hosts that makes this possible. You can find virt-hosts in my virt-utils GitHub repository:
https://raw.github.com/larsks/virt-utils/master/virt-hosts Run by itself, with no options, virt-hosts will scan through your running domains for interfaces on the libvirt default network, look up those MAC addresses up in the corresponding default.</description><content>&lt;p>Have you ever wished that you could use &lt;code>libvirt&lt;/code> domain names as
hostnames? So that you could do something like this:&lt;/p>
&lt;pre>&lt;code>$ virt-install -n anewhost ...
$ ssh clouduser@anewhost
&lt;/code>&lt;/pre>
&lt;p>Since this is something that would certainly make my life convenient,
I put together a small script called &lt;a href="https://raw.github.com/larsks/virt-utils/master/virt-hosts">virt-hosts&lt;/a> that makes this
possible. You can find &lt;a href="https://raw.github.com/larsks/virt-utils/master/virt-hosts">virt-hosts&lt;/a> in my &lt;a href="https://raw.github.com/larsks/virt-utils/">virt-utils&lt;/a> GitHub
repository:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://raw.github.com/larsks/virt-utils/master/virt-hosts">https://raw.github.com/larsks/virt-utils/master/virt-hosts&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Run by itself, with no options, &lt;code>virt-hosts&lt;/code> will scan through your
running domains for interfaces on the libvirt &lt;code>default&lt;/code> network, look
up those MAC addresses up in the corresponding &lt;code>default.leases&lt;/code> file,
and then generate a hosts file on &lt;code>stdout&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code>$ virt-hosts
192.168.122.221 compute-tmp0-net0.default.virt compute-tmp0.default.virt
192.168.122.101 centos-0-net0.default.virt centos-0.default.virt
192.168.122.214 controller-tmp-net0.default.virt controller-tmp.default.virt
&lt;/code>&lt;/pre>
&lt;p>Each address will be assigned the name
&lt;code>&amp;lt;domain_name&amp;gt;-&amp;lt;interface_name&amp;gt;.&amp;lt;network_name&amp;gt;.virt&lt;/code>. The first
interface on the network will also be given the alias
&lt;code>&amp;lt;domain_name&amp;gt;.&amp;lt;network_name&amp;gt;.virt&lt;/code>, so a host with multiple
interfaces on the same network would look like this:&lt;/p>
&lt;pre>&lt;code>$ virt-hosts
192.168.122.221 host0-net0.default.virt host0.default.virt
192.168.122.110 host0-net1.default.virt
&lt;/code>&lt;/pre>
&lt;p>Of course, this is only half the solution: having generated a hosts
file we need to put it somewhere where your system can find it.&lt;/p>
&lt;h2 id="an-aside-incron">An aside: incron&lt;/h2>
&lt;p>Both of the following solutions rely on &lt;a href="http://inotify.aiken.cz/?section=incron&amp;amp;page=about&amp;amp;lang=en">incron&lt;/a>, a tool that uses
the Linux &lt;a href="http://en.wikipedia.org/wiki/Inotify">inotify&lt;/a> subsystem to trigger scripts in reaction to
events on file and directories. In this case, we&amp;rsquo;ll be using &lt;code>incron&lt;/code>
to monitor the dnsmasq &lt;code>default.leases&lt;/code> file and firing off a script
when it changes.&lt;/p>
&lt;p>You could accomplish the same thing using the &lt;code>inotifywait&lt;/code> program
from the &lt;a href="https://github.com/rvoicilas/inotify-tools/wiki">inotify-tools&lt;/a> package and a small wrapper script, or you
could hook up something to the libvirt events framework.&lt;/p>
&lt;h2 id="using-etchosts">Using /etc/hosts&lt;/h2>
&lt;p>If you want to update your &lt;code>/etc/hosts&lt;/code> file, you can place the
following into a script called &lt;code>update-virt-hosts&lt;/code> (somewhere in
root&amp;rsquo;s &lt;code>PATH&lt;/code>) and run that via &lt;a href="http://inotify.aiken.cz/?section=incron&amp;amp;page=about&amp;amp;lang=en">incron&lt;/a>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
sed -i '/^# BEGIN VIRT HOSTS/,/^# END VIRT HOSTS/ d' /etc/hosts
cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt;/etc/hosts
# BEGIN VIRT HOSTS
$(virt-hosts)
# END VIRT HOSTS
EOF
&lt;/code>&lt;/pre>
&lt;p>Make sure you have &lt;code>incron&lt;/code> installed, and add the following to
&lt;code>/etc/incron.d/virt-hosts&lt;/code>:&lt;/p>
&lt;pre>&lt;code>/var/lib/libvirt/dnsmasq/default.leases IN_MODIFY update-virt-hosts
&lt;/code>&lt;/pre>
&lt;p>This will cause &lt;code>incron&lt;/code> to run your &lt;code>update-virt-hosts&lt;/code> script
whenever it sees an &lt;code>IN_MODIFY&lt;/code> event on the &lt;code>default.leases&lt;/code> file.&lt;/p>
&lt;h2 id="using-networkmanager--dnsmasq">Using NetworkManager + dnsmasq&lt;/h2>
&lt;p>I am running NetworkManager with the &lt;code>dnsmasq&lt;/code> dns plugin. I created
the file &lt;code>/etc/NetworkManager/dnsmasq.d/virthosts&lt;/code> containing:&lt;/p>
&lt;pre>&lt;code>addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts
&lt;/code>&lt;/pre>
&lt;p>This will cause the &lt;code>dnsmasq&lt;/code> process started by &lt;code>NetworkManager&lt;/code> to
use that file as an additional hosts file. I then installed the
&lt;code>incron&lt;/code> package and dropped the following in
&lt;code>/etc/incron.d/virt-hosts&lt;/code>:&lt;/p>
&lt;pre>&lt;code>/var/lib/libvirt/dnsmasq/default.leases IN_MODIFY /usr/local/bin/virt-hosts -ur
&lt;/code>&lt;/pre>
&lt;p>This has &lt;code>incron&lt;/code> listen for changes to the &lt;code>default.leases&lt;/code> file, and
whenever it receives the &lt;code>IN_MODIFY&lt;/code> event it runs &lt;code>virt-hosts&lt;/code> with
the &lt;code>-u&lt;/code> (aka &lt;code>--update&lt;/code>) and &lt;code>-r&lt;/code> (aka &lt;code>--reload-dnsmasq&lt;/code>) flags.
Thef former causes &lt;code>virt-hosts&lt;/code> to send output to
&lt;code>/var/lib/libvirt/dnsmasq/default.addnhosts&lt;/code> instead of &lt;code>stdout&lt;/code>, and
the latter does a &lt;code>killall -HUP dnsmasq&lt;/code> after installing the new
hosts file.&lt;/p></content></item><item><title>Did Arch Linux eat your KVM?</title><link>https://blog.oddbit.com/post/2013-04-08-did-archlinux-eat-yo/</link><pubDate>Mon, 08 Apr 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-04-08-did-archlinux-eat-yo/</guid><description>A recent update to Arch Linux replaced the qemu-kvm package with an updated version of qemu. A side effect of this change is that the qemu-kvm binary is no longer available, and any libvirt guests on your system utilizing that binary will no longer operate. As is typical with Arch, there is no announcement about this incompatible change, and queries to #archlinux will be met with the knowledge, grace and decorum you would expect of that channel:</description><content>&lt;p>A recent update to &lt;a href="https://www.archlinux.org/">Arch Linux&lt;/a> replaced the &lt;code>qemu-kvm&lt;/code> package with
an updated version of &lt;code>qemu&lt;/code>. A side effect of this change is that
the &lt;code>qemu-kvm&lt;/code> binary is no longer available, and any &lt;code>libvirt&lt;/code> guests
on your system utilizing that binary will no longer operate. As is
typical with Arch, there is no announcement about this incompatible
change, and queries to &lt;code>#archlinux&lt;/code> will be met with the knowledge,
grace and decorum you would expect of that channel:&lt;/p>
&lt;pre>&lt;code>2013-04-08T18:00 &amp;lt; gtmanfred&amp;gt; USE --enable-kvm for fucks sake
2013-04-08T18:00 &amp;lt; gtmanfred&amp;gt; DO I HAVE TO SAY IT AGAIN?
&lt;/code>&lt;/pre>
&lt;p>The emulator binary is hardcoded into your domain in the &lt;code>&amp;lt;emulator&amp;gt;&lt;/code>
emulator, and typically looks something like this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;emulator&amp;gt;/usr/bin/qemu-kvm&amp;lt;/emulator&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>In order to get your guests working again after the upgrade you&amp;rsquo;ll
need to replace this path with an appropriate selection from one of
the other binaries provided by the &lt;code>qemu&lt;/code> package, which include
&lt;code>qemu-system-i386&lt;/code> and &lt;code>qemu-system-x86_64&lt;/code>. You&amp;rsquo;ll want to select
the one appropriate for your &lt;em>guest&lt;/em> architecture. You can do this
manually running &lt;code>virsh edit&lt;/code> for each affected guest, but if you have
more than a couple that rapidly becomes annoying.&lt;/p>
&lt;p>We can use &lt;a href="https://en.wikipedia.org/wiki/XSLT">XSLT&lt;/a> to write a transformation that will set the
&lt;code>&amp;lt;emulator&amp;gt;&lt;/code> to an appropriate value, and we can set things up to run
this automatically across all of our guests. The following stylesheet
will replace the &lt;code>&amp;lt;emulator&amp;gt;&lt;/code> tag with a path to an appropriate &lt;code>qemu&lt;/code> (by
extracting the &lt;code>arch&lt;/code> attribute of the &lt;code>domain/os/type&lt;/code> element:&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version=&amp;quot;1.0&amp;quot;?&amp;gt;
&amp;lt;xsl:stylesheet version=&amp;quot;1.0&amp;quot; xmlns:xsl=&amp;quot;http://www.w3.org/1999/XSL/Transform&amp;quot;&amp;gt;
&amp;lt;!-- copy all elements verbatim... --&amp;gt;
&amp;lt;xsl:template match=&amp;quot;@*|node()&amp;quot;&amp;gt;
&amp;lt;xsl:copy&amp;gt;
&amp;lt;xsl:apply-templates select=&amp;quot;@*|node()&amp;quot;/&amp;gt;
&amp;lt;/xsl:copy&amp;gt;
&amp;lt;/xsl:template&amp;gt;
&amp;lt;!-- ...except for the 'emulator' element. --&amp;gt;
&amp;lt;xsl:template match=&amp;quot;emulator&amp;quot;&amp;gt;
&amp;lt;emulator&amp;gt;/usr/bin/qemu-system-&amp;lt;xsl:value-of select=&amp;quot;/*/os/type/@arch&amp;quot;/&amp;gt;&amp;lt;/emulator&amp;gt;
&amp;lt;/xsl:template&amp;gt;
&amp;lt;/xsl:stylesheet&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re going to apply this to all of our (inactive) guests via the
&lt;code>virsh edit&lt;/code> subcommand. This command runs an editor (selected based
on your &lt;code>VISUAL&lt;/code> or &lt;code>EDITOR&lt;/code> environment variables) on your domain
XML. We need to create an &amp;ldquo;editor&amp;rdquo; that will apply the above
transformation to its input file. Something like this will work:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
tmpfile=$(mktemp &amp;quot;$1.patched.XXXXXX&amp;quot;)
xsltproc -o &amp;quot;$tmpfile&amp;quot; patch-emulator.xsl &amp;quot;$1&amp;quot;
mv &amp;quot;$tmpfile&amp;quot; &amp;quot;$1&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Assuming the above script has been saved as &amp;ldquo;patch-emulator.sh&amp;rdquo; (and
made executable), we can run this across all of our inactive guests
like this:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
VISUAL=./patch-emulator.sh
export VISUAL
virsh list --inactive --name | while read vm; do
[ &amp;quot;$vm&amp;quot; ] || continue
virsh edit $vm
done
&lt;/code>&lt;/pre></content></item><item><title>Growing a filesystem on a virtual disk</title><link>https://blog.oddbit.com/post/2012-10-24-resizing-virtual-disk/</link><pubDate>Wed, 24 Oct 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-10-24-resizing-virtual-disk/</guid><description>Occasionally we will deploy a virtual instance into our KVM infrastructure and realize after the fact that we need more local disk space available. This is the process we use to expand the disk image. This process assumes the following:
You&amp;rsquo;re using legacy disk partitions. The process for LVM is similar and I will describe that in another post (it&amp;rsquo;s generally identical except for an additional pvresize thrown in and lvextend in place of resize2fs).</description><content>&lt;p>Occasionally we will deploy a virtual instance into our KVM
infrastructure and realize after the fact that we need more local disk
space available. This is the process we use to expand the disk image.
This process assumes the following:&lt;/p>
&lt;ul>
&lt;li>You&amp;rsquo;re using legacy disk partitions. The process for LVM is similar
and I will describe that in another post (it&amp;rsquo;s generally identical
except for an additional &lt;code>pvresize&lt;/code> thrown in and &lt;code>lvextend&lt;/code> in
place of &lt;code>resize2fs&lt;/code>).&lt;/li>
&lt;li>The partition you need to resize is the last partition on the disk.&lt;/li>
&lt;/ul>
&lt;p>This process will work with either a &lt;code>qcow2&lt;/code> or &lt;code>raw&lt;/code> disk image. For
&lt;code>raw&lt;/code> images you can also run &lt;code>fdisk&lt;/code> on the host, potentially saving
yourself a reboot, but that&amp;rsquo;s less convenient for &lt;code>qcow2&lt;/code> format
images.&lt;/p>
&lt;hr>
&lt;p>We start with a 5.5G root filesystem with 4.4G free:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda2 5.5G 864M 4.4G 17% /
&lt;/code>&lt;/pre>
&lt;p>We need to shut down the system to grow the underlying image:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# poweroff
&lt;/code>&lt;/pre>
&lt;p>On the host, we use the &lt;code>qemu-img resize&lt;/code> command to grow the image.
First we need the path to the underlying disk image:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system dumpxml lars-test-0 | grep file
&amp;lt;disk type='file' device='disk'&amp;gt;
&amp;lt;source file='/var/lib/libvirt/images/lars-test-0-1.img'/&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And now we increase the image size by 10G:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ sudo qemu-img resize /var/lib/libvirt/images/lars-test-0.img +10G
Image resized.
&lt;/code>&lt;/pre>
&lt;p>Now reboot the instance:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system start lars-test-0
&lt;/code>&lt;/pre>
&lt;p>And login in on the console:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system console lars-test-0
Connected to domain lars-test-0
Escape character is ^]
Fedora release 17 (Beefy Miracle)
Kernel 3.6.2-4.fc17.x86_64 on an x86_64 (ttyS0)
localhost login: root
Password:
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re going to use &lt;code>fdisk&lt;/code> to modify the partition layout. Run
&lt;code>fdisk&lt;/code> on the system disk:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# fdisk /dev/vda
Welcome to fdisk (util-linux 2.21.2).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.
&lt;/code>&lt;/pre>
&lt;p>Print out the existing partition table and verify that you really are
going to be modifying the final partition:&lt;/p>
&lt;pre>&lt;code>Command (m for help): p
Disk /dev/vda: 19.3 GB, 19327352832 bytes
16 heads, 63 sectors/track, 37449 cylinders, total 37748736 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00007d9f
Device Boot Start End Blocks Id System
/dev/vda1 * 2048 1026047 512000 83 Linux
/dev/vda2 1026048 5154815 2064384 82 Linux swap / Solaris
/dev/vda3 5154816 16777215 5811200 83 Linux
&lt;/code>&lt;/pre>
&lt;p>Delete and recreate the final partition, in this case &lt;code>/dev/vda3&lt;/code>&amp;hellip;&lt;/p>
&lt;pre>&lt;code>Command (m for help): d
Partition number (1-4): 3
Partition 3 is deleted
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and create a new partition, accepting all the defaults. This will
create a new partition starting in the same place and extending to the
end of the disk:&lt;/p>
&lt;pre>&lt;code>Command (m for help): n
Partition type:
p primary (2 primary, 0 extended, 2 free)
e extended
Select (default p): p
Partition number (1-4, default 3): 3
First sector (5154816-37748735, default 5154816):
Using default value 5154816
Last sector, +sectors or +size{K,M,G} (5154816-37748735, default 37748735):
Using default value 37748735
Partition 3 of type Linux and of size 15.6 GiB is set
&lt;/code>&lt;/pre>
&lt;p>You can print out the new partition table to see that indeed
&lt;code>/dev/vda3&lt;/code> is now larger:&lt;/p>
&lt;pre>&lt;code>Command (m for help): p
Disk /dev/vda: 19.3 GB, 19327352832 bytes
16 heads, 63 sectors/track, 37449 cylinders, total 37748736 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00007d9f
Device Boot Start End Blocks Id System
/dev/vda1 * 2048 1026047 512000 83 Linux
/dev/vda2 1026048 5154815 2064384 82 Linux swap / Solaris
/dev/vda3 5154816 37748735 16296960 83 Linux
&lt;/code>&lt;/pre>
&lt;p>Write the changes to disk:&lt;/p>
&lt;pre>&lt;code>Command (m for help): w
The partition table has been altered!
Calling ioctl() to re-read partition table.
WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>Note the warning!&lt;/strong> The kernel has cached a copy of the old
partition table. We need to reboot the system before our changes are
visible! So we reboot the system:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# reboot
&lt;/code>&lt;/pre>
&lt;p>And log back in. Run &lt;code>df&lt;/code> to see the current size of the root
filesystem:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda3 5.5G 864M 4.4G 17% /
&lt;/code>&lt;/pre>
&lt;p>Now run &lt;code>resize2fs&lt;/code> to resize the root filesystem so that it expands
to fill our extended &lt;code>/dev/vda3&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# resize2fs /dev/vda3
resize2fs 1.42.3 (14-May-2012)
Filesystem at /dev/vda3 is mounted on /; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vda3 is now 4074240 blocks long.
&lt;/code>&lt;/pre>
&lt;p>Run &lt;code>df&lt;/code> again to see that we now have additional space available:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda3 16G 867M 14G 6% /
[root@localhost ~]#
&lt;/code>&lt;/pre></content></item></channel></rss>