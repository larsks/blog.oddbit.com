<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>pull-request on blog.oddbit.com</title><link>https://blog.oddbit.com/tags/pull-request/</link><description>Recent content in pull-request on blog.oddbit.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Lars Kellogg-Stedman</copyright><lastBuildDate>Fri, 26 Apr 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.oddbit.com/tags/pull-request/rss.xml" rel="self" type="application/rss+xml"/><item><title>Adding support for privilege escalation to Ansible's docker connection driver</title><link>https://blog.oddbit.com/posts/adding-support-for-privilege-e/</link><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/posts/adding-support-for-privilege-e/</guid><description>Update 2019-05-09 Pull request #55816 has merged, so you can now use sudo with the docker connection driver even when sudo is configured to require a password.
I often use Docker to test out Ansible playbooks. While normally that works great, I recently ran into an unexpected problem with privilege escalation. Given a simple playbook like this:
--- - hosts: all gather_facts: false become: true tasks: - ping: And an inventory like this:</description><content>&lt;p>&lt;strong>Update 2019-05-09&lt;/strong> Pull request
&lt;a href="https://github.com/ansible/ansible/pull/55816" class="pull-request">#55816&lt;/a>
has merged, so you can now use &lt;code>sudo&lt;/code> with the &lt;code>docker&lt;/code> connection driver even when &lt;code>sudo&lt;/code> is configured to require a password.&lt;/p>
&lt;hr>
&lt;p>I often use Docker to test out Ansible playbooks. While normally that works great, I recently ran into an unexpected problem with privilege escalation. Given a simple playbook like this:&lt;/p>
&lt;pre>&lt;code>---
- hosts: all
gather_facts: false
become: true
tasks:
- ping:
&lt;/code>&lt;/pre>
&lt;p>And an inventory like this:&lt;/p>
&lt;pre>&lt;code>all:
vars:
ansible_user: example
ansible_connection: docker
hosts:
server1:
ansible_host: sudostuff_server1_1
server2:
ansible_host: sudostuff_server2_1
server3:
ansible_host: sudostuff_server3_1
&lt;/code>&lt;/pre>
&lt;p>And containers with &lt;code>sudo&lt;/code> configured to require a password, Ansible would fail like this (note that I&amp;rsquo;ve configured Ansible to use the &lt;code>debug&lt;/code> plugin for &lt;code>stdout_callback&lt;/code>):&lt;/p>
&lt;pre tabindex="0">&lt;code>fatal: [server1]: FAILED! =&amp;gt; {
&amp;#34;changed&amp;#34;: false,
&amp;#34;rc&amp;#34;: 1
}
MSG:
MODULE FAILURE
See stdout/stderr for the exact error
MODULE_STDERR:
We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:
#1) Respect the privacy of others.
#2) Think before you type.
#3) With great power comes great responsibility.
[sudo via ansible, key=rzrfiifcqoggklmehivtcrrlnnbphwbp] password:
&lt;/code>&lt;/pre>&lt;p>In the above output, you&amp;rsquo;ll note that there are no actual errors, but unexpectedly we&amp;rsquo;re seeing the privilege escalation prompt show up in the &lt;code>stderr&lt;/code> of the command. A quick search revealed bugs &lt;a href="https://github.com/ansible/ansible/issues/31759">#31759&lt;/a> and &lt;a href="https://github.com/ansible/ansible/issues/53385">#53385&lt;/a>, both of which confirm that privilege escalation simply doesn&amp;rsquo;t work using the &lt;code>docker&lt;/code> connection plugin.&lt;/p>
&lt;h2 id="use-the-source-luke">Use the source, Luke&lt;/h2>
&lt;figure class="left" >
&lt;img src="https://blog.oddbit.com/assets/2019/04/26/sausage.jpg" />
&lt;figcaption class="center" >Discovering how the sausage is made...&lt;/figcaption>
&lt;/figure>
&lt;p>Looking at the source, I was surprised: while Ansible has individual plugins for different privilege escalation methods, it is entirely up to the individual connection plugin to implement the logic necessary to make use of these mechanisms. I had expected privilege escalation support to be implemented in the base connection plugin (&lt;code>ConnectionBase&lt;/code> in &lt;code>lib/ansible/plugins/connection/__init__.py&lt;/code>), but it&amp;rsquo;s not. So while the &lt;a href="https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/connection/ssh.py">ssh plugin&lt;/a> has a fairly complex set of logic for handing the &lt;code>become&lt;/code> prompt, and the &lt;a href="https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/connection/local.py">local plugin&lt;/a> had a relatively simple solution, the &lt;code>docker&lt;/code> connection had none.&lt;/p>
&lt;p>Fortunately, in many ways the &lt;code>docker&lt;/code> plugin is almost identical to the &lt;code>local&lt;/code> plugin, which means that rather than doing actual work I was able to largely cut-and-paste the privilege escalation support from the &lt;code>local&lt;/code> plugin into the &lt;code>docker&lt;/code> plugin. You can find this work in pull request
&lt;a href="https://github.com/ansible/ansible/pull/55816" class="pull-request">#55816&lt;/a>
.&lt;/p></content></item><item><title>Integrating Bitwarden with Ansible</title><link>https://blog.oddbit.com/posts/integrating-bitwarden-with-ans/</link><pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/posts/integrating-bitwarden-with-ans/</guid><description>Bitwarden is a password management service (like LastPass or 1Password). It&amp;rsquo;s unique in that it is built entirely on open source software. In addition to the the web UI and mobile apps that you would expect, Bitwarden also provides a command-line tool for interacting with the your password store.
At $WORK(-ish) we&amp;rsquo;re looking into Bitwarden because we want a password sharing and management solution that was better than dropping files into directories on remote hosts or sharing things over Slack.</description><content>&lt;p>&lt;a href="https://bitwarden.com">Bitwarden&lt;/a> is a password management service (like &lt;a href="https://www.lastpass.com/">LastPass&lt;/a> or
&lt;a href="https://1password.com/">1Password&lt;/a>). It&amp;rsquo;s unique in that it is built entirely on open
source software. In addition to the the web UI and mobile apps that
you would expect, Bitwarden also provides a &lt;a href="https://help.bitwarden.com/article/cli/">command-line tool&lt;/a> for
interacting with the your password store.&lt;/p>
&lt;p>At $WORK(-ish) we&amp;rsquo;re looking into Bitwarden because we want a password
sharing and management solution that was better than dropping files
into directories on remote hosts or sharing things over Slack. At
the same time, we are also thinking about bringing more automation to
our operational environment, possibly by making more extensive use of
&lt;a href="https://ansible.com">Ansible&lt;/a>. It looked like all the pieces were available to use
Bitwarden as a credential storage mechanism for Ansible playbooks, so
I set out to write a lookup plugin to implement the integration&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;only to find that I was not the first person to have this idea;
Matt Stofko &lt;a href="https://github.com/c0sco/ansible-modules-bitwarden/">beat me to it&lt;/a>. While it worked, the directory
structure of Matt&amp;rsquo;s repository made it difficult to integrate into an
existing Ansible project. It was also missing some convenience
features I wanted to see, so I have submitted
&lt;a href="https://github.com/c0sco/ansible-modules-bitwarden/pull/1" class="pull-request">#1&lt;/a>
that
makes several changes to the module.&lt;/p>
&lt;p>You can find my fork of the Bitwarden lookup plugin at
&lt;a href="https://github.com/larsks/ansible-modules-bitwarden">https://github.com/larsks/ansible-modules-bitwarden&lt;/a>.&lt;/p>
&lt;h2 id="make-it-installable">Make it installable&lt;/h2>
&lt;p>By re-arranging the repository to following the standard Ansible role
structure, it is now possible to install it either a submodule of your
own git repository, or to install it using the &lt;code>ansible-galaxy&lt;/code> tool:&lt;/p>
&lt;pre>&lt;code>ansible-galaxy install git+https://github.com/larsks/ansible-modules-bitwarden
&lt;/code>&lt;/pre>
&lt;p>This command would place the role in &lt;code>$HOME/.ansible/roles&lt;/code>, where it
will be available to any playbooks you run on your system.&lt;/p>
&lt;h2 id="add-explicit-support-for-custom-fields">Add explicit support for custom fields&lt;/h2>
&lt;p>While it was possible to access custom fields by fetching the complete
JSON representation of an item in Bitwarden and then querying the
resulting document, it wasn&amp;rsquo;t particularly graceful. I&amp;rsquo;ve added
explicit support for looking up custom fields. Whereas the normal
lookup will the specific keys that Bitwarden supports in the &lt;code>bw get&lt;/code>:&lt;/p>
&lt;pre>&lt;code>lookup('bitwarden', 'Google', field=username)
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;adding &lt;code>custom_field=True&lt;/code> causes the lookup to be performed against
the list of custom fields:&lt;/p>
&lt;pre>&lt;code>lookup('bitwarden', 'Google', field=mycustomfield, custom_field=true)
&lt;/code>&lt;/pre>
&lt;h2 id="add-support-for-the-sync-operation">Add support for the sync operation&lt;/h2>
&lt;p>The Bitwarden CLI operates by keeping a local cache of your
credentials. This means that if you have just modified an item through
the web ui (or mobile app), you may still be querying stale data when
querying Bitwarden through the CLI. The &lt;code>bw sync&lt;/code> command refreshes
the local cache.&lt;/p>
&lt;p>You can add &lt;code>sync=true&lt;/code> to the lookup to have Ansible run &lt;code>bw sync&lt;/code>
before querying Bitwarden for data:&lt;/p>
&lt;pre>&lt;code>lookup('bitwarden', 'Google', field=username, sync=true)
&lt;/code>&lt;/pre>
&lt;h2 id="using-the-lookup-module-in-practice">Using the lookup module in practice&lt;/h2>
&lt;p>We&amp;rsquo;re using &lt;a href="https://docs.openstack.org/tripleo-docs/latest/">TripleO&lt;/a> to deploy OpenStack. TripleO requires as input
to the deployment process a number of parameters, including various
credentials. For example, to set the password that will be assigned
to the Keystone admin user, one would pass in a file that looks
something like:&lt;/p>
&lt;pre>&lt;code>---
parameter_defaults:
AdminPassword: &amp;quot;secret.password.goes.here&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Because our deployment configuration is public, we don&amp;rsquo;t want to store
credentials there. We&amp;rsquo;ve been copying around a credentials file that
lives outside the repository, but that&amp;rsquo;s not a great solution.&lt;/p>
&lt;p>Using the Bitwarden lookup module, we can replace the above with:&lt;/p>
&lt;pre>&lt;code>---
parameter_defaults:
AdminPassword: &amp;quot;{{ lookup('bitwarden', 'keystone admin') }}&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>With this change, we can use Ansible to query Bitwarden to get the
Keystone admin password and generate as output a file with the
passwords included.&lt;/p>
&lt;p>Using the custom field support, we can include metadata associated
with a credential in the same place as the credential itself. To
configure access to a remote Ceph installation, we need to provide a
client key and cluster id. By putting the cluster id in a custom
field, we can do something like this:&lt;/p>
&lt;pre>&lt;code>CephClientKey: &amp;quot;{{ lookup('bitwarden', 'ceph client key') }}&amp;quot;
CephClusterFSID: &amp;quot;{{ ((lookup('bitwarden', 'ceph client key', field='clusterid', custom_field=true) }}&amp;quot;
&lt;/code>&lt;/pre>
&lt;h2 id="an-example-playbook">An example playbook&lt;/h2>
&lt;p>Before you can run a playbook making use of the Bitwarden lookup
module, you need to &lt;a href="https://help.bitwarden.com/article/cli/#download--install">install&lt;/a> the Bitwarden CLI. This is as simple
as grabbing an appropriate binary and dropping it somewhere in
your &lt;code>$PATH&lt;/code>. I&amp;rsquo;ve been doing this:&lt;/p>
&lt;pre>&lt;code>$ curl -L 'https://vault.bitwarden.com/download/?app=cli&amp;amp;platform=linux' |
funzip &amp;gt; $HOME/bin/bw
$ chmod 755 $HOME/bin/bw
&lt;/code>&lt;/pre>
&lt;p>For the following example, assume that we have a template named
&lt;code>no-passwords-here.yml&lt;/code> matching the earlier example:&lt;/p>
&lt;pre>&lt;code>---
parameter_defaults:
AdminPassword: &amp;quot;{{ lookup('bitwarden', 'keystone admin') }}&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>We can generate a version of the file named &lt;code>yes-passwords-here.yml&lt;/code>
that includes the actual passwords by running the following playbook:&lt;/p>
&lt;pre>&lt;code>---
- hosts: localhost
# we need to include the role in order to make the lookup plugin
# available.
roles:
- ansible-modules-bitwarden
tasks:
- name: inject passwords into a file
template:
src: ./no-passwords-here.yml
dest: ./yes-passwords-here.yml
&lt;/code>&lt;/pre>
&lt;p>To actually run the playbook, we need to be authenticated to Bitwarden
first. That means:&lt;/p>
&lt;ol>
&lt;li>Run &lt;code>bw login&lt;/code> (or &lt;code>bw unlock&lt;/code>) to log in and get a session key.&lt;/li>
&lt;li>Set the &lt;code>BW_SESSION&lt;/code> environment variable to this value.&lt;/li>
&lt;li>Run the playbook.&lt;/li>
&lt;/ol>
&lt;p>The above tasks would look something like this:&lt;/p>
&lt;pre>&lt;code>bash$ bw login
? Email address: lars@redhat.com
? Master password: [hidden]
You are logged in!
To unlock your vault, set your session key to the `BW_SESSION`
environment variable. ex:
$ export BW_SESSION=&amp;quot;...&amp;quot;
[...]
bash$ export BW_SESSION=&amp;quot;...&amp;quot;
bash$ ansible-playbook inject-passwords.yml
&lt;/code>&lt;/pre></content></item><item><title>A systemd-nspawn connection driver for Ansible</title><link>https://blog.oddbit.com/posts/a-systemd-nspawn-connection-dr/</link><pubDate>Mon, 08 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/posts/a-systemd-nspawn-connection-dr/</guid><description>I wrote earlier about systemd-nspawn, and how it can take much of the fiddly work out of setting up functional chroot environments. I&amp;rsquo;m a regular Ansible user, and I wanted to be able to apply some of those techniques to my playbooks.
Ansible already has a chroot module, of course, but for some situations &amp;ndash; such as targeting an emulated chroot environment &amp;ndash; that just means a lot of extra work.</description><content>&lt;p>I wrote &lt;a href="https://blog.oddbit.com/posts/systemd-nspawn-for-fun-and-wel/">earlier&lt;/a> about &lt;a href="https://www.freedesktop.org/software/systemd/man/systemd-nspawn.html">systemd-nspawn&lt;/a>, and how it can take much
of the fiddly work out of setting up functional &lt;code>chroot&lt;/code> environments.
I&amp;rsquo;m a regular &lt;a href="http://ansible.com/">Ansible&lt;/a> user, and I wanted to be able to apply some
of those techniques to my playbooks.&lt;/p>
&lt;p>Ansible already has a &lt;code>chroot&lt;/code> module, of course, but for some
situations &amp;ndash; such as targeting an emulated &lt;code>chroot&lt;/code> environment &amp;ndash;
that just means a lot of extra work. Using &lt;code>systemd-nspawn&lt;/code> makes
this trivial.&lt;/p>
&lt;p>I&amp;rsquo;ve submitted
&lt;a href="https://github.com/ansible/ansible/pull/14334" class="pull-request">#14334&lt;/a>
to the Ansible project,
which introduces a new connection driver named &lt;code>nspawn&lt;/code>. It acts very
much like the &lt;code>chroot&lt;/code> driver, but it adds a few new configuration
options:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>ansible_nspawn_args&lt;/code> &amp;ndash; analagous to &lt;code>ansible_ssh_args&lt;/code>, setting
this will override the arguments that are passed to &lt;code>systemd-nspawn&lt;/code>
by default.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ansible_nspawn_extra_args&lt;/code> &amp;ndash; analgous to &lt;code>ansible_ssh_extra_args&lt;/code>,
setting this will &lt;em>append&lt;/em> the values to the default
&lt;code>systemd-nspawn&lt;/code> command line.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="advantages-over-chroot">Advantages over chroot&lt;/h2>
&lt;p>Let&amp;rsquo;s say we had a Fedora filesystem mounted on &lt;code>/fedora&lt;/code> and we want
to run the following playbook:&lt;/p>
&lt;pre>&lt;code>- hosts: /fedora
tasks:
- raw: dnf -y install python libselinux-python python2-dnf
- dnf:
name: git
state: installed
&lt;/code>&lt;/pre>
&lt;p>Using the &lt;code>chroot&lt;/code> driver, we get:&lt;/p>
&lt;pre>&lt;code>$ sudo ansible-playbook -i /fedora, -c chroot playbook.yml
PLAY ***************************************************************************
TASK [raw] *********************************************************************
fatal: [/fedora]: FAILED! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;failed&amp;quot;: true, &amp;quot;rc&amp;quot;: -6, &amp;quot;stderr&amp;quot;: &amp;quot;Fatal Python error: Failed to open /dev/urandom\n&amp;quot;, &amp;quot;stdout&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;stdout_lines&amp;quot;: []}
&lt;/code>&lt;/pre>
&lt;p>Adding the necessary tasks to our playbook to set up the chroot
environment properly will add a lot of additional complexity and will
make the playbook substantially less generic. Now compare that to the
result of running the same playbook using the &lt;code>nspawn&lt;/code> driver:&lt;/p>
&lt;pre>&lt;code>$ sudo ansible-playbook -i /fedora, -c nspawn playbook.yml
PLAY ***************************************************************************
TASK [raw] *********************************************************************
ok: [/fedora]
TASK [dnf] *********************************************************************
changed: [/fedora]
PLAY RECAP *********************************************************************
/fedora : ok=2 changed=1 unreachable=0 failed=0
&lt;/code>&lt;/pre>
&lt;h2 id="ansible-in-emulation">Ansible in emulation&lt;/h2>
&lt;p>By taking advantage of &lt;code>ansible_nspawn_extra_args&lt;/code> you can create
more complex containers. For example, in my &lt;a href="https://blog.oddbit.com/posts/systemd-nspawn-for-fun-and-wel/">last post&lt;/a> on
&lt;code>systemd-nspawn&lt;/code> I showed how to start a container for a different
architecture through the use of QEMU user-mode emulation. We can
apply the same idea to Ansible with an inventory entry like this:&lt;/p>
&lt;pre>&lt;code>target
ansible_host=/fedora
ansible_connection=nspawn
ansible_nspawn_extra_args=&amp;quot;--bind /usr/bin/qemu-arm&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The above will allow you to run a playbook against a filesystem
containing ARM architecture binaries, even though you&amp;rsquo;re running on an
x86_64 host.&lt;/p></content></item><item><title>Folding long lines in Ansible inventory files</title><link>https://blog.oddbit.com/posts/folding-long-lines-in-ansible-/</link><pubDate>Sun, 07 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/posts/folding-long-lines-in-ansible-/</guid><description>If you have an Ansible inventory file that includes lots of per host variables, it&amp;rsquo;s not unusual for lines to get long enough that they become unwieldly, particularly if you want to discuss them in an email or write about them in some context (e.g., a blog post).
I&amp;rsquo;ve just submitted pull request #14359 to Ansible which implements support for folding long lines using the INI-format convention of using indent to mark extended logical lines.</description><content>&lt;p>If you have an Ansible inventory file that includes lots of per host
variables, it&amp;rsquo;s not unusual for lines to get long enough that they
become unwieldly, particularly if you want to discuss them in an email
or write about them in some context (e.g., a blog post).&lt;/p>
&lt;p>I&amp;rsquo;ve just submitted pull request &lt;a href="https://github.com/ansible/ansible/pull/14359">#14359&lt;/a> to Ansible which
implements support for folding long lines using the INI-format
convention of using indent to mark extended logical lines.&lt;/p>
&lt;p>With this patch in place, you can turn this:&lt;/p>
&lt;pre>&lt;code>myhost ansible_host=a.b.c.d ansible_user=alice ansible_become=true ansible_ssh_extra_args=&amp;quot;-F ~/.ssh/specialconfig&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Into this:&lt;/p>
&lt;pre>&lt;code>myhost
ansible_host=a.b.c.d
ansible_user=alice
ansible_become=true
ansible_ssh_extra_args=&amp;quot;-F ~/.ssh/specialconfig&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>I think that&amp;rsquo;s a lot easier to read.&lt;/p>
&lt;p>If you think this is a good idea (or not!), feel free to comment on
the
&lt;a href="https://github.com/ansible/ansible/pull/14359" class="pull-request">#14359&lt;/a>
. I considered (and implemented, then discarded)
using a backslash-based model instead&amp;hellip;&lt;/p>
&lt;pre>&lt;code>myhost \
ansible_host=a.b.c.d \
...
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;but I was swayed by the fact that the indent-style model is at
least documented &lt;a href="https://docs.python.org/3/library/configparser.html#supported-ini-file-structure">somewhere&lt;/a>, and with the backslash
model it&amp;rsquo;s easy to end up with something like this:&lt;/p>
&lt;pre>&lt;code>myhost \
ansible_host=a.b.c.d # &amp;lt;--- OOOPS NO BACKSLASH
ansible_user=alice \
ansible_become=true
&lt;/code>&lt;/pre></content></item><item><title>Ansible 2.0: New OpenStack modules</title><link>https://blog.oddbit.com/posts/ansible-20-new-openstack-modul/</link><pubDate>Mon, 26 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/posts/ansible-20-new-openstack-modul/</guid><description>This is the second in a loose sequence of articles looking at new features in Ansible 2.0. In the previous article I looked at the Docker connection driver. In this article, I would like to provide an overview of the new-and-much-improved suite of modules for interacting with an OpenStack environment, and provide a few examples of their use.
In versions of Ansible prior to 2.0, there was a small collection of OpenStack modules.</description><content>&lt;p>This is the second in a loose sequence of articles looking at new
features in Ansible 2.0. In the previous article I looked at the
&lt;a href="https://blog.oddbit.com/posts/ansible-20-the-docker-connecti/">Docker connection driver&lt;/a>. In this article, I would like to
provide an overview of the new-and-much-improved suite of modules for
interacting with an &lt;a href="http://www.openstack.org/">OpenStack&lt;/a> environment, and provide a few
examples of their use.&lt;/p>
&lt;p>In versions of Ansible prior to 2.0, there was a small collection of
OpenStack modules. There was the minimum necessary to boot a Nova
instance:&lt;/p>
&lt;ul>
&lt;li>&lt;code>glance_image.py&lt;/code>&lt;/li>
&lt;li>&lt;code>keystone_user.py&lt;/code>&lt;/li>
&lt;li>&lt;code>nova_compute.py&lt;/code>&lt;/li>
&lt;li>&lt;code>nova_keypair.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>And a collection of modules for interacting with &lt;a href="https://wiki.openstack.org/wiki/Neutron">Neutron&lt;/a> (previously
Quantum):&lt;/p>
&lt;ul>
&lt;li>&lt;code>quantum_floating_ip_associate.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_floating_ip.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_network.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router_gateway.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router_interface.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_subnet.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>While functional, these modules did not provide very comprehensive
coverage of even basic OpenStack services, and they suffered from
having a great deal of duplicated code (which made ensuring
consistent behavior across all the modules more difficult). The
behavior of these modules was not always what you would expect (e.g.,
the &lt;code>nova_compute&lt;/code> module would return information in different forms
depending on whether it had to create an instance or not).&lt;/p>
&lt;h2 id="throwing-shade">Throwing Shade&lt;/h2>
&lt;p>The situation is much improved in Ansible 2.0, which introduces a new
suite of OpenStack modules in which the common code has been factored
out into the &lt;a href="https://pypi.python.org/pypi/shade">Shade&lt;/a> project, a Python package that provides a
simpler interface to OpenStack than is available using the native
clients. Collecting this code in one place will help ensure both that
these Ansible modules share consistent behavior and that they are
easier to maintain.&lt;/p>
&lt;p>There are modules for managing Keystone:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_auth.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_user_group.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_user.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Glance:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_image_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_image.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Cinder:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_server_volume.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_volume.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Nova:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_keypair.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_nova_flavor.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server_actions.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Ironic:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_ironic_node.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_ironic.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Neutron and Nova Networking:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_floating_ip.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_network.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_networks_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_port.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_router.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_security_group.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_security_group_rule.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_subnet.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_subnets_facts.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>and Swift:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_object.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="authentication">Authentication&lt;/h2>
&lt;p>Shade uses the &lt;a href="https://pypi.python.org/pypi/os-client-config/">os-client-config&lt;/a> library to configure
authentication credentials for your OpenStack environment.&lt;/p>
&lt;p>In the absence of any authentication information provided in your
Ansible playbook, these modules will attempt to use the standard suite
of &lt;code>OS_*&lt;/code> variables (&lt;code>OS_USERNAME&lt;/code>, &lt;code>OS_PASSWORD&lt;/code>, etc). This is fine
for testing, but you usually want to provide some sort of
authentication configuration in your Ansible environment.&lt;/p>
&lt;p>You can provide credentials directly in your plays by providing an
&lt;code>auth&lt;/code> argument to any of the modules. For example:&lt;/p>
&lt;pre>&lt;code>- os_image:
auth:
auth_url: http://openstack.local:5000/v2.0
username: admin
password: secret
project_name: admin
[...]
&lt;/code>&lt;/pre>
&lt;p>But that can get complicated, especially if you are maintaining
multiple sets of credentials. The &lt;code>shade&lt;/code> library allows you to
manage credentials in a file named (by default) &lt;code>clouds.yml&lt;/code>, which
&lt;code>shade&lt;/code> searches for in:&lt;/p>
&lt;ul>
&lt;li>The current directory&lt;/li>
&lt;li>&lt;code>$HOME/.config/openstack/&lt;/code>&lt;/li>
&lt;li>&lt;code>/etc/xdg/openstack/&lt;/code>&lt;/li>
&lt;li>&lt;code>/etc/openstack&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>This file may contain credentials for one or more cloud environments,
for example:&lt;/p>
&lt;pre>&lt;code>clouds:
testing:
auth:
auth_url: http://openstack.local:5000/v2.0
username: admin
password: secret
project_name: admin
&lt;/code>&lt;/pre>
&lt;p>If you have the above in &lt;code>clouds.yml&lt;/code> along with your playbook, the
above &lt;code>os_image&lt;/code> example can be rewritten as:&lt;/p>
&lt;pre>&lt;code>- os_image:
cloud: testing
[...]
&lt;/code>&lt;/pre>
&lt;h2 id="return-values">Return values&lt;/h2>
&lt;p>The new modules all return useful information about the objects they
have created. For example, if you create a network using
&lt;a href="http://docs.ansible.com/ansible/os_network_module.html">os_network&lt;/a> and register that result:&lt;/p>
&lt;pre>&lt;code>- os_network:
cloud: testing
name: mynetwork
register: mynetwork
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll get back a dictionary containing a top-level &lt;code>id&lt;/code> attribute,
which is the UUID of the created network, along with a &lt;code>network&lt;/code>
attribute containing a dictionary of information about the created
object. The &lt;a href="http://docs.ansible.com/ansible/debug_module.html">debug&lt;/a> module is an excellent tool for exploring these
return values. If we put the following in our playbook immediately
after the above task:&lt;/p>
&lt;pre>&lt;code>- debug:
var: mynetwork
&lt;/code>&lt;/pre>
&lt;p>We would get output that looks something like:&lt;/p>
&lt;pre>&lt;code>ok: [localhost] =&amp;gt; {
&amp;quot;changed&amp;quot;: false,
&amp;quot;mynetwork&amp;quot;: {
&amp;quot;changed&amp;quot;: true,
&amp;quot;id&amp;quot;: &amp;quot;02b77e32-794a-4102-ab1b-1b90e6d4d92f&amp;quot;,
&amp;quot;invocation&amp;quot;: {
&amp;quot;module_args&amp;quot;: {
&amp;quot;cloud&amp;quot;: &amp;quot;testing&amp;quot;,
&amp;quot;name&amp;quot;: &amp;quot;mynetwork&amp;quot;
},
&amp;quot;module_name&amp;quot;: &amp;quot;os_network&amp;quot;
},
&amp;quot;network&amp;quot;: {
&amp;quot;admin_state_up&amp;quot;: true,
&amp;quot;id&amp;quot;: &amp;quot;02b77e32-794a-4102-ab1b-1b90e6d4d92f&amp;quot;,
&amp;quot;mtu&amp;quot;: 0,
&amp;quot;name&amp;quot;: &amp;quot;mynetwork&amp;quot;,
&amp;quot;provider:network_type&amp;quot;: &amp;quot;vxlan&amp;quot;,
&amp;quot;provider:physical_network&amp;quot;: null,
&amp;quot;provider:segmentation_id&amp;quot;: 79,
&amp;quot;router:external&amp;quot;: false,
&amp;quot;shared&amp;quot;: false,
&amp;quot;status&amp;quot;: &amp;quot;ACTIVE&amp;quot;,
&amp;quot;subnets&amp;quot;: [],
&amp;quot;tenant_id&amp;quot;: &amp;quot;349a8b95c5ad4a3383149f65f8c44cff&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>I have written a set of basic &lt;a href="https://github.com/ansible/ansible/pull/12875">integration tests&lt;/a> for these modules.
I hope the pull request is merged, but even if not it provides an
example of how to make use of many of these new modules.&lt;/p>
&lt;p>I&amp;rsquo;d like to present a few brief examples here to give you a sense of
what working with the new modules is like.&lt;/p>
&lt;h3 id="uploading-an-image-to-glance">Uploading an image to Glance&lt;/h3>
&lt;p>The &lt;a href="http://docs.ansible.com/ansible/os_image_module.html">os_image&lt;/a> module is used to upload an image to Glance. Assuming
that you have file named &lt;code>cirros.qcow2&lt;/code> available locally, this will
create an image named &lt;code>cirros&lt;/code> in Glance:&lt;/p>
&lt;pre>&lt;code>- os_image:
cloud: testing
name: cirros
state: present
disk_format: qcow2
container_format: bare
filename: cirros.qcow2
&lt;/code>&lt;/pre>
&lt;h3 id="booting-a-nova-server">Booting a Nova server&lt;/h3>
&lt;p>The &lt;a href="http://docs.ansible.com/ansible/os_server_module.html">os_server&lt;/a> module, which is used for booting virtual servers
(&amp;ldquo;instances&amp;rdquo;) in Nova, replaces the &lt;a href="http://docs.ansible.com/ansible/nova_compute_module.html">nova_compute&lt;/a> module available
in Ansible versions before 2.0:&lt;/p>
&lt;pre>&lt;code>- name: create a nova server
os_server:
cloud: testing
name: myserver
state: present
nics:
- net-name: private
image: cirros
flavor: m1.small
key_name: my_ssh_key
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>nics&lt;/code> parameter can accept net names, net ids, port names, and
port ids. So you could also do this (assuming you were attaching your
server to two different tenant networks):&lt;/p>
&lt;pre>&lt;code>nics:
- net-id: c875770c-a20b-45b5-a9da-5aca97153053
- net-name: private
&lt;/code>&lt;/pre>
&lt;p>The above examples are using a YAML list of dictionaries to provide
the information. You can also pass in a comma-delimited key=value
string, like this:&lt;/p>
&lt;pre>&lt;code>nics: net-name=private,net-name=database
&lt;/code>&lt;/pre>
&lt;p>This syntax is particular useful if you are running ad-hoc commands on
the command line:&lt;/p>
&lt;pre>&lt;code>ansible localhost -m os_server -a '
cloud=testing name=myserver nics=net-name=private
image=cirros flavor=m1.small key_name=my_ssh_key'
&lt;/code>&lt;/pre>
&lt;h3 id="adding-a-nova-server-to-your-ansible-inventory">Adding a Nova server to your Ansible inventory&lt;/h3>
&lt;p>I&amp;rsquo;d like to conclude this post with a longer example, that
demonstrates how you can use the &lt;a href="http://docs.ansible.com/ansible/add_host_module.html">add_host&lt;/a> module to add a freshly
created server to your inventory, and then target that new server in
your playbook. I&amp;rsquo;ve split up this playbook with commentary; in
practice, the pre-formatted text in this section would all be in a
single playbook (like &lt;a href="https://blog.oddbit.com/assets/2015/10/26/playbook.yml">this&lt;/a>).&lt;/p>
&lt;pre>&lt;code>- hosts: localhost
tasks:
&lt;/code>&lt;/pre>
&lt;p>This first task boots the server. The values for &lt;code>image&lt;/code>, &lt;code>nics&lt;/code>, and
`key_name will need to be adjusted for your environment.&lt;/p>
&lt;pre>&lt;code> - os_server:
cloud: testing
name: myserver
image: centos-7-atomic
nics:
- net-name: private
flavor: m1.small
key_name: lars
auto_ip: true
register: myserver
&lt;/code>&lt;/pre>
&lt;p>This &lt;code>debug&lt;/code> entry simply shows us what values were returned in the
&lt;code>myserver&lt;/code> variable.&lt;/p>
&lt;pre>&lt;code> - debug:
var: myserver
&lt;/code>&lt;/pre>
&lt;p>Now we add the new host to our Ansible inventory. For this to work,
you need to have assigned a floating ip to the server (either using
&lt;code>auto_ip&lt;/code>, as in this example, or by assigning one explicitly), and
you need to be running this playbook somewhere that has a route to the
floating ip address.&lt;/p>
&lt;pre>&lt;code> - add_host:
name: myserver
groups: openstack
ansible_host: &amp;quot;{{myserver.server.public_v4}}&amp;quot;
ansible_user: centos
ansible_become: true
&lt;/code>&lt;/pre>
&lt;p>Note that in the above play you can&amp;rsquo;t use information from the
inventory because that new host won&amp;rsquo;t exist in the inventory until
&lt;em>after&lt;/em> this play completes.&lt;/p>
&lt;p>We&amp;rsquo;ll need to wait for the server to finish booting and provisioning
before we are able to target it with ansible. A typical cloud image
is configured to run &lt;a href="https://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> when it boots, which will take
care of a number of initial configuration tasks, including
provisioning the ssh key we configured using &lt;code>os_server&lt;/code>. Until this
process is complete, we won&amp;rsquo;t have remote access to the server.&lt;/p>
&lt;p>We can&amp;rsquo;t use the &lt;code>wait_for&lt;/code> module because that will only
check for an open port. Instead, we use a &lt;a href="http://docs.ansible.com/ansible/playbooks_loops.html#do-until-loops">do-until loop&lt;/a> to
wait until we are able to successfully run a command on the server via
ssh.&lt;/p>
&lt;pre>&lt;code> - command: &amp;gt;
ssh -o BatchMode=yes
centos@{{myserver.server.public_v4}} true
register: result
until: result|success
retries: 300
delay: 5
&lt;/code>&lt;/pre>
&lt;p>Now that we have added the new server to our inventory
we can target it in subsequent plays (such as this one):&lt;/p>
&lt;pre>&lt;code>- hosts: myserver
tasks:
- service:
name: docker
state: running
enabled: true
&lt;/code>&lt;/pre></content></item><item><title>Teach git about GIT_SSL_CIPHER_LIST</title><link>https://blog.oddbit.com/posts/git-ssl-cipher-list/</link><pubDate>Fri, 08 May 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/posts/git-ssl-cipher-list/</guid><description>Someone named hithard on StackOverflow was trying to clone a git repository via https, and was running into an odd error: &amp;ldquo;Cannot communicate securely with peer: no common encryption algorithm(s).&amp;rdquo;. This was due to the fact that the server (openhatch.org) was configured to use a cipher suite that was not supported by default in the underlying SSL library (which could be either OpenSSL or NSS, depending on how git was built).</description><content>&lt;p>Someone named &lt;a href="https://stackoverflow.com/users/4713895/hithard">hithard&lt;/a> on &lt;a href="https://stackoverflow.com/">StackOverflow&lt;/a> was trying to clone a git repository via https, and was &lt;a href="https://stackoverflow.com/a/30090725/147356">running into an odd error&lt;/a>: &amp;ldquo;Cannot communicate securely with peer: no common encryption algorithm(s).&amp;rdquo;. This was due to the fact that the server (&lt;code>openhatch.org&lt;/code>) was configured to use a cipher suite that was not supported by default in the underlying SSL library (which could be either &lt;a href="https://www.openssl.org/">OpenSSL&lt;/a> or &lt;a href="https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS">NSS&lt;/a>, depending on how git was built).&lt;/p>
&lt;p>Many applications allow the user to configure an explicit list of ciphers to consider when negotiating a secure connection. For example, &lt;a href="https://curl.haxx.se/">curl&lt;/a> has the &lt;a href="https://curl.haxx.se/libcurl/c/CURLOPT_SSL_CIPHER_LIST.html">CURLOPT_SSL_CIPHER_LIST&lt;/a> option. This turns out to be especially relevant because git relies on &lt;a href="https://curl.haxx.se/libcurl/">libcurl&lt;/a> for all of its http operations, which means all we need to do is (a) create a new configuration option for git, and then (b) pass that value through to libcurl.&lt;/p>
&lt;p>I took a look at the code and it turned out to be surprisingly easy. The functional part of the patch ends up being less than 10 lines total:&lt;/p>
&lt;pre tabindex="0">&lt;code>diff --git a/http.c b/http.c
index 679862006..c5e947965 100644
--- a/http.c
+++ b/http.c
@@ -35,6 +35,7 @@ char curl_errorstr[CURL_ERROR_SIZE];
static int curl_ssl_verify = -1;
static int curl_ssl_try;
static const char *ssl_cert;
+static const char *ssl_cipherlist;
#if LIBCURL_VERSION_NUM &amp;gt;= 0x070903
static const char *ssl_key;
#endif
@@ -153,6 +154,8 @@ static int http_options(const char *var, const char *value, void *cb)
curl_ssl_verify = git_config_bool(var, value);
return 0;
}
+ if (!strcmp(&amp;#34;http.sslcipherlist&amp;#34;, var))
+ return git_config_string(&amp;amp;ssl_cipherlist, var, value);
if (!strcmp(&amp;#34;http.sslcert&amp;#34;, var))
return git_config_string(&amp;amp;ssl_cert, var, value);
#if LIBCURL_VERSION_NUM &amp;gt;= 0x070903
@@ -327,6 +330,13 @@ static CURL *get_curl_handle(void)
if (http_proactive_auth)
init_curl_http_auth(result);
+ if (getenv(&amp;#34;GIT_SSL_CIPHER_LIST&amp;#34;))
+ ssl_cipherlist = getenv(&amp;#34;GIT_SSL_CIPHER_LIST&amp;#34;);
+
+ if (ssl_cipherlist != NULL &amp;amp;&amp;amp; *ssl_cipherlist)
+ curl_easy_setopt(result, CURLOPT_SSL_CIPHER_LIST,
+ ssl_cipherlist);
+
if (ssl_cert != NULL)
curl_easy_setopt(result, CURLOPT_SSLCERT, ssl_cert);
if (has_cert_password())
&lt;/code>&lt;/pre>&lt;p>I &lt;a href="https://marc.info/?l=git&amp;amp;m=143100824118409&amp;amp;w=2">submitted this patch&lt;/a> to the git mailing list, and after some discussion and a few revisions it was accepted. This changed was &lt;a href="https://github.com/git/git/commit/f6f2a9e42d14e61429af418d8038aa67049b3821">committed to git&lt;/a> on May 8, 2015.&lt;/p></content></item><item><title>Docker plugin bugs</title><link>https://blog.oddbit.com/posts/docker-plugin-bugs/</link><pubDate>Mon, 01 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/posts/docker-plugin-bugs/</guid><description>This is a companion to my article on the Docker plugin for Heat.
While writing that article, I encountered a number of bugs in the Docker plugin and elsewhere. I&amp;rsquo;ve submitted patches for most of the issues I encountered:
Bugs in the Heat plugin https://bugs.launchpad.net/heat/+bug/1364017
docker plugin fails to delete a container resource in CREATE_FAILED state.
https://bugs.launchpad.net/heat/+bug/1364041
docker plugin volumes_from parameter should be a list.
https://bugs.launchpad.net/heat/+bug/1364039
docker plugin volumes_from parameter results in an error</description><content>&lt;p>This is a companion to my &lt;a href="https://blog.oddbit.com/posts/docker-plugin-for-openstack-he/">article on the Docker plugin for Heat&lt;/a>.&lt;/p>
&lt;p>While writing that article, I encountered a number of bugs in the
Docker plugin and elsewhere. I&amp;rsquo;ve submitted patches for most of the
issues I encountered:&lt;/p>
&lt;h2 id="bugs-in-the-heat-plugin">Bugs in the Heat plugin&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364017">https://bugs.launchpad.net/heat/+bug/1364017&lt;/a>&lt;/p>
&lt;p>docker plugin fails to delete a container resource in
&lt;code>CREATE_FAILED&lt;/code> state.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364041">https://bugs.launchpad.net/heat/+bug/1364041&lt;/a>&lt;/p>
&lt;p>docker plugin &lt;code>volumes_from&lt;/code> parameter should be a list.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364039">https://bugs.launchpad.net/heat/+bug/1364039&lt;/a>&lt;/p>
&lt;p>docker plugin &lt;code>volumes_from&lt;/code> parameter results in an error&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364019">https://bugs.launchpad.net/heat/+bug/1364019&lt;/a>&lt;/p>
&lt;p>docker plugin does not actually remove containers on delete&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugs-in-docker-python-module">Bugs in docker Python module&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/docker/docker-py/pull/310">https://github.com/docker/docker-py/pull/310&lt;/a>&lt;/p>
&lt;p>allow ports to be specified as &lt;code>port/proto&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul></content></item><item><title>Docker plugin for OpenStack Heat</title><link>https://blog.oddbit.com/posts/docker-plugin-for-openstack-he/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/posts/docker-plugin-for-openstack-he/</guid><description>I have been looking at both Docker and OpenStack recently. In my last post I talked a little about the Docker driver for Nova; in this post I&amp;rsquo;ll be taking an in-depth look at the Docker plugin for Heat, which has been available since the Icehouse release but is surprisingly under-documented.
The release announcement on the Docker blog includes an example Heat template, but it is unfortunately grossly inaccurate and has led many people astray.</description><content>&lt;p>I have been looking at both Docker and OpenStack recently. In my &lt;a href="https://blog.oddbit.com/posts/novadocker-and-environment-var/">last
post&lt;/a> I talked a little about the &lt;a href="https://github.com/stackforge/nova-docker">Docker driver for Nova&lt;/a>; in
this post I&amp;rsquo;ll be taking an in-depth look at the Docker plugin for
Heat, which has been available &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">since the Icehouse release&lt;/a> but is
surprisingly under-documented.&lt;/p>
&lt;p>The &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">release announcement&lt;/a> on the Docker blog includes an
example Heat template, but it is unfortunately grossly inaccurate and
has led many people astray. In particular:&lt;/p>
&lt;ul>
&lt;li>It purports to but does not actually install Docker, due to a basic
&lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a> syntax error, and&lt;/li>
&lt;li>Even if you were to fix that problem, the lack of synchronization
between the two resources in the template would mean that you would
never be able to successfully launch a container.&lt;/li>
&lt;/ul>
&lt;p>In this post, I will present a fully functional example that will work
with the Icehouse release of Heat. We will install the Docker plugin
for Heat, then write a template that will (a) launch a Fedora 20
server and automatically install Docker, and then (b) use the Docker
plugin to launch some containers on that server.&lt;/p>
&lt;p>The &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a> referenced in this article can be found on GitHub:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/heat-docker-example">https://github.com/larsks/heat-docker-example&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="installing-the-docker-plugin">Installing the Docker plugin&lt;/h2>
&lt;p>The first thing we need to do is install the Docker plugin. I am
running &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> packages for Icehouse locally, which do not include
the Docker plugin. We&amp;rsquo;r going to install the plugin from the Heat
sources.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Download the Heat repository:&lt;/p>
&lt;pre>&lt;code> $ git clone https://github.com/openstack/heat.git
Cloning into 'heat'...
remote: Counting objects: 50382, done.
remote: Compressing objects: 100% (22/22), done.
remote: Total 50382 (delta 7), reused 1 (delta 0)
Receiving objects: 100% (50382/50382), 19.84 MiB | 1.81 MiB/s, done.
Resolving deltas: 100% (34117/34117), done.
Checking connectivity... done.
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>This will result in a directory called &lt;code>heat&lt;/code> in your current
working directory. Change into this directory:&lt;/p>
&lt;pre>&lt;code> $ cd heat
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Patch the Docker plugin.&lt;/p>
&lt;p>You have now checked out the &lt;code>master&lt;/code> branch of the Heat
repository; this is the most recent code committed to the project.
At this point we could check out the &lt;code>stable/icehouse&lt;/code> branch of
the repository to get the version of the plugin released at the
same time as the version of Heat that we&amp;rsquo;re running, but we would
find that the Docker plugin was, at that point in time, somewhat
crippled; in particular:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It does not support mapping container ports to host ports, so
there is no easy way to expose container services for external
access, and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It does not know how to automatically &lt;code>pull&lt;/code> missing images, so
you must arrange to run &lt;code>docker pull&lt;/code> a priori for each image you
plan to use in your Heat template.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>That would make us sad, so instead we&amp;rsquo;re going to use the plugin
from the &lt;code>master&lt;/code> branch, which only requires a trivial change in
order to work with the Icehouse release of Heat.&lt;/p>
&lt;p>Look at the file
&lt;code>contrib/heat_docker/heat_docker/resources/docker_container.py&lt;/code>.
Locate the following line:&lt;/p>
&lt;pre>&lt;code> attributes_schema = {
&lt;/code>&lt;/pre>
&lt;p>Add a line immediately before that so that the file look like
this:&lt;/p>
&lt;pre>&lt;code> attributes.Schema = lambda x: x
attributes_schema = {
&lt;/code>&lt;/pre>
&lt;p>If you&amp;rsquo;re curious, here is what we accomplished with that
additional line:&lt;/p>
&lt;p>The code following that point contains multiple stanzas of the
form:&lt;/p>
&lt;pre>&lt;code> INFO: attributes.Schema(
_('Container info.')
),
&lt;/code>&lt;/pre>
&lt;p>In Icehouse, the &lt;code>heat.engine.attributes&lt;/code> module does not have a
&lt;code>Schema&lt;/code> class so this fails. Our patch above adds a module
member named &lt;code>Schema&lt;/code> that simply returns it&amp;rsquo;s arguments (that
is, it is an identity function).&lt;/p>
&lt;p>(&lt;strong>NB&lt;/strong>: At the time this was written, Heat&amp;rsquo;s &lt;code>master&lt;/code> branch was
at &lt;code>a767880&lt;/code>.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Install the Docker plugin into your Heat plugin directory, which
on my system is &lt;code>/usr/lib/heat&lt;/code> (you can set this explicitly using
the &lt;code>plugin_dirs&lt;/code> directive in &lt;code>/etc/heat/heat.conf&lt;/code>):&lt;/p>
&lt;pre>&lt;code> $ rsync -a --exclude=tests/ contrib/heat_docker/heat_docker \
/usr/lib/heat
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re excluding the &lt;code>tests&lt;/code> directory here because it has
additional prerequisites that aren&amp;rsquo;t operationally necessary but
that will prevent Heat from starting up if they are missing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Restart your &lt;code>heat-engine&lt;/code> service. On Fedora, that would be:&lt;/p>
&lt;pre>&lt;code> # systemctl restart openstack-heat-engine
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Verify that the new &lt;code>DockerInc::Docker::Container&lt;/code> resource is
available:&lt;/p>
&lt;pre>&lt;code> $ heat resource-type-list | grep Docker
| DockerInc::Docker::Container |
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="templates-installing-docker">Templates: Installing docker&lt;/h2>
&lt;p>We would like our template to automatically install Docker on a Nova
server. The example in the &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">Docker blog&lt;/a> mentioned earlier
attempts to do this by setting the &lt;code>user_data&lt;/code> parameter of a
&lt;code>OS::Nova::Server&lt;/code> resource like this:&lt;/p>
&lt;pre>&lt;code>user_data: #include https://get.docker.io
&lt;/code>&lt;/pre>
&lt;p>Unfortunately, an unquoted &lt;code>#&lt;/code> introduces a comment in &lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a>, so
this is completely ignored. It would be written more correctly like
this (the &lt;code>|&lt;/code> introduces a block of literal text):&lt;/p>
&lt;pre>&lt;code>user_data: |
#include https://get.docker.io
&lt;/code>&lt;/pre>
&lt;p>Or possibly like this, although this would restrict you to a single
line and thus wouldn&amp;rsquo;t be used much in practice:&lt;/p>
&lt;pre>&lt;code>user_data: &amp;quot;#include https://get.docker.io&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And, all other things being correct, this would install Docker on a
system&amp;hellip;but would not necessarily start it, nor would it configure
Docker to listen on a TCP socket. On my Fedora system, I ended up
creating the following &lt;code>user_data&lt;/code> script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
yum -y upgrade
# I have occasionally seen 'yum install' fail with errors
# trying to contact mirrors. Because it can be a pain to
# delete and re-create the stack, just loop here until it
# succeeds.
while :; do
yum -y install docker-io
[ -x /usr/bin/docker ] &amp;amp;&amp;amp; break
sleep 5
done
# Add a tcp socket for docker
cat &amp;gt; /etc/systemd/system/docker-tcp.socket &amp;lt;&amp;lt;EOF
[Unit]
Description=Docker remote access socket
[Socket]
ListenStream=2375
BindIPv6Only=both
Service=docker.service
[Install]
WantedBy=sockets.target
EOF
# Start and enable the docker service.
for sock in docker.socket docker-tcp.socket; do
systemctl start $sock
systemctl enable $sock
done
&lt;/code>&lt;/pre>
&lt;p>This takes care of making sure our packages are current, installing
Docker, and arranging for it to listen on a tcp socket. For that last
bit, we&amp;rsquo;re creating a new &lt;code>systemd&lt;/code> socket file
(&lt;code>/etc/systemd/system/docker-tcp.socket&lt;/code>), which means that &lt;code>systemd&lt;/code>
will actually open the socket for listening and start &lt;code>docker&lt;/code> if
necessary when a client connects.&lt;/p>
&lt;h2 id="templates-synchronizing-resources">Templates: Synchronizing resources&lt;/h2>
&lt;p>In our Heat template, we are starting a Nova server that will run
Docker, and then we are instantiating one or more Docker containers
that will run on this server. This means that timing is suddenly very
important. If we use the &lt;code>user_data&lt;/code> script as presented in the
previous section, we would probably end up with an error like this in
our &lt;code>heat-engine.log&lt;/code>:&lt;/p>
&lt;pre>&lt;code>2014-08-29 17:10:37.598 15525 TRACE heat.engine.resource ConnectionError:
HTTPConnectionPool(host='192.168.200.11', port=2375): Max retries exceeded
with url: /v1.12/containers/create (Caused by &amp;lt;class 'socket.error'&amp;gt;:
[Errno 113] EHOSTUNREACH)
&lt;/code>&lt;/pre>
&lt;p>This happens because it takes &lt;em>time&lt;/em> to install packages. Absent any
dependencies, Heat creates resources in parallel, so Heat is happily
trying to spawn our Docker containers when our server is still
fetching the Docker package.&lt;/p>
&lt;p>Heat does have a &lt;code>depends_on&lt;/code> property that can be applied to
resources. For example, if we have:&lt;/p>
&lt;pre>&lt;code>docker_server:
type: &amp;quot;OS::Nova::Server&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>We can make a Docker container depend on that resource:&lt;/p>
&lt;pre>&lt;code>docker_container_mysql:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
depends_on:
- docker_server
&lt;/code>&lt;/pre>
&lt;p>Looks good, but this does not, in fact, help us. From Heat&amp;rsquo;s
perspective, the dependency is satisfied as soon as the Nova server
&lt;em>boots&lt;/em>, so really we&amp;rsquo;re back where we started.&lt;/p>
&lt;p>The Heat solution to this is the &lt;code>AWS::CloudFormation::WaitCondition&lt;/code>
resource (and its boon companion, the and
&lt;code>AWS::CloudFormation::WaitConditionHandle&lt;/code> resource). A
&lt;code>WaitCondition&lt;/code> is a resource this is not &amp;ldquo;created&amp;rdquo; until it has
received an external signal. We define a wait condition like this:&lt;/p>
&lt;pre>&lt;code>docker_wait_handle:
type: &amp;quot;AWS::CloudFormation::WaitConditionHandle&amp;quot;
docker_wait_condition:
type: &amp;quot;AWS::CloudFormation::WaitCondition&amp;quot;
depends_on:
- docker_server
properties:
Handle:
get_resource: docker_wait_handle
Timeout: &amp;quot;6000&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And then we make our container depend on the wait condition:&lt;/p>
&lt;pre>&lt;code>docker_container_mysql:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
depends_on:
- docker_wait_condition
&lt;/code>&lt;/pre>
&lt;p>With this in place, Heat will not attempt to create the Docker
container until we signal the wait condition resource. In order to do
that, we need to modify our &lt;code>user_data&lt;/code> script to embed the
notification URL generated by heat. We&amp;rsquo;ll use both the &lt;code>get_resource&lt;/code>
and &lt;code>str_replace&lt;/code> &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#intrinsic-functions">intrinsic function&lt;/a> in order to generate the appropriate
script:&lt;/p>
&lt;pre>&lt;code> user_data:
# We're using Heat's 'str_replace' function in order to
# substitute into this script the Heat-generated URL for
# signaling the docker_wait_condition resource.
str_replace:
template: |
#!/bin/sh
yum -y upgrade
# I have occasionally seen 'yum install' fail with errors
# trying to contact mirrors. Because it can be a pain to
# delete and re-create the stack, just loop here until it
# succeeds.
while :; do
yum -y install docker-io
[ -x /usr/bin/docker ] &amp;amp;&amp;amp; break
sleep 5
done
# Add a tcp socket for docker
cat &amp;gt; /etc/systemd/system/docker-tcp.socket &amp;lt;&amp;lt;EOF
[Unit]
Description=Docker remote access socket
[Socket]
ListenStream=2375
BindIPv6Only=both
Service=docker.service
[Install]
WantedBy=sockets.target
EOF
# Start and enable the docker service.
for sock in docker.socket docker-tcp.socket; do
systemctl start $sock
systemctl enable $sock
done
# Signal heat that we are finished settings things up.
cfn-signal -e0 --data 'OK' -r 'Setup complete' '$WAIT_HANDLE'
params:
&amp;quot;$WAIT_HANDLE&amp;quot;:
get_resource: docker_wait_handle
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>str_replace&lt;/code> function probably deserves a closer look; the
general format is:&lt;/p>
&lt;pre>&lt;code>str_replace:
template:
params:
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>template&lt;/code> is text content containing 0 or more things to be
replaced, and &lt;code>params&lt;/code> is a list of tokens to search for and replace
in the &lt;code>template&lt;/code>.&lt;/p>
&lt;p>We use &lt;code>str_replace&lt;/code> to substitute the token &lt;code>$WAIT_HANDLE&lt;/code> with the
result of calling &lt;code>get_resource&lt;/code> on our &lt;code>docker_wait_handle&lt;/code> resource.
This results in a URL that contains an EC2-style signed URL that will
deliver the necessary notification to Heat. In this example we&amp;rsquo;re
using the &lt;code>cfn-signal&lt;/code> tool, which is included in the Fedora cloud
images, but you could accomplish the same thing with &lt;code>curl&lt;/code>:&lt;/p>
&lt;pre>&lt;code>curl -X PUT -H 'Content-Type: application/json' \
--data-binary '{&amp;quot;Status&amp;quot;: &amp;quot;SUCCESS&amp;quot;,
&amp;quot;Reason&amp;quot;: &amp;quot;Setup complete&amp;quot;,
&amp;quot;Data&amp;quot;: &amp;quot;OK&amp;quot;, &amp;quot;UniqueId&amp;quot;: &amp;quot;00000&amp;quot;}' \
&amp;quot;$WAIT_HANDLE&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>You need to have correctly configured Heat in order for this to work;
I&amp;rsquo;ve written a short &lt;a href="https://blog.oddbit.com/posts/using-wait-conditions-with-hea/">companion article&lt;/a> that contains a checklist
and pointers to additional documentation to help work around some
common issues.&lt;/p>
&lt;h2 id="templates-defining-docker-containers">Templates: Defining Docker containers&lt;/h2>
&lt;p>&lt;strong>UPDATE&lt;/strong>: I have generated some &lt;a href="https://blog.oddbit.com/posts/docker-contain-doc/">annotated documentation for the
Docker plugin&lt;/a>.&lt;/p>
&lt;p>Now that we have arranged for Heat to wait for the server to finish
configuration before starting Docker contains, how do we create a
container? As Scott Lowe noticed in his &lt;a href="http://blog.scottlowe.org/2014/08/22/a-heat-template-for-docker-containers/">blog post about Heat and
Docker&lt;/a>, there is very little documentation available out there
for the Docker plugin (something I am trying to remedy with this blog
post!). Things are not quite as bleak as you might think, because
Heat resources are to a certain extent self-documenting. If you run:&lt;/p>
&lt;pre>&lt;code>$ heat resource-template DockerInc::Docker::Container
&lt;/code>&lt;/pre>
&lt;p>You will get a complete description of the attributes and properties
available in the named resource. The &lt;code>parameters&lt;/code> section is probably
the most descriptive:&lt;/p>
&lt;pre>&lt;code>parameters:
cmd:
Default: []
Description: Command to run after spawning the container.
Type: CommaDelimitedList
dns: {Description: Set custom dns servers., Type: CommaDelimitedList}
docker_endpoint: {Description: Docker daemon endpoint (by default the local docker
daemon will be used)., Type: String}
env: {Description: Set environment variables., Type: CommaDelimitedList}
hostname: {Default: '', Description: Hostname of the container., Type: String}
image: {Description: Image name., Type: String}
links: {Description: Links to other containers., Type: Json}
memory: {Default: 0, Description: Memory limit (Bytes)., Type: Number}
name: {Description: Name of the container., Type: String}
open_stdin:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Open stdin.
Type: String
port_bindings: {Description: TCP/UDP ports bindings., Type: Json}
port_specs: {Description: TCP/UDP ports mapping., Type: CommaDelimitedList}
privileged:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Enable extended privileges.
Type: String
stdin_once:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: If true, close stdin after the 1 attached client disconnects.
Type: String
tty:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Allocate a pseudo-tty.
Type: String
user: {Default: '', Description: Username or UID., Type: String}
volumes:
Default: {}
Description: Create a bind mount.
Type: Json
volumes_from: {Default: '', Description: Mount all specified volumes., Type: String}
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>port_specs&lt;/code> and &lt;code>port_bindings&lt;/code> parameters require a little
additional explanation.&lt;/p>
&lt;p>The &lt;code>port_specs&lt;/code> parameter is a list of (TCP) ports that will be
&amp;ldquo;exposed&amp;rdquo; by the container (similar to the &lt;code>EXPOSE&lt;/code> directive in a
Dockerfile). This corresponds to the &lt;code>PortSpecs&lt;/code> argument in the the
&lt;a href="https://docs.docker.com/reference/api/docker_remote_api_v1.14/#create-a-container">/containers/create&lt;/a> call of the &lt;a href="https://docs.docker.com/reference/api/docker_remote_api/">Docker remote API&lt;/a>.
For example:&lt;/p>
&lt;pre>&lt;code>port_specs:
- 3306
- 53/udp
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>port_bindings&lt;/code> parameter is a mapping that allows you to bind
host ports to ports in the container, similar to the &lt;code>-p&lt;/code> argument to
&lt;code>docker run&lt;/code>. This corresponds to the
&lt;a href="https://docs.docker.com/reference/api/docker_remote_api_v1.14/#start-a-container">/containers/(id)/start&lt;/a> call in the &lt;a href="https://docs.docker.com/reference/api/docker_remote_api/">Docker remote API&lt;/a>.
In the mappings, the key (left-hand side) is the &lt;em>container&lt;/em> port, and
the value (right-hand side) is the &lt;em>host&lt;/em> port.&lt;/p>
&lt;p>For example, to bind container port 3306 to host port 3306:&lt;/p>
&lt;pre>&lt;code>port_bindings:
3306: 3306
&lt;/code>&lt;/pre>
&lt;p>To bind port 9090 in a container to port 80 on the host:&lt;/p>
&lt;pre>&lt;code>port_bindings:
9090: 80
&lt;/code>&lt;/pre>
&lt;p>And in theory, this should also work for UDP ports (but in practice
there is an issue between the Docker plugin and the &lt;code>docker-py&lt;/code> Python
module which makes it impossible to expose UDP ports via &lt;code>port_specs&lt;/code>;
this is fixed in
&lt;a href="https://github.com/docker/docker-py/pull/310" class="pull-request">#310&lt;/a>
on GitHub).&lt;/p>
&lt;pre>&lt;code>port_bindings:
53/udp: 5300
&lt;/code>&lt;/pre>
&lt;p>With all of this in mind, we can create a container resource
definition:&lt;/p>
&lt;pre>&lt;code>docker_dbserver:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
# here's where we set the dependency on the WaitCondition
# resource we mentioned earlier.
depends_on:
- docker_wait_condition
properties:
docker_endpoint:
str_replace:
template: &amp;quot;tcp://$HOST:2375&amp;quot;
params:
&amp;quot;$HOST&amp;quot;:
get_attr:
- docker_server_floating
- floating_ip_address
image: mysql
env:
# The official MySQL docker image expect the database root
# password to be provided in the MYSQL_ROOT_PASSWORD
# environment variable.
- str_replace:
template: MYSQL_ROOT_PASSWORD=$PASSWORD
params:
&amp;quot;$PASSWORD&amp;quot;:
get_param:
mysql_root_password
port_specs:
- 3306
port_bindings:
3306: 3306
&lt;/code>&lt;/pre>
&lt;p>Take a close look at how we&amp;rsquo;re setting the &lt;code>docker_endpoint&lt;/code> property:&lt;/p>
&lt;pre>&lt;code>docker_endpoint:
str_replace:
template: &amp;quot;tcp://$HOST:2375&amp;quot;
params:
&amp;quot;$HOST&amp;quot;:
get_attr:
- docker_server_floating
- floating_ip_address
&lt;/code>&lt;/pre>
&lt;p>This uses the &lt;code>get_attr&lt;/code> function to get the &lt;code>floating_ip_address&lt;/code>
attribute from the &lt;code>docker_server_floating&lt;/code> resource, which you can
find in the &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a>. We take the return value from that
function and use &lt;code>str_replace&lt;/code> to substitute that into the
&lt;code>docker_endpoint&lt;/code> URL.&lt;/p>
&lt;h2 id="the-pudding">The pudding&lt;/h2>
&lt;p>Using the &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a> with an appropriate local environment
file, I can launch this stack by runnign:&lt;/p>
&lt;pre>&lt;code>$ heat stack-create -f docker-server.yml -e local.env docker
&lt;/code>&lt;/pre>
&lt;p>And after a while, I can run&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
&lt;/code>&lt;/pre>
&lt;p>And see that the stack has been created successfully:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+------------+-----------------+----------------------+
| id | stack_name | stack_status | creation_time |
+--------------------------------------+------------+-----------------+----------------------+
| c0fd793e-a1f7-4b35-afa9-12ba1005925a | docker | CREATE_COMPLETE | 2014-08-31T03:01:14Z |
+--------------------------------------+------------+-----------------+----------------------+
&lt;/code>&lt;/pre>
&lt;p>And I can ask for status information on the individual resources in
the stack:&lt;/p>
&lt;pre>&lt;code>$ heat resource-list docker
+------------------------+------------------------------------------+-----------------+
| resource_name | resource_type | resource_status |
+------------------------+------------------------------------------+-----------------+
| fixed_network | OS::Neutron::Net | CREATE_COMPLETE |
| secgroup_db | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| secgroup_docker | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| secgroup_webserver | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| docker_wait_handle | AWS::CloudFormation::WaitConditionHandle | CREATE_COMPLETE |
| extrouter | OS::Neutron::Router | CREATE_COMPLETE |
| fixed_subnet | OS::Neutron::Subnet | CREATE_COMPLETE |
| secgroup_common | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| docker_server_eth0 | OS::Neutron::Port | CREATE_COMPLETE |
| extrouter_inside | OS::Neutron::RouterInterface | CREATE_COMPLETE |
| docker_server | OS::Nova::Server | CREATE_COMPLETE |
| docker_server_floating | OS::Neutron::FloatingIP | CREATE_COMPLETE |
| docker_wait_condition | AWS::CloudFormation::WaitCondition | CREATE_COMPLETE |
| docker_webserver | DockerInc::Docker::Container | CREATE_COMPLETE |
| docker_dbserver | DockerInc::Docker::Container | CREATE_COMPLETE |
+------------------------+------------------------------------------+-----------------+
&lt;/code>&lt;/pre>
&lt;p>I can run &lt;code>nova list&lt;/code> and see information about my running Nova
server:&lt;/p>
&lt;pre>&lt;code>+--------...+-----------------...+------------------------------------------------------------+
| ID ...| Name ...| Networks |
+--------...+-----------------...+------------------------------------------------------------+
| 301c5ec...| docker-docker_se...| docker-fixed_network-whp3fxhohkxk=10.0.0.2, 192.168.200.46 |
+--------...+-----------------...+------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>I can point a Docker client at the remote address and see the running
containers:&lt;/p>
&lt;pre>&lt;code>$ docker-1.2 -H tcp://192.168.200.46:2375 ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
f2388c871b20 mysql:5 /entrypoint.sh mysql 5 minutes ago Up 5 minutes 0.0.0.0:3306-&amp;gt;3306/tcp grave_almeida
9596cbe51291 larsks/simpleweb:latest /bin/sh -c '/usr/sbi 11 minutes ago Up 11 minutes 0.0.0.0:80-&amp;gt;80/tcp hungry_tesla
&lt;/code>&lt;/pre>
&lt;p>And I can point a &lt;code>mysql&lt;/code> client at the remote address and access the
database server:&lt;/p>
&lt;pre>&lt;code>$ mysql -h 192.168.200.46 -u root -psecret mysql
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
[...]
MySQL [mysql]&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="when-things-go-wrong">When things go wrong&lt;/h2>
&lt;p>Your &lt;code>heat-engine&lt;/code> log, generally &lt;code>/var/log/heat/engine.log&lt;/code>, is going
to be your best source of information if things go wrong. The &lt;code>heat stack-show&lt;/code> command will generally provide useful fault information if
your stack ends up in the &lt;code>CREATE_FAILED&lt;/code> (or &lt;code>DELETE_FAILED&lt;/code>) state.&lt;/p></content></item></channel></rss>