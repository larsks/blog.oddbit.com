<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>rdo on blog.oddbit.com</title><link>https://blog.oddbit.com/tags/rdo/</link><description>Recent content in rdo on blog.oddbit.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Lars Kellogg-Stedman</copyright><lastBuildDate>Fri, 19 Feb 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.oddbit.com/tags/rdo/rss.xml" rel="self" type="application/rss+xml"/><item><title>Deploying an HA OpenStack development environment with tripleo-quickstart</title><link>https://blog.oddbit.com/post/2016-02-19-deploy-an-ha-openstack-develop/</link><pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-19-deploy-an-ha-openstack-develop/</guid><description>In this article I would like to introduce tripleo-quickstart, a tool that will automatically provision a virtual environment and then use TripleO to deploy an HA OpenStack on top of it.
Introducing Tripleo-Quickstart The goal of the Tripleo-Quickstart project is to replace the instack-virt-setup tool for quickly setting up virtual TripleO environments, and to ultimately become the tool used by both developers and upstream CI for this purpose. The project is a set of Ansible playbooks that will take care of:</description><content>&lt;p>In this article I would like to introduce &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart">tripleo-quickstart&lt;/a>, a
tool that will automatically provision a virtual environment and then
use &lt;a href="http://docs.openstack.org/developer/tripleo-docs/">TripleO&lt;/a> to deploy an HA OpenStack on top of it.&lt;/p>
&lt;h2 id="introducing-tripleo-quickstart">Introducing Tripleo-Quickstart&lt;/h2>
&lt;p>The goal of the &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart">Tripleo-Quickstart&lt;/a> project is to replace the
&lt;code>instack-virt-setup&lt;/code> tool for quickly setting up virtual TripleO
environments, and to ultimately become the tool used by both
developers and upstream CI for this purpose. The project is a set of
&lt;a href="http://ansible.com/">Ansible&lt;/a> playbooks that will take care of:&lt;/p>
&lt;ul>
&lt;li>Creating virtual undercloud node&lt;/li>
&lt;li>Creating virtual overcloud nodes&lt;/li>
&lt;li>Deploying the undercloud&lt;/li>
&lt;li>Deploying the overcloud&lt;/li>
&lt;li>Validating the overcloud&lt;/li>
&lt;/ul>
&lt;p>In this article, I will be using &lt;code>tripleo-quickstart&lt;/code> to set up a
development environment on a 32GB desktop. This is probably the
minimum sized system if your goal is to create an HA install (a
single controller/single compute environment could be deployed on
something smaller).&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>Before we get started, you will need:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>A target system with at least 32GB of RAM.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ansible 2.0.x. This is what you get if you &lt;code>pip install ansible&lt;/code>;
it is also available in the Fedora &lt;code>updates-testing&lt;/code> repository and
in the EPEL &lt;code>epel-testing&lt;/code> repository.&lt;/p>
&lt;p>Do &lt;strong>not&lt;/strong> use Ansible from the HEAD of the git repository; the
development version is not necessarily backwards compatible with
2.0.x and may break in unexpected ways.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A user account on the target system with which you can (a) log in
via ssh without a password and (b) use &lt;code>sudo&lt;/code> without a password to
gain root privileges. In other words, this should work:&lt;/p>
&lt;pre>&lt;code> ssh -tt targetuser@targethost sudo echo it worked
&lt;/code>&lt;/pre>
&lt;p>Your &lt;em>targetuser&lt;/em> could be &lt;code>root&lt;/code>, in which case the &lt;code>sudo&lt;/code> is
superfluous and you should be all set.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A copy of the tripleo-quickstart repository:&lt;/p>
&lt;pre>&lt;code> git clone https://github.com/redhat-openstack/tripleo-quickstart/
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>The remainder of this document assumes that you are running things
from inside the &lt;code>tripleo-quickstart&lt;/code> directory.&lt;/p>
&lt;h2 id="the-quick-way">The quick way&lt;/h2>
&lt;p>If you just want to take things out for a spin using the defaults
&lt;em>and&lt;/em> you can ssh to your target host as &lt;code>root&lt;/code>, you can skip the
remainder of this article and just run:&lt;/p>
&lt;pre>&lt;code>ansible-playbook playbooks/centosci/minimal.yml \
-e virthost=my.target.host
&lt;/code>&lt;/pre>
&lt;p>Or for an HA deployment:&lt;/p>
&lt;pre>&lt;code>ansible-playbook playbooks/centosci/ha.yml \
-e virthost=my.target.host
&lt;/code>&lt;/pre>
&lt;p>(Where you replace &lt;code>my.target.host&lt;/code> with the hostname of the host on
which you want to install your virtual environment.)&lt;/p>
&lt;p>In the remainder of this article I will discuss ways in which you can
customize this process (and make subsequent deployments faster).&lt;/p>
&lt;h2 id="create-an-inventory-file">Create an inventory file&lt;/h2>
&lt;p>An inventory file tells Ansible to which hosts it should connect and
provides information about how it should connect. For the quickstart,
your inventory needs to have your target host listed in the &lt;code>virthost&lt;/code>
group. For example:&lt;/p>
&lt;pre>&lt;code>[virthost]
my.target.host ansible_user=targetuser
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m going to assume you put this into a file named &lt;code>inventory&lt;/code>.&lt;/p>
&lt;h2 id="creating-a-playbook">Creating a playbook&lt;/h2>
&lt;p>A playbook tells Ansible what do to do.&lt;/p>
&lt;p>First, we want to tear down any existing virtual environment, and then
spin up a new undercloud node and create guests that will be used as
overcloud nodes. We do this with the &lt;code>libvirt/teardown&lt;/code> and
&lt;code>libvirt/setup&lt;/code> roles:&lt;/p>
&lt;pre>&lt;code>- hosts: virthost
roles:
- libvirt/teardown
- libvirt/setup
&lt;/code>&lt;/pre>
&lt;p>The next play will generate an Ansible inventory file (by default
&lt;code>$HOME/.quickstart/hosts&lt;/code>) that we can use in the future to refer to
our deployment:&lt;/p>
&lt;pre>&lt;code>- hosts: localhost
roles:
- rebuild-inventory
&lt;/code>&lt;/pre>
&lt;p>Lastly, we install the undercloud host and deploy the overcloud:&lt;/p>
&lt;pre>&lt;code>- hosts: undercloud
roles:
- overcloud
&lt;/code>&lt;/pre>
&lt;p>Put this content in a file named &lt;code>ha.yml&lt;/code> (the actual name doesn&amp;rsquo;t
matter, but this gives us something to refer to later on in this
article).&lt;/p>
&lt;h2 id="configuring-the-deployment">Configuring the deployment&lt;/h2>
&lt;p>Before we run tripleo-quickstart, we need to make a few configuration
changes. We&amp;rsquo;ll do this by creating a &lt;a href="http://yaml.org/">YAML&lt;/a> file that describes our
configuration, and we&amp;rsquo;ll feed this to ansible using the &lt;a href="http://docs.ansible.com/ansible/playbooks_variables.html#passing-variables-on-the-command-line">-e
@filename.yml&lt;/a> syntax.&lt;/p>
&lt;h3 id="describing-your-virtual-servers">Describing your virtual servers&lt;/h3>
&lt;p>By default, tripleo-quickstart will deploy an environment consisting
of four overcloud nodes:&lt;/p>
&lt;ul>
&lt;li>3 controller nodes&lt;/li>
&lt;li>1 compute node&lt;/li>
&lt;/ul>
&lt;p>All of these will have 4GB of memory, which when added to the default
overcloud node size of 12GB comes to a total memory footprint of 24GB.
These defaults are defined in
&lt;code>playbooks/roles/nodes/defaults/main.yml&lt;/code>. There are a number of ways
we can override this default configuration.&lt;/p>
&lt;p>To simply change the amount of memory assigned to each class of
server, we can set the &lt;code>undercloud_memory&lt;/code>, &lt;code>control_memory&lt;/code>, and
&lt;code>compute_memory&lt;/code> keys. For example:&lt;/p>
&lt;pre>&lt;code>control_memory: 6000
compute_memory: 2048
&lt;/code>&lt;/pre>
&lt;p>To change the number of CPUs assigned to a server, we can change the
corresponding &lt;code>_vcpu&lt;/code> key. Your deployments will generally run faster
if your undercloud host has more CPUs available:&lt;/p>
&lt;pre>&lt;code>undercloud_vcpu: 4
&lt;/code>&lt;/pre>
&lt;p>To change the number and type of nodes, you can provide an
&lt;code>overcloud_nodes&lt;/code> key with entries for each virtual system. The
default looks like this:&lt;/p>
&lt;pre>&lt;code>overcloud_nodes:
- name: control_0
flavor: control
- name: control_1
flavor: control
- name: control_2
flavor: control
- name: compute_0
flavor: compute
&lt;/code>&lt;/pre>
&lt;p>To create a minimal environment with a single controller and a single
compute node, we could instead put the following into our configuration
file:&lt;/p>
&lt;pre>&lt;code>overcloud_nodes:
- name: control_0
flavor: control
- name: compute_0
flavor: compute
&lt;/code>&lt;/pre>
&lt;p>You may intuit from the above examples that you can actually describe
custom flavors. This is true, but is beyond the scope of this post;
take a look at &lt;code>playbooks/roles/nodes/defaults/main.yml&lt;/code> for an
example.&lt;/p>
&lt;h3 id="configuring-ha">Configuring HA&lt;/h3>
&lt;p>To actually deploy an HA OpenStack environment, we need to pass a few
additional options to the &lt;code>openstack overcloud deploy&lt;/code> command. Based
on &lt;a href="http://docs.openstack.org/developer/tripleo-docs/basic_deployment/basic_deployment_cli.html#deploy-the-overcloud">the docs&lt;/a> I need:&lt;/p>
&lt;pre>&lt;code>--control-scale 3 \
-e /usr/share/openstack-tripleo-heat-templates/environments/puppet-pacemaker.yaml \
--ntp-server pool.ntp.org
&lt;/code>&lt;/pre>
&lt;p>We configure deploy arguments in the &lt;code>extra_args&lt;/code> variable, so for the
above configuration we would add:&lt;/p>
&lt;pre>&lt;code>extra_args: &amp;gt;
--control-scale 3
-e /usr/share/openstack-tripleo-heat-templates/environments/puppet-pacemaker.yaml
--ntp-server pool.ntp.org
&lt;/code>&lt;/pre>
&lt;h3 id="configuring-nested-kvm">Configuring nested KVM&lt;/h3>
&lt;p>I want &lt;a href="https://www.kernel.org/doc/Documentation/virtual/kvm/nested-vmx.txt">nested KVM&lt;/a> on my compute hosts,
which requires changes both to the libvirt XML used to deploy the
&amp;ldquo;baremetal&amp;rdquo; hosts and the nova.conf configuration. I was able to
accomplish this by adding the following to the configuration:&lt;/p>
&lt;pre>&lt;code>baremetal_vm_xml: |
&amp;lt;cpu mode='host-passthrough'/&amp;gt;
libvirt_args: --libvirt-type kvm
&lt;/code>&lt;/pre>
&lt;p>For this to work, you will need to have your target host correctly
configured to support nested KVM, which generally means adding the
following to &lt;code>/etc/modprobe.d/kvm.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code>options kvm_intel nested=1
&lt;/code>&lt;/pre>
&lt;p>(And possibly unloading/reloading the &lt;code>kvm_intel&lt;/code> module if it was
already loaded.)&lt;/p>
&lt;h3 id="disable-some-steps">Disable some steps&lt;/h3>
&lt;p>The default behavior is to:&lt;/p>
&lt;ul>
&lt;li>Install the undercloud&lt;/li>
&lt;li>Deploy the overcloud&lt;/li>
&lt;li>Validate the overcloud&lt;/li>
&lt;/ul>
&lt;p>You can enable or disable individual steps with the following
variables:&lt;/p>
&lt;ul>
&lt;li>&lt;code>step_install_undercloud&lt;/code>&lt;/li>
&lt;li>&lt;code>step_deploy_overcloud&lt;/code>&lt;/li>
&lt;li>&lt;code>step_validate_overcloud&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>These all default to &lt;code>true&lt;/code>. If, for example, overcloud validation is
failing because of a known issue, we could add the following to
&lt;code>nodes.yml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>step_validate_overcloud: false
&lt;/code>&lt;/pre>
&lt;h2 id="pre-caching-the-undercloud-image">Pre-caching the undercloud image&lt;/h2>
&lt;p>Fetching the undercloud image from the CentOS CI environment can take
a really long time. If you&amp;rsquo;re going to be deploying often, you can
speed up this step by manually saving the image and the corresponding
&lt;code>.md5&lt;/code> file to a file on your target host:&lt;/p>
&lt;pre>&lt;code>mkdir -p /usr/share/quickstart_images/mitaka/
cd /usr/share/quickstart_images/mitaka/
wget https://ci.centos.org/artifacts/rdo/images/mitaka/delorean/stable/undercloud.qcow2.md5 \
https://ci.centos.org/artifacts/rdo/images/mitaka/delorean/stable/undercloud.qcow2
&lt;/code>&lt;/pre>
&lt;p>And then providing the path to that file in the &lt;code>url&lt;/code> variable when
you run the playbook. I&amp;rsquo;ve added the following to my &lt;code>nodes.yml&lt;/code>
file, but you could also do this on the command line:&lt;/p>
&lt;pre>&lt;code>url: file:///usr/share/quickstart_images/mitaka/undercloud.qcow2
&lt;/code>&lt;/pre>
&lt;h2 id="intermission">Intermission&lt;/h2>
&lt;p>I&amp;rsquo;ve made the examples presented in this article available for
download at the following URLs:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="ha.yml">ha.yml&lt;/a> playbook&lt;/li>
&lt;li>&lt;a href="nodes.yml">nodes.yml&lt;/a> example configuration file&lt;/li>
&lt;li>&lt;a href="nodes-minimal.yml">nodes-minimal.yml&lt;/a> example configuration file for a minimal environment&lt;/li>
&lt;/ul>
&lt;h2 id="running-tripleo-quickstart">Running tripleo-quickstart&lt;/h2>
&lt;p>With all of the above in place, we can run:&lt;/p>
&lt;pre>&lt;code>ansible-playbook ha.yml -i inventory -e @nodes.yml
&lt;/code>&lt;/pre>
&lt;p>Which will proceed through the following phases:&lt;/p>
&lt;h3 id="tear-down-existing-environment">Tear down existing environment&lt;/h3>
&lt;p>This step deletes any libvirt guests matching the ones we are about to
deploy, removes the &lt;code>stack&lt;/code> user from the target host, and otherwise
ensures a clean slate from which to start.&lt;/p>
&lt;h3 id="create-overcloud-vms">Create overcloud vms&lt;/h3>
&lt;p>This uses the node definitions in &lt;code>vm.overcloud.nodes&lt;/code> to create a set
of libvirt guests. They will &lt;em>not&lt;/em> be booted at this stage; that
happens later during the ironic discovery process.&lt;/p>
&lt;h3 id="fetch-the-undercloud-image">Fetch the undercloud image&lt;/h3>
&lt;p>This will fetch the undercloud appliance image either from the CentOS
CI environment or from wherever you point the &lt;code>url&lt;/code> variable.&lt;/p>
&lt;h3 id="configure-the-undercloud-image">Configure the undercloud image&lt;/h3>
&lt;p>This performs some initial configuration steps such as injecting ssh
keys into the image.&lt;/p>
&lt;h3 id="create-undercloud-vm">Create undercloud vm&lt;/h3>
&lt;p>In this step, tripleo-quickstart uses the configured appliance image
to create a new &lt;code>undercloud&lt;/code> libvirt guest.&lt;/p>
&lt;h3 id="install-undercloud">Install undercloud&lt;/h3>
&lt;p>This runs &lt;code>openstack undercloud install&lt;/code>.&lt;/p>
&lt;h3 id="deploy-overcloud">Deploy overcloud&lt;/h3>
&lt;p>This does everything else:&lt;/p>
&lt;ul>
&lt;li>Discover the available nodes via the Ironic discovery process&lt;/li>
&lt;li>Use &lt;code>openstack overcloud deploy&lt;/code> to kick off the provisioning
process. This feeds &lt;a href="https://wiki.openstack.org/wiki/Heat">Heat&lt;/a> a collection of templates that will be
used to configure the overcloud nodes.&lt;/li>
&lt;/ul>
&lt;h2 id="accessing-the-undercloud">Accessing the undercloud&lt;/h2>
&lt;p>You can ssh directly into the undercloud host by taking advantage of
the ssh configuration that tripleo-quickstart generated for you. By
default this will be &lt;code>$HOME/.quickstart/ssh.config.ansible&lt;/code>, but you
can override that directory by specifying a value for the
&lt;code>local_working_dir&lt;/code> variable when you run Ansible. You use the &lt;code>-F&lt;/code>
option to ssh to point it at that file:&lt;/p>
&lt;pre>&lt;code>ssh -F $HOME/.quickstart/ssh.config.ansible undercloud
&lt;/code>&lt;/pre>
&lt;p>The configuration uses an ssh &lt;code>ProxyConnection&lt;/code> configuration to
automatically proxy your connection to the undercloud vm through your
physical host.&lt;/p>
&lt;h2 id="accessing-the-overcloud-hosts">Accessing the overcloud hosts&lt;/h2>
&lt;p>Once you have logged into the undercloud, you&amp;rsquo;ll need to source in
some credentials. The file &lt;code>stackrc&lt;/code> contains credentials for the
undercloud:&lt;/p>
&lt;pre>&lt;code>. stackrc
&lt;/code>&lt;/pre>
&lt;p>Now you can run &lt;code>nova list&lt;/code> to get a list of your overcloud nodes,
investigate the &lt;code>overcloud&lt;/code> heat stack, and so forth:&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
+----------...+------------+-----------------+--------------...+--------------+
| id ...| stack_name | stack_status | creation_time...| updated_time |
+----------...+------------+-----------------+--------------...+--------------+
| b6cfd621-...| overcloud | CREATE_COMPLETE | 2016-02-19T20...| None |
+----------...+------------+-----------------+--------------...+--------------+
&lt;/code>&lt;/pre>
&lt;p>You can find the ip addresses of your overcloud nodes by running &lt;code>nova list&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova list
+----------...+-------------------------+--------+...+---------------------+
| ID ...| Name | Status |...| Networks |
+----------...+-------------------------+--------+...+---------------------+
| 1fc5d5e8-...| overcloud-controller-0 | ACTIVE |...| ctlplane=192.0.2.9 |
| ab6439e8-...| overcloud-controller-1 | ACTIVE |...| ctlplane=192.0.2.10 |
| 82e12f81-...| overcloud-controller-2 | ACTIVE |...| ctlplane=192.0.2.11 |
| 53402a35-...| overcloud-novacompute-0 | ACTIVE |...| ctlplane=192.0.2.8 |
+----------...+-------------------------+--------+...+---------------------+
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll use the &lt;code>ctlplane&lt;/code> address to log into each host as the
&lt;code>heat-admin&lt;/code> user. For example, to log into my compute host:&lt;/p>
&lt;pre>&lt;code>$ ssh heat-admin@192.0.2.8
&lt;/code>&lt;/pre>
&lt;h2 id="accessing-the-overcloud-openstack-environment">Accessing the overcloud OpenStack environment&lt;/h2>
&lt;p>The file &lt;code>overcloudrc&lt;/code> on the undercloud host has administrative
credentials for the overcloud environment:&lt;/p>
&lt;pre>&lt;code>. overcloudrc
&lt;/code>&lt;/pre>
&lt;p>After sourcing in the overcloud credentials you can use OpenStack
clients to interact with your deployed cloud environment.&lt;/p>
&lt;h2 id="if-you-find-bugs">If you find bugs&lt;/h2>
&lt;p>If anything in the above process doesn&amp;rsquo;t work as described or
expected, feel free to visit the &lt;code>#rdo&lt;/code> channel on &lt;a href="https://freenode.net/">freenode&lt;/a>, or
open a bug report on the &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart/issues">issue tracker&lt;/a>.&lt;/p></content></item><item><title>Cloud-init and the case of the changing hostname</title><link>https://blog.oddbit.com/post/2014-12-10-cloudinit-and-the-case-of-the-/</link><pubDate>Wed, 10 Dec 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-12-10-cloudinit-and-the-case-of-the-/</guid><description>Setting the stage I ran into a problem earlier this week deploying RDO Icehouse under RHEL 6. My target systems were a set of libvirt guests deployed from the RHEL 6 KVM guest image, which includes cloud-init in order to support automatic configuration in cloud environments. I take advantage of this when using libvirt by attaching a configuration drive so that I can pass in ssh keys and a user-data script.</description><content>&lt;h2 id="setting-the-stage">Setting the stage&lt;/h2>
&lt;p>I ran into a problem earlier this week deploying RDO Icehouse under
RHEL 6. My target systems were a set of libvirt guests deployed from
the RHEL 6 KVM guest image, which includes &lt;a href="https://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> in order to
support automatic configuration in cloud environments. I take
advantage of this when using &lt;code>libvirt&lt;/code> by attaching a configuration
drive so that I can pass in ssh keys and a &lt;code>user-data&lt;/code> script.&lt;/p>
&lt;p>Once the systems were up, I used &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a> to deploy OpenStack
onto a single controller and two compute nodes, and at the conclusion
of the &lt;code>packstack&lt;/code> run everything was functioning correctly. Running
&lt;code>neutron agent-list&lt;/code> showed all agents in good order:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+--------------------+------------+-------+----------------+
| id | agent_type | host | alive | admin_state_up |
+--------------------------------------+--------------------+------------+-------+----------------+
| 0d51d200-d902-4e05-847a-858b69c03088 | DHCP agent | controller | :-) | True |
| 192f76e9-a816-4bd9-8a90-a263a1d54031 | Open vSwitch agent | compute-0 | :-) | True |
| 3d97d7ba-1b1f-43f8-9582-f860fbfe50df | Open vSwitch agent | controller | :-) | True |
| 54d387a6-dca1-4ace-8c1b-7788fb0bc090 | Metadata agent | controller | :-) | True |
| 92fc83bf-0995-43c3-92d1-70002c734604 | L3 agent | controller | :-) | True |
| e06575c2-43b3-4691-80bc-454f501debfe | Open vSwitch agent | compute-1 | :-) | True |
+--------------------------------------+--------------------+------------+-------+----------------+
&lt;/code>&lt;/pre>
&lt;h2 id="a-problem-rears-its-ugly-head">A problem rears its ugly head&lt;/h2>
&lt;p>After rebooting the system, I found that I was missing an expected
Neutron router namespace. Specifically, given:&lt;/p>
&lt;pre>&lt;code># neutron router-list
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| id | name | external_gateway_info |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| e83eec10-0de2-4bfa-8e58-c1bcbe702f51 | router1 | {&amp;quot;network_id&amp;quot;: &amp;quot;b53a9ecd-01fc-4bee-b20d-8fbe0cd2e010&amp;quot;, &amp;quot;enable_snat&amp;quot;: true} |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>I expected to see:&lt;/p>
&lt;pre>&lt;code># ip netns
qrouter-e83eec10-0de2-4bfa-8e58-c1bcbe702f51
&lt;/code>&lt;/pre>
&lt;p>But the &lt;code>qrouter&lt;/code> namespace was missing.&lt;/p>
&lt;p>The output of &lt;code>neutron agent-list&lt;/code> shed some light on the problem:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+--------------------+------------------------+-------+----------------+
| id | agent_type | host | alive | admin_state_up |
+--------------------------------------+--------------------+------------------------+-------+----------------+
| 0832e8f3-61f9-49cf-b49c-886cc94d3d28 | Metadata agent | controller.localdomain | :-) | True |
| 0d51d200-d902-4e05-847a-858b69c03088 | DHCP agent | controller | xxx | True |
| 192f76e9-a816-4bd9-8a90-a263a1d54031 | Open vSwitch agent | compute-0 | :-) | True |
| 3be34828-ca8d-4638-9b3a-4e2f688a9ca9 | L3 agent | controller.localdomain | :-) | True |
| 3d97d7ba-1b1f-43f8-9582-f860fbfe50df | Open vSwitch agent | controller | xxx | True |
| 54d387a6-dca1-4ace-8c1b-7788fb0bc090 | Metadata agent | controller | xxx | True |
| 87b53741-f28b-4582-9ea8-6062ab9962e9 | Open vSwitch agent | controller.localdomain | :-) | True |
| 92fc83bf-0995-43c3-92d1-70002c734604 | L3 agent | controller | xxx | True |
| e06575c2-43b3-4691-80bc-454f501debfe | Open vSwitch agent | compute-1 | :-) | True |
| e327b7f9-c9ce-49f8-89c1-b699d9f7d253 | DHCP agent | controller.localdomain | :-) | True |
+--------------------------------------+--------------------+------------------------+-------+----------------+
&lt;/code>&lt;/pre>
&lt;p>There were two sets of Neutron agents registered using different
hostnames &amp;ndash; one set using the short name of the host, and the other
set using the fully qualified hostname.&lt;/p>
&lt;h2 id="whats-up-with-that">What&amp;rsquo;s up with that?&lt;/h2>
&lt;p>In the &lt;code>cc_set_hostname.py&lt;/code> module, &lt;code>cloud-init&lt;/code> performs the
following operation:&lt;/p>
&lt;pre>&lt;code>(hostname, fqdn) = util.get_hostname_fqdn(cfg, cloud)
try:
log.debug(&amp;quot;Setting the hostname to %s (%s)&amp;quot;, fqdn, hostname)
cloud.distro.set_hostname(hostname, fqdn)
except Exception:
util.logexc(log, &amp;quot;Failed to set the hostname to %s (%s)&amp;quot;, fqdn,
hostname)
raise
&lt;/code>&lt;/pre>
&lt;p>It starts by retrieving the hostname (both the qualified and
unqualified version) from the cloud environment, and then calls
&lt;code>cloud.distro.set_hostname(hostname, fqdn)&lt;/code>. This ends up calling:&lt;/p>
&lt;pre>&lt;code>def set_hostname(self, hostname, fqdn=None):
writeable_hostname = self._select_hostname(hostname, fqdn)
self._write_hostname(writeable_hostname, self.hostname_conf_fn)
self._apply_hostname(hostname)
&lt;/code>&lt;/pre>
&lt;p>Where, on a RHEL system, &lt;code>_select_hostname&lt;/code> is:&lt;/p>
&lt;pre>&lt;code>def _select_hostname(self, hostname, fqdn):
# See: http://bit.ly/TwitgL
# Should be fqdn if we can use it
if fqdn:
return fqdn
return hostname
&lt;/code>&lt;/pre>
&lt;p>So:&lt;/p>
&lt;ul>
&lt;li>&lt;code>cloud-init&lt;/code> sets &lt;code>writeable_hostname&lt;/code> to the fully qualified name
of the system (assuming it is available).&lt;/li>
&lt;li>&lt;code>cloud-init&lt;/code> writes the fully qualified hostname to &lt;code>/etc/sysconfig/network&lt;/code>.&lt;/li>
&lt;li>&lt;code>cloud-init&lt;/code> sets the hostname to the &lt;em>unqualified&lt;/em> hostname&lt;/li>
&lt;/ul>
&lt;p>The result is that your system will probably have a different hostname
after your first reboot, which throws off Neutron.&lt;/p>
&lt;h2 id="and-they-all-lived-happily-ever-after">And they all lived happily ever after?&lt;/h2>
&lt;p>It turns out this bug was reported upstream back in October of 2013 as
&lt;a href="https://bugs.launchpad.net/cloud-init/+bug/1246485">bug 1246485&lt;/a>, and while there are patches available the bug has
been marked as &amp;ldquo;low&amp;rdquo; priority and has been fixed. There are patches
attached to the bug report that purport to fix the problem.&lt;/p></content></item><item><title>Heat Hangout</title><link>https://blog.oddbit.com/post/2014-09-05-heat-hangout/</link><pubDate>Fri, 05 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-05-heat-hangout/</guid><description>I ran a Google Hangout this morning on Deploying with Heat. You can find the slides for the presentation on line here, and the Heat templates (as well as slide sources) are available on github.
If you have any questions about the presentation, please feel free to ping me on irc (larsks).</description><content>&lt;p>I ran a Google Hangout this morning on &lt;a href="https://plus.google.com/events/c9u4sjn7ksb8jrmma7vd25aok94">Deploying with Heat&lt;/a>. You
can find the slides for the presentation on line &lt;a href="http://oddbit.com/rdo-hangout-heat-intro/#/">here&lt;/a>, and the
Heat templates (as well as slide sources) are available &lt;a href="https://github.com/larsks/rdo-hangout-heat-intro/">on
github&lt;/a>.&lt;/p>
&lt;p>If you have any questions about the presentation, please feel free to
ping me on irc (&lt;code>larsks&lt;/code>).&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/qH-qYE1Kmpg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></content></item><item><title>Multinode OpenStack with Packstack</title><link>https://blog.oddbit.com/post/2014-02-27-multinode-packstack/</link><pubDate>Thu, 27 Feb 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-02-27-multinode-packstack/</guid><description>I was the presenter for this morning&amp;rsquo;s RDO hangout, where I ran through a simple demonstration of setting up a multinode OpenStack deployment using packstack.
The slides are online here.
Here&amp;rsquo;s the video (also available on the event page):</description><content>&lt;p>I was the presenter for this morning&amp;rsquo;s &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> hangout, where I ran
through a simple demonstration of setting up a multinode OpenStack
deployment using &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a>.&lt;/p>
&lt;p>The slides are online &lt;a href="http://goo.gl/Yvmd0P">here&lt;/a>.&lt;/p>
&lt;p>Here&amp;rsquo;s the video (also available on the &lt;a href="https://plus.google.com/events/cm9ff549vmsim737lj7hopk4gao">event page&lt;/a>):&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/DGf-ny25OAw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div></content></item></channel></rss>