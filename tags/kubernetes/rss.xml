<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on blog.oddbit.com</title><link>https://blog.oddbit.com/tags/kubernetes/</link><description>Recent content in Kubernetes on blog.oddbit.com</description><generator>Hugo</generator><language>en</language><copyright>Lars Kellogg-Stedman</copyright><lastBuildDate>Fri, 17 Nov 2023 23:20:19 -0500</lastBuildDate><atom:link href="https://blog.oddbit.com/tags/kubernetes/rss.xml" rel="self" type="application/rss+xml"/><item><title>Applying custom configuration to Nginx Gateway Fabric</title><link>https://blog.oddbit.com/post/2023-11-17-nginx-gateway-configuration/</link><pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2023-11-17-nginx-gateway-configuration/</guid><description>&lt;p&gt;In this post, we take a look at how to apply custom Nginx configuration directives when you&amp;rsquo;re using the &lt;a href="https://github.com/nginxinc/nginx-gateway-fabric"&gt;NGINX Gateway Fabric&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="whats-the-nginx-gateway-fabric"&gt;What&amp;rsquo;s the NGINX Gateway Fabric?&lt;/h2&gt;
&lt;p&gt;The NGINX Gateway Fabric is an implementation of the Kubernetes &lt;a href="https://gateway-api.sigs.k8s.io/"&gt;Gateway API&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="whats-the-gateway-api"&gt;What&amp;rsquo;s the Gateway API?&lt;/h2&gt;
&lt;p&gt;The Gateway API is an evolution of the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress&lt;/a&gt; API; it aims to provide a flexible mechanism for managing north/south network traffic (that is, traffic entering or exiting your Kubernetes cluster), with additional work to support east/west traffic (traffic between pods in your cluster).&lt;/p&gt;</description></item><item><title>Kubernetes, connection timeouts, and the importance of labels</title><link>https://blog.oddbit.com/post/2022-09-10-kubernetes-labels/</link><pubDate>Sat, 10 Sep 2022 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2022-09-10-kubernetes-labels/</guid><description>&lt;p&gt;We are working with an application that produces resource utilization reports for clients of our OpenShift-based cloud environments. The developers working with the application have been reporting mysterious issues concerning connection timeouts between the application and the database (a MariaDB instance). For a long time we had only high-level verbal descriptions of the problem (&amp;ldquo;I&amp;rsquo;m seeing a lot of connection timeouts!&amp;rdquo;) and a variety of unsubstantiated theories (from multiple sources) about the cause. Absent a solid reproducer of the behavior in question, we looked at other aspects of our infrastructure:&lt;/p&gt;</description></item><item><title>Kubernetes External Secrets</title><link>https://blog.oddbit.com/post/2021-09-03-kubernetes-external-secrets/</link><pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-09-03-kubernetes-external-secrets/</guid><description>&lt;p&gt;At &lt;em&gt;$JOB&lt;/em&gt; we maintain the configuration for our OpenShift clusters in a public git repository. Changes in the git repository are applied automatically using &lt;a href="https://argo-cd.readthedocs.io/en/stable/"&gt;ArgoCD&lt;/a&gt; and &lt;a href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt;. This works great, but the public nature of the repository means we need to find a secure solution for managing secrets (such as passwords and other credentials necessary for authenticating to external services). In particular, we need a solution that permits our public repository to be the source of truth for our cluster configuration, without compromising our credentials.&lt;/p&gt;</description></item><item><title>Connecting OpenShift to an External Ceph Cluster</title><link>https://blog.oddbit.com/post/2021-08-23-external-ocs/</link><pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-08-23-external-ocs/</guid><description>&lt;p&gt;Red Hat&amp;rsquo;s &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation"&gt;OpenShift Data Foundation&lt;/a&gt; (formerly &amp;ldquo;OpenShift
Container Storage&amp;rdquo;, or &amp;ldquo;OCS&amp;rdquo;) allows you to either (a) automatically
set up a Ceph cluster as an application running on your OpenShift
cluster, or (b) connect your OpenShift cluster to an externally
managed Ceph cluster. While setting up Ceph as an OpenShift
application is a relatively polished experienced, connecting to an
external cluster still has some rough edges.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB&lt;/strong&gt; I am not a Ceph expert. If you read this and think I&amp;rsquo;ve made a
mistake with respect to permissions or anything else, please feel free
to leave a comment and I will update the article as necessary. In
particular, I think it may be possible to further restrict the &lt;code&gt;mgr&lt;/code&gt;
permissions shown in this article and I&amp;rsquo;m interested in feedback on
that topic.&lt;/p&gt;</description></item><item><title>Getting started with KSOPS</title><link>https://blog.oddbit.com/post/2021-03-09-getting-started-with-ksops/</link><pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-03-09-getting-started-with-ksops/</guid><description>&lt;p&gt;&lt;a href="https://kustomize.io/"&gt;Kustomize&lt;/a&gt; is a tool for assembling Kubernetes manifests from a
collection of files. We&amp;rsquo;re making extensive use of Kustomize in the
&lt;a href="https://www.operate-first.cloud/"&gt;operate-first&lt;/a&gt; project. In order to keep secrets stored in our
configuration repositories, we&amp;rsquo;re using the &lt;a href="https://github.com/viaduct-ai/kustomize-sops"&gt;KSOPS&lt;/a&gt; plugin, which
enables Kustomize to use &lt;a href="https://github.com/mozilla/sops"&gt;sops&lt;/a&gt; to encrypt/files using GPG.&lt;/p&gt;
&lt;p&gt;In this post, I&amp;rsquo;d like to walk through the steps necessary to get
everything up and running.&lt;/p&gt;
&lt;h2 id="set-up-gpg"&gt;Set up GPG&lt;/h2&gt;
&lt;p&gt;We encrypt files using GPG, so the first step is making sure that you
have a GPG keypair and that your public key is published where other
people can find it.&lt;/p&gt;</description></item><item><title>Object storage with OpenShift Container Storage</title><link>https://blog.oddbit.com/post/2021-02-10-object-storage-with-openshift/</link><pubDate>Wed, 10 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-02-10-object-storage-with-openshift/</guid><description>&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage"&gt;OpenShift Container Storage&lt;/a&gt; (OCS) from Red Hat deploys Ceph in your
OpenShift cluster (or allows you to integrate with an external Ceph
cluster). In addition to the file- and block- based volume services
provided by Ceph, OCS includes two S3-api compatible object storage
implementations.&lt;/p&gt;
&lt;p&gt;The first option is the &lt;a href="https://docs.ceph.com/en/latest/radosgw/"&gt;Ceph Object Gateway&lt;/a&gt; (radosgw),
Ceph&amp;rsquo;s native object storage interface. The second option called the
&amp;ldquo;&lt;a href="https://www.openshift.com/blog/introducing-multi-cloud-object-gateway-for-openshift"&gt;Multicloud Object Gateway&lt;/a&gt;&amp;rdquo;, which is in fact a piece of software
named &lt;a href="https://www.noobaa.io/"&gt;Noobaa&lt;/a&gt;, a storage abstraction layer that was &lt;a href="https://www.redhat.com/en/blog/faq-red-hat-acquires-noobaa"&gt;acquired by
Red Hat&lt;/a&gt; in 2018. In this article I&amp;rsquo;d like to demonstrate how to
take advantage of these storage options.&lt;/p&gt;</description></item><item><title>Heat-kubernetes Demo with Autoscaling</title><link>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</link><pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</guid><description>&lt;p&gt;Next week is the &lt;a href="http://www.redhat.com/summit/"&gt;Red Hat Summit&lt;/a&gt; in Boston, and I&amp;rsquo;ll be taking part
in a &lt;a href="http://www.projectatomic.io/"&gt;Project Atomic&lt;/a&gt; presentation in which I will discuss various
(well, two) options for deploying Atomic into an OpenStack
environment, focusing on my &lt;a href="https://github.com/projectatomic/heat-kubernetes/"&gt;heat-kubernetes&lt;/a&gt; templates.&lt;/p&gt;
&lt;p&gt;As part of that presentation, I&amp;rsquo;ve put together a short demonstration video:&lt;/p&gt;
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"&gt;
 &lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/tS5X0qi04ZU?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"&gt;&lt;/iframe&gt;
 &lt;/div&gt;

&lt;p&gt;This shows off the autoscaling behavior available with recent versions
of these templates (and also serves as a very brief introduction to
working with Kubernetes).&lt;/p&gt;</description></item><item><title>External networking for Kubernetes services</title><link>https://blog.oddbit.com/post/2015-02-10-external-networking-for-kubern/</link><pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-10-external-networking-for-kubern/</guid><description>&lt;p&gt;I have recently started running some &amp;ldquo;real&amp;rdquo; services (that is,
&amp;ldquo;services being consumed by someone other than myself&amp;rdquo;) on top of
Kubernetes (running on bare metal), which means I suddenly had to
confront the question of how to provide external access to Kubernetes
hosted services. Kubernetes provides two solutions to this problem,
neither of which is particularly attractive out of the box:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;There is a field &lt;code&gt;createExternalLoadBalancer&lt;/code&gt; that can be set in a
service description. This is meant to integrate with load
balancers provided by your local cloud environment, but at the
moment there is only support for this when running under &lt;a href="https://cloud.google.com/compute/"&gt;GCE&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>Building a minimal web server for testing Kubernetes</title><link>https://blog.oddbit.com/post/2015-01-04-building-a-minimal-web-server-/</link><pubDate>Sun, 04 Jan 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-01-04-building-a-minimal-web-server-/</guid><description>&lt;p&gt;I have recently been doing some work with &lt;a href="https://github.com/googlecloudplatform/kubernetes"&gt;Kubernetes&lt;/a&gt;, and wanted
to put together a minimal image with which I could test service and
pod deployment. Size in this case was critical: I wanted something
that would download quickly when initially deployed, because I am
often setting up and tearing down Kubernetes as part of my testing
(and some of my test environments have poor external bandwidth).&lt;/p&gt;
&lt;h2 id="building-thttpd"&gt;Building thttpd&lt;/h2&gt;
&lt;p&gt;My go-to minimal webserver is &lt;a href="http://acme.com/software/thttpd/"&gt;thttpd&lt;/a&gt;. For the normal case,
building the software is a simple matter of &lt;code&gt;./configure&lt;/code&gt; followed by
&lt;code&gt;make&lt;/code&gt;. This gets you a dynamically linked binary; using &lt;code&gt;ldd&lt;/code&gt; you
could build a Docker image containing only the necessary shared
libraries:&lt;/p&gt;</description></item><item><title>Fedora Atomic, OpenStack, and Kubernetes (oh my)</title><link>https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/</link><pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/</guid><description>&lt;p&gt;While experimenting with &lt;a href="http://www.projectatomic.io/"&gt;Fedora Atomic&lt;/a&gt;, I was looking for an
elegant way to automatically deploy Atomic into an &lt;a href="http://openstack.org/"&gt;OpenStack&lt;/a&gt;
environment and then automatically schedule some &lt;a href="http://docker.com/"&gt;Docker&lt;/a&gt; containers
on the Atomic host. This post describes my solution.&lt;/p&gt;
&lt;p&gt;Like many other cloud-targeted distributions, Fedora Atomic runs
&lt;a href="http://cloudinit.readthedocs.org/"&gt;cloud-init&lt;/a&gt; when the system boots. We can take advantage of this
to configure the system at first boot by providing a &lt;code&gt;user-data&lt;/code&gt; blob
to Nova when we boot the instance. A &lt;code&gt;user-data&lt;/code&gt; blob can be as
simple as a shell script, and while we could arguably mash everything
into a single script it wouldn&amp;rsquo;t be particularly maintainable or
flexible in the face of different pod/service/etc descriptions.&lt;/p&gt;</description></item><item><title>Docker networking with dedicated network containers</title><link>https://blog.oddbit.com/post/2014-10-06-docker-networking-with-dedicat/</link><pubDate>Mon, 06 Oct 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-10-06-docker-networking-with-dedicat/</guid><description>&lt;p&gt;The current version of Docker has a very limited set of networking
options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bridge&lt;/code&gt; &amp;ndash; connect a container to the Docker bridge&lt;/li&gt;
&lt;li&gt;&lt;code&gt;host&lt;/code&gt; &amp;ndash; run the container in the global network namespace&lt;/li&gt;
&lt;li&gt;&lt;code&gt;container:xxx&lt;/code&gt; &amp;ndash; connect a container to the network namespace of
another container&lt;/li&gt;
&lt;li&gt;&lt;code&gt;none&lt;/code&gt; &amp;ndash; do not configure any networking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you need something more than that, you can use a tool like
&lt;a href="https://github.com/jpetazzo/pipework"&gt;pipework&lt;/a&gt; to provision additional network interfaces inside the
container, but this leads to a synchronization problem: &lt;code&gt;pipework&lt;/code&gt; can
only be used after your container is running. This means that when
starting your container, you must have logic that will wait until the
necessary networking is available before starting your service.&lt;/p&gt;</description></item></channel></rss>