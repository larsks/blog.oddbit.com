<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>openstack on blog.oddbit.com</title><link>https://blog.oddbit.com/tags/openstack/</link><description>Recent content in openstack on blog.oddbit.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Lars Kellogg-Stedman</copyright><lastBuildDate>Thu, 19 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.oddbit.com/tags/openstack/rss.xml" rel="self" type="application/rss+xml"/><item><title>OVN and DHCP: A minimal example</title><link>https://blog.oddbit.com/post/2019-12-19-ovn-and-dhcp/</link><pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-12-19-ovn-and-dhcp/</guid><description>Introduction A long time ago, I wrote an article all about OpenStack Neutron (which at that time was called Quantum). That served as an excellent reference for a number of years, but if you&amp;rsquo;ve deployed a recent version of OpenStack you may have noticed that the network architecture looks completely different. The network namespaces previously used to implement routers and dhcp servers are gone (along with iptables rules and other features), and have been replaced by OVN (&amp;ldquo;Open Virtual Network&amp;rdquo;).</description><content>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>A long time ago, I wrote an article &lt;a href="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/">all about OpenStack Neutron&lt;/a> (which at that time was called Quantum). That served as an excellent reference for a number of years, but if you&amp;rsquo;ve deployed a recent version of OpenStack you may have noticed that the network architecture looks completely different. The network namespaces previously used to implement routers and dhcp servers are gone (along with iptables rules and other features), and have been replaced by OVN (&amp;ldquo;Open Virtual Network&amp;rdquo;). What is OVN? How does it work? In this article, I&amp;rsquo;d like to explore a minimal OVN installation to help answer these questions.&lt;/p>
&lt;h2 id="goals">Goals&lt;/h2>
&lt;p>We&amp;rsquo;re going to create a single OVN logical switch to which we will attach a few ports. We will demonstrate how we can realize a port on a physical node and configure it using DHCP, using a virtual DHCP server provided by OVN.&lt;/p>
&lt;h2 id="so-what-is-ovn-anyway">So what is OVN anyway?&lt;/h2>
&lt;p>If you&amp;rsquo;re just getting started with OVN, you&amp;rsquo;ll find that&amp;rsquo;s a hard question to answer: there is no dedicated OVN website; there&amp;rsquo;s no OVN landing page at &lt;a href="http://openvswitch.org">http://openvswitch.org&lt;/a>; in fact, there&amp;rsquo;s really no documentation for OVN at all other than the man pages. The only high-level description you&amp;rsquo;ll find comes from the &lt;code>ovn-architecture(7)&lt;/code> man page:&lt;/p>
&lt;blockquote>
&lt;p>OVN, the Open Virtual Network, is a system to support virtual network
abstraction. OVN complements the existing capabilities of OVS to add
native support for virtual network abstractions, such as virtual L2 and L3
overlays and security groups.&lt;/p>
&lt;/blockquote>
&lt;p>Where Open vSwitch (OVS) provides a virtual switch on a single host, OVN extends this abstraction to span multiple hosts. You can create virtual switches that span many physical nodes, and OVN will take care of creating overlay networks to support this abstraction. While OVS is primarily just a layer 2 device, OVN also operates at layer 3: you can create virtual routers to connect your virtual networks as well a variety of access control mechanisms such as security groups and ACLs.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>You&amp;rsquo;re going to need a recent version of OVN. Packages are available for &lt;a href="http://docs.openvswitch.org/en/latest/intro/install/distributions/">most major distributions&lt;/a>. I used &lt;a href="https://getfedora.org/">Fedora 31&lt;/a> for my testing, which includes OVS and OVN version 2.12.0. You can of course also &lt;a href="http://docs.openvswitch.org/en/latest/intro/install/">install from source&lt;/a>.&lt;/p>
&lt;p>This post assumes that you are logged in to your system as the &lt;code>root&lt;/code> user. Most of the commands require root access in order to function correctly.&lt;/p>
&lt;h2 id="concepts">Concepts&lt;/h2>
&lt;p>OVN operates with a pair of databases. The &lt;em>Northbound&lt;/em> database contains the &lt;em>logical&lt;/em> structure of your networks: this is where you define switches, routers, ports, and so on.&lt;/p>
&lt;p>The &lt;em>Southbound&lt;/em> database is concerned with the &lt;em>physical&lt;/em> structure of your network. This database maintains information about which ports are realized on which hosts.&lt;/p>
&lt;p>The &lt;a href="http://www.openvswitch.org/support/dist-docs/ovn-northd.8.html">&lt;code>ovn-northd&lt;/code>&lt;/a> service &amp;ldquo;translates the logical network configuration in terms of conventional network concepts, taken from the OVN Northâ€ bound Database, into logical datapath flows in the OVN Southbound Database below it.&amp;rdquo; (&lt;a href="http://www.openvswitch.org/support/dist-docs/ovn-architecture.7.html">ovn-architecture(7)&lt;/a>)&lt;/p>
&lt;p>The &lt;a href="http://www.openvswitch.org/support/dist-docs/ovn-controller.8.html">&lt;code>ovn-controller&lt;/code>&lt;/a> service running on each host connects to the Southbound database and is responsible for configuring OVS as instructed by the database configuration.&lt;/p>
&lt;h2 id="test-environment">Test environment&lt;/h2>
&lt;p>This article assumes a test environment with three nodes running Fedora 31. All nodes have a single interface connecting to a shared layer 2 network:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>MAC address&lt;/th>
&lt;th>IP address&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ovn0&lt;/td>
&lt;td>de:ca:ff:00:00:64&lt;/td>
&lt;td>192.168.122.100&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ovn1&lt;/td>
&lt;td>de:ca:ff:00:00:65&lt;/td>
&lt;td>192.168.122.101&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ovn2&lt;/td>
&lt;td>de:ca:ff:00:00:66&lt;/td>
&lt;td>192.168.122.102&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h1 id="setting-up-ovn">Setting up OVN&lt;/h1>
&lt;h2 id="initial-configuration-steps">Initial configuration steps&lt;/h2>
&lt;p>Our first step will be to activate &lt;code>openvswitch&lt;/code> and &lt;code>ovn-controller&lt;/code> on all of the nodes in our test environment. On all nodes, run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl enable --now openvswitch ovn-controller
&lt;/code>&lt;/pre>&lt;p>The &lt;code>--now&lt;/code> flag causes &lt;code>systemd&lt;/code> to start the service as well as enabling it in future boots.&lt;/p>
&lt;p>By default, OVN manages an &lt;code>openvswitch&lt;/code> bridge named &lt;code>br-int&lt;/code> (for &amp;ldquo;integration&amp;rdquo;). We&amp;rsquo;ll need to create this on all of our nodes. On all nodes, run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-br br-int
&lt;/code>&lt;/pre>&lt;h2 id="configuring-the-controller">Configuring the controller&lt;/h2>
&lt;p>We will designate the node &lt;code>ovn0&lt;/code> as our controller (which simply means &amp;ldquo;this node will run &lt;code>ovn-northd&lt;/code>). The first thing we need to do is enable the &lt;code>ovn-northd&lt;/code> service. On node &lt;code>ovn0&lt;/code>, run:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl enable --now ovn-northd
&lt;/code>&lt;/pre>&lt;p>In addition to starting the &lt;code>ovn-northd&lt;/code> service itself, this will also starts two instances of &lt;a href="http://www.openvswitch.org/support/dist-docs/ovsdb-server.1.html">&lt;code>ovsdb-server&lt;/code>&lt;/a>: one serving the Northbound database, listening on &lt;code>/run/ovn/ovnnb_db.sock&lt;/code>, and the second for the Southbound database, listening on &lt;code>/run/ovn/ovnsb_db.sock&lt;/code>. In order for the &lt;code>ovn-controller&lt;/code> service on the other nodes to connect to the Southbound database, we will need to configure that instance of &lt;code>ovsdb-server&lt;/code> to listen for tcp connections. We can do that using the &lt;code>ovn-sbctl set-connection&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-sbctl set-connection ptcp:6642
&lt;/code>&lt;/pre>&lt;p>The &lt;code>ptcp&lt;/code> in the above setting means &amp;ldquo;passive tcp&amp;rdquo;, which means &amp;ldquo;listen on port 6642 for connections&amp;rdquo;. After running the above command, we see that there is now an &lt;code>ovsdb-server&lt;/code> instance listening on port 6642:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ss -tlnp | grep 6642
LISTEN 0 10 0.0.0.0:6642 0.0.0.0:* users:((&amp;#34;ovsdb-server&amp;#34;,pid=1798,fd=21))
&lt;/code>&lt;/pre>&lt;h2 id="connecting-nodes-to-the-controller">Connecting nodes to the controller&lt;/h2>
&lt;p>Now that we have our controller configured, we have to connect the &lt;code>ovn-controller&lt;/code> service on our nodes to the Southbound database. We do this by creating several entries in the &lt;code>external_ids&lt;/code> column of the OVS &lt;code>open_vswitch&lt;/code> database on each host:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ovn-remote&lt;/code> &amp;ndash; this is the address of the controller&lt;/li>
&lt;li>&lt;code>ovn-encap-ip&lt;/code> &amp;ndash; this is the local address that will be used for tunnel endpoints&lt;/li>
&lt;li>&lt;code>ovn-encap-type&lt;/code> &amp;ndash; the encapsulation mechanism to use for tunnels&lt;/li>
&lt;li>&lt;code>system-id&lt;/code> &amp;ndash; a unique identifier for the local host&lt;/li>
&lt;/ul>
&lt;p>On all nodes, run the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl set open_vswitch . \
external_ids:ovn-remote=tcp:192.168.122.100:6642 \
external_ids:ovn-encap-ip=$(ip addr show eth0 | awk &amp;#39;$1 == &amp;#34;inet&amp;#34; {print $2}&amp;#39; | cut -f1 -d/) \
external_ids:ovn-encap-type=geneve \
external_ids:system-id=$(hostname)
&lt;/code>&lt;/pre>&lt;p>This points &lt;code>ovn-remote&lt;/code> at the address of the controller, sets &lt;code>ovn-encap-ip&lt;/code> to the address of &lt;code>eth0&lt;/code> on the local host, sets &lt;code>systemd-id&lt;/code> to the local hostname, and selects &lt;a href="https://tools.ietf.org/html/draft-ietf-nvo3-geneve-08">geneve&lt;/a> encapsulation for tunnels (see &lt;a href="https://blog.russellbryant.net/2017/05/30/ovn-geneve-vs-vxlan-does-it-matter/">this post&lt;/a> for information on why OVN prefers Geneve encapsulation).&lt;/p>
&lt;p>We can verify these settings by using the &lt;code>ovs-vsctl list&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn1 ~]# ovs-vsctl --columns external_ids list open_vswitch
external_ids : {hostname=&amp;#34;ovn1.virt&amp;#34;, ovn-encap-ip=&amp;#34;192.168.122.101&amp;#34;, ovn-encap-type=geneve, ovn-remote=&amp;#34;192.168.122.100&amp;#34;, rundir=&amp;#34;/var/run/openvswitch&amp;#34;, system-id=&amp;#34;ovn1&amp;#34;}
&lt;/code>&lt;/pre>&lt;p>After running the above commands, each node should now have tunnels interfaces connecting to the other nodes in the test environment. For example, running &lt;code>ovs-vsctl show&lt;/code> on node &lt;code>ovn1&lt;/code> looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>f0087676-7f93-419c-9da0-32321d2d3668
Bridge br-int
fail_mode: secure
Port &amp;#34;ovn-ovn0-0&amp;#34;
Interface &amp;#34;ovn-ovn0-0&amp;#34;
type: geneve
options: {csum=&amp;#34;true&amp;#34;, key=flow, remote_ip=&amp;#34;192.168.122.100&amp;#34;}
Port br-int
Interface br-int
type: internal
Port &amp;#34;ovn-ovn2-0&amp;#34;
Interface &amp;#34;ovn-ovn2-0&amp;#34;
type: geneve
options: {csum=&amp;#34;true&amp;#34;, key=flow, remote_ip=&amp;#34;192.168.122.102&amp;#34;}
ovs_version: &amp;#34;2.12.0&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Due to what appears to be &lt;a href="https://mail.openvswitch.org/pipermail/ovs-discuss/2020-January/049692.html">some sort of race condition in OVN&lt;/a>, you may not see the geneve tunnels in the &lt;code>ovs-vsctl show&lt;/code> output. If this is the case, restart &lt;code>ovn-controller&lt;/code> on all your ovn nodes:&lt;/p>
&lt;pre tabindex="0">&lt;code>systemctl restart ovn-controller
&lt;/code>&lt;/pre>&lt;p>The issue with the geneve tunnels appears to be resolved by &lt;a href="https://patchwork.ozlabs.org/patch/1222380/">this patch&lt;/a>, which will hopefully land in OVN in the near future.&lt;/p>
&lt;h1 id="creating-a-virtual-network">Creating a virtual network&lt;/h1>
&lt;p>Now that we have a functioning OVN environment, we&amp;rsquo;re ready to create our virtual network.&lt;/p>
&lt;h2 id="create-a-logical-switch">Create a logical switch&lt;/h2>
&lt;p>We&amp;rsquo;ll start by creating a logical switch, which we will call &lt;code>net0&lt;/code>. We create that using the &lt;code>ovn-nbctl ls-add&lt;/code> command. Run the following on &lt;code>ovn0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl ls-add net0
&lt;/code>&lt;/pre>&lt;p>After running the above command, the output of &lt;code>ovn-nbctl show&lt;/code> will look something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-nbctl show
switch d8d96fb2-e1e7-469d-8c72-b7e891fb16ba (net0)
&lt;/code>&lt;/pre>&lt;p>Next, we need to set some configuration options on the switch that will be used to set the range from which we allocate addresses via DHCP. We&amp;rsquo;re going to have OVN manage the &lt;code>10.0.0.0/24&lt;/code> network, which means we need to set &lt;code>other_config:subnet&lt;/code> to &lt;code>10.0.0.0/24&lt;/code>. I generally like to reserve some addresses from the DHCP range to use for static allocations, so I have also set &lt;code>other_config:exclude_ips&lt;/code> to &lt;code>10.0.0.1..10.0.0.10&lt;/code>. This means that DHCP allocations will come from the range &lt;code>10.0.0.11&lt;/code> - &lt;code>10.0.0.254&lt;/code>.&lt;/p>
&lt;p>To apply these settings, run the following commands on &lt;code>ovn0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl set logical_switch net0 \
other_config:subnet=&amp;#34;10.0.0.0/24&amp;#34; \
other_config:exclude_ips=&amp;#34;10.0.0.1..10.0.0.10&amp;#34;
&lt;/code>&lt;/pre>&lt;h2 id="create-dhcp-options">Create DHCP options&lt;/h2>
&lt;p>Each port that we want to configure using DHCP needs to be associated with a set of DHCP options. We accomplish this by creating a new entry in the Northbound &lt;code>dhcp_options&lt;/code> table, and then set the &lt;code>dhcp_options&lt;/code> column of the port to the id of the object we created in the &lt;code>dhcp_options&lt;/code> table.&lt;/p>
&lt;p>Looking at &lt;a href="https://github.com/ovn-org/ovn/blob/master/northd/ovn-northd.c#L4113">the source&lt;/a>, there are three required options that must be set in order for DHCP to operate:&lt;/p>
&lt;ul>
&lt;li>&lt;code>server_id&lt;/code> &amp;ndash; the ip address of the virtual dhcp server&lt;/li>
&lt;li>&lt;code>server_mac&lt;/code> &amp;ndash; the MAC address of the virtual dhcp server&lt;/li>
&lt;li>&lt;code>lease_time&lt;/code> &amp;ndash; the lifetime of DHCP leases&lt;/li>
&lt;/ul>
&lt;p>While not actually required, we can also set the &lt;code>router&lt;/code> key to provide information about the default gateway. We&amp;rsquo;re not going to make use of it in this example, but in practice you will probably want to set the &lt;code>router&lt;/code> option.&lt;/p>
&lt;p>We also need to set the CIDR range that will be served by the DHCP server.&lt;/p>
&lt;p>We can create the appropriate options using the &lt;code>ovn-nbctl dhcp-options-create&lt;/code> command. Run the following on &lt;code>ovn0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl dhcp-options-create 10.0.0.0/24
&lt;/code>&lt;/pre>&lt;p>Despite the name of that command, it doesn&amp;rsquo;t actually let us set DHCP options. For that, we need to first look up the uuid of our newly created entry in the &lt;code>dhcp_options&lt;/code> table. Let&amp;rsquo;s store that in the &lt;code>CIDR_UUID&lt;/code> variable, which we will use in a few places in the remainder of this post:&lt;/p>
&lt;pre tabindex="0">&lt;code>CIDR_UUID=$(ovn-nbctl --bare --columns=_uuid find dhcp_options cidr=&amp;#34;10.0.0.0/24&amp;#34;)
&lt;/code>&lt;/pre>&lt;p>With that uuid in hand, we can now set the required options:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl dhcp-options-set-options ${CIDR_UUID} \
lease_time=3600 \
router=10.0.0.1 \
server_id=10.0.0.1 \
server_mac=c0:ff:ee:00:00:01
&lt;/code>&lt;/pre>&lt;p>We can use the database &lt;code>list&lt;/code> command to inspect the &lt;code>dhcp_options&lt;/code> table to verify that things look as we expect:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-nbctl list dhcp_options
_uuid : f8a6abc5-b8e4-4209-8809-b95435b4d48b
cidr : &amp;#34;10.0.0.0/24&amp;#34;
external_ids : {lease_time=&amp;#34;3600&amp;#34;, router=&amp;#34;10.0.0.1&amp;#34;, server_id=&amp;#34;10.0.0.1&amp;#34;, server_mac=&amp;#34;c0:ff:ee:00:00:01&amp;#34;}
options : {}
&lt;/code>&lt;/pre>&lt;p>Instead of using the &lt;code>dhcp-options-create&lt;/code> command, as we did in this section, we could instead have used the database &lt;code>create&lt;/code> command. The quoting requirements for that command are a little more complex, but unlike the &lt;code>dhcp-options-create&lt;/code> command the &lt;code>create&lt;/code> command returns the id of the row it creates. This can be useful if you&amp;rsquo;re using the command as part of a script. The equivalent &lt;code>create&lt;/code> command would look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>CIDR_UUID=$(ovn-nbctl create dhcp_options \
cidr=10.0.0.0/24 \
options=&amp;#39;&amp;#34;lease_time&amp;#34;=&amp;#34;3600&amp;#34; &amp;#34;router&amp;#34;=&amp;#34;10.0.0.1&amp;#34; &amp;#34;server_id&amp;#34;=&amp;#34;10.0.0.1&amp;#34; &amp;#34;server_mac&amp;#34;=&amp;#34;c0:ff:ee:00:00:01&amp;#34;&amp;#39;)
&lt;/code>&lt;/pre>&lt;h2 id="create-logical-ports">Create logical ports&lt;/h2>
&lt;p>Let&amp;rsquo;s add the following three logical ports to the switch:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>MAC Address&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>port1&lt;/td>
&lt;td>c0:ff:ee:00:00:11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>port2&lt;/td>
&lt;td>c0:ff:ee:00:00:12&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>port3&lt;/td>
&lt;td>c0:ff:ee:00:00:13&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For each port, we&amp;rsquo;ll need to run three commands. First, we create the port on the switch:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-add net0 port1
&lt;/code>&lt;/pre>&lt;p>Next, we set the port addresses. For this example, I&amp;rsquo;m using static MAC addresses and dynamic (assigned by DHCP) IP addresses, so the command will look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-set-addresses port1 &amp;#34;c0:ff:ee:00:00:11 dynamic&amp;#34;
&lt;/code>&lt;/pre>&lt;p>If you want OVN to set MAC addresses for the ports as well, you would instead run:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-set-addresses port1 &amp;#34;dynamic&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Finally, we associate the port with the DHCP options we created in the previous section:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-set-dhcpv4-options port1 $CIDR_UUID
&lt;/code>&lt;/pre>&lt;p>Repeat the above sequence for &lt;code>port2&lt;/code> and &lt;code>port3&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-nbctl lsp-add net0 port2
ovn-nbctl lsp-set-addresses port2 &amp;#34;c0:ff:ee:00:00:12 dynamic&amp;#34;
ovn-nbctl lsp-set-dhcpv4-options port2 $CIDR_UUID
ovn-nbctl lsp-add net0 port3
ovn-nbctl lsp-set-addresses port3 &amp;#34;c0:ff:ee:00:00:13 dynamic&amp;#34;
ovn-nbctl lsp-set-dhcpv4-options port3 $CIDR_UUID
&lt;/code>&lt;/pre>&lt;p>When you&amp;rsquo;re done, &lt;code>ovn-nbctl show&lt;/code> should return output similar to the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>switch 3c03342f-f762-410b-9f4e-572d266c8ff7 (net0)
port port2
addresses: [&amp;#34;c0:ff:ee:00:00:12 dynamic&amp;#34;]
port port3
addresses: [&amp;#34;c0:ff:ee:00:00:13 dynamic&amp;#34;]
port port1
addresses: [&amp;#34;c0:ff:ee:00:00:11 dynamic&amp;#34;]
&lt;/code>&lt;/pre>&lt;p>We can see additional details using the database command &lt;code>ovn-nbctl list logical_switch_port&lt;/code>. The entry for &lt;code>port1&lt;/code> might look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>_uuid : 8ad6a4c0-4c7b-4817-bf13-8e7b1a86bab1
addresses : [&amp;#34;c0:ff:ee:00:00:11 dynamic&amp;#34;]
dhcpv4_options : f8a6abc5-b8e4-4209-8809-b95435b4d48b
dhcpv6_options : []
dynamic_addresses : &amp;#34;c0:ff:ee:00:00:11 10.0.0.11&amp;#34;
enabled : []
external_ids : {}
ha_chassis_group : []
name : port1
options : {}
parent_name : []
port_security : []
tag : []
tag_request : []
type : &amp;#34;&amp;#34;
up : false
&lt;/code>&lt;/pre>&lt;p>Looking at the &lt;code>dynamic_addresses&lt;/code> column we can see that &lt;code>port1&lt;/code> has been assigned the ip address &lt;code>10.0.0.11&lt;/code>. We can see the assigned addresses for all of our ports like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-nbctl --columns dynamic_addresses list logical_switch_port
dynamic_addresses : &amp;#34;c0:ff:ee:00:00:13 10.0.0.13&amp;#34;
dynamic_addresses : &amp;#34;c0:ff:ee:00:00:11 10.0.0.11&amp;#34;
dynamic_addresses : &amp;#34;c0:ff:ee:00:00:12 10.0.0.12&amp;#34;
&lt;/code>&lt;/pre>&lt;h2 id="simulating-a-dhcp-request-with-ovn-trace">Simulating a DHCP request with ovn-trace&lt;/h2>
&lt;p>At this point, we have a functioning switch, although we haven&amp;rsquo;t actually realized the ports anywhere yet. This is the perfect time to introduce the &lt;code>ovn-trace&lt;/code> tool, which can be used to simulate how your OVN network will handle a packet of data.&lt;/p>
&lt;p>We can show how OVN will respond to a DHCP &lt;code>DISCOVER&lt;/code> message with the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovn-trace --summary net0 &amp;#39;
inport==&amp;#34;port1&amp;#34; &amp;amp;&amp;amp;
eth.src==c0:ff:ee:00:00:11 &amp;amp;&amp;amp;
ip4.src==0.0.0.0 &amp;amp;&amp;amp;
ip.ttl==1 &amp;amp;&amp;amp;
ip4.dst==255.255.255.255 &amp;amp;&amp;amp;
udp.src==68 &amp;amp;&amp;amp;
udp.dst==67&amp;#39;
&lt;/code>&lt;/pre>&lt;p>The above command simulates a packet originating on &lt;code>port1&lt;/code> with the appropriate MAC address (&lt;code>eth.src&lt;/code>, &lt;code>c0:ff:ee:00:00:11&lt;/code>) and a source address (&lt;code>ip4.src&lt;/code>) of &lt;code>0.0.0.0&lt;/code> (port 68 (&lt;code>udp.src&lt;/code>)), targeting (&lt;code>ip4.dst&lt;/code>) the broadcast address &lt;code>255.255.255.255&lt;/code> (port 67 (&lt;code>udp.dst&lt;/code>)).&lt;/p>
&lt;p>Assuming everything is functioning correctly, this should produce the following output:&lt;/p>
&lt;pre tabindex="0">&lt;code># udp,reg14=0x2,vlan_tci=0x0000,dl_src=c0:ff:ee:00:00:11,dl_dst=c0:ff:ee:00:00:01,nw_src=0.0.0.0,nw_dst=255.255.255.255,nw_tos=0,nw_ecn=0,nw_ttl=1,tp_src=68,tp_dst=67
ingress(dp=&amp;#34;net0&amp;#34;, inport=&amp;#34;port1&amp;#34;) {
next;
reg0[3] = put_dhcp_opts(offerip = 10.0.0.11, lease_time = 3600, netmask = 255.255.255.0, router = 10.0.0.1, server_id = 10.0.0.1);
/* We assume that this packet is DHCPDISCOVER or DHCPREQUEST. */;
next;
eth.dst = eth.src;
eth.src = c0:ff:ee:00:00:01;
ip4.dst = 10.0.0.11;
ip4.src = 10.0.0.1;
udp.src = 67;
udp.dst = 68;
outport = inport;
flags.loopback = 1;
output;
egress(dp=&amp;#34;net0&amp;#34;, inport=&amp;#34;port1&amp;#34;, outport=&amp;#34;port1&amp;#34;) {
next;
output;
/* output to &amp;#34;port1&amp;#34;, type &amp;#34;&amp;#34; */;
};
};
&lt;/code>&lt;/pre>&lt;p>In the above output, you can see that OVN is filling in the details of the DHCP lease (that&amp;rsquo;s the &lt;code>put_dhcp_options&lt;/code> command), and then sending the packet back out &lt;code>port1&lt;/code> with the ethernet source and destination addresses reversed (so that the destination address is now the MAC address of &lt;code>port1&lt;/code>).&lt;/p>
&lt;p>It looks like everything is working in theory. Let&amp;rsquo;s attach some actual network interfaces and see what happens!&lt;/p>
&lt;h1 id="attaching-network-interfaces">Attaching network interfaces&lt;/h1>
&lt;p>In this section, we will attach network interfaces to our logical switch and demonstrate that they can be properly configured using DHCP.&lt;/p>
&lt;h2 id="create-an-ovs-port">Create an OVS port&lt;/h2>
&lt;p>On host &lt;code>ovn1&lt;/code>, let&amp;rsquo;s create port &lt;code>port1&lt;/code>. We&amp;rsquo;ll want to ensure that (a) the MAC address of this port matches the MAC address we configured earlier (&lt;code>c0:ff:ee:00:00:11&lt;/code>), and we need to make sure that the &lt;code>iface-id&lt;/code> external id matches the port name we registered in the Northbound database. We can do that with the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br-int port1 -- \
set interface port1 \
type=internal \
mac=&amp;#39;[&amp;#34;c0:ff:ee:00:00:11&amp;#34;]&amp;#39; \
external_ids:iface-id=port1
&lt;/code>&lt;/pre>&lt;p>After running this command, running &lt;code>ovs-vsctl show&lt;/code> on &lt;code>ovn1&lt;/code> should produce:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn1 ~]# ovs-vsctl show
f359ad7a-5fcd-49b3-8557-e61be3a0b130
Bridge br-int
fail_mode: secure
Port br-int
Interface br-int
type: internal
Port &amp;#34;port1&amp;#34;
Interface &amp;#34;port1&amp;#34;
type: internal
Port &amp;#34;ovn-ovn2-0&amp;#34;
Interface &amp;#34;ovn-ovn2-0&amp;#34;
type: geneve
options: {csum=&amp;#34;true&amp;#34;, key=flow, remote_ip=&amp;#34;192.168.122.102&amp;#34;}
Port &amp;#34;ovn-ovn0-0&amp;#34;
Interface &amp;#34;ovn-ovn0-0&amp;#34;
type: geneve
options: {csum=&amp;#34;true&amp;#34;, key=flow, remote_ip=&amp;#34;192.168.122.100&amp;#34;}
ovs_version: &amp;#34;2.12.0&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Furthermore, OVN should also be aware of this port. If we run &lt;code>ovn-sbctl show&lt;/code> on &lt;code>ovn0&lt;/code>, we see a binding for host &lt;code>ovn1&lt;/code> (look for the &lt;code>Port_Binding port1&lt;/code> line under &lt;code>Chassis ovn1&lt;/code>):&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-sbctl show
Chassis ovn0
hostname: ovn0.virt
Encap geneve
ip: &amp;#34;192.168.122.100&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Chassis ovn1
hostname: ovn1.virt
Encap geneve
ip: &amp;#34;192.168.122.101&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Port_Binding port1
Chassis ovn2
hostname: ovn2.virt
Encap geneve
ip: &amp;#34;192.168.122.102&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
&lt;/code>&lt;/pre>&lt;h2 id="configure-the-port-using-dhcp">Configure the port using DHCP&lt;/h2>
&lt;p>We can now try to configure this interface with DHCP. Let&amp;rsquo;s first move the interface into a network namespace; this means we don&amp;rsquo;t need to worry about messing up routing on the host. We&amp;rsquo;ll create a namespace named &lt;code>vm1&lt;/code> and make &lt;code>port1&lt;/code> part of that namespace:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip netns add vm1
ip link set netns vm1 port1
ip -n vm1 addr add 127.0.0.1/8 dev lo
ip -n vm1 link set lo up
&lt;/code>&lt;/pre>&lt;p>We can now configure the interface using DHCP by running the &lt;code>dhclient&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip netns exec vm1 dhclient -v -i port1 --no-pid
&lt;/code>&lt;/pre>&lt;p>After &lt;code>dhclient&lt;/code> goes to the background, we see that it was able to successfully request an address:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn1 ~]# ip netns exec vm1 dhclient -v -i port1 --no-pid
Internet Systems Consortium DHCP Client 4.4.1
Copyright 2004-2018 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/
Listening on LPF/port1/c0:ff:ee:00:00:11
Sending on LPF/port1/c0:ff:ee:00:00:11
Sending on Socket/fallback
Created duid &amp;#34;\000\004\344J\012\236\007\033AF\261\354\246\273\206\011\226g&amp;#34;.
DHCPDISCOVER on port1 to 255.255.255.255 port 67 interval 7 (xid=0xffc0820a)
DHCPOFFER of 10.0.0.11 from 10.0.0.1
DHCPREQUEST for 10.0.0.11 on port1 to 255.255.255.255 port 67 (xid=0xffc0820a)
DHCPACK of 10.0.0.11 from 10.0.0.1 (xid=0xffc0820a)
bound to 10.0.0.11 -- renewal in 1378 seconds.
&lt;/code>&lt;/pre>&lt;p>And it has correctly configured the interface:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn1 ~]# ip netns exec vm1 ip addr show port1
6: port1: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000
link/ether c0:ff:ee:00:00:11 brd ff:ff:ff:ff:ff:ff
inet 10.0.0.11/24 brd 10.0.0.255 scope global dynamic port1
valid_lft 577sec preferred_lft 577sec
inet6 fe80::c2ff:eeff:fe00:11/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;h2 id="configuring-port2-on-ovn1">Configuring port2 on ovn1&lt;/h2>
&lt;p>Let&amp;rsquo;s repeat the above process with &lt;code>port2&lt;/code>, again using host &lt;code>ovn1&lt;/code>. First we add the port:&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br-int port2 -- \
set interface port2 \
type=internal \
mac=&amp;#39;[&amp;#34;c0:ff:ee:00:00:12&amp;#34;]&amp;#39; \
external_ids:iface-id=port2
&lt;/code>&lt;/pre>&lt;p>Add it to a namespace:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip netns add vm2
ip link set netns vm2 port2
ip -n vm2 addr add 127.0.0.1/8 dev lo
ip -n vm2 link set lo up
&lt;/code>&lt;/pre>&lt;p>Configure it using &lt;code>dhclient&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>ip netns exec vm2 dhclient -v -i port2 --no-pid
&lt;/code>&lt;/pre>&lt;p>And finally look at the OVN port bindings on &lt;code>ovn0&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-sbctl show
Chassis ovn1
hostname: ovn1.virt
Encap geneve
ip: &amp;#34;192.168.122.101&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Port_Binding port2
Port_Binding port1
Chassis ovn0
hostname: ovn0.virt
Encap geneve
ip: &amp;#34;192.168.122.100&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Chassis ovn2
hostname: ovn2.virt
Encap geneve
ip: &amp;#34;192.168.122.102&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
&lt;/code>&lt;/pre>&lt;h2 id="configuring-port3-on-ovn2">Configuring port3 on ovn2&lt;/h2>
&lt;p>Lastly, let&amp;rsquo;s repeat the above process for &lt;code>port3&lt;/code> on host &lt;code>ovn2&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>ovs-vsctl add-port br-int port3 -- \
set interface port3 \
type=internal \
mac=&amp;#39;[&amp;#34;c0:ff:ee:00:00:13&amp;#34;]&amp;#39; \
external_ids:iface-id=port3
ip netns add vm3
ip link set netns vm3 port3
ip -n vm3 addr add 127.0.0.1/8 dev lo
ip -n vm3 link set lo up
ip netns exec vm3 dhclient -v -i port3 --no-pid
&lt;/code>&lt;/pre>&lt;p>When we&amp;rsquo;re done, &lt;code>ovn-sbctl show&lt;/code> looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn0 ~]# ovn-sbctl show
Chassis ovn1
hostname: ovn1.virt
Encap geneve
ip: &amp;#34;192.168.122.101&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Port_Binding port2
Port_Binding port1
Chassis ovn0
hostname: ovn0.virt
Encap geneve
ip: &amp;#34;192.168.122.100&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Chassis ovn2
hostname: ovn2.virt
Encap geneve
ip: &amp;#34;192.168.122.102&amp;#34;
options: {csum=&amp;#34;true&amp;#34;}
Port_Binding port3
&lt;/code>&lt;/pre>&lt;h2 id="verify-connectivity">Verify connectivity&lt;/h2>
&lt;p>We can verify that the network namespaces we&amp;rsquo;ve created in the above examples are able to communicate with each other regardless of the host on which they have been created. For example, if we log into &lt;code>ovn2&lt;/code> we can show that we are able to reach the address of &lt;code>port1&lt;/code> (&lt;code>10.0.0.11&lt;/code>) from &lt;code>port3&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@ovn2 ~]# ip netns exec vm3 ping -c1 10.0.0.11
PING 10.0.0.11 (10.0.0.11) 56(84) bytes of data.
64 bytes from 10.0.0.11: icmp_seq=1 ttl=64 time=0.266 ms
--- 10.0.0.11 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.266/0.266/0.266/0.000 ms
&lt;/code>&lt;/pre>&lt;h1 id="thats-all-folks">That&amp;rsquo;s all folks!&lt;/h1>
&lt;p>I hope this post helps you understand how to set up a simple OVN environment with DHCP. Please feel free to leave comments and questions!&lt;/p>
&lt;h2 id="thanks-to">Thanks to&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/LorenzoBianconi">Lorenzo Bianconi&lt;/a> for helping sort this out over email.&lt;/li>
&lt;li>&lt;a href="https://twitter.com/zhouhanok">Han Zhou&lt;/a> for helping solve the issue around Geneve tunnels coming up appropriately.&lt;/li>
&lt;/ul>
&lt;h2 id="see-also">See also&lt;/h2>
&lt;p>Below are some of the resources to which I referred while figuring out how to put this all together:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/09/03/ovn-dynamic-ip-address-management/">Dynamic IP address management in Open Virtual Network (OVN): Part One&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/">Dynamic IP address management in Open Virtual Network (OVN): Part Two&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://qyx.me/2018/07/10/run-and-test-ovn/">Run Open Virtual Network (OVN) in Ubuntu&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://dani.foroselectronica.es/simple-ovn-setup-in-5-minutes-491/">Simple OVN setup in 5 minutes&lt;/a>&lt;/li>
&lt;/ul></content></item><item><title>Running Keystone with Docker Compose</title><link>https://blog.oddbit.com/post/2019-06-07-running-keystone-with-docker-c/</link><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2019-06-07-running-keystone-with-docker-c/</guid><description>In this article, we will look at what is necessary to run OpenStack&amp;rsquo;s Keystone service (and the requisite database server) in containers using Docker Compose.
Running MariaDB The standard mariadb docker image can be configured via a number of environment variables. It also benefits from persistent volume storage, since in most situations you don&amp;rsquo;t want to lose your data when you remove a container. A simple docker command line for starting MariaDB might look something like:</description><content>&lt;p>In this article, we will look at what is necessary to run OpenStack&amp;rsquo;s &lt;a href="https://docs.openstack.org/keystone/latest/">Keystone&lt;/a> service (and the requisite database server) in containers using &lt;a href="https://docs.docker.com/compose/">Docker Compose&lt;/a>.&lt;/p>
&lt;h2 id="running-mariadb">Running MariaDB&lt;/h2>
&lt;p>The standard &lt;a href="https://hub.docker.com/_/mariadb/">mariadb docker image&lt;/a> can be configured via a number of environment variables. It also benefits from persistent volume storage, since in most situations you don&amp;rsquo;t want to lose your data when you remove a container. A simple &lt;code>docker&lt;/code> command line for starting MariaDB might look something like:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker run \
-v mariadb_data:/var/lib/mysql \
-e MYSQL_ROOT_PASSWORD=secret.password
mariadb
&lt;/code>&lt;/pre>&lt;p>The above assumes that we have previously created a Docker volume named &lt;code>mariadb_data&lt;/code> by running:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker volume create mariadb_data
&lt;/code>&lt;/pre>&lt;p>An equivalent &lt;code>docker-compose.yml&lt;/code> would look like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb
environment:
MYSQL_ROOT_PASSWORD: secret.password
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;p>Now, rather than typing a long &lt;code>docker run&lt;/code> command line (and possibly forgetting something), you can simply run:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker-compose up
&lt;/code>&lt;/pre>&lt;h3 id="pre-creating-a-database">Pre-creating a database&lt;/h3>
&lt;p>For the purposes of setting up Keystone in Docker, we will need to make a few changes. In particular, we will need to have the &lt;code>mariadb&lt;/code> container create the &lt;code>keystone&lt;/code> database (and user) for us, and as a matter of best practice we will want to specify an explicit tag for the &lt;code>mariadb&lt;/code> image rather than relying on the default &lt;code>latest&lt;/code>.&lt;/p>
&lt;p>We can have the &lt;code>mariadb&lt;/code> image create a database for us at startup by setting the &lt;code>MYSQL_DATABASE&lt;/code>, &lt;code>MYSQL_USER&lt;/code>, and &lt;code>MYSQL_PASSWORD&lt;/code> environment variables:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb:10.4.5-bionic
environment:
MYSQL_ROOT_PASSWORD: secret.password
MYSQL_USER: keystone
MYSQL_PASSWORD: another.password
MYSQL_DATABASE: keystone
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;p>When the &lt;code>database&lt;/code> service starts up, it will create a &lt;code>keystone&lt;/code> database accessible by the &lt;code>keystone&lt;/code> user.&lt;/p>
&lt;h3 id="parameterize-all-the-things">Parameterize all the things&lt;/h3>
&lt;p>The above example is pretty much what we want, but there is one problem: we&amp;rsquo;ve hardcoded our passwords (and database name) into the &lt;code>docker-compose.yml&lt;/code> file, which makes it hard to share: it would be unsuitable for hosting on a public git repository, because anybody who wanted to use it would need to modify the file first, which would make it difficult to contribute changes or bring in new changes from the upstream repository. We can solve that problem by using environment variables in our &lt;code>docker-compose.yml&lt;/code>. Much like the shell, &lt;code>docker-compose&lt;/code> will replace an expression of the form &lt;code>${MY_VARIABLE}&lt;/code> with the value of the &lt;code>MY_VARIABLE&lt;/code> environment variable. It is possible to provide a fallback value in the event that an environment variable is undefined by writing &lt;code>${MY_VARIABLE:-some_default_value}&lt;/code>.&lt;/p>
&lt;p>You have a couple options for providing values for this variables. You can of course simply set them in the environment, either like this:&lt;/p>
&lt;pre>&lt;code>export MY_VARIABLE=some_value
&lt;/code>&lt;/pre>
&lt;p>Or as part of the &lt;code>docker-compose&lt;/code> command line, like this:&lt;/p>
&lt;pre>&lt;code>MY_VARIABLE=some_value docker-compose up
&lt;/code>&lt;/pre>
&lt;p>Alternatively, you can also set them in a &lt;code>.env&lt;/code> file in the same directory as your &lt;code>docker-compose.yml&lt;/code> file; &lt;code>docker-compose&lt;/code> reads this file automatically when it runs. A &lt;code>.env&lt;/code> file looks like this:&lt;/p>
&lt;pre>&lt;code>MY_VARIABLE=some_value
&lt;/code>&lt;/pre>
&lt;p>With the above in mind, we can restructure our example &lt;code>docker-compose.yml&lt;/code> so that it looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>version: 3
services:
database:
image: mariadb:${MARIADB_IMAGE_TAG:-10.4.5-bionic}
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
MYSQL_USER: ${KEYSTONE_DB_USER:-keystone}
MYSQL_PASSWORD: ${KEYSTONE_DB_PASSWORD}
MYSQL_DATABASE: ${KEYSTONE_DB_NAME:-keystone}
volumes:
- &amp;#34;mariadb_data:/var/lib/mysql&amp;#34;
volumes:
mariadb_data:
&lt;/code>&lt;/pre>&lt;h2 id="running-keystone">Running Keystone&lt;/h2>
&lt;h3 id="selecting-a-base-image">Selecting a base image&lt;/h3>
&lt;p>While there is an official MariaDB image available in Docker Hub, there is no such thing as an official Keystone image. A search for &lt;code>keystone&lt;/code> yields over 300 results. I have elected to use the Keystone image produced as part of the &lt;a href="https://wiki.openstack.org/wiki/TripleO">TripleO&lt;/a> project, &lt;a href="https://hub.docker.com/r/tripleomaster/centos-binary-keystone">tripleo-master/centos-binary-keystone&lt;/a>. The &lt;code>current-rdo&lt;/code> tag follows the head of the Keystone repository, and the images are produced automatically as part of the CI process. Unlike the MariaDB image, which is designed to pretty much be &amp;ldquo;plug and play&amp;rdquo;, the Keystone image is going to require some configuration before it provides us with a useful service.&lt;/p>
&lt;p>Using the &lt;code>centos-binary-keystone&lt;/code> image, there are two required configuration tasks we will have to complete when starting the container:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>We will need to inject an appropriate configuration file to run Keystone as a &lt;a href="https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface">WSGI&lt;/a> binary under Apache &lt;a href="http://httpd.apache.org/">httpd&lt;/a>. This is certainly not the only way to run Keystone, but the &lt;code>centos-binary-keystone&lt;/code> image has both &lt;code>httpd&lt;/code> and &lt;code>mod_wsgi&lt;/code> installed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We will need to inject a minimal configuration for Keystone (for example, we will need to provide Keystone with connection information for the database instance).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="keystone-wsgi-configuration">Keystone WSGI configuration&lt;/h3>
&lt;p>We need to configure Keystone as a WSGI service running on port 5000. We will do this with the following configuration file:&lt;/p>
&lt;pre tabindex="0">&lt;code>Listen 5000
ErrorLog &amp;#34;/dev/stderr&amp;#34;
CustomLog &amp;#34;/dev/stderr&amp;#34; combined
&amp;lt;VirtualHost *:5000&amp;gt;
ServerName keystone
ServerSignature Off
DocumentRoot &amp;#34;/var/www/cgi-bin/keystone&amp;#34;
&amp;lt;Directory &amp;#34;/var/www/cgi-bin/keystone&amp;#34;&amp;gt;
Options Indexes FollowSymLinks MultiViews
AllowOverride None
Require all granted
&amp;lt;/Directory&amp;gt;
WSGIApplicationGroup %{GLOBAL}
WSGIDaemonProcess keystone_main display-name=keystone-main \
processes=12 threads=1 user=keystone group=keystone
WSGIProcessGroup keystone_main
WSGIScriptAlias / &amp;#34;/var/www/cgi-bin/keystone/main&amp;#34;
WSGIPassAuthorization On
&amp;lt;/VirtualHost&amp;gt;
&lt;/code>&lt;/pre>&lt;p>The easiest way to inject this custom configuration is to bake it into a custom image. Using the &lt;code>tripleomaster/centos-binary-keystone&lt;/code> base image identified earlier, we can start with a custom &lt;code>Dockerfile&lt;/code> that looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>ARG KEYSTONE_IMAGE_TAG=current-tripleo
FROM tripleomaster/centos-binary-keystone:${KEYSTONE_IMAGE_TAG}
COPY keystone-wsgi-main.conf /etc/httpd/conf.d/keystone-wsgi-main.conf
&lt;/code>&lt;/pre>&lt;p>The &lt;code>ARG&lt;/code> directive permits us to select an image tag via a build argument (but defaults to &lt;code>current-tripleo&lt;/code>).&lt;/p>
&lt;p>We can ask &lt;code>docker-compose&lt;/code> to build our custom image for us when we run &lt;code>docker-compose up&lt;/code>. Instead of specifying an &lt;code>image&lt;/code> as we did with the MariaDB container, we use the &lt;code>build&lt;/code> directive:&lt;/p>
&lt;pre tabindex="0">&lt;code>[...]
keystone:
build:
context: .
args:
KEYSTONE_IMAGE_TAG: current-tripleo
[...]
&lt;/code>&lt;/pre>&lt;p>This tells &lt;code>docker-compose&lt;/code> to use the &lt;code>Dockerfile&lt;/code> in the current directory (and to set the &lt;code>KEYSTONE_IMAGE_TAG&lt;/code> build argument to &lt;code>current-tripleo&lt;/code>). Note that &lt;code>docker-compose&lt;/code> will only build this image for us by default if it doesn&amp;rsquo;t already exist; we can ask &lt;code>docker-compose&lt;/code> to build it explicitly by running &lt;code>docker-compose build&lt;/code>, or by providing the &lt;code>--build&lt;/code> option to &lt;code>docker-compose up&lt;/code>.&lt;/p>
&lt;h3 id="configuring-at-build-time-vs-run-time">Configuring at build time vs run time&lt;/h3>
&lt;p>In the previous section, we used a &lt;code>Dockerfile&lt;/code> to build on a selected base image by adding custom content. Other sorts of configuration must happen when the container starts up (for example, we probably want to be able to set passwords at runtime). One way of solving this problem is to embed some scripts into our custom image and then run them when the container starts in order to perform any necessary initialization.&lt;/p>
&lt;p>I have placed some custom scripts and templates into the &lt;code>runtime&lt;/code> directory and arranged to copy that directory into the custom image like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>ARG KEYSTONE_IMAGE_TAG=current-tripleo
FROM tripleomaster/centos-binary-keystone:${KEYSTONE_IMAGE_TAG}
COPY keystone-wsgi-main.conf /etc/httpd/conf.d/keystone-wsgi-main.conf
COPY runtime /runtime
CMD [&amp;#34;/bin/sh&amp;#34;, &amp;#34;/runtime/startup.sh&amp;#34;]
&lt;/code>&lt;/pre>&lt;p>The &lt;code>runtime&lt;/code> directory contains the following files:&lt;/p>
&lt;ul>
&lt;li>&lt;code>runtime/dtu.py&lt;/code> &amp;ndash; a short Python script for generating files from templates.&lt;/li>
&lt;li>&lt;code>runtime/startup.sh&lt;/code> &amp;ndash; a shell script that performs all the necessary initialization tasks before starting Keystone&lt;/li>
&lt;li>&lt;code>runtime/keystone.j2.conf&lt;/code> &amp;ndash; template for the Keystone configuration file&lt;/li>
&lt;li>&lt;code>runtime/clouds.j2.yaml&lt;/code> &amp;ndash; template for a &lt;code>clouds.yaml&lt;/code> for use by the &lt;code>openshift&lt;/code> command line client.&lt;/li>
&lt;/ul>
&lt;h3 id="starting-up">Starting up&lt;/h3>
&lt;p>The &lt;code>startup.sh&lt;/code> script performs the following actions:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Generates &lt;code>/etc/keystone/keystone.conf&lt;/code> from &lt;code>/runtime/keystone.j2.conf&lt;/code>.&lt;/p>
&lt;p>The file &lt;code>/runtime/keystone.j2.conf&lt;/code> is a minimal Keystone configuration template. It ensures that Keystone logs to &lt;code>stderr&lt;/code> (by setting &lt;code>log_file&lt;/code> to an empty value) and configures the database connection using values from the environment.&lt;/p>
&lt;pre tabindex="0">&lt;code>[DEFAULT]
debug = {{ environ.KEYSTONE_DEBUG|default(&amp;#39;false&amp;#39;) }}
log_file =
[database]
{% set keystone_db_user = environ.KEYSTONE_DB_USER|default(&amp;#39;keystone&amp;#39;) %}
{% set keystone_db_host = environ.KEYSTONE_DB_HOST|default(&amp;#39;localhost&amp;#39;) %}
{% set keystone_db_port = environ.KEYSTONE_DB_PORT|default(&amp;#39;3306&amp;#39;) %}
{% set keystone_db_name = environ.KEYSTONE_DB_NAME|default(&amp;#39;keystone&amp;#39;) %}
{% set keystone_db_pass = environ.KEYSTONE_DB_PASSWORD|default(&amp;#39;insert-password-here&amp;#39;) %}
connection = mysql+pymysql://{{ keystone_db_user }}:{{ keystone_db_pass }}@{{ keystone_db_host }}:{{ keystone_db_port }}/{{ keystone_db_name }}
[token]
provider = fernet
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Generates &lt;code>/root/clouds.yaml&lt;/code> from &lt;code>/runtime/clouds.j2.yaml&lt;/code>.&lt;/p>
&lt;p>The &lt;code>clouds.yaml&lt;/code> file can be used with to provide authentication information to the &lt;code>openshift&lt;/code> command line client (and other applications that use the OpenStack Python SDK). We&amp;rsquo;ll see an example of this further on in this article.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Initializes Keystone&amp;rsquo;s fernet token mechanism by running &lt;code>keystone-manage fernet_setup&lt;/code>.&lt;/p>
&lt;p>Keystone supports various token generation mechanisms. Fernet tokens provide some advantages over the older UUID token mechanism. From the &lt;a href="fernet-faq">FAQ&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Even though fernet tokens operate very similarly to UUID tokens, they do not require persistence or leverage the configured token persistence driver in any way. The keystone token database no longer suffers bloat as a side effect of authentication. Pruning expired tokens from the token database is no longer required when using fernet tokens. Because fernet tokens do not require persistence, they do not have to be replicated. As long as each keystone node shares the same key repository, fernet tokens can be created and validated instantly across nodes.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>Initializes the Keystone database schema by running &lt;code>keystone-manage db_sync&lt;/code>.&lt;/p>
&lt;p>The &lt;code>db_sync&lt;/code> command creates the database tables that Keystone requires to operate.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Creates the Keystone &lt;code>admin&lt;/code> user and initial service catalog entries by running &lt;code>keystone-manage bootstrap&lt;/code>&lt;/p>
&lt;p>Before we can authenticate to Keystone, there needs to exist a user with administrative privileges (so that we can create other users, projects, and so forth).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Starts &lt;code>httpd&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="avoiding-race-conditions">Avoiding race conditions&lt;/h3>
&lt;p>When we run &lt;code>docker-compose up&lt;/code>, it will bring up both the &lt;code>keystone&lt;/code> container and the &lt;code>database&lt;/code> container in parallel. This is going to cause problems if we try to initialize the Keystone database schema before the database server is actually up and running. There is a &lt;code>depends_on&lt;/code> keyword that can be used to order the startup of containers in your &lt;code>docker-compose.yml&lt;/code> file, but this isn&amp;rsquo;t useful to us: this only delays the startup of the dependent container until the indicated container is &lt;em>running&lt;/em>. It doesn&amp;rsquo;t know anything about application startup, and so it would not wait for the database to be ready.&lt;/p>
&lt;p>We need to explicitly wait until we can successfully connect to the database before we can complete initializing the Keystone service. It turns out the easiest solution to this problem is to imply run the database schema initialization in a loop until it is successful, like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>echo &amp;#34;* initializing database schema&amp;#34;
while ! keystone-manage db_sync; do
echo &amp;#34;! database schema initialization failed; retrying in 5 seconds...&amp;#34;
sleep 5
done
&lt;/code>&lt;/pre>&lt;p>This will attempt the &lt;code>db_sync&lt;/code> command every five seconds until it is sucessful.&lt;/p>
&lt;h2 id="the-final-docker-compose-file">The final docker-compose file&lt;/h2>
&lt;p>Taking all of the above into account, this is what the final &lt;code>docker-compose.yml&lt;/code> file looks like:&lt;/p>
&lt;pre tabindex="0">&lt;code>---
version: &amp;#34;3&amp;#34;
services:
database:
image: mariadb:10.4.5-bionic
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
MYSQL_USER: ${KEYSTONE_DB_USER:-keystone}
MYSQL_PASSWORD: ${KEYSTONE_DB_PASSWORD}
MYSQL_DATABASE: ${KEYSTONE_DB_NAME:-keystone}
volumes:
- mysql:/var/lib/mysql
keystone:
build:
context: .
args:
KEYSTONE_IMAGE_TAG: current-tripleo
environment:
MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
KEYSTONE_ADMIN_PASSWORD: ${KEYSTONE_ADMIN_PASSWORD}
KEYSTONE_DB_PASSWORD: ${KEYSTONE_DB_PASSWORD}
KEYSTONE_DB_USER: ${KEYSTONE_DB_USER:-keystone}
KEYSTONE_DB_NAME: ${KEYSTONE_DB_NAME:-keystone}
KEYSTONE_DEBUG: ${KEYSTONE_DEBUG:-&amp;#34;false&amp;#34;}
ports:
- &amp;#34;127.0.0.1:5000:5000&amp;#34;
volumes:
mysql:
&lt;/code>&lt;/pre>&lt;h2 id="interacting-with-keystone">Interacting with Keystone&lt;/h2>
&lt;p>Once Keystone is up and running, we can grab the generated &lt;code>clouds.yaml&lt;/code> file like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>docker-compose exec keystone cat /root/clouds.yaml &amp;gt; clouds.yaml
&lt;/code>&lt;/pre>&lt;p>Now we can run the &lt;code>openstack&lt;/code> command line client:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ export OS_CLOUD=openstack-public
$ openstack catalog list
+----------+----------+-----------------------------------+
| Name | Type | Endpoints |
+----------+----------+-----------------------------------+
| keystone | identity | RegionOne |
| | | internal: http://localhost:5000 |
| | | RegionOne |
| | | public: http://localhost:5000 |
| | | |
+----------+----------+-----------------------------------+
$ openstack user list
+----------------------------------+-------+
| ID | Name |
+----------------------------------+-------+
| e8f460619a854c849feaf278b8d68e2c | admin |
+----------------------------------+-------+
&lt;/code>&lt;/pre>&lt;h2 id="project-sources">Project sources&lt;/h2>
&lt;p>You can find everything reference in this article in the &lt;a href="https://github.com/cci-moc/flocx-keystone-dev">flocx-keystone-dev&lt;/a> repository.&lt;/p></content></item><item><title>Grouping aggregation queries in Gnocchi 4.0.x</title><link>https://blog.oddbit.com/post/2018-02-26-grouping-aggregation-queries-i/</link><pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-02-26-grouping-aggregation-queries-i/</guid><description>In this article, we&amp;rsquo;re going to ask Gnocchi (the OpenStack telemetry storage service) how much memory was used, on average, over the course of each day by each project in an OpenStack environment.
Environment I&amp;rsquo;m working with an OpenStack &amp;ldquo;Pike&amp;rdquo; deployment, which means I have Gnocchi 4.0.x. More recent versions of Gnocchi (4.1.x and later) have a new aggregation API called dynamic aggregates, but that isn&amp;rsquo;t available in 4.0.x so in this article we&amp;rsquo;ll be using the legacy /v1/aggregations API.</description><content>&lt;p>In this article, we&amp;rsquo;re going to ask Gnocchi (the OpenStack telemetry
storage service) how much memory was used, on average, over the course
of each day by each project in an OpenStack environment.&lt;/p>
&lt;h2 id="environment">Environment&lt;/h2>
&lt;p>I&amp;rsquo;m working with an OpenStack &amp;ldquo;Pike&amp;rdquo; deployment, which means I have
Gnocchi 4.0.x. More recent versions of Gnocchi (4.1.x and later) have
a new aggregation API called &lt;a href="https://gnocchi.xyz/rest.html#dynamic-aggregates">dynamic aggregates&lt;/a>, but that isn&amp;rsquo;t
available in 4.0.x so in this article we&amp;rsquo;ll be using the legacy
&lt;code>/v1/aggregations&lt;/code> API.&lt;/p>
&lt;h2 id="the-gnocchi-data-model">The Gnocchi data model&lt;/h2>
&lt;p>In Gnocchi, named metrics are associated with &lt;em>resources&lt;/em>. A
&lt;em>resource&lt;/em> corresponds to an object in OpenStack, such as a Nova
instance, or a Neutron network, etc. Metrics on a resource are
created dynamically and depend entirely on which metrics you are
collecting with Ceilometer and delivering to Gnocchi, and two
different resources of the same resource type may have different sets
of metrics.&lt;/p>
&lt;p>The list of metrics available from OpenStack is available in the
&lt;a href="https://docs.openstack.org/ceilometer/latest/admin/telemetry-measurements.html">telemetry documentation&lt;/a>. The &lt;a href="https://docs.openstack.org/ceilometer/latest/admin/telemetry-measurements.html#openstack-compute">metrics available for Nova
servers&lt;/a> include statistics on cpu utilization,
memory utilization, disk read/write operations, network traffic, and
more.&lt;/p>
&lt;p>In this example, we&amp;rsquo;re going to look at the &lt;code>memory.usage&lt;/code> metric.&lt;/p>
&lt;h2 id="building-a-query">Building a query&lt;/h2>
&lt;p>The amount of memory used is available in the &lt;code>memory.usage&lt;/code> metric
associated with each &lt;code>instance&lt;/code> resource. We&amp;rsquo;re using the legacy
&lt;code>/v1/aggregations&lt;/code> API, so the request url will initially look like:&lt;/p>
&lt;pre>&lt;code>/v1/aggregation/resource/instance/metric/memory.usage
&lt;/code>&lt;/pre>
&lt;p>We need to specify which granularity we want to fetch. We&amp;rsquo;re using
the &lt;code>low&lt;/code> archive policy from the default Gnocchi configuration which
means we only have one option (metrics are aggregated into 5 minute
intervals). We use the &lt;code>granularity&lt;/code> parameter to tell Gnocchi which
granularity we want:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300
&lt;/code>&lt;/pre>
&lt;p>We want the average utilization, so that means:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;d like to limit this to the past 30 days of data, so we&amp;rsquo;ll add an
appropriate &lt;code>start&lt;/code> parameter. There are various ways of specifying
time in Gnocchi, documented in the &lt;a href="https://gnocchi.xyz/rest.html#timestamp-format">Timestamps&lt;/a> section of the docs.
Specifying &amp;ldquo;30 days ago&amp;rdquo; is as simple as: &lt;code>start=-30+days&lt;/code>. So our
request now looks like:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days
&lt;/code>&lt;/pre>
&lt;p>We have values at 5 minute intervals, but we would like daily
averages, so we&amp;rsquo;ll need to use the &lt;a href="https://gnocchi.xyz/rest.html#resample">resample&lt;/a> parameter to ask
Gnocchi to reshape the data for us. The argument to &lt;code>resample&lt;/code> is a
time period specified as a number of seconds; since we want daily
averages, that means &lt;code>resample=86400&lt;/code>:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days&amp;amp;resample=86400
&lt;/code>&lt;/pre>
&lt;p>We want to group the results by project, which means we&amp;rsquo;ll need the
&lt;code>groupby&lt;/code> parameter. Instances (and most other resources) store this
information in the &lt;code>project_id&lt;/code> attribute, so &lt;code>groupby=project_id&lt;/code>:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days&amp;amp;resample=86400&amp;amp;groupby=project_id
&lt;/code>&lt;/pre>
&lt;p>Lastly, not all metrics will have measurements covering the same
period. If we were to try the query as we have it right now, we would
probably see:&lt;/p>
&lt;pre>&lt;code>400 Bad Request
The server could not comply with the request since it is either
malformed or otherwise incorrect.
One of the metrics being aggregated doesn't have matching
granularity: Metrics &amp;lt;list of metrics here&amp;gt;...can't be aggregated:
No overlap
&lt;/code>&lt;/pre>
&lt;p>Gnocchi provides a &lt;code>fill&lt;/code> parameter to indicate what should happen
with missing data. In Gnocchi 4.0.x, the options are &lt;code>fill=0&lt;/code>
(replace missing data with &lt;code>0&lt;/code>) and &lt;code>fill=null&lt;/code> (compute the
aggregation using only available data points). Selecting &lt;code>fill=0&lt;/code> gets
us:&lt;/p>
&lt;pre>&lt;code>.../metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days&amp;amp;resample=86400&amp;amp;groupby=project_id&amp;amp;fill=0
&lt;/code>&lt;/pre>
&lt;p>That should give us the results we want. The return value from that
query looks something like this:&lt;/p>
&lt;pre>&lt;code>[
{
&amp;quot;group&amp;quot;: {
&amp;quot;project_id&amp;quot;: &amp;quot;00a8d5e942bb442b86733f0f92280fcc&amp;quot;
},
&amp;quot;measures&amp;quot;: [
[
&amp;quot;2018-02-14T00:00:00+00:00&amp;quot;,
86400.0,
2410.014338235294
],
[
&amp;quot;2018-02-15T00:00:00+00:00&amp;quot;,
86400.0,
2449.4921970791206
],
.
.
.
{
&amp;quot;group&amp;quot;: {
&amp;quot;project_id&amp;quot;: &amp;quot;03d2a1de5b2342d78d14e5294a81e0b0&amp;quot;
},
&amp;quot;measures&amp;quot;: [
[
&amp;quot;2018-02-14T00:00:00+00:00&amp;quot;,
86400.0,
381.0
],
[
&amp;quot;2018-02-15T00:00:00+00:00&amp;quot;,
86400.0,
380.99004975124376
],
[
&amp;quot;2018-02-16T00:00:00+00:00&amp;quot;,
86400.0,
380.99305555555554
],
.
.
.
&lt;/code>&lt;/pre>
&lt;h2 id="authenticating-to-gnocchi">Authenticating to Gnocchi&lt;/h2>
&lt;p>Since we&amp;rsquo;re using Gnocchi as part of an OpenStack deployment, we&amp;rsquo;ll
be authenticating to Gnocchi using a Keystone token. Let&amp;rsquo;s take a
look at how you could do that from the command line using &lt;code>curl&lt;/code>.&lt;/p>
&lt;h3 id="acquiring-a-token">Acquiring a token&lt;/h3>
&lt;p>You can acquire a token using the &lt;code>openstack token issue&lt;/code> command,
which will produce output like:&lt;/p>
&lt;pre>&lt;code>+------------+------------------------------------------------------------------------+
| Field | Value |
+------------+------------------------------------------------------------------------+
| expires | 2018-02-26T23:09:36+0000 |
| id | ... |
| project_id | c53c18b2d29641e0877bbbd8d87f8267 |
| user_id | 533ad9ab4fed403fb98f1ffc2f2b4436 |
+------------+------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>While it is possible to parse that with, say, &lt;code>awk&lt;/code>, it&amp;rsquo;s not
really designed to be machine readable. We can get just the token
value by instead calling:&lt;/p>
&lt;pre>&lt;code>openstack token issue -c id -f value
&lt;/code>&lt;/pre>
&lt;p>We should probably store that in a variable:&lt;/p>
&lt;pre>&lt;code>TOKEN=$(openstack token issue -c id -f value)
&lt;/code>&lt;/pre>
&lt;h3 id="querying-gnocchi">Querying Gnocchi&lt;/h3>
&lt;p>In order to make our command line shorter, let&amp;rsquo;s set &lt;code>ENDPOINT&lt;/code> to the
URL of our Gnocchi endpoint:&lt;/p>
&lt;pre>&lt;code>ENDPOINT=http://cloud.example.com:8041/v1
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ll pass the Keystone token in the &lt;code>X-Auth-Token&lt;/code> header. Gnocchi
is expecting a JSON document body as part of this request (you can use
that to specify a filter; see the &lt;a href="https://gnocchi.xyz/rest.html">API documentation&lt;/a> for details),
so we need to both set a &lt;code>Content-type&lt;/code> header and provide at least an
empty JSON object. That makes our final request look like:&lt;/p>
&lt;pre>&lt;code>curl \
-H &amp;quot;X-Auth-Token: $TOKEN&amp;quot; \
-H &amp;quot;Content-type: application/json&amp;quot; \
-d &amp;quot;{}&amp;quot; \
&amp;quot;$ENDPOINT/aggregation/resource/instance/metric/memory.usage?granularity=300&amp;amp;aggregation=mean&amp;amp;start=-30+days&amp;amp;resample=86400&amp;amp;groupby=project_id&amp;amp;fill=0&amp;quot;
&lt;/code>&lt;/pre></content></item><item><title>Safely restarting an OpenStack server with Ansible</title><link>https://blog.oddbit.com/post/2018-01-24-safely-restarting-an-openstack/</link><pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-01-24-safely-restarting-an-openstack/</guid><description>The other day on #ansible, someone was looking for a way to safely shut down a Nova server, wait for it to stop, and then start it up again using the openstack cli. The first part seemed easy:
- hosts: myserver tasks: - name: shut down the server command: poweroff become: true &amp;hellip;but that will actually fail with the following result:
TASK [shut down server] ************************************* fatal: [myserver]: UNREACHABLE! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;msg&amp;quot;: &amp;quot;Failed to connect to the host via ssh: Shared connection to 10.</description><content>&lt;p>The other day on &lt;a href="http://docs.ansible.com/ansible/latest/community.html#irc-channel">#ansible&lt;/a>, someone was looking for a way to safely
shut down a Nova server, wait for it to stop, and then start it up
again using the &lt;code>openstack&lt;/code> cli. The first part seemed easy:&lt;/p>
&lt;pre>&lt;code>- hosts: myserver
tasks:
- name: shut down the server
command: poweroff
become: true
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;but that will actually fail with the following result:&lt;/p>
&lt;pre>&lt;code>TASK [shut down server] *************************************
fatal: [myserver]: UNREACHABLE! =&amp;gt; {&amp;quot;changed&amp;quot;: false, &amp;quot;msg&amp;quot;:
&amp;quot;Failed to connect to the host via ssh: Shared connection to
10.0.0.103 closed.\r\n&amp;quot;, &amp;quot;unreachable&amp;quot;: true}
&lt;/code>&lt;/pre>
&lt;p>This happens because running &lt;code>poweroff&lt;/code> immediately closes Ansible&amp;rsquo;s
ssh connection. The workaround here is to use a &amp;ldquo;fire-and-forget&amp;rdquo;
&lt;a href="http://docs.ansible.com/ansible/latest/playbooks_async.html">asynchronous task&lt;/a>:&lt;/p>
&lt;pre>&lt;code>- hosts: myserver
tasks:
- name: shut down the server
command: poweroff
become: true
async: 30
poll: 0
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>poll: 0&lt;/code> means that Ansible won&amp;rsquo;t wait around to check the result
of this task. Control will return to ansible immediately, so our
playbook can continue without errors resulting from the closed
connection.&lt;/p>
&lt;p>Now that we&amp;rsquo;ve started the shutdown process on the host, how do we
wait for it to complete? You&amp;rsquo;ll see people solve this with a
&lt;a href="http://docs.ansible.com/ansible/latest/pause_module.html">pause&lt;/a> task, but that&amp;rsquo;s fragile because the timing there can be
tricky to get right.&lt;/p>
&lt;p>Another option is to use something like Ansible&amp;rsquo;s &lt;a href="http://docs.ansible.com/ansible/latest/wait_for_module.html">wait_for&lt;/a> module.
For example, we could wait for &lt;code>sshd&lt;/code> to become unresponsive:&lt;/p>
&lt;pre>&lt;code>- name: wait for server to finishing shutting down
wait_for:
port: 22
state: stopped
&lt;/code>&lt;/pre>
&lt;p>Be this is really just checking whether or not &lt;code>sshd&lt;/code> is listening for
a connection, and &lt;code>sshd&lt;/code> may shut down long before the system shutdown
process completes.&lt;/p>
&lt;p>The best option is to ask Nova. We can query Nova for information
about a server using the Ansible&amp;rsquo;s &lt;a href="http://docs.ansible.com/ansible/latest/os_server_facts_module.html">os_server_facts&lt;/a> module. Like
the other OpenStack modules, this uses &lt;a href="https://docs.openstack.org/os-client-config/latest/">os-client-config&lt;/a> to find
authentication credentials for your OpenStack environment. If you&amp;rsquo;re
not explicitly providing authentication information in your playbook,
the module will use the &lt;code>OS_*&lt;/code> environment variables that are commonly
used with the OpenStack command line tools.&lt;/p>
&lt;p>The &lt;code>os_server_facts&lt;/code> module creates an &lt;code>openstack_servers&lt;/code> fact, the
value of which is a list of dictionaries which contains keys like
&lt;code>status&lt;/code>, which is the one in which we&amp;rsquo;re interested. A running
server has &lt;code>status == &amp;quot;ACTIVE&amp;quot;&lt;/code> and a server that has been powered off
has &lt;code>status == &amp;quot;SHUTOFF&lt;/code>.&lt;/p>
&lt;p>Given the above, the &amp;ldquo;wait for shutdown&amp;rdquo; task would look something
like the following:&lt;/p>
&lt;pre>&lt;code>- hosts: localhost
tasks:
- name: wait for server to stop
os_server_facts:
server: myserver
register: results
until: openstack_servers.0.status == &amp;quot;SHUTOFF&amp;quot;
retries: 120
delay: 5
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll note that I&amp;rsquo;m targeting &lt;code>localhost&lt;/code> right now, because my local
system has access to my OpenStack environment and I have the necessary
credentials in my environment. If you need to run these tasks
elsewhere, you&amp;rsquo;ll want to read up on how to provide credentials in
your playbook.&lt;/p>
&lt;p>This task will retry up to &lt;code>120&lt;/code> times, waiting &lt;code>5&lt;/code> seconds between
tries, until the server reaches the desired state.&lt;/p>
&lt;p>Next, we want to start the server back up. We can do this using
the &lt;a href="http://docs.ansible.com/ansible/latest/os_server_action_module.html">os_server_action&lt;/a> module, using the &lt;code>start&lt;/code> action. This task
also runs on &lt;code>localhost&lt;/code>:&lt;/p>
&lt;pre>&lt;code> - name: start the server
os_server_action:
server: larstest
action: start
&lt;/code>&lt;/pre>
&lt;p>Finally, we want to wait for the host to come back up before we run
any additional tasks on it. In this case, we can&amp;rsquo;t just look at the
status reported by Nova: the host will be &lt;code>ACTIVE&lt;/code> from Nova&amp;rsquo;s
perspective long before it is ready to accept &lt;code>ssh&lt;/code> connections. Nor
can we use the &lt;code>wait_for&lt;/code> module, since the &lt;code>ssh&lt;/code> port may be open
before we are actually able to log in. The solution to this is the
&lt;a href="http://docs.ansible.com/ansible/latest/wait_for_connection_module.html">wait_for_connection&lt;/a> module, which waits until Ansible is able to
successful execute an action on the target host:&lt;/p>
&lt;pre>&lt;code>- hosts: larstest
gather_facts: false
tasks:
- name: wait for server to start
wait_for_connection:
&lt;/code>&lt;/pre>
&lt;p>We need to set &lt;code>gather_facts: false&lt;/code> here because fact gathering
requires a functioning connection to the target host.&lt;/p>
&lt;p>And that&amp;rsquo;s it: a recipe for gently shutting down a remote host,
waiting for the shutdown to complete, then later on spinning it back
up and waiting until subsequent Ansible tasks will work correctly.&lt;/p></content></item><item><title>Ansible for Infrastructure Testing</title><link>https://blog.oddbit.com/post/2017-08-02-ansible-for-infrastructure-tes/</link><pubDate>Wed, 02 Aug 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-08-02-ansible-for-infrastructure-tes/</guid><description>At $JOB we often find ourselves at customer sites where we see the same set of basic problems that we have previously encountered elsewhere (&amp;ldquo;your clocks aren&amp;rsquo;t in sync&amp;rdquo; or &amp;ldquo;your filesystem is full&amp;rdquo; or &amp;ldquo;you haven&amp;rsquo;t installed a critical update&amp;rdquo;, etc). We would like a simple tool that could be run either by the customer or by our own engineers to test for and report on these common issues. Fundamentally, we want something that acts like a typical code test suite, but for infrastructure.</description><content>&lt;p>At &lt;code>$JOB&lt;/code> we often find ourselves at customer sites where we see the
same set of basic problems that we have previously encountered
elsewhere (&amp;ldquo;your clocks aren&amp;rsquo;t in sync&amp;rdquo; or &amp;ldquo;your filesystem is full&amp;rdquo;
or &amp;ldquo;you haven&amp;rsquo;t installed a critical update&amp;rdquo;, etc). We would like a
simple tool that could be run either by the customer or by our own
engineers to test for and report on these common issues.
Fundamentally, we want something that acts like a typical code test
suite, but for infrastructure.&lt;/p>
&lt;p>It turns out that Ansible is &lt;em>almost&lt;/em> the right tool for the job:&lt;/p>
&lt;ul>
&lt;li>It&amp;rsquo;s easy to write simple tests.&lt;/li>
&lt;li>It works well in distributed environments.&lt;/li>
&lt;li>It&amp;rsquo;s easy to extend with custom modules and plugins.&lt;/li>
&lt;/ul>
&lt;p>The only real problem is that Ansible has, by default, &amp;ldquo;fail fast&amp;rdquo;
behavior: once a task fails on a host, no more tasks will run on that
host. That&amp;rsquo;s great if you&amp;rsquo;re actually making configuration changes,
but for our purposes we are running a set of read-only independent
checks, and we want to know the success or failure of all of those
checks in a single operation (and in many situations we may not have
the option of correcting the underlying problem ourselves).&lt;/p>
&lt;p>In this post, I would like to discuss a few Ansible extensions I&amp;rsquo;ve
put together to make it more useful as an infrastructure testing tool.&lt;/p>
&lt;h2 id="the-ansible-assertive-project">The ansible-assertive project&lt;/h2>
&lt;p>The &lt;a href="https://github.com/larsks/ansible-assertive/">ansible-assertive&lt;/a> project contains two extensions for Ansible:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The &lt;code>assert&lt;/code> action plugin replaces Ansible&amp;rsquo;s native &lt;code>assert&lt;/code>
behavior with something more appropriate for infrastructure testing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The &lt;code>assertive&lt;/code> callback plugin modifies the output of &lt;code>assert&lt;/code>
tasks and collects and reports results.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The idea is that you write all of your tests using the &lt;code>assert&lt;/code>
plugin, which means you can run your playbooks in a stock environment
and see the standard Ansible fail-fast behavior, or you can activate
the &lt;code>assert&lt;/code> plugin from the ansible-assertive project and get
behavior more useful for infrastructure testing.&lt;/p>
&lt;h2 id="a-simple-example">A simple example&lt;/h2>
&lt;p>Ansible&amp;rsquo;s native &lt;code>assert&lt;/code> plugin will trigger a task failure when an
assertion evaluates to &lt;code>false&lt;/code>. Consider the following example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">hosts&lt;/span>: &lt;span style="color:#ae81ff">localhost&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">vars&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">fruits&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">oranges&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#ae81ff">lemons&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tasks&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">assert&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">that&lt;/span>: &amp;gt;-&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &lt;/span> &lt;span style="color:#e6db74">&amp;#39;apples&amp;#39;&lt;/span> &lt;span style="color:#ae81ff">in fruits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#ae81ff">you have no apples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">assert&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">that&lt;/span>: &amp;gt;-&lt;span style="color:#e6db74">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#e6db74"> &lt;/span> &lt;span style="color:#e6db74">&amp;#39;lemons&amp;#39;&lt;/span> &lt;span style="color:#ae81ff">in fruits&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#ae81ff">you have no lemons&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If we run this in a stock Ansible environment, we will see the
following:&lt;/p>
&lt;pre tabindex="0">&lt;code>PLAY [localhost] ***************************************************************
TASK [Gathering Facts] *********************************************************
ok: [localhost]
TASK [assert] ******************************************************************
fatal: [localhost]: FAILED! =&amp;gt; {
&amp;#34;assertion&amp;#34;: &amp;#34;&amp;#39;apples&amp;#39; in fruits&amp;#34;,
&amp;#34;changed&amp;#34;: false,
&amp;#34;evaluated_to&amp;#34;: false,
&amp;#34;failed&amp;#34;: true,
&amp;#34;msg&amp;#34;: &amp;#34;you have no apples&amp;#34;
}
to retry, use: --limit @/home/lars/projects/ansible-assertive/examples/ex-005/playbook1.retry
PLAY RECAP *********************************************************************
localhost : ok=1 changed=0 unreachable=0 failed=1
&lt;/code>&lt;/pre>&lt;h2 id="a-modified-assert-plugin">A modified assert plugin&lt;/h2>
&lt;p>Let&amp;rsquo;s activate the &lt;code>assert&lt;/code> plugin in &lt;a href="https://github.com/larsks/ansible-assertive/">ansible-assertive&lt;/a>. We&amp;rsquo;ll
start by cloning the project into our local directory:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/larsks/ansible-assertive
&lt;/code>&lt;/pre>
&lt;p>And we&amp;rsquo;ll activate the plugin by creating an &lt;code>ansible.cfg&lt;/code> file with
the following content:&lt;/p>
&lt;pre>&lt;code>[defaults]
action_plugins = ./ansible-assertive/action_plugins
&lt;/code>&lt;/pre>
&lt;p>Now when we re-run the playbook we see that a failed assertion now
registers as &lt;code>changed&lt;/code> rather than &lt;code>failed&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>PLAY [localhost] ***************************************************************
TASK [Gathering Facts] *********************************************************
ok: [localhost]
TASK [assert] ******************************************************************
changed: [localhost]
TASK [assert] ******************************************************************
ok: [localhost]
PLAY RECAP *********************************************************************
localhost : ok=3 changed=1 unreachable=0 failed=0
&lt;/code>&lt;/pre>&lt;p>While that doesn&amp;rsquo;t look like much of a change, there are two things of
interest going on here. The first is that the &lt;code>assert&lt;/code> plugin
provides detailed information about the assertions specified in the
task; if we were to &lt;code>register&lt;/code> the result of the failed assertion and
display it in a &lt;code>debug&lt;/code> task, it would look like:&lt;/p>
&lt;pre tabindex="0">&lt;code>TASK [debug] *******************************************************************
ok: [localhost] =&amp;gt; {
&amp;#34;apples&amp;#34;: {
&amp;#34;ansible_stats&amp;#34;: {
&amp;#34;aggregate&amp;#34;: true,
&amp;#34;data&amp;#34;: {
&amp;#34;assertions&amp;#34;: 1,
&amp;#34;assertions_failed&amp;#34;: 1,
&amp;#34;assertions_passed&amp;#34;: 0
},
&amp;#34;per_host&amp;#34;: true
},
&amp;#34;assertions&amp;#34;: [
{
&amp;#34;assertion&amp;#34;: &amp;#34;&amp;#39;apples&amp;#39; in fruits&amp;#34;,
&amp;#34;evaluated_to&amp;#34;: false
}
],
&amp;#34;changed&amp;#34;: true,
&amp;#34;failed&amp;#34;: false,
&amp;#34;msg&amp;#34;: &amp;#34;you have no apples&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;p>The &lt;code>assertions&lt;/code> key in the result dictionary contains of a list of
tests and their results. The &lt;code>ansible_stats&lt;/code> key contains metadata
that will be consumed by the custom statistics support in recent
versions of Ansible. If you have Ansible 2.3.0.0 or later, add
the following to the &lt;code>defaults&lt;/code> section of your &lt;code>ansible.cfg&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>show_custom_stats = yes
&lt;/code>&lt;/pre>&lt;p>With this feature enabled, your playbook run will conclude with:&lt;/p>
&lt;pre tabindex="0">&lt;code>CUSTOM STATS: ******************************************************************
localhost: { &amp;#34;assertions&amp;#34;: 2, &amp;#34;assertions_failed&amp;#34;: 1, &amp;#34;assertions_passed&amp;#34;: 1}
&lt;/code>&lt;/pre>&lt;h2 id="a-callback-plugin-for-better-output">A callback plugin for better output&lt;/h2>
&lt;p>The &lt;code>assertive&lt;/code> callback plugin provided by the &lt;a href="https://github.com/larsks/ansible-assertive/">ansible-assertive&lt;/a>
project will provide more useful output concerning the result of
failed assertions. We activate it by adding the following to our
&lt;code>ansible.cfg&lt;/code>:&lt;/p>
&lt;pre>&lt;code>callback_plugins = ./ansible-assertive/callback_plugins
stdout_callback = assertive
&lt;/code>&lt;/pre>
&lt;p>Now when we run our playbook we see:&lt;/p>
&lt;pre tabindex="0">&lt;code>PLAY [localhost] ***************************************************************
TASK [Gathering Facts] *********************************************************
ok: [localhost]
TASK [assert] ******************************************************************
failed: [localhost] ASSERT(&amp;#39;apples&amp;#39; in fruits)
failed: you have no apples
TASK [assert] ******************************************************************
passed: [localhost] ASSERT(&amp;#39;lemons&amp;#39; in fruits)
PLAY RECAP *********************************************************************
localhost : ok=3 changed=1 unreachable=0 failed=0
&lt;/code>&lt;/pre>&lt;h2 id="machine-readable-statistics">Machine readable statistics&lt;/h2>
&lt;p>The above is nice but is still primarily human-consumable. What if we
want to collect test statistics for machine processing (maybe we want
to produce a nicely formatted report of some kind, or maybe we want to
aggregate information from multiple test runs, or maybe we want to
trigger some action in the event there are failed tests, or&amp;hellip;)? You
can ask the &lt;code>assertive&lt;/code> plugin to write a YAML format document with
this information by adding the following to your &lt;code>ansible.cfg&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[assertive]
results = testresult.yml
&lt;/code>&lt;/pre>
&lt;p>After running our playbook, this file would contain:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">groups&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- &lt;span style="color:#f92672">hosts&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">localhost&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">stats&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_failed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_passed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_skipped&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">tests&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">assertions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">test&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;apples&amp;#39;&amp;#39; in fruits&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testresult&lt;/span>: &lt;span style="color:#ae81ff">failed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#ae81ff">you have no apples&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testresult&lt;/span>: &lt;span style="color:#ae81ff">failed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testtime&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;2017-08-04T21:20:58.624789&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">assertions&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#f92672">test&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;&amp;#39;&amp;#39;lemons&amp;#39;&amp;#39; in fruits&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testresult&lt;/span>: &lt;span style="color:#ae81ff">passed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">msg&lt;/span>: &lt;span style="color:#ae81ff">All assertions passed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testresult&lt;/span>: &lt;span style="color:#ae81ff">passed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">testtime&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;2017-08-04T21:20:58.669144&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">name&lt;/span>: &lt;span style="color:#ae81ff">localhost&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">stats&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_failed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_passed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_skipped&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">stats&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions&lt;/span>: &lt;span style="color:#ae81ff">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_failed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_passed&lt;/span>: &lt;span style="color:#ae81ff">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">assertions_skipped&lt;/span>: &lt;span style="color:#ae81ff">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">timing&lt;/span>:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">test_finished_at&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;2017-08-04T21:20:58.670802&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#f92672">test_started_at&lt;/span>: &lt;span style="color:#e6db74">&amp;#39;2017-08-04T21:20:57.918412&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With these tools it becomes much easier to design playbooks for
testing your infrastructure.&lt;/p></content></item><item><title>OpenStack, Containers, and Logging</title><link>https://blog.oddbit.com/post/2017-06-14-openstack-containers-and-loggi/</link><pubDate>Wed, 14 Jun 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-06-14-openstack-containers-and-loggi/</guid><description>I&amp;rsquo;ve been thinking about logging in the context of OpenStack and containerized service deployments. I&amp;rsquo;d like to lay out some of my thoughts on this topic and see if people think I am talking crazy or not.
There are effectively three different mechanisms that an application can use to emit log messages:
Via some logging-specific API, such as the legacy syslog API By writing a byte stream to stdout/stderr By writing a byte stream to a file A substantial advantage to the first mechanism (using a logging API) is that the application is logging messages rather than bytes.</description><content>&lt;p>I&amp;rsquo;ve been thinking about logging in the context of OpenStack and containerized service deployments. I&amp;rsquo;d like to lay out some of my thoughts on this topic and see if people think I am talking crazy or not.&lt;/p>
&lt;p>There are effectively three different mechanisms that an application can use to emit log messages:&lt;/p>
&lt;ul>
&lt;li>Via some logging-specific API, such as the legacy syslog API&lt;/li>
&lt;li>By writing a byte stream to stdout/stderr&lt;/li>
&lt;li>By writing a byte stream to a file&lt;/li>
&lt;/ul>
&lt;p>A substantial advantage to the first mechanism (using a logging API) is that the application is logging &lt;em>messages&lt;/em> rather than &lt;em>bytes&lt;/em>. This means that if you log a message containing embedded newlines (e.g., python or java tracebacks), you can collect that as a single message rather than having to impose some sort of structure on the byte stream after the fact in order to reconstruct those message.&lt;/p>
&lt;p>Another advantage to the use of a logging API is that whatever is receiving logs from your application may be able to annotate the message in various interesting ways.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>We&amp;rsquo;re probably going to need to support all three of the above mechanisms. Some applications (such as &lt;code>haproxy&lt;/code>) will only log to syslog. Others may only log to files (such as &lt;code>mariadb&lt;/code>), and still others may only log to stdout.&lt;/p>
&lt;h2 id="comparing-different-log-mechanisms">Comparing different log mechanisms&lt;/h2>
&lt;h3 id="logging-via-syslog">Logging via syslog&lt;/h3>
&lt;p>In RHEL, the &lt;code>journald&lt;/code> process is what listens to &lt;code>/dev/log&lt;/code>. If you bind mount journald&amp;rsquo;s &lt;code>/dev/log&lt;/code> inside a container and then run the following Python code inside that container&amp;hellip;&lt;/p>
&lt;pre>&lt;code>import logging
import logging.handlers
handler = logging.handlers.SysLogHandler(address='/dev/log')
log = logging.getLogger(__name__)
log.setLevel('DEBUG')
log.addHandler(handler)
log.warning('This is a test')
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;you will find that your simple log message has been annotated with
a variety of useful metadata (the output below is the result of
running &lt;code>journalctl -o verbose ...&lt;/code>):&lt;/p>
&lt;pre>&lt;code>Wed 2017-06-14 12:35:57.577061 EDT [s=dc1dd9d61cf045e991f265aa17c5af03;i=6eb6e;b=0d9dc78871c34f43a4a6c27f43cf4167;m=a171206ec6;t=551ee258c6492;x=4e3c71faa52ba9d8]
_BOOT_ID=0d9dc78871c34f43a4a6c27f43cf4167
_MACHINE_ID=229916fba5b54252ad4d08efbc581213
_HOSTNAME=lkellogg-pc0dzzve
_UID=0
_GID=0
_SYSTEMD_SLICE=-.slice
_TRANSPORT=syslog
PRIORITY=4
SYSLOG_FACILITY=1
_EXE=/usr/bin/python3.5
_CAP_EFFECTIVE=a80425fb
_SELINUX_CONTEXT=system_u:system_r:unconfined_service_t:s0
_COMM=python3
MESSAGE=This is a test
_PID=13849
_CMDLINE=python3 logtest.py
_SYSTEMD_CGROUP=/docker/7ed1e97d5bb4076caf99393ae3f88b07102a26b0ade2176ed07890bee9a84d24
_SOURCE_REALTIME_TIMESTAMP=1497458157577061
&lt;/code>&lt;/pre>
&lt;p>There are several items of interest there:&lt;/p>
&lt;ul>
&lt;li>A high resolution timestamp&lt;/li>
&lt;li>The kernel cgroup, which corresponds to the docker container id and thus uniquely identifies the container that originated the message&lt;/li>
&lt;li>The executable path inside the container that generated the message&lt;/li>
&lt;li>The machine id, which uniquely identifies the host&lt;/li>
&lt;/ul>
&lt;p>By logging via syslog you have removed the necessity of either (a) handling log rotation in your application or (b) handling log rotation in your container or (c) having to communicate log rotation configuration from the container to the host. Additionally, you can rely on journald to take care of rate limiting and log size management to prevent a broken application from performing a local DOS of the server.&lt;/p>
&lt;h3 id="logging-via-stdoutstderr">Logging via stdout/stderr&lt;/h3>
&lt;p>Applications that write a byte stream to stdout/stderr will have their output handled by the Docker log driver. If we run Docker with the &lt;code>journald&lt;/code> log driver (using the &lt;code>--log-driver=journald&lt;/code> option to the Docker server), then Docker will add metadata lines read from stdout/stderr. For example, if we run&amp;hellip;&lt;/p>
&lt;pre>&lt;code>docker run fedora echo This is a test.
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;then our journal will contain:&lt;/p>
&lt;pre>&lt;code>Wed 2017-06-14 12:46:45.511515 EDT [s=dc1dd9d61cf045e991f265aa17c5af03;i=6ee72;b=0d9dc78871c34f43a4a6c27f43cf4167;m=a197bf222b;t=551ee4c2b17f7;x=e7c1a220c93ef3cf]
_BOOT_ID=0d9dc78871c34f43a4a6c27f43cf4167
_MACHINE_ID=229916fba5b54252ad4d08efbc581213
_HOSTNAME=lkellogg-pc0dzzve
PRIORITY=6
_TRANSPORT=journal
_UID=0
_GID=0
_CAP_EFFECTIVE=3fffffffff
_SYSTEMD_SLICE=system.slice
_SELINUX_CONTEXT=system_u:system_r:unconfined_service_t:s0
_COMM=dockerd
_EXE=/usr/bin/dockerd
_SYSTEMD_CGROUP=/system.slice/docker.service
_SYSTEMD_UNIT=docker.service
_PID=14309
_CMDLINE=/usr/bin/dockerd -G docker --dns 172.23.254.1 --log-driver journald -s overlay2
MESSAGE=This is a test.
CONTAINER_NAME=happy_euclid
CONTAINER_TAG=82b87e8902e8
CONTAINER_ID=82b87e8902e8
CONTAINER_ID_FULL=82b87e8902e8ac36f3365012ef10c66444fbb8c00e8cec7d7c2a14c05b054127
&lt;/code>&lt;/pre>
&lt;p>Like the messages logged via syslog, this also containers information that identifies the source container. It does not identify the particular binary responsible for emitting the message.&lt;/p>
&lt;h3 id="logging-to-a-file">Logging to a file&lt;/h3>
&lt;p>When logging to a file, the system is unable to add any metadata for us automatically. We can derive similar information by logging to a container-specific location (&lt;code>/var/log/CONTAINERNAME/...&lt;/code>, for example), or by configuring our application to include specific information in the log messages, but ultimately this is the least information-rich mechanism available to us. Furthermore, it necessitates that we configure some sort of container-aware log rotation strategy to avoid eating up all the available disk space over time.&lt;/p>
&lt;h2 id="log-collection">Log collection&lt;/h2>
&lt;p>Our goal is not simply to make log messages available locally. In
general, we also want to aggregate log messages from several machines
into a single location where we can perform various sorts of queries,
analysis, and visualization. There are a number of solutions in place
for getting logs off a local server to a central collector, including both &lt;a href="http://www.fluentd.org/">fluentd&lt;/a> and &lt;a href="http://www.rsyslog.com/">rsyslog&lt;/a>.&lt;/p>
&lt;p>In the context of the above discussion, it turns out that &lt;code>rsyslog&lt;/code> has some very desirable features. In particular, the &lt;a href="http://www.rsyslog.com/doc/v8-stable/configuration/modules/imjournal.html">imjournal&lt;/a> input module has support for reading structured messages from journald and exporting those to a remote collector (such as &lt;a href="https://www.elastic.co/">ElasticSearch&lt;/a>) with their structure intact. Fluentd does not ship with journald support as a core plugin.&lt;/p>
&lt;p>Rsyslog version 8.x and later have a rich language for filtering, annotating, and otherwise modifying log messages that would allow us to do things such as add host-specific tags to messages, normalize log messages from applications with poorly designed log messages, and perform other transformations before sending them on to a remote collector.&lt;/p>
&lt;p>For example, we would ensure that messages from containerized services logged via syslog &lt;em>or&lt;/em> via stdout/stderr have a &lt;code>CONTAINER_ID_FULL&lt;/code> field with something like the following:&lt;/p>
&lt;pre>&lt;code>if re_match($!_SYSTEMD_CGROUP, &amp;quot;^/docker/&amp;quot;) then {
set $!CONTAINER_ID_FULL = re_extract($!_SYSTEMD_CGROUP, &amp;quot;^/docker/(.*)&amp;quot;, 0, 1, &amp;quot;unknown&amp;quot;);
}
&lt;/code>&lt;/pre>
&lt;p>This matches the &lt;code>_SYSTEMD_CGROUP&lt;/code> field of the message, extracts the container id, and uses that to set the &lt;code>CONTAINER_ID_FULL&lt;/code> property on the message.&lt;/p>
&lt;h2 id="recommendations">Recommendations&lt;/h2>
&lt;ol>
&lt;li>Provide a consistent logging environment to containerized services. Provide every container both with &lt;code>/dev/log&lt;/code> and a container-specific host directory mounted on &lt;code>/var/log&lt;/code>.&lt;/li>
&lt;li>For applications that support logging to syslog (such as all consumers of &lt;code>oslo.log&lt;/code>), configure them to log exclusively via syslog.&lt;/li>
&lt;li>For applications that are unable to log via syslog but are able to log to stdout/stderr, ensure that Docker is using the &lt;code>journald&lt;/code> log driver.&lt;/li>
&lt;li>For applications that can only log to files, configure rsyslog on the host to read those files using the &lt;a href="http://www.rsyslog.com/doc/v8-stable/configuration/modules/imfile.html">imfile&lt;/a> input plugin.&lt;/li>
&lt;li>Use rsyslog on the host to forward structured messages to a remote collector.&lt;/li>
&lt;/ol></content></item><item><title>Making sure your Gerrit changes aren't broken</title><link>https://blog.oddbit.com/post/2017-01-22-making-sure-your-gerrit-change/</link><pubDate>Sun, 22 Jan 2017 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2017-01-22-making-sure-your-gerrit-change/</guid><description>It&amp;rsquo;s a bit of an embarrassment when you submit a review to Gerrit only to have it fail CI checks immediately because of something as simple as a syntax error or pep8 failure that you should have caught yourself before submitting&amp;hellip;but you forgot to run your validations before submitting the change.
In many cases you can alleviate this through the use of the git pre-commit hook, which will run every time you commit changes locally.</description><content>&lt;p>It&amp;rsquo;s a bit of an embarrassment when you submit a review to Gerrit only
to have it fail CI checks immediately because of something as simple
as a syntax error or pep8 failure that you should have caught yourself
before submitting&amp;hellip;but you forgot to run your validations before
submitting the change.&lt;/p>
&lt;p>In many cases you can alleviate this through the use of the git
&lt;code>pre-commit&lt;/code> hook, which will run every time you commit changes
locally. You can have the hook run &lt;code>tox&lt;/code> or whatever tool your
project uses for validation on every commit. This works okay for
simple cases, but if the validation takes more than a couple of
seconds the delay can be disruptive to the flow of your work.&lt;/p>
&lt;p>What you want is something that stays out of the way while you are
working locally, but that will prevent you from submitting something
for review that&amp;rsquo;s going to fail CI immediately. If you are using the
&lt;a href="http://docs.openstack.org/infra/git-review/">git-review&lt;/a> tool to interact with Gerrit (and if you&amp;rsquo;re not, you
should be), you&amp;rsquo;re in luck! The &lt;code>git-review&lt;/code> tool supports a
&lt;code>pre-review&lt;/code> hook that does exactly what we want. &lt;code>git-review&lt;/code> looks
for hooks in a global location (&lt;code>~/.config/git-review/hooks&lt;/code>) and in a
per-project location (in &lt;code>.git/hooks/&lt;/code>). As with standard Git hooks,
the &lt;code>pre-review&lt;/code> hook must be executable (i.e., &lt;code>chmod u+x .git/hooks/pre-review&lt;/code>).&lt;/p>
&lt;p>The &lt;code>pre-review&lt;/code> script will be run before attempting to submit your
changes to Gerrit. If the script exits successfully, the output is
hidden and your changes will be submitted normally. If the hook
fails, you will see output along the lines of&amp;hellip;&lt;/p>
&lt;pre>&lt;code>Custom script execution failed.
The following command failed with exit code 1
&amp;quot;.git/hooks/pre-review&amp;quot;
-----------------------
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;followed by the output of the &lt;code>pre-review&lt;/code> script.&lt;/p>
&lt;p>For my work on the &lt;a href="https://github.com/openstack/tripleo-quickstart">tripleo-quickstart&lt;/a> project, the contents of my
&lt;code>.git/hooks/pre-review&lt;/code> script is as simple as:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
tox
&lt;/code>&lt;/pre></content></item><item><title>Exploring YAQL Expressions</title><link>https://blog.oddbit.com/post/2016-08-11-exploring-yaql-expressions/</link><pubDate>Thu, 11 Aug 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-08-11-exploring-yaql-expressions/</guid><description>The Newton release of Heat adds support for a yaql intrinsic function, which allows you to evaluate yaql expressions in your Heat templates. Unfortunately, the existing yaql documentation is somewhat limited, and does not offer examples of many of yaql&amp;rsquo;s more advanced features.
I am working on a Fluentd composable service for TripleO. I want to allow each service to specify a logging source configuration fragment, for example:
parameters: NovaAPILoggingSource: type: json description: Fluentd logging configuration for nova-api.</description><content>&lt;p>The Newton release of &lt;a href="https://wiki.openstack.org/wiki/Heat">Heat&lt;/a> adds support for a &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#yaql">yaql&lt;/a>
intrinsic function, which allows you to evaluate &lt;a href="https://yaql.readthedocs.io/en/latest/">yaql&lt;/a> expressions
in your Heat templates. Unfortunately, the existing yaql
documentation is somewhat limited, and does not offer examples of many
of yaql&amp;rsquo;s more advanced features.&lt;/p>
&lt;p>I am working on a &lt;a href="http://www.fluentd.org/">Fluentd&lt;/a> composable service for &lt;a href="https://wiki.openstack.org/wiki/TripleO">TripleO&lt;/a>. I
want to allow each service to specify a logging source configuration
fragment, for example:&lt;/p>
&lt;pre>&lt;code>parameters:
NovaAPILoggingSource:
type: json
description: Fluentd logging configuration for nova-api.
default:
tag: openstack.nova.api
type: tail
format: |
/(?&amp;lt;time&amp;gt;\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}.\d+) (?&amp;lt;pid&amp;gt;\d+) (?&amp;lt;priority&amp;gt;\S+) (?&amp;lt;message&amp;gt;.*)/
path: /var/log/nova/nova-api.log
pos_file: /var/run/fluentd/openstack.nova.api.pos
&lt;/code>&lt;/pre>
&lt;p>This generally works, but several parts of this fragment are going to
be the same across all OpenStack services. I wanted to reduce the
above to just the unique attributes, which would look something like:&lt;/p>
&lt;pre>&lt;code>parameters:
NovaAPILoggingSource:
type: json
description: Fluentd logging configuration for nova-api.
default:
tag: openstack.nova.api
path: /var/log/nova/nova-api.log
&lt;/code>&lt;/pre>
&lt;p>This would ultimately give me a list of dictionaries of the form:&lt;/p>
&lt;pre>&lt;code>[
{
&amp;quot;tag&amp;quot;: &amp;quot;openstack.nova.api&amp;quot;,
&amp;quot;path&amp;quot;: &amp;quot;/var/log/nova/nova-api.log&amp;quot;
},
{
&amp;quot;tag&amp;quot;: &amp;quot;openstack.nova.scheduler&amp;quot;,
&amp;quot;path&amp;quot;: &amp;quot;/var/log/nova/nova-scheduler.log&amp;quot;
}
]
&lt;/code>&lt;/pre>
&lt;p>I want to iterate over this list, adding default values for attributes
that are not explicitly provided.&lt;/p>
&lt;p>The yaql language has a &lt;code>select&lt;/code> function, somewhat analagous to the
SQL &lt;code>select&lt;/code> statement, that can be used to construct a new data
structure from an existing one. For example, given the above data in
a parameter called &lt;code>sources&lt;/code>, I could write:&lt;/p>
&lt;pre>&lt;code>outputs:
sources:
yaql:
data:
sources: {get_param: sources}
expression: &amp;gt;
$.data.sources.select({
'path' =&amp;gt; $.path,
'tag' =&amp;gt; $.tag,
'type' =&amp;gt; $.get('type', 'tail')})
&lt;/code>&lt;/pre>
&lt;p>This makes use of the &lt;code>.get&lt;/code> method to insert a default value of
&lt;code>tail&lt;/code> for the &lt;code>type&lt;/code> attribute for items that don&amp;rsquo;t specify it
explicitly. This would produce a list that looks like:&lt;/p>
&lt;pre>&lt;code>[
{
&amp;quot;path&amp;quot;: &amp;quot;/var/log/nova/nova-api.log&amp;quot;,
&amp;quot;tag&amp;quot;: &amp;quot;openstack.nova.api&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;tail&amp;quot;
},
{
&amp;quot;path&amp;quot;: &amp;quot;/var/log/nova/nova-scheduler.log&amp;quot;,
&amp;quot;tag&amp;quot;: &amp;quot;openstack.nova.scheduler&amp;quot;,
&amp;quot;type&amp;quot;: &amp;quot;tail&amp;quot;
}
]
&lt;/code>&lt;/pre>
&lt;p>That works fine, but what if I want to parameterize the default value
such that it can be provided as part of the template? I wanted to be
able to pass the yaql expression something like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>outputs:
sources:
yaql:
data:
sources: {get_param: sources}
default_type: tail
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then within the yaql expression, insert the value of
&lt;code>default_type&lt;/code> into items that don&amp;rsquo;t provide an explicit value for the
&lt;code>type&lt;/code> attribute.&lt;/p>
&lt;p>This is trickier than it might sound at first because within the
context of the &lt;code>select&lt;/code> method, &lt;code>$&lt;/code> is bound to the &lt;em>local&lt;/em> context,
which will be an individual item from the list. So while I can ask
for &lt;code>$.path&lt;/code>, there&amp;rsquo;s no way to refer to items from the top-level
context. Or is there?&lt;/p>
&lt;p>The &lt;a href="https://yaql.readthedocs.io/en/latest/getting_started.html#operators">operators&lt;/a> documentation for yaql mentions the &amp;ldquo;context pass&amp;rdquo;
operator, &lt;code>-&amp;gt;&lt;/code>, but doesn&amp;rsquo;t provide any examples of how it can be
used. It turns out that this operator will be the key to our solution.
But before we look at that in more detail, we need to introduce the
&lt;code>let&lt;/code> statement, which can be used to define variables. The &lt;code>let&lt;/code>
statement isn&amp;rsquo;t mentioned in the documentation at all, but it looks
like this:&lt;/p>
&lt;pre>&lt;code>let(var =&amp;gt; value, ...)
&lt;/code>&lt;/pre>
&lt;p>By itself, this isn&amp;rsquo;t particularly useful. In fact, if you were to
type a bare &lt;code>let&lt;/code> statement in a yaql evaluator, you would get an
error:&lt;/p>
&lt;pre>&lt;code>yaql&amp;gt; let(foo =&amp;gt; 10, bar =&amp;gt; 20)
Execution exception: &amp;lt;yaql.language.contexts.Context object at 0x7fbaf9772e50&amp;gt; is not JSON serializable
&lt;/code>&lt;/pre>
&lt;p>This is where the &lt;code>-&amp;gt;&lt;/code> operator comes into play. We use that to pass
the context created by the &lt;code>let&lt;/code> statement into a yaql expression. For
example:&lt;/p>
&lt;pre>&lt;code>yaql&amp;gt; let(foo =&amp;gt; 10, bar =&amp;gt; 20) -&amp;gt; $foo
10
yaql&amp;gt; let(foo =&amp;gt; 10, bar =&amp;gt; 20) -&amp;gt; $bar
20
&lt;/code>&lt;/pre>
&lt;p>With that in mind, we can return to our earlier task, and rewrite the
yaql expression like this:&lt;/p>
&lt;pre>&lt;code>outputs:
sources:
yaql:
data:
sources: {get_param: sources}
default_type: tail
expression: &amp;gt;
let(default_type =&amp;gt; $.data.default_type) -&amp;gt;
$.data.sources.select({
'path' =&amp;gt; $.path,
'tag' =&amp;gt; $.tag,
'type' =&amp;gt; $.get('type', $default_type)})
&lt;/code>&lt;/pre>
&lt;p>Which will give us exactly what we want. This can of course be
extended to support additional default values:&lt;/p>
&lt;pre>&lt;code>outputs:
sources:
yaql:
data:
sources: {get_param: sources}
default_type: tail
default_format: &amp;gt;
/some regular expression/
expression: &amp;gt;
let(
default_type =&amp;gt; $.data.default_type,
default_format =&amp;gt; $.data.default_format
) -&amp;gt;
$.data.sources.select({
'path' =&amp;gt; $.path,
'tag' =&amp;gt; $.tag,
'type' =&amp;gt; $.get('type', $default_type),
'format' =&amp;gt; $.get('format', $default_format)
})
&lt;/code>&lt;/pre>
&lt;p>Going out on a bit of a tangent, there is another statement not
mentioned in the documentation: the &lt;code>def&lt;/code> statement lets you defined a
yaql function. The general format is:&lt;/p>
&lt;pre>&lt;code>def(func_name, func_body)
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>func_body&lt;/code> is a yaql expresion. For example:&lt;/p>
&lt;pre>&lt;code>def(upperpath, $.path.toUpper()) -&amp;gt;
$.data.sources.select(upperpath($))
&lt;/code>&lt;/pre>
&lt;p>Which would generate:&lt;/p>
&lt;pre>&lt;code>[
&amp;quot;/VAR/LOG/NOVA/NOVA-API.LOG&amp;quot;,
&amp;quot;/VAR/LOG/NOVA/NOVA-SCHEDULER.LOG&amp;quot;
]
&lt;/code>&lt;/pre>
&lt;p>This obviously becomes more useful as you use user-defined functions
to encapsulate more complex yaql expressions for re-use.&lt;/p>
&lt;p>Thanks to &lt;a href="https://github.com/sergmelikyan">sergmelikyan&lt;/a> for his help figuring this out.&lt;/p></content></item><item><title>Connecting another vm to your tripleo-quickstart deployment</title><link>https://blog.oddbit.com/post/2016-05-19-connecting-another-vm-to-your-/</link><pubDate>Thu, 19 May 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-05-19-connecting-another-vm-to-your-/</guid><description>Let&amp;rsquo;s say that you have set up an environment using tripleo-quickstart and you would like to add another virtual machine to the mix that has both &amp;ldquo;external&amp;rdquo; connectivity (&amp;ldquo;external&amp;rdquo; in quotes because I am using it in the same way as the quickstart does w/r/t the undercloud) and connectivity to the overcloud nodes. How would you go about setting that up?
For a concrete example, let&amp;rsquo;s presume you have deployed an environment using the default tripleo-quickstart configuration, which looks like this:</description><content>&lt;p>Let&amp;rsquo;s say that you have set up an environment using
&lt;a href="https://github.com/openstack/tripleo-quickstart/">tripleo-quickstart&lt;/a> and you would like to add another virtual
machine to the mix that has both &amp;ldquo;external&amp;rdquo; connectivity (&amp;ldquo;external&amp;rdquo;
in quotes because I am using it in the same way as the quickstart does
w/r/t the undercloud) and connectivity to the overcloud nodes. How
would you go about setting that up?&lt;/p>
&lt;p>For a concrete example, let&amp;rsquo;s presume you have deployed an environment
using the default tripleo-quickstart configuration, which looks like
this:&lt;/p>
&lt;pre>&lt;code>overcloud_nodes:
- name: control_0
flavor: control
- name: compute_0
flavor: compute
extra_args: &amp;gt;-
--neutron-network-type vxlan
--neutron-tunnel-types vxlan
--ntp-server pool.ntp.org
network_isolation: true
&lt;/code>&lt;/pre>
&lt;p>That gets you one controller, one compute node, and enables network
isolation. When your deployment is complete, networking from the
perspective of the undercloud looks like this:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>eth0&lt;/code> is connected to the host&amp;rsquo;s &lt;code>brext&lt;/code> bridge and gives the
undercloud NAT access to the outside world. The interface will have
an address on the &lt;code>192.168.23.0/24&lt;/code> network.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>eth1&lt;/code> is connected to the host&amp;rsquo;s &lt;code>brovc&lt;/code> bridge, which is the
internal network for the overcloud. The interface is attached to
the OVS bridge &lt;code>br-ctlplane&lt;/code>.&lt;/p>
&lt;p>The &lt;code>br-ctlplane&lt;/code> bridge has the address &lt;code>192.0.2.1&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>And your overcloud environment probably looks something like this:&lt;/p>
&lt;pre>&lt;code>[stack@undercloud ~]$ nova list
+-------...+-------------------------+--------+...+--------------------+
| ID ...| Name | Status |...| Networks |
+-------...+-------------------------+--------+...+--------------------+
| 32f6ec...| overcloud-controller-0 | ACTIVE |...| ctlplane=192.0.2.7 |
| d98474...| overcloud-novacompute-0 | ACTIVE |...| ctlplane=192.0.2.8 |
+-------...+-------------------------+--------+...+--------------------+
&lt;/code>&lt;/pre>
&lt;p>We want to set up a new machine that has the same connectivity as the
undercloud.&lt;/p>
&lt;h2 id="upload-an-image">Upload an image&lt;/h2>
&lt;p>Before we can boot a new vm we&amp;rsquo;ll need an image; let&amp;rsquo;s start with the
standard CentOS 7 cloud image. First we&amp;rsquo;ll download it:&lt;/p>
&lt;pre>&lt;code>curl -O http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s add a root password to the image and disable &lt;a href="https://cloudinit.readthedocs.io/en/latest/">cloud-init&lt;/a>,
since we&amp;rsquo;re not booting in a cloud environment:&lt;/p>
&lt;pre>&lt;code>virt-customize -a CentOS-7-x86_64-GenericCloud.qcow2 \
--root-password password:changeme \
--run-command &amp;quot;yum -y remove cloud-init&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Now let&amp;rsquo;s upload it to libvirt:&lt;/p>
&lt;pre>&lt;code>virsh vol-create-as oooq_pool centos-7-cloud.qcow2 8G \
--format qcow2 \
--allocation 0
virsh vol-upload --pool oooq_pool centos-7-cloud.qcow2 \
CentOS-7-x86_64-GenericCloud.qcow2
&lt;/code>&lt;/pre>
&lt;h2 id="a-idboot-the-vmboot-the-vma">&lt;!-- raw HTML omitted -->Boot the vm&lt;!-- raw HTML omitted -->&lt;/h2>
&lt;p>I like to boot from a copy-on-write clone of the image, so that I can
use the base image multiple times or quickly revert to a pristine
state, so let&amp;rsquo;s first create that clone:&lt;/p>
&lt;pre>&lt;code>virsh vol-create-as oooq_pool myguest.qcow2 10G \
--allocation 0 --format qcow2 \
--backing-vol centos-7-cloud.qcow2 \
--backing-vol-format qcow2
&lt;/code>&lt;/pre>
&lt;p>And then boot our vm:
&lt;!-- raw HTML omitted -->&lt;!-- raw HTML omitted -->&lt;/p>
&lt;pre>&lt;code>virt-install --disk vol=oooq_pool/myguest.qcow2,bus=virtio \
--import \
-r 2048 -n myguest --cpu host \
--os-variant rhel7 \
-w bridge=brext,model=virtio \
-w bridge=brovc,model=virtio \
--serial pty \
--noautoconsole
&lt;/code>&lt;/pre>
&lt;p>The crucial parts of the above command are the two &lt;code>-w ...&lt;/code> arguments,
which create interfaces attached to the named bridges.&lt;/p>
&lt;p>We can now connect to the console and log in as &lt;code>root&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ virsh console myguest
.
.
.
localhost login: root
Password:
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ll see that the system already has an ip address on the &amp;ldquo;external&amp;rdquo;
network:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ip addr show eth0
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
link/ether 52:54:00:7f:5c:5a brd ff:ff:ff:ff:ff:ff
inet 192.168.23.27/24 brd 192.168.23.255 scope global dynamic eth0
valid_lft 3517sec preferred_lft 3517sec
inet6 fe80::5054:ff:fe7f:5c5a/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>And we have external connectivity:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ping -c1 google.com
PING google.com (216.58.219.206) 56(84) bytes of data.
64 bytes from lga25s40-in-f14.1e100.net (216.58.219.206): icmp_seq=1 ttl=56 time=20.6 ms
--- google.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 20.684/20.684/20.684/0.000 ms
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s give &lt;code>eth1&lt;/code> an address on the ctlplane network:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ip addr add 192.0.2.254/24 dev eth1
[root@localhost ~]# ip link set eth1 up
&lt;/code>&lt;/pre>
&lt;p>Now we can access the undercloud:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ping -c1 192.0.2.1
PING 192.0.2.1 (192.0.2.1) 56(84) bytes of data.
64 bytes from 192.0.2.1: icmp_seq=1 ttl=64 time=0.464 ms
--- 192.0.2.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.464/0.464/0.464/0.000 ms
&lt;/code>&lt;/pre>
&lt;p>As well as all of the overcloud hosts using their addresses on the
same network:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# ping -c1 192.0.2.1
PING 192.0.2.1 (192.0.2.1) 56(84) bytes of data.
64 bytes from 192.0.2.1: icmp_seq=1 ttl=64 time=0.464 ms
--- 192.0.2.1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.464/0.464/0.464/0.000 ms
&lt;/code>&lt;/pre>
&lt;h2 id="allocating-an-address-using-dhcp">Allocating an address using DHCP&lt;/h2>
&lt;p>In the above instructions we&amp;rsquo;ve manually assigned an ip address on the
ctlplane network. This works fine for testing, but it could
ultimately prove problematic if neutron were to allocate the same
address to another overcloud host. We can use neutron to configure a
static dhcp lease for our new host.&lt;/p>
&lt;p>First, we need the MAC address of our guest:&lt;/p>
&lt;pre>&lt;code>virthost$ virsh dumpxml myguest |
xmllint --xpath '//interface[source/@bridge=&amp;quot;brovc&amp;quot;]' -
&amp;lt;interface type=&amp;quot;bridge&amp;quot;&amp;gt;
&amp;lt;mac address=&amp;quot;52:54:00:42:d6:c2&amp;quot;/&amp;gt;
&amp;lt;source bridge=&amp;quot;brovc&amp;quot;/&amp;gt;
&amp;lt;target dev=&amp;quot;tap9&amp;quot;/&amp;gt;
&amp;lt;model type=&amp;quot;virtio&amp;quot;/&amp;gt;
&amp;lt;alias name=&amp;quot;net1&amp;quot;/&amp;gt;
&amp;lt;address type=&amp;quot;pci&amp;quot; domain=&amp;quot;0x0000&amp;quot; bus=&amp;quot;0x00&amp;quot; slot=&amp;quot;0x04&amp;quot; function=&amp;quot;0x0&amp;quot;/&amp;gt;
&amp;lt;/interface&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And then on the undercloud we run &lt;code>neutron port-create&lt;/code> to create a
port and associate it with our MAC address:&lt;/p>
&lt;pre>&lt;code>[stack@undercloud]$ neutron port-create --mac-address 52:54:00:42:d6:c2 ctlplane
&lt;/code>&lt;/pre>
&lt;p>Now if we run &lt;code>dhclient&lt;/code> on our guest, it will acquire a lease from
the neutron-managed DHCP server:&lt;/p>
&lt;pre>&lt;code>[root@localhost]# dhclient -d eth1
Internet Systems Consortium DHCP Client 4.2.5
Copyright 2004-2013 Internet Systems Consortium.
All rights reserved.
For info, please visit https://www.isc.org/software/dhcp/
Listening on LPF/eth1/52:54:00:42:d6:c2
Sending on LPF/eth1/52:54:00:42:d6:c2
Sending on Socket/fallback
DHCPREQUEST on eth1 to 255.255.255.255 port 67 (xid=0xc90c0ba)
DHCPACK from 192.0.2.5 (xid=0xc90c0ba)
bound to 192.0.2.9 -- renewal in 42069 seconds.
&lt;/code>&lt;/pre>
&lt;p>We can make this persistent by creating
&lt;code>/etc/sysconfig/network-scripts/ifcfg-eth1&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[root@localhost]# cd /etc/sysconfig/network-scripts
[root@localhost]# sed s/eth0/eth1/g ifcfg-eth0 &amp;gt; ifcfg-eth1
[root@localhost]# ifup eth1
Determining IP information for eth1... done.
&lt;/code>&lt;/pre></content></item><item><title>Deploying an HA OpenStack development environment with tripleo-quickstart</title><link>https://blog.oddbit.com/post/2016-02-19-deploy-an-ha-openstack-develop/</link><pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2016-02-19-deploy-an-ha-openstack-develop/</guid><description>In this article I would like to introduce tripleo-quickstart, a tool that will automatically provision a virtual environment and then use TripleO to deploy an HA OpenStack on top of it.
Introducing Tripleo-Quickstart The goal of the Tripleo-Quickstart project is to replace the instack-virt-setup tool for quickly setting up virtual TripleO environments, and to ultimately become the tool used by both developers and upstream CI for this purpose. The project is a set of Ansible playbooks that will take care of:</description><content>&lt;p>In this article I would like to introduce &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart">tripleo-quickstart&lt;/a>, a
tool that will automatically provision a virtual environment and then
use &lt;a href="http://docs.openstack.org/developer/tripleo-docs/">TripleO&lt;/a> to deploy an HA OpenStack on top of it.&lt;/p>
&lt;h2 id="introducing-tripleo-quickstart">Introducing Tripleo-Quickstart&lt;/h2>
&lt;p>The goal of the &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart">Tripleo-Quickstart&lt;/a> project is to replace the
&lt;code>instack-virt-setup&lt;/code> tool for quickly setting up virtual TripleO
environments, and to ultimately become the tool used by both
developers and upstream CI for this purpose. The project is a set of
&lt;a href="http://ansible.com/">Ansible&lt;/a> playbooks that will take care of:&lt;/p>
&lt;ul>
&lt;li>Creating virtual undercloud node&lt;/li>
&lt;li>Creating virtual overcloud nodes&lt;/li>
&lt;li>Deploying the undercloud&lt;/li>
&lt;li>Deploying the overcloud&lt;/li>
&lt;li>Validating the overcloud&lt;/li>
&lt;/ul>
&lt;p>In this article, I will be using &lt;code>tripleo-quickstart&lt;/code> to set up a
development environment on a 32GB desktop. This is probably the
minimum sized system if your goal is to create an HA install (a
single controller/single compute environment could be deployed on
something smaller).&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>Before we get started, you will need:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>A target system with at least 32GB of RAM.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ansible 2.0.x. This is what you get if you &lt;code>pip install ansible&lt;/code>;
it is also available in the Fedora &lt;code>updates-testing&lt;/code> repository and
in the EPEL &lt;code>epel-testing&lt;/code> repository.&lt;/p>
&lt;p>Do &lt;strong>not&lt;/strong> use Ansible from the HEAD of the git repository; the
development version is not necessarily backwards compatible with
2.0.x and may break in unexpected ways.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A user account on the target system with which you can (a) log in
via ssh without a password and (b) use &lt;code>sudo&lt;/code> without a password to
gain root privileges. In other words, this should work:&lt;/p>
&lt;pre>&lt;code> ssh -tt targetuser@targethost sudo echo it worked
&lt;/code>&lt;/pre>
&lt;p>Your &lt;em>targetuser&lt;/em> could be &lt;code>root&lt;/code>, in which case the &lt;code>sudo&lt;/code> is
superfluous and you should be all set.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A copy of the tripleo-quickstart repository:&lt;/p>
&lt;pre>&lt;code> git clone https://github.com/redhat-openstack/tripleo-quickstart/
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>The remainder of this document assumes that you are running things
from inside the &lt;code>tripleo-quickstart&lt;/code> directory.&lt;/p>
&lt;h2 id="the-quick-way">The quick way&lt;/h2>
&lt;p>If you just want to take things out for a spin using the defaults
&lt;em>and&lt;/em> you can ssh to your target host as &lt;code>root&lt;/code>, you can skip the
remainder of this article and just run:&lt;/p>
&lt;pre>&lt;code>ansible-playbook playbooks/centosci/minimal.yml \
-e virthost=my.target.host
&lt;/code>&lt;/pre>
&lt;p>Or for an HA deployment:&lt;/p>
&lt;pre>&lt;code>ansible-playbook playbooks/centosci/ha.yml \
-e virthost=my.target.host
&lt;/code>&lt;/pre>
&lt;p>(Where you replace &lt;code>my.target.host&lt;/code> with the hostname of the host on
which you want to install your virtual environment.)&lt;/p>
&lt;p>In the remainder of this article I will discuss ways in which you can
customize this process (and make subsequent deployments faster).&lt;/p>
&lt;h2 id="create-an-inventory-file">Create an inventory file&lt;/h2>
&lt;p>An inventory file tells Ansible to which hosts it should connect and
provides information about how it should connect. For the quickstart,
your inventory needs to have your target host listed in the &lt;code>virthost&lt;/code>
group. For example:&lt;/p>
&lt;pre>&lt;code>[virthost]
my.target.host ansible_user=targetuser
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m going to assume you put this into a file named &lt;code>inventory&lt;/code>.&lt;/p>
&lt;h2 id="creating-a-playbook">Creating a playbook&lt;/h2>
&lt;p>A playbook tells Ansible what do to do.&lt;/p>
&lt;p>First, we want to tear down any existing virtual environment, and then
spin up a new undercloud node and create guests that will be used as
overcloud nodes. We do this with the &lt;code>libvirt/teardown&lt;/code> and
&lt;code>libvirt/setup&lt;/code> roles:&lt;/p>
&lt;pre>&lt;code>- hosts: virthost
roles:
- libvirt/teardown
- libvirt/setup
&lt;/code>&lt;/pre>
&lt;p>The next play will generate an Ansible inventory file (by default
&lt;code>$HOME/.quickstart/hosts&lt;/code>) that we can use in the future to refer to
our deployment:&lt;/p>
&lt;pre>&lt;code>- hosts: localhost
roles:
- rebuild-inventory
&lt;/code>&lt;/pre>
&lt;p>Lastly, we install the undercloud host and deploy the overcloud:&lt;/p>
&lt;pre>&lt;code>- hosts: undercloud
roles:
- overcloud
&lt;/code>&lt;/pre>
&lt;p>Put this content in a file named &lt;code>ha.yml&lt;/code> (the actual name doesn&amp;rsquo;t
matter, but this gives us something to refer to later on in this
article).&lt;/p>
&lt;h2 id="configuring-the-deployment">Configuring the deployment&lt;/h2>
&lt;p>Before we run tripleo-quickstart, we need to make a few configuration
changes. We&amp;rsquo;ll do this by creating a &lt;a href="http://yaml.org/">YAML&lt;/a> file that describes our
configuration, and we&amp;rsquo;ll feed this to ansible using the &lt;a href="http://docs.ansible.com/ansible/playbooks_variables.html#passing-variables-on-the-command-line">-e
@filename.yml&lt;/a> syntax.&lt;/p>
&lt;h3 id="describing-your-virtual-servers">Describing your virtual servers&lt;/h3>
&lt;p>By default, tripleo-quickstart will deploy an environment consisting
of four overcloud nodes:&lt;/p>
&lt;ul>
&lt;li>3 controller nodes&lt;/li>
&lt;li>1 compute node&lt;/li>
&lt;/ul>
&lt;p>All of these will have 4GB of memory, which when added to the default
overcloud node size of 12GB comes to a total memory footprint of 24GB.
These defaults are defined in
&lt;code>playbooks/roles/nodes/defaults/main.yml&lt;/code>. There are a number of ways
we can override this default configuration.&lt;/p>
&lt;p>To simply change the amount of memory assigned to each class of
server, we can set the &lt;code>undercloud_memory&lt;/code>, &lt;code>control_memory&lt;/code>, and
&lt;code>compute_memory&lt;/code> keys. For example:&lt;/p>
&lt;pre>&lt;code>control_memory: 6000
compute_memory: 2048
&lt;/code>&lt;/pre>
&lt;p>To change the number of CPUs assigned to a server, we can change the
corresponding &lt;code>_vcpu&lt;/code> key. Your deployments will generally run faster
if your undercloud host has more CPUs available:&lt;/p>
&lt;pre>&lt;code>undercloud_vcpu: 4
&lt;/code>&lt;/pre>
&lt;p>To change the number and type of nodes, you can provide an
&lt;code>overcloud_nodes&lt;/code> key with entries for each virtual system. The
default looks like this:&lt;/p>
&lt;pre>&lt;code>overcloud_nodes:
- name: control_0
flavor: control
- name: control_1
flavor: control
- name: control_2
flavor: control
- name: compute_0
flavor: compute
&lt;/code>&lt;/pre>
&lt;p>To create a minimal environment with a single controller and a single
compute node, we could instead put the following into our configuration
file:&lt;/p>
&lt;pre>&lt;code>overcloud_nodes:
- name: control_0
flavor: control
- name: compute_0
flavor: compute
&lt;/code>&lt;/pre>
&lt;p>You may intuit from the above examples that you can actually describe
custom flavors. This is true, but is beyond the scope of this post;
take a look at &lt;code>playbooks/roles/nodes/defaults/main.yml&lt;/code> for an
example.&lt;/p>
&lt;h3 id="configuring-ha">Configuring HA&lt;/h3>
&lt;p>To actually deploy an HA OpenStack environment, we need to pass a few
additional options to the &lt;code>openstack overcloud deploy&lt;/code> command. Based
on &lt;a href="http://docs.openstack.org/developer/tripleo-docs/basic_deployment/basic_deployment_cli.html#deploy-the-overcloud">the docs&lt;/a> I need:&lt;/p>
&lt;pre>&lt;code>--control-scale 3 \
-e /usr/share/openstack-tripleo-heat-templates/environments/puppet-pacemaker.yaml \
--ntp-server pool.ntp.org
&lt;/code>&lt;/pre>
&lt;p>We configure deploy arguments in the &lt;code>extra_args&lt;/code> variable, so for the
above configuration we would add:&lt;/p>
&lt;pre>&lt;code>extra_args: &amp;gt;
--control-scale 3
-e /usr/share/openstack-tripleo-heat-templates/environments/puppet-pacemaker.yaml
--ntp-server pool.ntp.org
&lt;/code>&lt;/pre>
&lt;h3 id="configuring-nested-kvm">Configuring nested KVM&lt;/h3>
&lt;p>I want &lt;a href="https://www.kernel.org/doc/Documentation/virtual/kvm/nested-vmx.txt">nested KVM&lt;/a> on my compute hosts,
which requires changes both to the libvirt XML used to deploy the
&amp;ldquo;baremetal&amp;rdquo; hosts and the nova.conf configuration. I was able to
accomplish this by adding the following to the configuration:&lt;/p>
&lt;pre>&lt;code>baremetal_vm_xml: |
&amp;lt;cpu mode='host-passthrough'/&amp;gt;
libvirt_args: --libvirt-type kvm
&lt;/code>&lt;/pre>
&lt;p>For this to work, you will need to have your target host correctly
configured to support nested KVM, which generally means adding the
following to &lt;code>/etc/modprobe.d/kvm.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code>options kvm_intel nested=1
&lt;/code>&lt;/pre>
&lt;p>(And possibly unloading/reloading the &lt;code>kvm_intel&lt;/code> module if it was
already loaded.)&lt;/p>
&lt;h3 id="disable-some-steps">Disable some steps&lt;/h3>
&lt;p>The default behavior is to:&lt;/p>
&lt;ul>
&lt;li>Install the undercloud&lt;/li>
&lt;li>Deploy the overcloud&lt;/li>
&lt;li>Validate the overcloud&lt;/li>
&lt;/ul>
&lt;p>You can enable or disable individual steps with the following
variables:&lt;/p>
&lt;ul>
&lt;li>&lt;code>step_install_undercloud&lt;/code>&lt;/li>
&lt;li>&lt;code>step_deploy_overcloud&lt;/code>&lt;/li>
&lt;li>&lt;code>step_validate_overcloud&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>These all default to &lt;code>true&lt;/code>. If, for example, overcloud validation is
failing because of a known issue, we could add the following to
&lt;code>nodes.yml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>step_validate_overcloud: false
&lt;/code>&lt;/pre>
&lt;h2 id="pre-caching-the-undercloud-image">Pre-caching the undercloud image&lt;/h2>
&lt;p>Fetching the undercloud image from the CentOS CI environment can take
a really long time. If you&amp;rsquo;re going to be deploying often, you can
speed up this step by manually saving the image and the corresponding
&lt;code>.md5&lt;/code> file to a file on your target host:&lt;/p>
&lt;pre>&lt;code>mkdir -p /usr/share/quickstart_images/mitaka/
cd /usr/share/quickstart_images/mitaka/
wget https://ci.centos.org/artifacts/rdo/images/mitaka/delorean/stable/undercloud.qcow2.md5 \
https://ci.centos.org/artifacts/rdo/images/mitaka/delorean/stable/undercloud.qcow2
&lt;/code>&lt;/pre>
&lt;p>And then providing the path to that file in the &lt;code>url&lt;/code> variable when
you run the playbook. I&amp;rsquo;ve added the following to my &lt;code>nodes.yml&lt;/code>
file, but you could also do this on the command line:&lt;/p>
&lt;pre>&lt;code>url: file:///usr/share/quickstart_images/mitaka/undercloud.qcow2
&lt;/code>&lt;/pre>
&lt;h2 id="intermission">Intermission&lt;/h2>
&lt;p>I&amp;rsquo;ve made the examples presented in this article available for
download at the following URLs:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="ha.yml">ha.yml&lt;/a> playbook&lt;/li>
&lt;li>&lt;a href="nodes.yml">nodes.yml&lt;/a> example configuration file&lt;/li>
&lt;li>&lt;a href="nodes-minimal.yml">nodes-minimal.yml&lt;/a> example configuration file for a minimal environment&lt;/li>
&lt;/ul>
&lt;h2 id="running-tripleo-quickstart">Running tripleo-quickstart&lt;/h2>
&lt;p>With all of the above in place, we can run:&lt;/p>
&lt;pre>&lt;code>ansible-playbook ha.yml -i inventory -e @nodes.yml
&lt;/code>&lt;/pre>
&lt;p>Which will proceed through the following phases:&lt;/p>
&lt;h3 id="tear-down-existing-environment">Tear down existing environment&lt;/h3>
&lt;p>This step deletes any libvirt guests matching the ones we are about to
deploy, removes the &lt;code>stack&lt;/code> user from the target host, and otherwise
ensures a clean slate from which to start.&lt;/p>
&lt;h3 id="create-overcloud-vms">Create overcloud vms&lt;/h3>
&lt;p>This uses the node definitions in &lt;code>vm.overcloud.nodes&lt;/code> to create a set
of libvirt guests. They will &lt;em>not&lt;/em> be booted at this stage; that
happens later during the ironic discovery process.&lt;/p>
&lt;h3 id="fetch-the-undercloud-image">Fetch the undercloud image&lt;/h3>
&lt;p>This will fetch the undercloud appliance image either from the CentOS
CI environment or from wherever you point the &lt;code>url&lt;/code> variable.&lt;/p>
&lt;h3 id="configure-the-undercloud-image">Configure the undercloud image&lt;/h3>
&lt;p>This performs some initial configuration steps such as injecting ssh
keys into the image.&lt;/p>
&lt;h3 id="create-undercloud-vm">Create undercloud vm&lt;/h3>
&lt;p>In this step, tripleo-quickstart uses the configured appliance image
to create a new &lt;code>undercloud&lt;/code> libvirt guest.&lt;/p>
&lt;h3 id="install-undercloud">Install undercloud&lt;/h3>
&lt;p>This runs &lt;code>openstack undercloud install&lt;/code>.&lt;/p>
&lt;h3 id="deploy-overcloud">Deploy overcloud&lt;/h3>
&lt;p>This does everything else:&lt;/p>
&lt;ul>
&lt;li>Discover the available nodes via the Ironic discovery process&lt;/li>
&lt;li>Use &lt;code>openstack overcloud deploy&lt;/code> to kick off the provisioning
process. This feeds &lt;a href="https://wiki.openstack.org/wiki/Heat">Heat&lt;/a> a collection of templates that will be
used to configure the overcloud nodes.&lt;/li>
&lt;/ul>
&lt;h2 id="accessing-the-undercloud">Accessing the undercloud&lt;/h2>
&lt;p>You can ssh directly into the undercloud host by taking advantage of
the ssh configuration that tripleo-quickstart generated for you. By
default this will be &lt;code>$HOME/.quickstart/ssh.config.ansible&lt;/code>, but you
can override that directory by specifying a value for the
&lt;code>local_working_dir&lt;/code> variable when you run Ansible. You use the &lt;code>-F&lt;/code>
option to ssh to point it at that file:&lt;/p>
&lt;pre>&lt;code>ssh -F $HOME/.quickstart/ssh.config.ansible undercloud
&lt;/code>&lt;/pre>
&lt;p>The configuration uses an ssh &lt;code>ProxyConnection&lt;/code> configuration to
automatically proxy your connection to the undercloud vm through your
physical host.&lt;/p>
&lt;h2 id="accessing-the-overcloud-hosts">Accessing the overcloud hosts&lt;/h2>
&lt;p>Once you have logged into the undercloud, you&amp;rsquo;ll need to source in
some credentials. The file &lt;code>stackrc&lt;/code> contains credentials for the
undercloud:&lt;/p>
&lt;pre>&lt;code>. stackrc
&lt;/code>&lt;/pre>
&lt;p>Now you can run &lt;code>nova list&lt;/code> to get a list of your overcloud nodes,
investigate the &lt;code>overcloud&lt;/code> heat stack, and so forth:&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
+----------...+------------+-----------------+--------------...+--------------+
| id ...| stack_name | stack_status | creation_time...| updated_time |
+----------...+------------+-----------------+--------------...+--------------+
| b6cfd621-...| overcloud | CREATE_COMPLETE | 2016-02-19T20...| None |
+----------...+------------+-----------------+--------------...+--------------+
&lt;/code>&lt;/pre>
&lt;p>You can find the ip addresses of your overcloud nodes by running &lt;code>nova list&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova list
+----------...+-------------------------+--------+...+---------------------+
| ID ...| Name | Status |...| Networks |
+----------...+-------------------------+--------+...+---------------------+
| 1fc5d5e8-...| overcloud-controller-0 | ACTIVE |...| ctlplane=192.0.2.9 |
| ab6439e8-...| overcloud-controller-1 | ACTIVE |...| ctlplane=192.0.2.10 |
| 82e12f81-...| overcloud-controller-2 | ACTIVE |...| ctlplane=192.0.2.11 |
| 53402a35-...| overcloud-novacompute-0 | ACTIVE |...| ctlplane=192.0.2.8 |
+----------...+-------------------------+--------+...+---------------------+
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll use the &lt;code>ctlplane&lt;/code> address to log into each host as the
&lt;code>heat-admin&lt;/code> user. For example, to log into my compute host:&lt;/p>
&lt;pre>&lt;code>$ ssh heat-admin@192.0.2.8
&lt;/code>&lt;/pre>
&lt;h2 id="accessing-the-overcloud-openstack-environment">Accessing the overcloud OpenStack environment&lt;/h2>
&lt;p>The file &lt;code>overcloudrc&lt;/code> on the undercloud host has administrative
credentials for the overcloud environment:&lt;/p>
&lt;pre>&lt;code>. overcloudrc
&lt;/code>&lt;/pre>
&lt;p>After sourcing in the overcloud credentials you can use OpenStack
clients to interact with your deployed cloud environment.&lt;/p>
&lt;h2 id="if-you-find-bugs">If you find bugs&lt;/h2>
&lt;p>If anything in the above process doesn&amp;rsquo;t work as described or
expected, feel free to visit the &lt;code>#rdo&lt;/code> channel on &lt;a href="https://freenode.net/">freenode&lt;/a>, or
open a bug report on the &lt;a href="https://github.com/redhat-openstack/tripleo-quickstart/issues">issue tracker&lt;/a>.&lt;/p></content></item><item><title>Ansible 2.0: New OpenStack modules</title><link>https://blog.oddbit.com/post/2015-10-26-ansible-20-new-openstack-modul/</link><pubDate>Mon, 26 Oct 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-10-26-ansible-20-new-openstack-modul/</guid><description>This is the second in a loose sequence of articles looking at new features in Ansible 2.0. In the previous article I looked at the Docker connection driver. In this article, I would like to provide an overview of the new-and-much-improved suite of modules for interacting with an OpenStack environment, and provide a few examples of their use.
In versions of Ansible prior to 2.0, there was a small collection of OpenStack modules.</description><content>&lt;p>This is the second in a loose sequence of articles looking at new
features in Ansible 2.0. In the previous article I looked at the
&lt;a href="https://blog.oddbit.com/post/2015-10-13-ansible-20-the-docker-connecti/">Docker connection driver&lt;/a>. In this article, I would like to
provide an overview of the new-and-much-improved suite of modules for
interacting with an &lt;a href="http://www.openstack.org/">OpenStack&lt;/a> environment, and provide a few
examples of their use.&lt;/p>
&lt;p>In versions of Ansible prior to 2.0, there was a small collection of
OpenStack modules. There was the minimum necessary to boot a Nova
instance:&lt;/p>
&lt;ul>
&lt;li>&lt;code>glance_image.py&lt;/code>&lt;/li>
&lt;li>&lt;code>keystone_user.py&lt;/code>&lt;/li>
&lt;li>&lt;code>nova_compute.py&lt;/code>&lt;/li>
&lt;li>&lt;code>nova_keypair.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>And a collection of modules for interacting with &lt;a href="https://wiki.openstack.org/wiki/Neutron">Neutron&lt;/a> (previously
Quantum):&lt;/p>
&lt;ul>
&lt;li>&lt;code>quantum_floating_ip_associate.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_floating_ip.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_network.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router_gateway.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router_interface.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_router.py&lt;/code>&lt;/li>
&lt;li>&lt;code>quantum_subnet.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>While functional, these modules did not provide very comprehensive
coverage of even basic OpenStack services, and they suffered from
having a great deal of duplicated code (which made ensuring
consistent behavior across all the modules more difficult). The
behavior of these modules was not always what you would expect (e.g.,
the &lt;code>nova_compute&lt;/code> module would return information in different forms
depending on whether it had to create an instance or not).&lt;/p>
&lt;h2 id="throwing-shade">Throwing Shade&lt;/h2>
&lt;p>The situation is much improved in Ansible 2.0, which introduces a new
suite of OpenStack modules in which the common code has been factored
out into the &lt;a href="https://pypi.python.org/pypi/shade">Shade&lt;/a> project, a Python package that provides a
simpler interface to OpenStack than is available using the native
clients. Collecting this code in one place will help ensure both that
these Ansible modules share consistent behavior and that they are
easier to maintain.&lt;/p>
&lt;p>There are modules for managing Keystone:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_auth.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_user_group.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_user.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Glance:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_image_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_image.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Cinder:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_server_volume.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_volume.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Nova:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_keypair.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_nova_flavor.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server_actions.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_server.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Ironic:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_ironic_node.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_ironic.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Neutron and Nova Networking:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_floating_ip.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_network.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_networks_facts.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_port.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_router.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_security_group.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_security_group_rule.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_subnet.py&lt;/code>&lt;/li>
&lt;li>&lt;code>os_subnets_facts.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>and Swift:&lt;/p>
&lt;ul>
&lt;li>&lt;code>os_object.py&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="authentication">Authentication&lt;/h2>
&lt;p>Shade uses the &lt;a href="https://pypi.python.org/pypi/os-client-config/">os-client-config&lt;/a> library to configure
authentication credentials for your OpenStack environment.&lt;/p>
&lt;p>In the absence of any authentication information provided in your
Ansible playbook, these modules will attempt to use the standard suite
of &lt;code>OS_*&lt;/code> variables (&lt;code>OS_USERNAME&lt;/code>, &lt;code>OS_PASSWORD&lt;/code>, etc). This is fine
for testing, but you usually want to provide some sort of
authentication configuration in your Ansible environment.&lt;/p>
&lt;p>You can provide credentials directly in your plays by providing an
&lt;code>auth&lt;/code> argument to any of the modules. For example:&lt;/p>
&lt;pre>&lt;code>- os_image:
auth:
auth_url: http://openstack.local:5000/v2.0
username: admin
password: secret
project_name: admin
[...]
&lt;/code>&lt;/pre>
&lt;p>But that can get complicated, especially if you are maintaining
multiple sets of credentials. The &lt;code>shade&lt;/code> library allows you to
manage credentials in a file named (by default) &lt;code>clouds.yml&lt;/code>, which
&lt;code>shade&lt;/code> searches for in:&lt;/p>
&lt;ul>
&lt;li>The current directory&lt;/li>
&lt;li>&lt;code>$HOME/.config/openstack/&lt;/code>&lt;/li>
&lt;li>&lt;code>/etc/xdg/openstack/&lt;/code>&lt;/li>
&lt;li>&lt;code>/etc/openstack&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>This file may contain credentials for one or more cloud environments,
for example:&lt;/p>
&lt;pre>&lt;code>clouds:
testing:
auth:
auth_url: http://openstack.local:5000/v2.0
username: admin
password: secret
project_name: admin
&lt;/code>&lt;/pre>
&lt;p>If you have the above in &lt;code>clouds.yml&lt;/code> along with your playbook, the
above &lt;code>os_image&lt;/code> example can be rewritten as:&lt;/p>
&lt;pre>&lt;code>- os_image:
cloud: testing
[...]
&lt;/code>&lt;/pre>
&lt;h2 id="return-values">Return values&lt;/h2>
&lt;p>The new modules all return useful information about the objects they
have created. For example, if you create a network using
&lt;a href="http://docs.ansible.com/ansible/os_network_module.html">os_network&lt;/a> and register that result:&lt;/p>
&lt;pre>&lt;code>- os_network:
cloud: testing
name: mynetwork
register: mynetwork
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll get back a dictionary containing a top-level &lt;code>id&lt;/code> attribute,
which is the UUID of the created network, along with a &lt;code>network&lt;/code>
attribute containing a dictionary of information about the created
object. The &lt;a href="http://docs.ansible.com/ansible/debug_module.html">debug&lt;/a> module is an excellent tool for exploring these
return values. If we put the following in our playbook immediately
after the above task:&lt;/p>
&lt;pre>&lt;code>- debug:
var: mynetwork
&lt;/code>&lt;/pre>
&lt;p>We would get output that looks something like:&lt;/p>
&lt;pre>&lt;code>ok: [localhost] =&amp;gt; {
&amp;quot;changed&amp;quot;: false,
&amp;quot;mynetwork&amp;quot;: {
&amp;quot;changed&amp;quot;: true,
&amp;quot;id&amp;quot;: &amp;quot;02b77e32-794a-4102-ab1b-1b90e6d4d92f&amp;quot;,
&amp;quot;invocation&amp;quot;: {
&amp;quot;module_args&amp;quot;: {
&amp;quot;cloud&amp;quot;: &amp;quot;testing&amp;quot;,
&amp;quot;name&amp;quot;: &amp;quot;mynetwork&amp;quot;
},
&amp;quot;module_name&amp;quot;: &amp;quot;os_network&amp;quot;
},
&amp;quot;network&amp;quot;: {
&amp;quot;admin_state_up&amp;quot;: true,
&amp;quot;id&amp;quot;: &amp;quot;02b77e32-794a-4102-ab1b-1b90e6d4d92f&amp;quot;,
&amp;quot;mtu&amp;quot;: 0,
&amp;quot;name&amp;quot;: &amp;quot;mynetwork&amp;quot;,
&amp;quot;provider:network_type&amp;quot;: &amp;quot;vxlan&amp;quot;,
&amp;quot;provider:physical_network&amp;quot;: null,
&amp;quot;provider:segmentation_id&amp;quot;: 79,
&amp;quot;router:external&amp;quot;: false,
&amp;quot;shared&amp;quot;: false,
&amp;quot;status&amp;quot;: &amp;quot;ACTIVE&amp;quot;,
&amp;quot;subnets&amp;quot;: [],
&amp;quot;tenant_id&amp;quot;: &amp;quot;349a8b95c5ad4a3383149f65f8c44cff&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>I have written a set of basic &lt;a href="https://github.com/ansible/ansible/pull/12875">integration tests&lt;/a> for these modules.
I hope the pull request is merged, but even if not it provides an
example of how to make use of many of these new modules.&lt;/p>
&lt;p>I&amp;rsquo;d like to present a few brief examples here to give you a sense of
what working with the new modules is like.&lt;/p>
&lt;h3 id="uploading-an-image-to-glance">Uploading an image to Glance&lt;/h3>
&lt;p>The &lt;a href="http://docs.ansible.com/ansible/os_image_module.html">os_image&lt;/a> module is used to upload an image to Glance. Assuming
that you have file named &lt;code>cirros.qcow2&lt;/code> available locally, this will
create an image named &lt;code>cirros&lt;/code> in Glance:&lt;/p>
&lt;pre>&lt;code>- os_image:
cloud: testing
name: cirros
state: present
disk_format: qcow2
container_format: bare
filename: cirros.qcow2
&lt;/code>&lt;/pre>
&lt;h3 id="booting-a-nova-server">Booting a Nova server&lt;/h3>
&lt;p>The &lt;a href="http://docs.ansible.com/ansible/os_server_module.html">os_server&lt;/a> module, which is used for booting virtual servers
(&amp;ldquo;instances&amp;rdquo;) in Nova, replaces the &lt;a href="http://docs.ansible.com/ansible/nova_compute_module.html">nova_compute&lt;/a> module available
in Ansible versions before 2.0:&lt;/p>
&lt;pre>&lt;code>- name: create a nova server
os_server:
cloud: testing
name: myserver
state: present
nics:
- net-name: private
image: cirros
flavor: m1.small
key_name: my_ssh_key
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>nics&lt;/code> parameter can accept net names, net ids, port names, and
port ids. So you could also do this (assuming you were attaching your
server to two different tenant networks):&lt;/p>
&lt;pre>&lt;code>nics:
- net-id: c875770c-a20b-45b5-a9da-5aca97153053
- net-name: private
&lt;/code>&lt;/pre>
&lt;p>The above examples are using a YAML list of dictionaries to provide
the information. You can also pass in a comma-delimited key=value
string, like this:&lt;/p>
&lt;pre>&lt;code>nics: net-name=private,net-name=database
&lt;/code>&lt;/pre>
&lt;p>This syntax is particular useful if you are running ad-hoc commands on
the command line:&lt;/p>
&lt;pre>&lt;code>ansible localhost -m os_server -a '
cloud=testing name=myserver nics=net-name=private
image=cirros flavor=m1.small key_name=my_ssh_key'
&lt;/code>&lt;/pre>
&lt;h3 id="adding-a-nova-server-to-your-ansible-inventory">Adding a Nova server to your Ansible inventory&lt;/h3>
&lt;p>I&amp;rsquo;d like to conclude this post with a longer example, that
demonstrates how you can use the &lt;a href="http://docs.ansible.com/ansible/add_host_module.html">add_host&lt;/a> module to add a freshly
created server to your inventory, and then target that new server in
your playbook. I&amp;rsquo;ve split up this playbook with commentary; in
practice, the pre-formatted text in this section would all be in a
single playbook (like &lt;a href="playbook.yml">this&lt;/a>).&lt;/p>
&lt;pre>&lt;code>- hosts: localhost
tasks:
&lt;/code>&lt;/pre>
&lt;p>This first task boots the server. The values for &lt;code>image&lt;/code>, &lt;code>nics&lt;/code>, and
`key_name will need to be adjusted for your environment.&lt;/p>
&lt;pre>&lt;code> - os_server:
cloud: testing
name: myserver
image: centos-7-atomic
nics:
- net-name: private
flavor: m1.small
key_name: lars
auto_ip: true
register: myserver
&lt;/code>&lt;/pre>
&lt;p>This &lt;code>debug&lt;/code> entry simply shows us what values were returned in the
&lt;code>myserver&lt;/code> variable.&lt;/p>
&lt;pre>&lt;code> - debug:
var: myserver
&lt;/code>&lt;/pre>
&lt;p>Now we add the new host to our Ansible inventory. For this to work,
you need to have assigned a floating ip to the server (either using
&lt;code>auto_ip&lt;/code>, as in this example, or by assigning one explicitly), and
you need to be running this playbook somewhere that has a route to the
floating ip address.&lt;/p>
&lt;pre>&lt;code> - add_host:
name: myserver
groups: openstack
ansible_host: &amp;quot;{{myserver.server.public_v4}}&amp;quot;
ansible_user: centos
ansible_become: true
&lt;/code>&lt;/pre>
&lt;p>Note that in the above play you can&amp;rsquo;t use information from the
inventory because that new host won&amp;rsquo;t exist in the inventory until
&lt;em>after&lt;/em> this play completes.&lt;/p>
&lt;p>We&amp;rsquo;ll need to wait for the server to finish booting and provisioning
before we are able to target it with ansible. A typical cloud image
is configured to run &lt;a href="https://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> when it boots, which will take
care of a number of initial configuration tasks, including
provisioning the ssh key we configured using &lt;code>os_server&lt;/code>. Until this
process is complete, we won&amp;rsquo;t have remote access to the server.&lt;/p>
&lt;p>We can&amp;rsquo;t use the &lt;code>wait_for&lt;/code> module because that will only
check for an open port. Instead, we use a &lt;a href="http://docs.ansible.com/ansible/playbooks_loops.html#do-until-loops">do-until loop&lt;/a> to
wait until we are able to successfully run a command on the server via
ssh.&lt;/p>
&lt;pre>&lt;code> - command: &amp;gt;
ssh -o BatchMode=yes
centos@{{myserver.server.public_v4}} true
register: result
until: result|success
retries: 300
delay: 5
&lt;/code>&lt;/pre>
&lt;p>Now that we have added the new server to our inventory
we can target it in subsequent plays (such as this one):&lt;/p>
&lt;pre>&lt;code>- hosts: myserver
tasks:
- service:
name: docker
state: running
enabled: true
&lt;/code>&lt;/pre></content></item><item><title>Migrating Cinder volumes between OpenStack environments using shared NFS storage</title><link>https://blog.oddbit.com/post/2015-09-29-migrating-cinder-volumes-betwe/</link><pubDate>Tue, 29 Sep 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-09-29-migrating-cinder-volumes-betwe/</guid><description>Many of the upgrade guides for OpenStack focus on in-place upgrades to your OpenStack environment. Some organizations may opt for a less risky (but more hardware intensive) option of setting up a parallel environment, and then migrating data into the new environment. In this article, we look at how to use Cinder backups with a shared NFS volume to facilitate the migration of Cinder volumes between two different OpenStack environments.</description><content>&lt;p>Many of the upgrade guides for OpenStack focus on in-place upgrades to
your OpenStack environment. Some organizations may opt for a less
risky (but more hardware intensive) option of setting up a parallel
environment, and then migrating data into the new environment. In
this article, we look at how to use Cinder backups with a shared NFS
volume to facilitate the migration of Cinder volumes between two
different OpenStack environments.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>This is how we&amp;rsquo;re going to proceed:&lt;/p>
&lt;p>In the source environment:&lt;/p>
&lt;ol>
&lt;li>Configure Cinder for NFS backups&lt;/li>
&lt;li>Create a backup&lt;/li>
&lt;li>Export the backup metadata&lt;/li>
&lt;/ol>
&lt;p>In the target environment:&lt;/p>
&lt;ol>
&lt;li>Configure Cinder for NFS backups&lt;/li>
&lt;li>Import the backup metadata&lt;/li>
&lt;li>Create a new volume matching the size of the source volume&lt;/li>
&lt;li>Restore the backup to the new volume&lt;/li>
&lt;/ol>
&lt;h2 id="cinder-configuration">Cinder configuration&lt;/h2>
&lt;p>We&amp;rsquo;ll be using the NFS backup driver for cinder, which means
&lt;code>cinder.conf&lt;/code> must contain:&lt;/p>
&lt;pre>&lt;code>backup_driver=cinder.backup.drivers.nfs
&lt;/code>&lt;/pre>
&lt;p>And you need to configure an NFS share to use for backups:&lt;/p>
&lt;pre>&lt;code>backup_share=fileserver:/vol/backups
&lt;/code>&lt;/pre>
&lt;p>Cinder in both environments should be pointing at the same
&lt;code>backup_share&lt;/code>. This is how we make backups made in the source
environment available in the target environment &amp;ndash; they will both have
access to the same storage, so that we only need to copy the metadata
into the target environment.&lt;/p>
&lt;p>After making changes to your Cinder configuration
you will need to restart Cinder. If you are using RDO or RHEL-OSP,
this is:&lt;/p>
&lt;pre>&lt;code>openstack-service restart cinder
&lt;/code>&lt;/pre>
&lt;h2 id="creating-a-backup">Creating a backup&lt;/h2>
&lt;p>Assume we have a volume named &lt;code>testvol&lt;/code> that is currently attached to
a running Nova server. The output of &lt;code>cinder list&lt;/code> looks like:&lt;/p>
&lt;pre>&lt;code>$ cinder list
+----------...+--------+--------------+------+...+----------+-------------...+
| ID ...| Status | Display Name | Size |...| Bootable | Attached to...|
+----------...+--------+--------------+------+...+----------+-------------...+
| bec9b02f-...| in-use | testvol | 1 |...| false | d97e9193-cf2...|
+----------...+--------+--------------+------+...+----------+-------------...+
&lt;/code>&lt;/pre>
&lt;p>We can try to create a backup of this using &lt;code>cinder backup-create&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-create testvol
&lt;/code>&lt;/pre>
&lt;p>But this will fail because the volume is currently attached to a Nova
server:&lt;/p>
&lt;pre>&lt;code>ERROR: Invalid volume: Volume to be backed up must be available
(HTTP 400) (Request-ID: req-...)
&lt;/code>&lt;/pre>
&lt;p>There are two ways we can deal with this:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>We can pass the &lt;code>--force&lt;/code> flag to &lt;code>cinder backup-create&lt;/code>, which
will allow the backup to continue even if the source volume is
attached. This should be done with care, because the on-disk
filesystem may not be in consistent state.&lt;/p>
&lt;p>The &lt;code>--force&lt;/code> flag was introduced in OpenStack Liberty. If you
are using an earlier OpenStack release you will need to use the
following procedure.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We can make the volume available by detaching it from the server.
In this case, you probably want to shut down the server first:&lt;/p>
&lt;pre>&lt;code> $ nova stop d97e9193-cf2c-41c4-afa2-fdd201b575d9
Request to stop server d97e9193-cf2c-41c4-afa2-fdd201b575d9 has
been accepted.
&lt;/code>&lt;/pre>
&lt;p>And then detach the volume:&lt;/p>
&lt;pre>&lt;code> $ nova volume-detach \
d97e9193-cf2c-41c4-afa2-fdd201b575d9 \
bec9b02f-f66f-4f15-a254-9acebd9c7c34
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Now the backup will start successfully:&lt;/p>
&lt;pre>&lt;code> # cinder backup-create testvol
+-----------+--------------------------------------+
| Property | Value |
+-----------+--------------------------------------+
| id | 96128a75-e143-4d8a-9b93-e246af8e6a7d |
| name | None |
| volume_id | bec9b02f-f66f-4f15-a254-9acebd9c7c34 |
+-----------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;p>At this point, you can use the &lt;code>cinder backup-list&lt;/code> command to see the
status of the backup. Initially it will be in state &lt;code>creating&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-list
+----------...+------------...+----------+------+------+...+----------------...+
| ID ...| Volume ID ...| Status | Name | Size |...| Container ...|
+----------...+------------...+----------+------+------+...+----------------...+
| 96128a75-...| bec9b02f-f6...| creating | - | 1 |...| 96/12/96128a75-...|
+----------...+------------...+----------+------+------+...+----------------...+
&lt;/code>&lt;/pre>
&lt;p>When the backup is finished, the status will read &lt;code>available&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-list
+----------...+------------...+-----------+------+------+...+----------------...+
| ID ...| Volume ID ...| Status | Name | Size |...| Container ...|
+----------...+------------...+-----------+------+------+...+----------------...+
| 96128a75-...| bec9b02f-f6...| available | - | 1 |...| 96/12/96128a75-...|
+----------...+------------...+-----------+------+------+...+----------------...+
&lt;/code>&lt;/pre>
&lt;p>At this point, you may want to re-attach the volume to your Nova server and
restart the server:&lt;/p>
&lt;pre>&lt;code> $ nova volume-attach \
d97e9193-cf2c-41c4-afa2-fdd201b575d9 \
bec9b02f-f66f-4f15-a254-9acebd9c7c34
$ nova start d97e9193-cf2c-41c4-afa2-fdd201b575d9
&lt;/code>&lt;/pre>
&lt;h2 id="exporting-the-backup">Exporting the backup&lt;/h2>
&lt;p>Now that we have successfully created the backup, we need to export
the Cinder metadata regarding the backup using the &lt;code>cinder backup-export&lt;/code> command (which can only be run by a user with &lt;code>admin&lt;/code>
privileges):&lt;/p>
&lt;pre>&lt;code>$ cinder backup-export 96128a75-e143-4d8a-9b93-e246af8e6a7d
&lt;/code>&lt;/pre>
&lt;p>This will return something like the following:&lt;/p>
&lt;pre>&lt;code>+----------------+------------------------------------------------------------------------------+
| Property | Value |
+----------------+------------------------------------------------------------------------------+
| backup_service | cinder.backup.drivers.nfs |
| backup_url | eyJzdGF0dXMiOiAiYXZhaWxhYmxlIiwgIm9iamVjdF9jb3VudCI6IDIsICJkZWxldGVkX2F0Ijog |
| | bnVsbCwgInNlcnZpY2VfbWV0YWRhdGEiOiAiYmFja3VwIiwgInVzZXJfaWQiOiAiYTY1MzQ5NzU5 |
| | YjZmNGVjNWEwYmIwY2MzZmViMWU5ZmEiLCAic2VydmljZSI6ICJjaW5kZXIuYmFja3VwLmRyaXZl |
| | cnMubmZzIiwgImF2YWlsYWJpbGl0eV96b25lIjogIm5vdmEiLCAiZGVsZXRlZCI6IGZhbHNlLCAi |
| | Y3JlYXRlZF9hdCI6ICIyMDE1LTA5LTI5VDE4OjU5OjEwLjAwMDAwMCIsICJ1cGRhdGVkX2F0Ijog |
| | IjIwMTUtMDktMjlUMTg6NTk6MzEuMDAwMDAwIiwgImRpc3BsYXlfZGVzY3JpcHRpb24iOiBudWxs |
| | LCAicGFyZW50X2lkIjogbnVsbCwgImhvc3QiOiAiaWJtLWhzMjItMDMucmh0cy5lbmcuYnJxLnJl |
| | ZGhhdC5jb20iLCAiY29udGFpbmVyIjogIjk2LzEyLzk2MTI4YTc1LWUxNDMtNGQ4YS05YjkzLWUy |
| | NDZhZjhlNmE3ZCIsICJ2b2x1bWVfaWQiOiAiYmVjOWIwMmYtZjY2Zi00ZjE1LWEyNTQtOWFjZWJk |
| | OWM3YzM0IiwgImRpc3BsYXlfbmFtZSI6IG51bGwsICJmYWlsX3JlYXNvbiI6IG51bGwsICJwcm9q |
| | ZWN0X2lkIjogImJjYWUzM2JkZjViODRkYzlhYjljYTY1MThhNDM4NTYxIiwgImlkIjogIjk2MTI4 |
| | YTc1LWUxNDMtNGQ4YS05YjkzLWUyNDZhZjhlNmE3ZCIsICJzaXplIjogMX0= |
| | |
+----------------+------------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>That giant block of text labeled &lt;code>backup_url&lt;/code> is not, in fact, a URL.
In this case, the actual content is a base64 encoded JSON string. You
will need to copy the base64 data to your target OpenStack
environment. You can extract just the base64 data like this:&lt;/p>
&lt;pre>&lt;code>cinder backup-export 96128a75-e143-4d8a-9b93-e246af8e6a7d |
sed -n '/backup_url/,$ s/|.*| *\(.*\) |/\1/p'
&lt;/code>&lt;/pre>
&lt;p>Which will give you:&lt;/p>
&lt;pre>&lt;code>eyJzdGF0dXMiOiAiYXZhaWxhYmxlIiwgIm9iamVjdF9jb3VudCI6IDIsICJkZWxldGVkX2F0Ijog
bnVsbCwgInNlcnZpY2VfbWV0YWRhdGEiOiAiYmFja3VwIiwgInVzZXJfaWQiOiAiYTY1MzQ5NzU5
YjZmNGVjNWEwYmIwY2MzZmViMWU5ZmEiLCAic2VydmljZSI6ICJjaW5kZXIuYmFja3VwLmRyaXZl
cnMubmZzIiwgImF2YWlsYWJpbGl0eV96b25lIjogIm5vdmEiLCAiZGVsZXRlZCI6IGZhbHNlLCAi
Y3JlYXRlZF9hdCI6ICIyMDE1LTA5LTI5VDE4OjU5OjEwLjAwMDAwMCIsICJ1cGRhdGVkX2F0Ijog
IjIwMTUtMDktMjlUMTg6NTk6MzEuMDAwMDAwIiwgImRpc3BsYXlfZGVzY3JpcHRpb24iOiBudWxs
LCAicGFyZW50X2lkIjogbnVsbCwgImhvc3QiOiAiaWJtLWhzMjItMDMucmh0cy5lbmcuYnJxLnJl
ZGhhdC5jb20iLCAiY29udGFpbmVyIjogIjk2LzEyLzk2MTI4YTc1LWUxNDMtNGQ4YS05YjkzLWUy
NDZhZjhlNmE3ZCIsICJ2b2x1bWVfaWQiOiAiYmVjOWIwMmYtZjY2Zi00ZjE1LWEyNTQtOWFjZWJk
OWM3YzM0IiwgImRpc3BsYXlfbmFtZSI6IG51bGwsICJmYWlsX3JlYXNvbiI6IG51bGwsICJwcm9q
ZWN0X2lkIjogImJjYWUzM2JkZjViODRkYzlhYjljYTY1MThhNDM4NTYxIiwgImlkIjogIjk2MTI4
YTc1LWUxNDMtNGQ4YS05YjkzLWUyNDZhZjhlNmE3ZCIsICJzaXplIjogMX0=
&lt;/code>&lt;/pre>
&lt;p>While not critical to this process, it may be interesting to see that
this string actually decodes to:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;availability_zone&amp;quot;: &amp;quot;nova&amp;quot;,
&amp;quot;container&amp;quot;: &amp;quot;96/12/96128a75-e143-4d8a-9b93-e246af8e6a7d&amp;quot;,
&amp;quot;created_at&amp;quot;: &amp;quot;2015-09-29T18:59:10.000000&amp;quot;,
&amp;quot;deleted&amp;quot;: false,
&amp;quot;deleted_at&amp;quot;: null,
&amp;quot;display_description&amp;quot;: null,
&amp;quot;display_name&amp;quot;: null,
&amp;quot;fail_reason&amp;quot;: null,
&amp;quot;host&amp;quot;: &amp;quot;ibm-hs22-03.rhts.eng.brq.redhat.com&amp;quot;,
&amp;quot;id&amp;quot;: &amp;quot;96128a75-e143-4d8a-9b93-e246af8e6a7d&amp;quot;,
&amp;quot;object_count&amp;quot;: 2,
&amp;quot;parent_id&amp;quot;: null,
&amp;quot;project_id&amp;quot;: &amp;quot;bcae33bdf5b84dc9ab9ca6518a438561&amp;quot;,
&amp;quot;service&amp;quot;: &amp;quot;cinder.backup.drivers.nfs&amp;quot;,
&amp;quot;service_metadata&amp;quot;: &amp;quot;backup&amp;quot;,
&amp;quot;size&amp;quot;: 1,
&amp;quot;status&amp;quot;: &amp;quot;available&amp;quot;,
&amp;quot;updated_at&amp;quot;: &amp;quot;2015-09-29T18:59:31.000000&amp;quot;,
&amp;quot;user_id&amp;quot;: &amp;quot;a65349759b6f4ec5a0bb0cc3feb1e9fa&amp;quot;,
&amp;quot;volume_id&amp;quot;: &amp;quot;bec9b02f-f66f-4f15-a254-9acebd9c7c34&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;h2 id="importing-the-backup">Importing the backup&lt;/h2>
&lt;p>In the target OpenStack environment, you need to import the backup
metadata to make Cinder aware of the backup. You do this with the
&lt;code>cinder backup-import&lt;/code> command, which requires both a &lt;code>backup_service&lt;/code>
parameter and a &lt;code>backup_url&lt;/code>. These are the values produces by the
&lt;code>cinder backup-export&lt;/code> command in the previous step.&lt;/p>
&lt;p>Assuming that we have dumped the base64 data into a file named
&lt;code>metadata.txt&lt;/code>, we can import the metadata using the following
command:&lt;/p>
&lt;pre>&lt;code># cinder backup-import cinder.backup.drivers.nfs $(tr -d '\n' &amp;lt; metadata.txt)
+----------+--------------------------------------+
| Property | Value |
+----------+--------------------------------------+
| id | a23891b2-e757-4d8f-9623-3d982e5616cb |
| name | None |
+----------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>And now if we run &lt;code>cinder backup-list&lt;/code> we should see a new backup
available:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-list
+----------...+----------...+-----------+------+------+...+-----------...+
| ID ...| Volume ID...| Status | Name | Size |...| Container ...|
+----------...+----------...+-----------+------+------+...+-----------...+
| a23891b2-...| 0000-0000...| available | - | 1 |...| 96/12/9612...|
+----------...+----------...+-----------+------+------+...+-----------...+
&lt;/code>&lt;/pre>
&lt;h2 id="creating-a-new-volume">Creating a new volume&lt;/h2>
&lt;p>At this point, we could simply run &lt;code>cinder backup-restore&lt;/code> on the
target system, and Cinder would restore the data onto a new volume
owned by the &lt;code>admin&lt;/code> user. If you want to restore to a volume owned
by another user, it is easiest to first create the volume as that
user. You will want to make sure that the size is at least as large
as the source volume:&lt;/p>
&lt;pre>&lt;code>$ cinder create --display_name mydata 1
+---------------------------------------+--------------------------------------+
| Property | Value |
+---------------------------------------+--------------------------------------+
[...]
| id | 145277e1-4733-4374-9b9c-677cb5334379 |
[...]
+---------------------------------------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>Note that is is possible to transfer a volume between tenants using
the &lt;code>cinder transfer-create&lt;/code> and &lt;code>cinder transfer-accept&lt;/code> commands,
but I will not be covering that in this article.&lt;/p>
&lt;h2 id="restoring-the-backup">Restoring the backup&lt;/h2>
&lt;p>Now that we have created a target volume we can restore the data from
our backup:&lt;/p>
&lt;pre>&lt;code>$ cinder backup-restore --volume 145277e1-4733-4374-9b9c-677cb5334379 \
a23891b2-e757-4d8f-9623-3d982e5616cb
+-----------+--------------------------------------+
| Property | Value |
+-----------+--------------------------------------+
| backup_id | a23891b2-e757-4d8f-9623-3d982e5616cb |
| volume_id | 145277e1-4733-4374-9b9c-677cb5334379 |
+-----------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>While the backup is running the backup status will be &lt;code>restoring&lt;/code>:&lt;/p>
&lt;pre>&lt;code>+----------...+------------...+-----------+------+------+...+----------------...+
| ID ...| Volume ID ...| Status | Name | Size |...| Container ...|
+----------...+------------...+-----------+------+------+...+----------------...+
| a23891b2-...| 0000-0000-0...| restoring | - | 1 |...| 96/12/96128a75-...|
+----------...+------------...+-----------+------+------+...+----------------...+
&lt;/code>&lt;/pre>
&lt;p>When the backup is complete that status will be &lt;code>available&lt;/code>:&lt;/p>
&lt;pre>&lt;code>+----------...+------------...+-----------+------+------+...+----------------...+
| ID ...| Volume ID ...| Status | Name | Size |...| Container ...|
+----------...+------------...+-----------+------+------+...+----------------...+
| a23891b2-...| 0000-0000-0...| available | - | 1 |...| 96/12/96128a75-...|
+----------...+------------...+-----------+------+------+...+----------------...+
&lt;/code>&lt;/pre>
&lt;h2 id="verification">Verification&lt;/h2>
&lt;p>If you spawn a Nova server in your target environment and attach the
volume we just created, you should find that it contains the same data
as the source volume contained at the time of the backup.&lt;/p>
&lt;h2 id="for-more-information">For more information&lt;/h2>
&lt;p>The &lt;a href="http://docs.openstack.org/admin-guide-cloud/">Cloud Adminstrator Guide&lt;/a> has more information about
&lt;a href="http://docs.openstack.org/admin-guide-cloud/blockstorage_volume_backups.html">volume backups and restores&lt;/a> and &lt;a href="http://docs.openstack.org/admin-guide-cloud/blockstorage_volume_backups_export_import.html">managing backup
metadata&lt;/a>.&lt;/p></content></item><item><title>Provider external networks (in an appropriate amount of detail)</title><link>https://blog.oddbit.com/post/2015-08-13-provider-external-networks-det/</link><pubDate>Thu, 13 Aug 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-08-13-provider-external-networks-det/</guid><description>In Quantum in Too Much Detail, I discussed the architecture of a Neutron deployment in detail. Since that article was published, Neutron gained the ability to handle multiple external networks with a single L3 agent. While I wrote about that back in 2014, I covered the configuration side of it in much more detail than I discussed the underlying network architecture. This post addresses the architecture side.
The players This document describes the architecture that results from a particular OpenStack configuration, specifically:</description><content>&lt;p>In &lt;a href="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/">Quantum in Too Much Detail&lt;/a>, I discussed the architecture of a
Neutron deployment in detail. Since that article was published,
Neutron gained the ability to handle multiple external networks with a
single L3 agent. While I &lt;a href="https://blog.oddbit.com/post/2014-05-28-multiple-external-networks-wit/">wrote about that&lt;/a> back in 2014, I
covered the configuration side of it in much more detail than I
discussed the underlying network architecture. This post addresses
the architecture side.&lt;/p>
&lt;h2 id="the-players">The players&lt;/h2>
&lt;p>This document describes the architecture that results from a
particular OpenStack configuration, specifically:&lt;/p>
&lt;ul>
&lt;li>Neutron networking using VXLAN or GRE tunnels;&lt;/li>
&lt;li>A dedicated network controller;&lt;/li>
&lt;li>Two external networks&lt;/li>
&lt;/ul>
&lt;h2 id="the-lay-of-the-land">The lay of the land&lt;/h2>
&lt;p>This is a simplified architecture diagram of the network connectivity
in this scenario:&lt;/p>
&lt;p>Everything on the compute hosts is identical to &lt;a href="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/">my previous
article&lt;/a>, so I will only be discussing the network host here.&lt;/p>
&lt;p>For the purposes of this article, we have two external networks and
two internal networks defined:&lt;/p>
&lt;pre>&lt;code>$ neutron net-list
+--------------------------------------+-----------+----------...------------------+
| id | name | subnets ... |
+--------------------------------------+-----------+----------...------------------+
| 6f0a5622-4d2b-4e4d-b34a-09b70cacf3f1 | net1 | beb767f8-... 192.168.101.0/24 |
| 972f2853-2ba6-474d-a4be-a400d4e3dc97 | net2 | f6d0ca0f-... 192.168.102.0/24 |
| 12136507-9bbe-406f-b68b-151d2a78582b | external2 | 106db3d6-... 172.24.5.224/28 |
| 973a6eb3-eaf8-4697-b90b-b30315b0e05d | external1 | fe8e8193-... 172.24.4.224/28 |
+--------------------------------------+-----------+----------...------------------+
&lt;/code>&lt;/pre>
&lt;p>And two routers:&lt;/p>
&lt;pre>&lt;code>$ neutron router-list
+--------------------------------------+---------+-----------------------...-------------------+...
| id | name | external_gateway_info ... |...
+--------------------------------------+---------+-----------------------...-------------------+...
| 1b19e179-5d67-4d80-8449-bab42119a4c5 | router2 | {&amp;quot;network_id&amp;quot;: &amp;quot;121365... &amp;quot;172.24.5.226&amp;quot;}]} |...
| e2117de3-58ca-420d-9ac6-c4eccf5e7a53 | router1 | {&amp;quot;network_id&amp;quot;: &amp;quot;973a6e... &amp;quot;172.24.4.227&amp;quot;}]} |...
+--------------------------------------+---------+-----------------------...-------------------+...
&lt;/code>&lt;/pre>
&lt;p>And our logical connectivity is:&lt;/p>
&lt;pre>&lt;code>+---------+ +----------+ +-------------+
| | | | | |
| net1 +----&amp;gt; router1 +----&amp;gt; external1 |
| | | | | |
+---------+ +----------+ +-------------+
+---------+ +----------+ +-------------+
| | | | | |
| net2 +----&amp;gt; router2 +----&amp;gt; external2 |
| | | | | |
+---------+ +----------+ +-------------+
&lt;/code>&lt;/pre>
&lt;h2 id="router-attachments-to-integration-bridge">Router attachments to integration bridge&lt;/h2>
&lt;p>In the &lt;a href="https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/">legacy model&lt;/a>, in which an L3 agent supported a single
external network, the &lt;code>qrouter-...&lt;/code> namespaces that implement Neutron
routers were attached to both the integration bridge &lt;code>br-int&lt;/code> and the
external network bridge (the &lt;code>external_network_bridge&lt;/code> configuration
option from your &lt;code>l3_agent.ini&lt;/code>, often named &lt;code>br-ex&lt;/code>).&lt;/p>
&lt;p>In the provider network model, &lt;em>both&lt;/em> interfaces in a &lt;code>qrouter&lt;/code>
namespace are attached to the integration bridge. For the
configuration we&amp;rsquo;ve described above, the configuration of the
integration bridge ends up looking something like:&lt;/p>
&lt;pre>&lt;code>Bridge br-int
fail_mode: secure
Port &amp;quot;qvoc532d46c-33&amp;quot;
tag: 3
Interface &amp;quot;qvoc532d46c-33&amp;quot;
Port br-int
Interface br-int
type: internal
Port &amp;quot;qg-09e9da38-fb&amp;quot;
tag: 4
Interface &amp;quot;qg-09e9da38-fb&amp;quot;
type: internal
Port &amp;quot;qvo3ccea690-c2&amp;quot;
tag: 2
Interface &amp;quot;qvo3ccea690-c2&amp;quot;
Port &amp;quot;int-br-ex2&amp;quot;
Interface &amp;quot;int-br-ex2&amp;quot;
type: patch
options: {peer=&amp;quot;phy-br-ex2&amp;quot;}
Port &amp;quot;tapd2ff89e7-16&amp;quot;
tag: 2
Interface &amp;quot;tapd2ff89e7-16&amp;quot;
type: internal
Port patch-tun
Interface patch-tun
type: patch
options: {peer=patch-int}
Port &amp;quot;int-br-ex1&amp;quot;
Interface &amp;quot;int-br-ex1&amp;quot;
type: patch
options: {peer=&amp;quot;phy-br-ex1&amp;quot;}
Port &amp;quot;qr-affdbcee-5c&amp;quot;
tag: 3
Interface &amp;quot;qr-affdbcee-5c&amp;quot;
type: internal
Port &amp;quot;qr-b37877cd-42&amp;quot;
tag: 2
Interface &amp;quot;qr-b37877cd-42&amp;quot;
type: internal
Port &amp;quot;qg-19250d3f-5c&amp;quot;
tag: 1
Interface &amp;quot;qg-19250d3f-5c&amp;quot;
type: internal
Port &amp;quot;tap0881edf5-e5&amp;quot;
tag: 3
Interface &amp;quot;tap0881edf5-e5&amp;quot;
type: internal
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>qr-...&lt;/code> interface on each router is attached to an internal
network. The VLAN tag associated with this interface is whatever VLAN
Neutron has selected internally for the private network. In the above
output, these ports are on the network named &lt;code>net1&lt;/code>:&lt;/p>
&lt;pre>&lt;code>Port &amp;quot;qr-affdbcee-5c&amp;quot;
tag: 3
Interface &amp;quot;qr-affdbcee-5c&amp;quot;
type: internal
Port &amp;quot;tap0881edf5-e5&amp;quot;
tag: 3
Interface &amp;quot;tap0881edf5-e5&amp;quot;
type: internal
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>qr-affdbcee-5c&lt;/code> is &lt;code>router1&lt;/code>&amp;rsquo;s interface on that network, and
&lt;code>tap0881edf5-e5&lt;/code> is the port attached to a &lt;code>dhcp-...&lt;/code> namespace. The
same router is attached to the &lt;code>external1&lt;/code> network; this attachment is
represented by:&lt;/p>
&lt;pre>&lt;code>Port &amp;quot;qg-09e9da38-fb&amp;quot;
tag: 4
Interface &amp;quot;qg-09e9da38-fb&amp;quot;
type: internal
&lt;/code>&lt;/pre>
&lt;p>The external bridges are connected to the integration bridge using OVS
&amp;ldquo;patch&amp;rdquo; interfaces (the &lt;code>int-br-ex1&lt;/code> on the integration bridge and the
&lt;code>phy-br-ex1&lt;/code> interface on the &lt;code>br-ex1&lt;/code>).&lt;/p>
&lt;h2 id="from-here-to-there">From here to there&lt;/h2>
&lt;p>Connectivity between the &lt;code>qg-...&lt;/code> interface and the appropriate
external bridge (&lt;code>br-ex1&lt;/code> in this case) happens due to the VLAN tag
assigned on egress by the &lt;code>qg-...&lt;/code> interface and the following
OpenFlow rules associated with &lt;code>br-ex1&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-ex1
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=794.876s, table=0, n_packets=0, n_bytes=0, idle_age=794, priority=1 actions=NORMAL
cookie=0x0, duration=785.833s, table=0, n_packets=0, n_bytes=0, idle_age=785, priority=4,in_port=3,dl_vlan=4 actions=strip_vlan,NORMAL
cookie=0x0, duration=792.945s, table=0, n_packets=24, n_bytes=1896, idle_age=698, priority=2,in_port=3 actions=drop
&lt;/code>&lt;/pre>
&lt;p>Each of these rules contains some state information (like the
packet/byte counts), some conditions (like
&lt;code>priority=4,in_port=3,dl_vlan=4&lt;/code>) and one or more actions (like
&lt;code>actions=strip_vlan,NORMAL&lt;/code>). So, the second rule there matches
packets associated with VLAN tag 4 and strips the VLAN tag (after
which the packet is delivered to any physical interfaces that are
attached to this OVS bridge).&lt;/p>
&lt;p>Putting this all together:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>An outbound packet from a Nova server running on a compute node
enters via &lt;code>br-tun&lt;/code> (&lt;strong>H&lt;/strong>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Flow rules on &lt;code>br-tun&lt;/code> translate the tunnel id into an internal
VLAN tag.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The packet gets delivered to the &lt;code>qr-...&lt;/code> interface of the
appropriate router. (&lt;strong>O&lt;/strong>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The packet exits the &lt;code>qg-...&lt;/code> interface of the router (where it
is assigned the VLAN tag associated with the external network).
(&lt;strong>N&lt;/strong>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The packet is delivered to the external bridge, where a flow rule
strip the VLAN tag. (&lt;strong>P&lt;/strong>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The packet is sent out the physical interface associated with the
bridge.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="for-the-sake-of-completeness">For the sake of completeness&lt;/h2>
&lt;p>The second private network, &lt;code>net2&lt;/code>, is attached to &lt;code>router2&lt;/code> on the
&lt;code>qr-b37877cd-42&lt;/code> interface. It exits on the &lt;code>qg-19250d3f-5c&lt;/code>
interface, where packets will be assigned to VLAN 1:&lt;/p>
&lt;pre>&lt;code>Port &amp;quot;qr-b37877cd-42&amp;quot;
tag: 2
Interface &amp;quot;qr-b37877cd-42&amp;quot;
type: internal
Port &amp;quot;qg-19250d3f-5c&amp;quot;
tag: 1
Interface &amp;quot;qg-19250d3f-5c&amp;quot;
type: internal
&lt;/code>&lt;/pre>
&lt;p>The network interface configuration in the associated router namespace
looks like this:&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-1b19e179-5d67-4d80-8449-bab42119a4c5 ip a
30: qg-19250d3f-5c: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:01:e9:e3 brd ff:ff:ff:ff:ff:ff
inet 172.24.5.226/28 brd 172.24.5.239 scope global qg-19250d3f-5c
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe01:e9e3/64 scope link
valid_lft forever preferred_lft forever
37: qr-b37877cd-42: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:4c:6c:f2 brd ff:ff:ff:ff:ff:ff
inet 192.168.102.1/24 brd 192.168.102.255 scope global qr-b37877cd-42
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe4c:6cf2/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>OpenFlow rules attached to &lt;code>br-ex2&lt;/code> will match these packets:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-ex2
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=3841.678s, table=0, n_packets=0, n_bytes=0, idle_age=3841, priority=1 actions=NORMAL
cookie=0x0, duration=3831.396s, table=0, n_packets=0, n_bytes=0, idle_age=3831, priority=4,in_port=3,dl_vlan=1 actions=strip_vlan,NORMAL
cookie=0x0, duration=3840.085s, table=0, n_packets=26, n_bytes=1980, idle_age=3742, priority=2,in_port=3 actions=drop
&lt;/code>&lt;/pre>
&lt;p>We can see that the second rule here will patch traffic on VLAN 1
(&lt;code>priority=4,in_port=3,dl_vlan=1&lt;/code>) and strip the VLAN tag, after which
the packet will be delivered to any other interfaces attached to this
bridge.&lt;/p></content></item><item><title>In which we are amazed it doesn't all fall apart</title><link>https://blog.oddbit.com/post/2015-07-26-in-which-we-are-amazed-it-does/</link><pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-07-26-in-which-we-are-amazed-it-does/</guid><description>So, the Kilo release notes say:
nova-manage migrate-flavor-data But nova-manage says:
nova-manage db migrate_flavor_data But that says:
Missing arguments: max_number And the help says:
usage: nova-manage db migrate_flavor_data [-h] [--max-number &amp;lt;number&amp;gt;] Which indicates that &amp;ndash;max-number is optional, but whatever, so you try:
nova-manage db migrate_flavor_data --max-number 100 And that says:
Missing arguments: max_number So just for kicks you try:
nova-manage db migrate_flavor_data --max_number 100 And that says:
nova-manage: error: unrecognized arguments: --max_number So finally you try:</description><content>&lt;p>So, the Kilo release notes say:&lt;/p>
&lt;pre>&lt;code>nova-manage migrate-flavor-data
&lt;/code>&lt;/pre>
&lt;p>But nova-manage says:&lt;/p>
&lt;pre>&lt;code>nova-manage db migrate_flavor_data
&lt;/code>&lt;/pre>
&lt;p>But that says:&lt;/p>
&lt;pre>&lt;code>Missing arguments: max_number
&lt;/code>&lt;/pre>
&lt;p>And the help says:&lt;/p>
&lt;pre>&lt;code>usage: nova-manage db migrate_flavor_data [-h]
[--max-number &amp;lt;number&amp;gt;]
&lt;/code>&lt;/pre>
&lt;p>Which indicates that &amp;ndash;max-number is optional, but whatever, so you
try:&lt;/p>
&lt;pre>&lt;code>nova-manage db migrate_flavor_data --max-number 100
&lt;/code>&lt;/pre>
&lt;p>And that says:&lt;/p>
&lt;pre>&lt;code>Missing arguments: max_number
&lt;/code>&lt;/pre>
&lt;p>So just for kicks you try:&lt;/p>
&lt;pre>&lt;code>nova-manage db migrate_flavor_data --max_number 100
&lt;/code>&lt;/pre>
&lt;p>And that says:&lt;/p>
&lt;pre>&lt;code>nova-manage: error: unrecognized arguments: --max_number
&lt;/code>&lt;/pre>
&lt;p>So finally you try:&lt;/p>
&lt;pre>&lt;code>nova-manage db migrate_flavor_data 100
&lt;/code>&lt;/pre>
&lt;p>And holy poorly implement client, Batman, it works.&lt;/p></content></item><item><title>OpenStack Networking without DHCP</title><link>https://blog.oddbit.com/post/2015-06-26-openstack-networking-without-d/</link><pubDate>Fri, 26 Jun 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-06-26-openstack-networking-without-d/</guid><description>In an OpenStack environment, cloud-init generally fetches information from the metadata service provided by Nova. It also has support for reading this information from a configuration drive, which under OpenStack means a virtual CD-ROM device attached to your instance containing the same information that would normally be available via the metadata service.
It is possible to generate your network configuration from this configuration drive, rather than relying on the DHCP server provided by your OpenStack environment.</description><content>&lt;p>In an OpenStack environment, &lt;a href="https://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> generally fetches
information from the metadata service provided by Nova. It also has
support for reading this information from a &lt;em>configuration drive&lt;/em>,
which under OpenStack means a virtual CD-ROM device attached to your
instance containing the same information that would normally be
available via the metadata service.&lt;/p>
&lt;p>It is possible to generate your network configuration from this
configuration drive, rather than relying on the DHCP server provided
by your OpenStack environment. In order to do this you will need to
make the following changes to your Nova configuration:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>You must be using a subnet that does have a DHCP server. This
means that you have created it using &lt;code>neutron subnet-create --disable-dhcp ...&lt;/code>, or that you disabled DHCP on an existing
network using &lt;code>neutron net-update --disable-dhcp ...&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You must set &lt;code>flat_inject&lt;/code> to &lt;code>true&lt;/code> in &lt;code>/etc/nova/nova.conf&lt;/code>.
This causes Nova to embed network configuration information in the
meta-data embedded on the configuration drive.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You must ensure that &lt;code>injected_network_template&lt;/code> in
&lt;code>/etc/nova/nova.conf&lt;/code> points to an appropriately formatted
template.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Cloud-init expects the network configuration information to be
presented in the format of a Debian &lt;code>/etc/network/interfaces&lt;/code> file,
even if you&amp;rsquo;re using it on RHEL (or a derivative). The template is
rendered using the &lt;a href="http://jinja.pocoo.org/docs/dev/">Jinja2&lt;/a> template engine, and receives a
top-level key called &lt;code>interfaces&lt;/code> that contains a list of
dictionaries, one for each interface.&lt;/p>
&lt;p>A template similar to the following ought to be sufficient:&lt;/p>
&lt;pre>&lt;code>{% for interface in interfaces %}
auto {{ interface.name }}
iface {{ interface.name }} inet static
address {{ interface.address }}
netmask {{ interface.netmask }}
broadcast {{ interface.broadcast }}
gateway {{ interface.gateway }}
dns-nameservers {{ interface.dns }}
{% endfor %}
&lt;/code>&lt;/pre>
&lt;p>This will directly populate &lt;code>/etc/network/interfaces&lt;/code> on an Ubuntu
system, or will get translated into
&lt;code>/etc/sysconfig/network-scripts/ifcfg-eth0&lt;/code> on a RHEL system (a RHEL
environment can only configure a single network interface using this
mechanism).&lt;/p></content></item><item><title>Heat-kubernetes Demo with Autoscaling</title><link>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</link><pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-06-19-heatkubernetes-demo-with-autos/</guid><description>Next week is the Red Hat Summit in Boston, and I&amp;rsquo;ll be taking part in a Project Atomic presentation in which I will discuss various (well, two) options for deploying Atomic into an OpenStack environment, focusing on my heat-kubernetes templates.
As part of that presentation, I&amp;rsquo;ve put together a short demonstration video:
This shows off the autoscaling behavior available with recent versions of these templates (and also serves as a very brief introduction to working with Kubernetes).</description><content>&lt;p>Next week is the &lt;a href="http://www.redhat.com/summit/">Red Hat Summit&lt;/a> in Boston, and I&amp;rsquo;ll be taking part
in a &lt;a href="http://www.projectatomic.io/">Project Atomic&lt;/a> presentation in which I will discuss various
(well, two) options for deploying Atomic into an OpenStack
environment, focusing on my &lt;a href="https://github.com/projectatomic/heat-kubernetes/">heat-kubernetes&lt;/a> templates.&lt;/p>
&lt;p>As part of that presentation, I&amp;rsquo;ve put together a short demonstration video:&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>This shows off the autoscaling behavior available with recent versions
of these templates (and also serves as a very brief introduction to
working with Kubernetes).&lt;/p></content></item><item><title>Diagnosing problems with an OpenStack deployment</title><link>https://blog.oddbit.com/post/2015-03-09-diagnosing-problems-with-an-op/</link><pubDate>Mon, 09 Mar 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-03-09-diagnosing-problems-with-an-op/</guid><description>I recently had the chance to help a colleague debug some problems in his OpenStack installation. The environment was unique because it was booting virtualized aarch64 instances, which at the time did not have any PCI bus support&amp;hellip;which in turn precluded things like graphic consoles (i.e., VNC or SPICE consoles) for the Nova instances.
This post began life as an email summarizing the various configuration changes we made on the systems to get things up and running.</description><content>&lt;p>I recently had the chance to help a colleague debug some problems in
his OpenStack installation. The environment was unique because it was
booting virtualized &lt;a href="https://fedoraproject.org/wiki/Architectures/AArch64">aarch64&lt;/a> instances, which at the time did not
have any PCI bus support&amp;hellip;which in turn precluded things like graphic
consoles (i.e., VNC or SPICE consoles) for the Nova instances.&lt;/p>
&lt;p>This post began life as an email summarizing the various configuration
changes we made on the systems to get things up and running. After
writing it, I decided it presented an interesting summary of some
common (and maybe not-so-common) issues, so I am posting it here in
the hopes that other folks will find it interesting.&lt;/p>
&lt;h2 id="serial-console-configuration">Serial console configuration&lt;/h2>
&lt;h3 id="the-problem">The problem&lt;/h3>
&lt;p>We needed console access to the Nova instances in order to diagnose
some networking issues, but there was no VGA console support in the
virtual machines. Recent versions of Nova provide serial console
support, but do not provide any client-side tool for &lt;em>accessing&lt;/em> the
serial console.&lt;/p>
&lt;p>We wanted to:&lt;/p>
&lt;ul>
&lt;li>Correctly configure Nova to provide serial console support, and&lt;/li>
&lt;li>Get the &lt;a href="https://github.com/larsks/novaconsole">novaconsole&lt;/a> tool installed in order to access the serial
consoles.&lt;/li>
&lt;/ul>
&lt;h3 id="making-novaconsole-work">Making novaconsole work&lt;/h3>
&lt;p>In order to get &lt;code>novaconsole&lt;/code> installed we needed the
&lt;code>websocket-client&lt;/code> library, which is listed in &lt;code>requirements.txt&lt;/code> at
the top level of the &lt;code>novaconsole&lt;/code> source. Normally one would just
&lt;code>pip install .&lt;/code> from the source directory, but &lt;code>python-pip&lt;/code> was not
available on our platform.&lt;/p>
&lt;p>That wasn&amp;rsquo;t a big issue because we &lt;em>did&lt;/em> have &lt;code>python-setuptools&lt;/code>
available, so I was able to simply run (inside the &lt;code>novaconsole&lt;/code>
source directory):&lt;/p>
&lt;pre>&lt;code>python setup.py install
&lt;/code>&lt;/pre>
&lt;p>And now we had a &lt;code>/usr/bin/novaconsole&lt;/code> script, and we were able to
use it like this to connect to the console of a nova instance named
&amp;ldquo;test0&amp;rdquo;:&lt;/p>
&lt;pre>&lt;code>novaconsole test0
&lt;/code>&lt;/pre>
&lt;p>(For this to work you need appropriate Keystone credentials loaded in
your environment. You can also provide a websocket URL in lieu of an
instance name.)&lt;/p>
&lt;h3 id="configuration-changes-on-the-controller">Configuration changes on the controller&lt;/h3>
&lt;p>The controller did not have the &lt;code>openstack-nova-serialproxy&lt;/code> package
installed, which provides the &lt;code>nova-serialproxy&lt;/code> service. This service
provides the websocket endpoint used by clients, so without this
service you you won&amp;rsquo;t be able to connect to serial consoles.&lt;/p>
&lt;p>Installing the service was a simple matter of:&lt;/p>
&lt;pre>&lt;code>yum -y install openstack-nova-serialproxy
systemctl enable openstack-nova-serialproxy
systemctl start openstack-nova-serialproxy
&lt;/code>&lt;/pre>
&lt;h3 id="configuration-changes-on-the-compute-nodes">Configuration changes on the compute nodes&lt;/h3>
&lt;p>We also need to enable the serial console support on our compute nodes,
and we need to change the following configuration options in
&lt;code>nova.conf&lt;/code> in the &lt;code>serial_console&lt;/code> section:&lt;/p>
&lt;pre>&lt;code># Set this to 'true' to enable serial console support.
enabled=true
# Enabling serial console support means that spawning an instance
# causes qemu to open up a listening TCP socket for the serial
# console. This socket binds to the `listen` address. It
# defaults to 127.0.0.1, which will not permit a remote host --
# such as your controller -- from connecting to the port. Setting
# this to 0.0.0.0 means &amp;quot;listen on all available addresses&amp;quot;, which
# is *usually* what you want.
listen=0.0.0.0
# `proxyclient_address` is the address to which the
# nova-serialproxy service will connect to access serial consoles
# of instances located on this physical host. That means it needs
# to be an address of a local interface (and so this value will be
# unique to each compute host).
proxyclient_address=10.16.184.118
&lt;/code>&lt;/pre>
&lt;p>In a production deployment, we would also need to modify the
&lt;code>base_url&lt;/code> option in this section, which is used to generate the URLs
provided via the &lt;code>nova get-serial-console&lt;/code> command. With the default
configuration, the URLs will point to 127.0.0.1, which is fine as long
as we are running &lt;code>novaconsole&lt;/code> on the same host as
&lt;code>nova-serialproxy&lt;/code>.&lt;/p>
&lt;p>After making these changes, we need to restart nova-compute on all the
compute hosts:&lt;/p>
&lt;pre>&lt;code># openstack-service restart nova
&lt;/code>&lt;/pre>
&lt;p>&lt;em>And&lt;/em> we will need to re-deploy any running instances, because they
will still have sockets listening on 127.0.0.1.&lt;/p>
&lt;p>The network ports opened for the serial console service are controlled
by the &lt;code>port_range&lt;/code> setting in the &lt;code>serial_console&lt;/code> section. We must
permit connections to these ports from our controller. I added the
following rule with iptables:&lt;/p>
&lt;pre>&lt;code># iptables -I INPUT 1 -p tcp --dport 10000:20000 -j ACCEPT
&lt;/code>&lt;/pre>
&lt;p>In practice, we would probably want to limit this specifically to our
controller(s).&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="networking-on-the-controller">Networking on the controller&lt;/h2>
&lt;h3 id="the-problem-1">The problem&lt;/h3>
&lt;p>Nova instances were not successfully obtaining ip addresses from the
Nova-managed DHCP service.&lt;/p>
&lt;h3 id="selinux-and-the-case-of-the-missing-interfaces">Selinux and the case of the missing interfaces&lt;/h3>
&lt;p>When I first looked at the system, it was obvious that something
fundamental was broken because the Neutron routers were missing
interfaces.&lt;/p>
&lt;p>Each neutron router is realized as a network namespace on the
network host. We can see these namespaces with the &lt;code>ip netns&lt;/code>
command:&lt;/p>
&lt;pre>&lt;code># ip netns
qrouter-42389195-c8c1-4d68-a16c-3937453f149d
qdhcp-d2719d67-fd00-4620-be00-ea8525dc6524
&lt;/code>&lt;/pre>
&lt;p>We can use the &lt;code>ip netns exec&lt;/code> command to run commands inside the
router namespace. For instance, we can run the following to see a
list of network interfaces inside the namespace:&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-42389195-c8c1-4d68-a16c-3937453f149d \
ip addr show
&lt;/code>&lt;/pre>
&lt;p>For a router we would expect to see something like this:&lt;/p>
&lt;pre>&lt;code>1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
18: qr-b3cd13d6-94: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:61:89:49 brd ff:ff:ff:ff:ff:ff
inet 10.0.0.1/24 brd 10.0.0.255 scope global qr-b3cd13d6-94
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe61:8949/64 scope link
valid_lft forever preferred_lft forever
19: qg-89591203-47: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether fa:16:3e:30:b5:05 brd ff:ff:ff:ff:ff:ff
inet 172.24.4.231/28 brd 172.24.4.239 scope global qg-89591203-47
valid_lft forever preferred_lft forever
inet 172.24.4.232/32 brd 172.24.4.232 scope global qg-89591203-47
valid_lft forever preferred_lft forever
inet6 fe80::f816:3eff:fe30:b505/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>But all I found was the loopback interface:&lt;/p>
&lt;pre>&lt;code>1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>After a few attempts to restore the router to health through API commands (such
as by clearing and re-setting the network gateway), I looked in the
logs for the &lt;code>neutron-l3-agent&lt;/code> service, which is the service
responsible for configuring the routers. There I found:&lt;/p>
&lt;pre>&lt;code>2015-02-20 17:16:52.324 22758 TRACE neutron.agent.l3_agent Stderr:
'Error: argument &amp;quot;qrouter-42389195-c8c1-4d68-a16c-3937453f149d&amp;quot; is
wrong: Invalid &amp;quot;netns&amp;quot; value\n\n'
&lt;/code>&lt;/pre>
&lt;p>This is weird because clearly a network namespace with a matching name
was available. When weird inexplicable errors happen, we often look
first to selinux, and indeed, running &lt;code>audit2allow -a&lt;/code> showed us that
neutron was apparently missing a privilege:&lt;/p>
&lt;pre>&lt;code>#============= neutron_t ==============
allow neutron_t unlabeled_t:file { read open };
&lt;/code>&lt;/pre>
&lt;p>After putting selinux in permissive mode&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> and restarting neutron
services, things looked a lot better. To completely restart neutron,
I usually do:&lt;/p>
&lt;pre>&lt;code>openstack-service stop neutron
neutron-ovs-cleanup
neutron-netns-cleanup
openstack-service start neutron
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>openstack-service&lt;/code> command is a wrapper over &lt;code>systemctl&lt;/code> or
&lt;code>chkconfig&lt;/code> and &lt;code>service&lt;/code> that operates on whatever openstack services
you have enabled on your host. Providing a additional arguments
limits the action to services that match that name, so in addition to
&lt;code>openstack-service stop neutron&lt;/code> you can do something like
&lt;code>openstack-service stop nova glance&lt;/code> to stop all Nova and Glance
services, etc.&lt;/p>
&lt;h3 id="iptables-and-the-case-of-the-missing-packets">Iptables and the case of the missing packets&lt;/h3>
&lt;p>After diagnosing the selinux issue noted above, virtual networking
layer looked fine, but we still weren&amp;rsquo;t able to get traffic between
the test instance and the router/dhcp server on the controller.&lt;/p>
&lt;p>Traffic was clearly traversing the VXLAN tunnels, as revealed by
running &lt;code>tcpdump&lt;/code> on both ends of the tunnel (where 4789 is the vxlan
port):&lt;/p>
&lt;pre>&lt;code>tcpdump -i eth0 -n port 4789
&lt;/code>&lt;/pre>
&lt;p>But that traffic was never reaching, e.g., the dhcp namespace.
Investigating the Open vSwitch (OVS) configuration on our host showed
that everything look correct; commands I use to look at things were:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>ovs-vsctl show&lt;/code> to look at the basic layout of switches and
interfaces,&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ovs-ofctl dump-flows &amp;lt;bridge&amp;gt;&lt;/code> to look at the openflow rules
associated with a particular OVS switch, and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ovs-dpctl-top&lt;/code>, which provides a top-like view of flow activity on
the OVS bridges.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Ultimately, it turns out that there were some iptables rule missing
from our configuration. On the host, looking for rules that match
vxlan traffic I found a single rule for vxlan traffic:&lt;/p>
&lt;pre>&lt;code># iptables -S | grep 4789
-A INPUT -s 10.16.184.117/32 -p udp -m multiport --dports 4789 ...
&lt;/code>&lt;/pre>
&lt;p>The compute node we were operating with was 10.16.184.118 (which is
not the address listed in the above rule), so vxlan traffic from this
host was being rejected by the kernel. I added a new rule to match
vxlan traffic from the compute host:&lt;/p>
&lt;pre>&lt;code># iptables -I INPUT 18 -s 10.16.184.118/32 -p udp -m multiport --dports 4789 ...
&lt;/code>&lt;/pre>
&lt;p>This seemed to take care of things, but it&amp;rsquo;s a bit of a mystery why
this wasn&amp;rsquo;t configured for us in the first place by the installer.
This may have been a bug in &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a>; we would need to do clean
re-deploy to verify this behavior.&lt;/p>
&lt;h3 id="access-to-floating-ip-addresses">Access to floating ip addresses&lt;/h3>
&lt;p>In order to access our instances using their floating ip addresses
from our host, we need a route to the floating ip network. The
easiest way to do this in a test environment, if you are happy with
host-only networking, is to assign interface &lt;code>br-ex&lt;/code> the address of
the default gateway for your floating ip network. The default
floating ip network configured by &lt;code>packstack&lt;/code> is 172.24.4.224/28, and
the gateway for that network is &lt;code>172.24.24.225&lt;/code>. We can assign this
address to &lt;code>br-ex&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code># ip addr add 172.24.4.225/28 dev br-ex
&lt;/code>&lt;/pre>
&lt;p>With this in place, connections to floating ips will route via br-ex,
which in turn is bridged to the external interface of your neutron
router.&lt;/p>
&lt;p>Setting the address by hand like this means it will be lost next time
we reboot. We can make this configuration persistent by modifying (or
creating)
&lt;code>/etc/sysconfig/network-scripts/ifcfg-br-ex&lt;/code> so that it looks like this:&lt;/p>
&lt;pre>&lt;code>DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROT=static
IPADDR=172.24.4.225
NETMASK=255.255.255.240
ONBOOT=yes
&lt;/code>&lt;/pre>
&lt;p>If you&amp;rsquo;re not able to map CIDR prefixes to dotted-quad netmasks in
your head, the &lt;code>ipcalc&lt;/code> tool is useful:&lt;/p>
&lt;pre>&lt;code>$ ipcalc -m 172.24.4.224/28
NETMASK=255.255.255.240
&lt;/code>&lt;/pre>
&lt;h2 id="the-state-of-things">The state of things&lt;/h2>
&lt;p>With all the above changes in place, we had a functioning OpenStack
environment.&lt;/p>
&lt;p>We could spawn an instance as the &amp;ldquo;demo&amp;rdquo; user:&lt;/p>
&lt;pre>&lt;code># . keystonerc_demo
# nova boot --image &amp;quot;rhelsa&amp;quot; --flavor m1.small example
&lt;/code>&lt;/pre>
&lt;p>Create a floating ip address:&lt;/p>
&lt;pre>&lt;code># nova floating-ip-create public
+--------------+-----------+----------+--------+
| Ip | Server Id | Fixed Ip | Pool |
+--------------+-----------+----------+--------+
| 172.24.4.233 | - | - | public |
+--------------+-----------+----------+--------+
&lt;/code>&lt;/pre>
&lt;p>Assign that address to our instance:&lt;/p>
&lt;pre>&lt;code># nova floating-ip-associate example 172.4.4.233
&lt;/code>&lt;/pre>
&lt;p>And finally we were able to access services on that instance (provided
that our security groups (and local iptables configuration on the
instance) permit access to that service).&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&amp;hellip;as a temporary measure, pending opening a bug report to get
things corrected so that this step would no longer be necessary.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></content></item><item><title>Installing nova-docker with devstack</title><link>https://blog.oddbit.com/post/2015-02-11-installing-novadocker-with-dev/</link><pubDate>Wed, 11 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-11-installing-novadocker-with-dev/</guid><description>This is a long-form response to this question, and describes how to get the nova-docker driver up running with devstack under Ubuntu 14.04 (Trusty). I wrote a similar post for Fedora 21, although that one was using the RDO Juno packages, while this one is using devstack and the upstream sources.
Getting started We&amp;rsquo;ll be using the Ubuntu 14.04 cloud image (because my test environment runs on OpenStack).
First, let&amp;rsquo;s install a few prerequisites:</description><content>&lt;p>This is a long-form response to &lt;a href="https://ask.openstack.org/en/question/60679/installing-docker-on-openstack-with-ubuntu/">this question&lt;/a>, and describes
how to get the &lt;a href="http://github.com/stackforge/nova-docker/">nova-docker&lt;/a> driver up running with &lt;a href="http://devstack.org/">devstack&lt;/a>
under Ubuntu 14.04 (Trusty). I wrote a &lt;a href="https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/">similar post&lt;/a> for Fedora
21, although that one was using the &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> Juno packages, while this
one is using &lt;a href="http://devstack.org/">devstack&lt;/a> and the upstream sources.&lt;/p>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>We&amp;rsquo;ll be using the &lt;a href="https://cloud-images.ubuntu.com/trusty/current/">Ubuntu 14.04 cloud image&lt;/a> (because my test
environment runs on &lt;a href="http://www.openstack.org/">OpenStack&lt;/a>).&lt;/p>
&lt;p>First, let&amp;rsquo;s install a few prerequisites:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get update
$ sudo apt-get -y install git git-review python-pip python-dev
&lt;/code>&lt;/pre>
&lt;p>And generally make sure things are up-to-date:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get -y upgrade
&lt;/code>&lt;/pre>
&lt;h2 id="installing-docker">Installing Docker&lt;/h2>
&lt;p>We need to install Docker if we&amp;rsquo;re going to use &lt;code>nova-docker&lt;/code>.&lt;/p>
&lt;p>Ubuntu 14.04 includes a fairly dated version of Docker, so I followed
&lt;a href="https://docs.docker.com/installation/ubuntulinux/#docker-maintained-package-installation">the instructions&lt;/a> on the Docker website for installing the current
version of Docker on Ubuntu; this ultimately got me:&lt;/p>
&lt;pre>&lt;code>$ sudo apt-get -y install lxc-docker
$ sudo docker version
Client version: 1.5.0
Client API version: 1.17
Go version (client): go1.4.1
Git commit (client): a8a31ef
OS/Arch (client): linux/amd64
Server version: 1.5.0
Server API version: 1.17
Go version (server): go1.4.1
Git commit (server): a8a31ef
&lt;/code>&lt;/pre>
&lt;p>Docker by default creates its socket (&lt;code>/var/run/docker.socket&lt;/code>) with
&lt;code>root:root&lt;/code> ownership. We&amp;rsquo;re going to be running devstack as the
&lt;code>ubuntu&lt;/code> user, so let&amp;rsquo;s change that by editing &lt;code>/etc/default/docker&lt;/code>
and setting:&lt;/p>
&lt;pre>&lt;code>DOCKER_OPTS='-G ubuntu'
&lt;/code>&lt;/pre>
&lt;p>And restart &lt;code>docker&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ sudo restart docker
&lt;/code>&lt;/pre>
&lt;p>And verify that we can access Docker as the &lt;code>ubuntu&lt;/code> user:&lt;/p>
&lt;pre>&lt;code>$ docker version
Client version: 1.5.0
Client API version: 1.17
[...]
&lt;/code>&lt;/pre>
&lt;h2 id="installing-nova-docker">Installing nova-docker&lt;/h2>
&lt;p>As the &lt;code>ubuntu&lt;/code> user, let&amp;rsquo;s get the &lt;code>nova-docker&lt;/code> source code:&lt;/p>
&lt;pre>&lt;code>$ git clone http://github.com/stackforge/nova-docker.git
$ cd nova-docker
&lt;/code>&lt;/pre>
&lt;p>As of this writing (&lt;code>HEAD&lt;/code> is &amp;ldquo;984900a Give some time for docker.stop
to work&amp;rdquo;), you need to apply &lt;a href="https://review.openstack.org/#/c/154750/">a patch&lt;/a> to &lt;code>nova-docker&lt;/code> to get it to
work with the current Nova &lt;code>master&lt;/code> branch:&lt;/p>
&lt;pre>&lt;code>$ git fetch https://review.openstack.org/stackforge/nova-docker \
refs/changes/50/154750/3 \
&amp;amp;&amp;amp; git checkout FETCH_HEAD
&lt;/code>&lt;/pre>
&lt;p>Once &lt;a href="https://review.openstack.org/#/c/154750/">that change&lt;/a> has merged (&lt;strong>update&lt;/strong>, 2015-02-12: the
patch has merged), this step should no longer be
necessary. With the patch we applied, we can install the
&lt;code>nova-docker&lt;/code> driver:&lt;/p>
&lt;pre>&lt;code>$ sudo pip install .
&lt;/code>&lt;/pre>
&lt;h2 id="configuring-devstack">Configuring devstack&lt;/h2>
&lt;p>Now we&amp;rsquo;re ready to get devstack up and running. Start by cloning the
repository:&lt;/p>
&lt;pre>&lt;code>$ git clone https://git.openstack.org/openstack-dev/devstack
$ cd devstack
&lt;/code>&lt;/pre>
&lt;p>Then create a &lt;code>local.conf&lt;/code> file with the following content:&lt;/p>
&lt;pre>&lt;code>[[local|localrc]]
ADMIN_PASSWORD=secret
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD
SERVICE_TOKEN=super-secret-admin-token
VIRT_DRIVER=novadocker.virt.docker.DockerDriver
DEST=$HOME/stack
SERVICE_DIR=$DEST/status
DATA_DIR=$DEST/data
LOGFILE=$DEST/logs/stack.sh.log
LOGDIR=$DEST/logs
# The default fixed range (10.0.0.0/24) conflicted with an address
# range I was using locally.
FIXED_RANGE=10.254.1.0/24
NETWORK_GATEWAY=10.254.1.1
# This enables Neutron, because that's how I roll.
disable_service n-net
enable_service q-svc
enable_service q-agt
enable_service q-dhcp
enable_service q-l3
enable_service q-meta
# I am disabling horizon (because I rarely use the web ui)
# and tempest in order to make the installer complete a
# little faster.
disable_service horizon
disable_service tempest
# Introduce glance to docker images
[[post-config|$GLANCE_API_CONF]]
[DEFAULT]
container_formats=ami,ari,aki,bare,ovf,ova,docker
# Configure nova to use the nova-docker driver
[[post-config|$NOVA_CONF]]
[DEFAULT]
compute_driver=novadocker.virt.docker.DockerDriver
&lt;/code>&lt;/pre>
&lt;p>This will result in things getting installed in subdirectories of
&lt;code>$HOME/stack&lt;/code>. We enable Neutron and leave pretty much everything
else set to default values.&lt;/p>
&lt;h2 id="start-the-installation">Start the installation&lt;/h2>
&lt;p>So, now we&amp;rsquo;re all ready to roll!&lt;/p>
&lt;pre>&lt;code>$ ./stack.sh
[Call Trace]
./stack.sh:151:source
/home/ubuntu/devstack/stackrc:665:die
[ERROR] /home/ubuntu/devstack/stackrc:665 Could not determine host ip address. See local.conf for suggestions on setting HOST_IP.
/home/ubuntu/devstack/functions-common: line 322: /home/ubuntu/stack/logs/error.log: No such file or directory
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;or not. This error happens if devstack is unable to turn your
hostname into an IP address. We can set &lt;code>HOST_IP&lt;/code> in our
environment:&lt;/p>
&lt;pre>&lt;code>$ HOST_IP=10.0.0.232 ./stack.sh
&lt;/code>&lt;/pre>
&lt;p>And then go grab a cup of coffee or something.&lt;/p>
&lt;h2 id="install-nova-docker-rootwrap-filters">Install nova-docker rootwrap filters&lt;/h2>
&lt;p>Once &lt;code>stack.sh&lt;/code> is finished running, we need to install a &lt;code>rootwrap&lt;/code>
configuration file for &lt;code>nova-docker&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ sudo cp nova-docker/etc/nova/rootwrap.d/docker.filters \
/etc/nova/rootwrap.d/
&lt;/code>&lt;/pre>
&lt;h2 id="starting-a-docker-container">Starting a Docker container&lt;/h2>
&lt;p>Now that our environment is up and running, we should be able to start
a container. We&amp;rsquo;ll start by grabbing some admin credentials for our
OpenStack environment:&lt;/p>
&lt;pre>&lt;code>$ . openrc admin
&lt;/code>&lt;/pre>
&lt;p>Next, we need an appropriate image; my &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">larsks/thttpd&lt;/a> image
is small (so it&amp;rsquo;s quick to download) and does not require any
interactive terminal (so it&amp;rsquo;s appropriate for nova-docker), so let&amp;rsquo;s
start with that:&lt;/p>
&lt;pre>&lt;code>$ docker pull larsks/thttpd
$ docker save larsks/thttpd |
glance image-create --name larsks/thttpd \
--is-public true --container-format docker \
--disk-format raw
&lt;/code>&lt;/pre>
&lt;p>And now we&amp;rsquo;ll boot it up. I like to do this as a non-admin user:&lt;/p>
&lt;pre>&lt;code>$ . openrc demo
$ nova boot --image larsks/thttpd --flavor m1.small test0
&lt;/code>&lt;/pre>
&lt;p>After a bit, we should see:&lt;/p>
&lt;pre>&lt;code>$ nova list
+----...+-------+--------+...+-------------+--------------------+
| ID ...| Name | Status |...| Power State | Networks |
+----...+-------+--------+...+-------------+--------------------+
| 0c3...| test0 | ACTIVE |...| Running | private=10.254.1.4 |
+----...+-------+--------+...+-------------+--------------------+
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s create a floating ip address:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-create
+------------+-----------+----------+--------+
| Ip | Server Id | Fixed Ip | Pool |
+------------+-----------+----------+--------+
| 172.24.4.3 | - | - | public |
+------------+-----------+----------+--------+
&lt;/code>&lt;/pre>
&lt;p>And assign it to our container:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-associate test0 172.24.4.3
&lt;/code>&lt;/pre>
&lt;p>And now access our service:&lt;/p>
&lt;pre>&lt;code>$ curl http://172.24.4.3
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Your web server is working&amp;lt;/title&amp;gt;
[...]
&lt;/code>&lt;/pre></content></item><item><title>Installing nova-docker on Fedora 21/RDO Juno</title><link>https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/</link><pubDate>Fri, 06 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-06-installing-nova-docker-on-fedo/</guid><description>This post comes about indirectly by a request on IRC in #rdo for help getting nova-docker installed on Fedora 21. I ran through the process from start to finish and decided to write everything down for posterity.
Getting started I started with the Fedora 21 Cloud Image, because I&amp;rsquo;m installing onto OpenStack and the cloud images include some features that are useful in this environment.
We&amp;rsquo;ll be using OpenStack packages from the RDO Juno repository.</description><content>&lt;p>This post comes about indirectly by a request on IRC in &lt;code>#rdo&lt;/code> for help getting &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> installed on Fedora 21. I ran through the process from start to finish and decided to write everything down for posterity.&lt;/p>
&lt;h2 id="getting-started">Getting started&lt;/h2>
&lt;p>I started with the &lt;a href="https://getfedora.org/en/cloud/download/">Fedora 21 Cloud Image&lt;/a>, because I&amp;rsquo;m
installing onto OpenStack and the cloud images include
some features that are useful in this environment.&lt;/p>
&lt;p>We&amp;rsquo;ll be using OpenStack packages from the &lt;a href="https://repos.fedorapeople.org/repos/openstack/openstack-juno/">RDO Juno&lt;/a> repository.
Because there is often some skew between the RDO packages and the
current Fedora selinux policy, we&amp;rsquo;re going to start by putting SELinux
into permissive mode (sorry, Dan):&lt;/p>
&lt;pre>&lt;code># setenforce 0
# sed -i '/^SELINUX=/ s/=.*/=permissive/' /etc/selinux/config
&lt;/code>&lt;/pre>
&lt;p>Next, install the RDO Juno repository:&lt;/p>
&lt;pre>&lt;code># yum -y install \
https://repos.fedorapeople.org/repos/openstack/openstack-juno/rdo-release-juno-1.noarch.rpm
&lt;/code>&lt;/pre>
&lt;p>And upgrade all our existing packages:&lt;/p>
&lt;pre>&lt;code># yum -y upgrade
&lt;/code>&lt;/pre>
&lt;h2 id="install-openstack">Install OpenStack&lt;/h2>
&lt;p>We&amp;rsquo;ll be using &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a> to install OpenStack onto this host.
Start by installing the package:&lt;/p>
&lt;pre>&lt;code># yum -y install openstack-packstack
&lt;/code>&lt;/pre>
&lt;p>And then run a &lt;code>--allinone&lt;/code> install, which sets up all OpenStack
services on a single host:&lt;/p>
&lt;pre>&lt;code># packstack --allinone
&lt;/code>&lt;/pre>
&lt;h2 id="install-nova-docker-prequisites">Install nova-docker prequisites&lt;/h2>
&lt;p>Once &lt;code>packstack&lt;/code> has completed successfully, we need to install some
prerequisites for &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a>.&lt;/p>
&lt;pre>&lt;code># yum -y install git python-pip python-pbr \
docker-io fedora-repos-rawhide
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>fedora-repos-rawhide&lt;/code> package provides a yum configuration for the
&lt;code>rawhide&lt;/code> repository (disabled by default). We&amp;rsquo;re going to need that
to pick up more recent versions of &lt;code>systemd&lt;/code> (because of &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1187882">this
bug&lt;/a>) and
&lt;code>python-six&lt;/code> (because &lt;code>nova-docker&lt;/code> needs the &lt;code>six.add_metaclass&lt;/code>
method):&lt;/p>
&lt;pre>&lt;code># yum --enablerepo=rawhide install python-six systemd
&lt;/code>&lt;/pre>
&lt;p>At this point, having upgraded &lt;code>systemd&lt;/code>, you should probably reboot:&lt;/p>
&lt;pre>&lt;code># reboot
&lt;/code>&lt;/pre>
&lt;h2 id="configure-docker">Configure Docker&lt;/h2>
&lt;p>Once things are up and running, we will expect the &lt;code>nova-compute&lt;/code>
service to launch Docker containers. In order for this to work, the
&lt;code>nova&lt;/code> user will need access to the Docker socket,
&lt;code>/var/run/docker.sock&lt;/code>. By default, this is owned by &lt;code>root:root&lt;/code> and
has mode &lt;code>660&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ls -l /var/run/docker.sock
srw-rw----. 1 root root 0 Feb 1 12:43 /var/run/docker.sock
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>nova-compute&lt;/code> service runs as the &lt;code>nova&lt;/code> user and will not have
access to that socket. There are a few ways of resolving this; an
expedient method is simply to make this socket owned by the &lt;code>nova&lt;/code>
group, which we can do with &lt;code>docker&lt;/code>&amp;rsquo;s &lt;code>-G&lt;/code> option.&lt;/p>
&lt;p>Edit &lt;code>/etc/sysconfig/docker&lt;/code>, and modify the &lt;code>OPTIONS=&lt;/code> line to look
like:&lt;/p>
&lt;pre>&lt;code>OPTIONS='--selinux-enabled -G nova'
&lt;/code>&lt;/pre>
&lt;p>Then enable and start the &lt;code>docker&lt;/code> service:&lt;/p>
&lt;pre>&lt;code># systemctl enable docker
# systemctl start docker
&lt;/code>&lt;/pre>
&lt;h2 id="install-nova-docker">Install nova-docker&lt;/h2>
&lt;p>Clone the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> repository:&lt;/p>
&lt;pre>&lt;code># git clone http://github.com/stackforge/nova-docker.git
# cd nova-docker
&lt;/code>&lt;/pre>
&lt;p>And check out the &lt;code>stable/juno&lt;/code> branch, since we&amp;rsquo;re operating with an
OpenStack Juno environment:&lt;/p>
&lt;pre>&lt;code># git checkout stable/juno
&lt;/code>&lt;/pre>
&lt;p>Now install the driver:&lt;/p>
&lt;pre>&lt;code># python setup.py install
&lt;/code>&lt;/pre>
&lt;h2 id="configure-nova">Configure Nova&lt;/h2>
&lt;p>Following the &lt;a href="https://github.com/stackforge/nova-docker/blob/master/README.rst">README&lt;/a> from &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a>, we need to modify
the Nova configuration to use the &lt;code>nova-docker&lt;/code> driver. Edit
&lt;code>/etc/nova/nova.conf&lt;/code> and add the following line to the &lt;code>DEFAULT&lt;/code>
section:&lt;/p>
&lt;pre>&lt;code>compute_driver=novadocker.virt.docker.DockerDriver
&lt;/code>&lt;/pre>
&lt;p>If there is already a line setting &lt;code>compute_driver&lt;/code>, then comment it
out or delete before adding the new one.&lt;/p>
&lt;p>Modify the Glance configuration to permit storage of Docker images.
Edit &lt;code>/etc/glance/glance-api.conf&lt;/code>, and add the following line to the
&lt;code>DEFAULT&lt;/code> section:&lt;/p>
&lt;pre>&lt;code>container_formats=ami,ari,aki,bare,ovf,ova,docker
&lt;/code>&lt;/pre>
&lt;p>Next, we need to augment the &lt;code>rootwrap&lt;/code> configuration such that
&lt;code>nova-docker&lt;/code> is able run the &lt;code>ln&lt;/code> command with &lt;code>root&lt;/code> privileges.
We&amp;rsquo;ll install the &lt;a href="https://github.com/stackforge/nova-docker/blob/master/etc/nova/rootwrap.d/docker.filters">docker.filters&lt;/a> file from the &lt;code>nova-docker&lt;/code>
source:&lt;/p>
&lt;pre>&lt;code># mkdir -p /etc/nova/rootwrap.d
# cp etc/nova/rootwrap.d/docker.filters \
/etc/nova/rootwrap.d/docker.filters
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;ve changed a number of configuration files, so we should restart
the affected services:&lt;/p>
&lt;pre>&lt;code># openstack-service restart nova glance
&lt;/code>&lt;/pre>
&lt;h2 id="testing-things-out">Testing things out&lt;/h2>
&lt;p>Let&amp;rsquo;s try starting a container! We need to select one that will run
in the &lt;code>nova-docker&lt;/code> environment. Generally, that means one that does
not expect to have an interactive terminal and that will automatically
start some sort of web-accessible service. I have a &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">minimal thttpd
container&lt;/a> that fits the bill nicely:&lt;/p>
&lt;pre>&lt;code># docker pull larsks/thttpd
&lt;/code>&lt;/pre>
&lt;p>We need to store this image into Glance using the same name:&lt;/p>
&lt;pre>&lt;code># docker save larsks/thttpd |
glance image-create --name larsks/thttpd \
--container-format docker --disk-format raw --is-public true
&lt;/code>&lt;/pre>
&lt;p>And now we should be able to start a container:&lt;/p>
&lt;pre>&lt;code># nova boot --image larsks/thttpd --flavor m1.tiny test0
&lt;/code>&lt;/pre>
&lt;p>After a bit, &lt;code>nova list&lt;/code> should show:&lt;/p>
&lt;pre>&lt;code>+------...+-------+--------+...+------------------+
| ID ...| Name | Status |...| Networks |
+------...+-------+--------+...+------------------+
| 430a1...| test0 | ACTIVE |...| private=10.0.0.6 |
+------...+-------+--------+...+------------------+
&lt;/code>&lt;/pre>
&lt;p>And we should also see the container if we run &lt;code>docker ps&lt;/code>:&lt;/p>
&lt;pre>&lt;code># docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
ee864da30cf1 larsks/thttpd:latest &amp;quot;/thttpd -D -l /dev/ 7 hours ago Up 7 hours nova-430a197e-a0ca-4e72-a7db-1969d0773cf7
&lt;/code>&lt;/pre>
&lt;h2 id="getting-connected">Getting connected&lt;/h2>
&lt;p>At this point, the container will &lt;em>not&lt;/em> be network accessible; it&amp;rsquo;s
attached to a private tenant network. Let&amp;rsquo;s assign it a floating ip
address:&lt;/p>
&lt;pre>&lt;code># nova floating-ip-create public
+--------------+-----------+----------+--------+
| Ip | Server Id | Fixed Ip | Pool |
+--------------+-----------+----------+--------+
| 172.24.4.229 | - | - | public |
+--------------+-----------+----------+--------+
# nova floating-ip-associate test0 172.24.4.229
&lt;/code>&lt;/pre>
&lt;p>This isn&amp;rsquo;t going to be immediately accessible because Packstack left
us without a route to the floating ip network. We can fix that
temporarily like this:&lt;/p>
&lt;pre>&lt;code># ip addr add 172.24.4.225/28 dev br-ex
&lt;/code>&lt;/pre>
&lt;p>And now we can ping our Docker container:&lt;/p>
&lt;pre>&lt;code># ping -c2 172.24.4.229
PING 172.24.4.229 (172.24.4.229) 56(84) bytes of data.
64 bytes from 172.24.4.229: icmp_seq=1 ttl=63 time=0.291 ms
64 bytes from 172.24.4.229: icmp_seq=2 ttl=63 time=0.074 ms
--- 172.24.4.229 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.074/0.182/0.291/0.109 ms
&lt;/code>&lt;/pre>
&lt;p>And access the webserver:&lt;/p>
&lt;pre>&lt;code># curl http://172.24.4.229
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Your web server is working&amp;lt;/title&amp;gt;
.
.
.&lt;/code>&lt;/pre></content></item><item><title>Filtering libvirt XML in Nova</title><link>https://blog.oddbit.com/post/2015-02-05-filtering-libvirt-xml-in-nova/</link><pubDate>Thu, 05 Feb 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-02-05-filtering-libvirt-xml-in-nova/</guid><description>I saw a request from a customer float by the other day regarding the ability to filter the XML used to create Nova instances in libvirt. The customer effectively wanted to blacklist a variety of devices (and device types). The consensus seems to be &amp;ldquo;you can&amp;rsquo;t do this right now and upstream is unlikely to accept patches that implement this behavior&amp;rdquo;, but it sounded like an interesting problem, so&amp;hellip;
https://github.com/larsks/nova/tree/feature/xmlfilter This is a fork of Nova (Juno) that includes support for an extensible filtering mechanism that is applied to the generated XML before it gets passed to libvirt.</description><content>&lt;p>I saw a request from a customer float by the other day regarding the
ability to filter the XML used to create Nova instances in libvirt.
The customer effectively wanted to blacklist a variety of devices (and
device types). The consensus seems to be &amp;ldquo;you can&amp;rsquo;t do this right now
and upstream is unlikely to accept patches that implement this
behavior&amp;rdquo;, but it sounded like an interesting problem, so&amp;hellip;&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/nova/tree/feature/xmlfilter">https://github.com/larsks/nova/tree/feature/xmlfilter&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a fork of Nova (Juno) that includes support for an extensible
filtering mechanism that is applied to the generated XML before it
gets passed to libvirt.&lt;/p>
&lt;h2 id="how-it-works">How it works&lt;/h2>
&lt;p>The code uses the &lt;a href="https://github.com/dreamhost/stevedore">stevedore&lt;/a> module to handle locating and loading
filters. A filter is a Python class that implements a &lt;code>filter&lt;/code>
method with the following signature:&lt;/p>
&lt;pre>&lt;code>def filter(self, xml, instance=None, context=None)
&lt;/code>&lt;/pre>
&lt;p>The code in &lt;code>nova.virt.libvirt.domxmlfilters&lt;/code> collects all filters
registered in the &lt;code>nova.filters.domxml&lt;/code> namespace, and then runs them
in sequence, passing the output of one filter as the output to the
next:&lt;/p>
&lt;pre>&lt;code>filters = stevedore.extension.ExtensionManager(
'nova.filters.domxml',
invoke_on_load=True,
)
def filter_domain_xml(xml,
instance=None,
context=None):
'''Filter the XML content in 'xml' through any filters registered in
the nova.filters.domxml namespace.'''
revised = xml
for filter in filters:
LOG.debug('filtering xml with filter %s',
filter.name)
revised = filter.obj.filter(revised,
instance=instance,
context=context
)
return revised
&lt;/code>&lt;/pre>
&lt;p>The filters are called from the &lt;code>_get_guest_xml&lt;/code> method in
&lt;code>nova/virt/libvirt/driver.py&lt;/code>.&lt;/p>
&lt;h2 id="an-example-filter">An example filter&lt;/h2>
&lt;p>This filter will add an interface to the libvirt &lt;code>default&lt;/code> network to
any instance created by Nova:&lt;/p>
&lt;pre>&lt;code>from lxml import etree
class AddNetworkFilter (object):
def filter(self, xml,
instance=None,
context=None):
doc = etree.fromstring(xml)
network = etree.fromstring('''
&amp;lt;interface type=&amp;quot;network&amp;quot;&amp;gt;
&amp;lt;source network=&amp;quot;default&amp;quot;/&amp;gt;
&amp;lt;model type=&amp;quot;virtio&amp;quot;/&amp;gt;
&amp;lt;/interface&amp;gt;
''')
devices = doc.xpath('/domain/devices')[0]
devices.append(network)
return etree.tostring(doc, pretty_print=True)
&lt;/code>&lt;/pre>
&lt;p>You can find this in my &lt;a href="https://github.com/larsks/demo_nova_filters/">demo_nova_filters&lt;/a> repository, along with a
few other trivial examples. The above filter is registered via the
&lt;code>entry_points&lt;/code> section of the &lt;code>setup.py&lt;/code> file:&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env python
import setuptools
setuptools.setup(
name=&amp;quot;demo_nova_filters&amp;quot;,
version=1,
packages=['demo_nova_filters'],
entry_points={
'nova.filters.domxml': [
'prettyprint=demo_nova_filters.prettyprint:PrettyPrintFilter',
'novideo=demo_nova_filters.novideo:NoVideoFilter',
'addnetwork=demo_nova_filters.addnetwork:AddNetworkFilter',
]
},
)
&lt;/code>&lt;/pre>
&lt;p>And that&amp;rsquo;s it. This is almost entirely untested. While it works in
some cases it doesn&amp;rsquo;t work in all cases, and it&amp;rsquo;s unlikely that I&amp;rsquo;m
going to update this to work with any future version of Nova. This
was really just an exercise in curiosity. Enjoy!&lt;/p></content></item><item><title>Running nova-libvirt and nova-docker on the same host</title><link>https://blog.oddbit.com/post/2015-01-17-running-novalibvirt-and-novado/</link><pubDate>Sat, 17 Jan 2015 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2015-01-17-running-novalibvirt-and-novado/</guid><description>I regularly use OpenStack on my laptop with libvirt as my hypervisor. I was interested in experimenting with recent versions of the nova-docker driver, but I didn&amp;rsquo;t have a spare system available on which to run the driver, and I use my regular nova-compute service often enough that I didn&amp;rsquo;t want to simply disable it temporarily in favor of nova-docker.
NB As pointed out by gustavo in the comments, running two neutron-openvswitch-agents on the same host &amp;ndash; as suggested in this article &amp;ndash; is going to lead to nothing but sadness and doom.</description><content>&lt;p>I regularly use &lt;a href="http://www.openstack.org/">OpenStack&lt;/a> on my laptop with &lt;a href="http://www.libvirt.org/">libvirt&lt;/a> as my
hypervisor. I was interested in experimenting with recent versions of
the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> driver, but I didn&amp;rsquo;t have a spare system available
on which to run the driver, and I use my regular &lt;code>nova-compute&lt;/code> service
often enough that I didn&amp;rsquo;t want to simply disable it temporarily in
favor of &lt;code>nova-docker&lt;/code>.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong> As pointed out by &lt;em>gustavo&lt;/em> in the comments, running two
&lt;code>neutron-openvswitch-agents&lt;/code> on the same host &amp;ndash; as suggested in this
article &amp;ndash; is going to lead to nothing but sadness and doom. So
kids, don&amp;rsquo;t try this at home. I&amp;rsquo;m leaving the article here because I
think it still has some interesting bits.&lt;/p>
&lt;hr>
&lt;p>I guess the simplest solution would be to spin up a vm on which to run
&lt;code>nova-docker&lt;/code>, but why use a simple solution when there are things to
be learned? I wanted to know if it were possible (and if so, how) to
run both hypervisors on the same physical host.&lt;/p>
&lt;p>The naive solution would be to start up another instance of
&lt;code>nova-compute&lt;/code> configured to use the Docker driver. Unfortunately,
Nova only permits a single service instance per &amp;ldquo;host&amp;rdquo;, so starting up
the second instance of &lt;code>nova-compute&lt;/code> would effectively &amp;ldquo;mask&amp;rdquo; the
original one.&lt;/p>
&lt;p>Fortunately, Nova&amp;rsquo;s definition of what constitutes a &amp;ldquo;host&amp;rdquo; is
somewhat flexible. Nova supports a &lt;code>host&lt;/code> configuration key in
&lt;code>nova.conf&lt;/code> that will cause Nova to identify the host on which it is
running using your explicitly configured value, rather than your
system hostname. We can take advantage of this to get a second
&lt;code>nova-compute&lt;/code> instance running on the same system.&lt;/p>
&lt;h2 id="install-nova-docker">Install nova-docker&lt;/h2>
&lt;p>We&amp;rsquo;ll start by installing the &lt;code>nova-docker&lt;/code> driver from
&lt;a href="https://github.com/stackforge/nova-docker">https://github.com/stackforge/nova-docker&lt;/a>. If you&amp;rsquo;re running the
Juno release of OpenStack (which I am), you&amp;rsquo;re going to want to use
the &lt;code>stable/juno&lt;/code> branch of the &lt;code>nova-docker&lt;/code> repository. So:&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/stackforge/nova-docker
$ cd nova-docker
$ git checkout stable/juno
$ sudo python setup.py install
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll want to read the project&amp;rsquo;s &lt;a href="https://github.com/stackforge/nova-docker/blob/master/README.rst">README&lt;/a> for complete installation
instructions.&lt;/p>
&lt;h2 id="configure-nova-docker">Configure nova-docker&lt;/h2>
&lt;p>Now, rather than configuring &lt;code>/etc/nova/nova.conf&lt;/code>, we&amp;rsquo;re going to
create a new configuration file, &lt;code>/etc/nova/nova-docker.conf&lt;/code>, with
only the configuration keys that differ from our primary Nova
configuration:&lt;/p>
&lt;pre>&lt;code>[DEFAULT]
host=nova-docker
compute_driver=novadocker.virt.docker.DockerDriver
log_file=/var/log/nova/nova-docker.log
state_path=/var/lib/nova-docker
&lt;/code>&lt;/pre>
&lt;p>You can see that we&amp;rsquo;ve set the value of &lt;code>host&lt;/code> to &lt;code>nova-docker&lt;/code>, to
differentiate this &lt;code>nova-compute&lt;/code> service from the &lt;code>libvirt&lt;/code>-backed
one that is already running. We&amp;rsquo;ve provided the service with a
dedicated log file and state directory to prevent conflicts with the
already-running &lt;code>nova-compute&lt;/code> service.&lt;/p>
&lt;p>To use this configuration file, we&amp;rsquo;ll launch a new instance of the
&lt;code>nova-compute&lt;/code> service pointing at both the original configuration
file, &lt;code>/etc/nova/nova.conf&lt;/code>, as well as this &lt;code>nova-docker&lt;/code>
configuration file. The command line would look something like:&lt;/p>
&lt;pre>&lt;code>nova-compute --config-file /etc/nova/nova.conf \
--config-file /etc/nova/nova-docker.conf
&lt;/code>&lt;/pre>
&lt;p>The ordering of configuration files on the command line is
significant: later configuration files will override values from
earlier files.&lt;/p>
&lt;p>I&amp;rsquo;m running &lt;a href="http://www.fedora.org/">Fedora&lt;/a> 21 on my laptop, which uses &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a>, so I
created a modified version of the &lt;code>openstack-nova-compute.service&lt;/code>
unit on my system, and saved it as
&lt;code>/etc/systemd/system/openstack-nova-docker.service&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=OpenStack Nova Compute Server (Docker)
After=syslog.target network.target
[Service]
Environment=LIBGUESTFS_ATTACH_METHOD=appliance
Type=notify
Restart=always
User=nova
ExecStart=/usr/bin/nova-compute --config-file /etc/nova/nova.conf --config-file /etc/nova/nova-docker.conf
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;p>And then activated the service;&lt;/p>
&lt;pre>&lt;code># systemctl enable openstack-nova-docker
# systemctl start openstack-nova-docker
&lt;/code>&lt;/pre>
&lt;p>Now, if I run &lt;code>nova service-list&lt;/code> with administrator credentials, I
can see both &lt;code>nova-compute&lt;/code> instances:&lt;/p>
&lt;pre>&lt;code>+----+------------------+------------------+----------+---------+-------...
| Id | Binary | Host | Zone | Status | State ...
+----+------------------+------------------+----------+---------+-------...
| 1 | nova-consoleauth | host.example.com | internal | enabled | up ...
| 2 | nova-scheduler | host.example.com | internal | enabled | up ...
| 3 | nova-conductor | host.example.com | internal | enabled | up ...
| 5 | nova-cert | host.example.com | internal | enabled | up ...
| 6 | nova-compute | host.example.com | nova | enabled | up ...
| 7 | nova-compute | nova-docker | nova | enabled | up ...
+----+------------------+------------------+----------+---------+-------...
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-docker-container-take-1">Booting a Docker container (take 1)&lt;/h2>
&lt;p>Let&amp;rsquo;s try starting a Docker container using the new &lt;code>nova-compute&lt;/code>
service. We&amp;rsquo;ll first need to load a Docker image into Glance (you
followed the &lt;code>nova-docker&lt;/code> &lt;a href="https://github.com/stackforge/nova-docker#1-enable-the-driver-in-glances-configuration">instructions for configuring
Glance&lt;/a>, right?). We&amp;rsquo;ll use my &lt;a href="https://registry.hub.docker.com/u/larsks/thttpd/">larsks/thttpd&lt;/a> image,
because it&amp;rsquo;s very small and doesn&amp;rsquo;t require any configuration:&lt;/p>
&lt;pre>&lt;code>$ docker pull larsks/thttpd
$ docker save larsks/thttpd |
glance image-create --is-public True --container-format docker \
--disk-format raw --name larsks/thttpd
&lt;/code>&lt;/pre>
&lt;p>(Note that you will probably require administrative credentials to load
this image into Glance.)&lt;/p>
&lt;p>Now that we have an appropriate image available we can try booting a container:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image larsks/thttpd --flavor m1.small test1
&lt;/code>&lt;/pre>
&lt;p>If we wait a moment and then run &lt;code>nova list&lt;/code>, we see:&lt;/p>
&lt;pre>&lt;code>| 9a783952-a888-4fcd-8f5d-cd9291ed1969 | test1 | ERROR | spawning ...
&lt;/code>&lt;/pre>
&lt;p>&lt;a href="http://www.sadtrombone.com/">What happened?&lt;/a> Looking at the appropriate log file
(&lt;code>/var/log/nova/nova-docker.log&lt;/code>), we find:&lt;/p>
&lt;pre>&lt;code>Cannot setup network: Unexpected vif_type=binding_failed
Traceback (most recent call last):
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/driver.py&amp;quot;, line 367, in _start_container
self.plug_vifs(instance, network_info)
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/driver.py&amp;quot;, line 187, in plug_vifs
self.vif_driver.plug(instance, vif)
File &amp;quot;/usr/lib/python2.7/site-packages/novadocker/virt/docker/vifs.py&amp;quot;, line 63, in plug
_(&amp;quot;Unexpected vif_type=%s&amp;quot;) % vif_type)
NovaException: Unexpected vif_type=binding_failed
&lt;/code>&lt;/pre>
&lt;p>The message &lt;code>vif_type=binding_failed&lt;/code> is Nova&amp;rsquo;s way of saying &amp;ldquo;I have
no idea what happened, go ask Neutron&amp;rdquo;. Looking in Neutron&amp;rsquo;s
&lt;code>/var/log/neutron/server.log&lt;/code>, we find:&lt;/p>
&lt;pre>&lt;code>Failed to bind port 82c07caa-b2c2-45e9-955d-e8b35112437c on host
nova-docker
&lt;/code>&lt;/pre>
&lt;p>And this tells us our problem: we have told our &lt;code>nova-docker&lt;/code> service
that it is running on a host called &amp;ldquo;nova-docker&amp;rdquo;, and Neutron doesn&amp;rsquo;t
know anything about that host.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong>&lt;/p>
&lt;p>If you were to try to delete this failed instance, you would find that
it is un-deletable. In the end, I was only able to delete it by
directly editing the &lt;code>nova&lt;/code> database using &lt;a href="delete-deleting-instances.sql">this sql script&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="adding-a-neutron-agent">Adding a Neutron agent&lt;/h2>
&lt;p>We&amp;rsquo;re going to need to set up an instance of
&lt;code>neutron-openvswitch-agent&lt;/code> to service network requests on our
&amp;ldquo;nova-docker&amp;rdquo; host. Like Nova, Neutron also supports a &lt;code>host&lt;/code>
configuration key, so we&amp;rsquo;re going to pursue a solution similar to what
we used with Nova by creating a new configuration file,
&lt;code>/etc/neutron/ovs-docker.conf&lt;/code>, with the following content:&lt;/p>
&lt;pre>&lt;code>[DEFAULT]
host = nova-docker
&lt;/code>&lt;/pre>
&lt;p>And then we&amp;rsquo;ll set up the corresponding service by dropping the
following into &lt;code>/etc/systemd/system/docker-openvswitch-agent.service&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[Unit]
Description=OpenStack Neutron Open vSwitch Agent (Docker)
After=syslog.target network.target
[Service]
Type=simple
User=neutron
ExecStart=/usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini --config-file /etc/neutron/ovs-docker.conf --log-file /var/log/neutron/docker-openvswitch-agent.log
PrivateTmp=true
KillMode=process
[Install]
WantedBy=multi-user.target
&lt;/code>&lt;/pre>
&lt;hr>
&lt;p>&lt;strong>NB&lt;/strong>&lt;/p>
&lt;p>While working on this configuration I ran into an undesirable
interaction between Docker and &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/">systemd&lt;/a>&amp;rsquo;s &lt;code>PrivateTmp&lt;/code> directive.&lt;/p>
&lt;p>This directive causes the service to run with
a private &lt;a href="http://lwn.net/Articles/531114/">mount namespace&lt;/a> such that &lt;code>/tmp&lt;/code> for the service is not
the same as &lt;code>/tmp&lt;/code> for other services. This is a great idea from a
security perspective, but can cause problems in the following
scenario:&lt;/p>
&lt;ol>
&lt;li>Start a Docker container with &lt;code>nova boot ...&lt;/code>&lt;/li>
&lt;li>Restart any service that uses the &lt;code>PrivateTmp&lt;/code> directive&lt;/li>
&lt;li>Attempt to delete the Docker container with &lt;code>nova delete ...&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>Docker will fail to destroy the container because the private
namespace created by the &lt;code>PrivateTmp&lt;/code> directive preserves a reference
to the Docker &lt;code>devicemapper&lt;/code> mount in
&lt;code>/var/lib/docker/devicemapper/mnt/...&lt;/code> that was active at the time the
service was restarted. To recover from this situation, you will need
to restart whichever service is still holding a reference to the
Docker mounts.&lt;/p>
&lt;p>I have &lt;a href="http://lists.freedesktop.org/archives/systemd-devel/2015-January/027162.html">posted to the systemd-devel&lt;/a> mailing
list to see if there are any solutions to this behavior. As I note in
that email, this behavior appears to be identical to that described in
Fedora bug &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=851970">851970&lt;/a>, which was closed two years ago.&lt;/p>
&lt;p>&lt;strong>Update&lt;/strong> I wrote a &lt;a href="https://blog.oddbit.com/post/2015-01-18-docker-vs-privatetmp/">separate post&lt;/a> about this issue, which
includes some discussion about what&amp;rsquo;s going on and a solution.&lt;/p>
&lt;hr>
&lt;p>If we activate this service&amp;hellip;&lt;/p>
&lt;pre>&lt;code># systemctl enable docker-openvswitch-agent
# systemctl start docker-openvswitch-agent
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and then run &lt;code>neutron agent-list&lt;/code> with administrative credentials,
we&amp;rsquo;ll see the new agent:&lt;/p>
&lt;pre>&lt;code>$ neutron agent-list
+--------------------------------------+--------------------+------------------+-------+...
| id | agent_type | host | alive |...
+--------------------------------------+--------------------+------------------+-------+...
| 2e40062a-1c30-46a3-8719-3ce93a56b4ce | Open vSwitch agent | nova-docker | :-) |...
| 63edb2a4-f980-4f88-b9c0-9610a1b20f13 | L3 agent | host.example.com | :-) |...
| 8482c5c3-208c-4145-9f7d-606be3da11ed | Loadbalancer agent | host.example.com | :-) |...
| 9922ed54-00fa-41d4-96e8-ac8af8c291fd | Open vSwitch agent | host.example.com | :-) |...
| b8becb9c-7290-42be-9faf-fd3baeea3dcf | Metadata agent | host.example.com | :-) |...
| c46be41b-e93a-40ab-a37e-4d67b770a3df | DHCP agent | host.example.com | :-) |...
+--------------------------------------+--------------------+------------------+-------+...
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-docker-container-take-2">Booting a Docker container (take 2)&lt;/h2>
&lt;p>Now that we have both the &lt;code>nova-docker&lt;/code> service running and a
corresponding &lt;code>neutron-openvswitch-agent&lt;/code> available, let&amp;rsquo;s try
starting our container one more time:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image larsks/thttpd --flavor m1.small test1
$ nova list
+--------------------------------------+---------+--------+...
| ID | Name | Status |...
+--------------------------------------+---------+--------+...
| 642b7d61-9189-40ea-86f5-2424c3c86028 | test1 | ACTIVE |...
+--------------------------------------+---------+--------+...
&lt;/code>&lt;/pre>
&lt;p>If we assign a floating IP address:&lt;/p>
&lt;pre>&lt;code>$ nova floating-ip-create ext-nat
+-----------------+-----------+----------+---------+
| Ip | Server Id | Fixed Ip | Pool |
+-----------------+-----------+----------+---------+
| 192.168.200.211 | - | - | ext-nat |
+-----------------+-----------+----------+---------+
$ nova floating-ip-associate test1 192.168.200.211
&lt;/code>&lt;/pre>
&lt;p>We can then browse to &lt;code>http://192.168.200.211&lt;/code> and see the sample
page:&lt;/p>
&lt;pre>&lt;code>$ curl http://192.168.200.211/
.
.
.
____ _ _ _ _
/ ___|___ _ __ __ _ _ __ __ _| |_ _ _| | __ _| |_(_) ___ _ __ ___
| | / _ \| '_ \ / _` | '__/ _` | __| | | | |/ _` | __| |/ _ \| '_ \/ __|
| |__| (_) | | | | (_| | | | (_| | |_| |_| | | (_| | |_| | (_) | | | \__ \
\____\___/|_| |_|\__, |_| \__,_|\__|\__,_|_|\__,_|\__|_|\___/|_| |_|___/
|___/
.
.
.
&lt;/code>&lt;/pre>
&lt;h2 id="booting-a-libvirt-instance">Booting a libvirt instance&lt;/h2>
&lt;p>To show that we really are running two hypervisors on the same host,
we can launch a traditional &lt;code>libvirt&lt;/code> instance alongside our Docker
container:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image cirros --flavor m1.small --key-name lars test2
&lt;/code>&lt;/pre>
&lt;p>Wait a bit, then:&lt;/p>
&lt;pre>&lt;code>$ nova list
+--------------------------------------+-------+--------+...
| ID | Name | Status |...
+--------------------------------------+-------+--------+...
| 642b7d61-9189-40ea-86f5-2424c3c86028 | test1 | ACTIVE |...
| 7fec33c9-d50f-477e-957c-a05ee9bd0b0b | test2 | ACTIVE |...
+--------------------------------------+-------+--------+...
&lt;/code>&lt;/pre></content></item><item><title>Accessing the serial console of your Nova servers</title><link>https://blog.oddbit.com/post/2014-12-22-accessing-the-serial-console-o/</link><pubDate>Mon, 22 Dec 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-12-22-accessing-the-serial-console-o/</guid><description>One of the new features available in the Juno release of OpenStack is support for serial console access to your Nova servers. This post looks into how to configure the serial console feature and then how to access the serial consoles of your Nova servers.
Configuring serial console support In previous release of OpenStack, read-only access to the serial console of your servers was available through the os-getConsoleOutput server action (exposed via nova console-log on the command line).</description><content>&lt;p>One of the new features available in the Juno release of OpenStack is
support for &lt;a href="https://blueprints.launchpad.net/nova/+spec/serial-ports">serial console access to your Nova
servers&lt;/a>. This post looks into how to configure the
serial console feature and then how to access the serial consoles of
your Nova servers.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="configuring-serial-console-support">Configuring serial console support&lt;/h2>
&lt;p>In previous release of OpenStack, read-only access to the serial
console of your servers was available through the
&lt;code>os-getConsoleOutput&lt;/code> server action (exposed via &lt;code>nova console-log&lt;/code> on
the command line). Most cloud-specific Linux images are configured
with a command line that includes something like &lt;code>console=tty0 console=ttyS0,115200n81&lt;/code>, which ensures that kernel output and other
messages are available on the serial console. This is a useful
mechanism for diagnosing problems in the event that you do not have
network access to a server.&lt;/p>
&lt;p>In Juno, you can exchange this read-only view of the console for
read-write access by setting &lt;code>enabled=true&lt;/code> in the &lt;code>[serial_console]&lt;/code>
section of your &lt;code>nova.conf&lt;/code> file:&lt;/p>
&lt;pre>&lt;code>[serial_console]
enabled=true
&lt;/code>&lt;/pre>
&lt;p>This enables the new &lt;code>os-getSerialConsole&lt;/code> server action.&lt;/p>
&lt;p>Much like the configuration for graphical console access, you will also
probably need to provide values for &lt;code>base_url&lt;/code>, &lt;code>listen&lt;/code>, and
&lt;code>proxyclient_address&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[serial_console]
enabled=true
# Location of serial console proxy. (string value)
base_url=ws://127.0.0.1:6083/
# IP address on which instance serial console should listen
# (string value)
listen=127.0.0.1
# The address to which proxy clients (like nova-serialproxy)
# should connect (string value)
proxyclient_address=127.0.0.1
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>base_url&lt;/code> setting is what gets passed to clients, so this will
probably be the address of one of your &amp;ldquo;front-end&amp;rdquo; controllers (e.g.,
wherever you are running other public APIs or services like Horizon).&lt;/p>
&lt;p>The &lt;code>listen&lt;/code> address is used by &lt;code>nova-compute&lt;/code> to control on which
address the virtual console will listen (this can be set to &lt;code>0.0.0.0&lt;/code>
to listen on all available addresses). The &lt;code>proxyclient_address&lt;/code>
controls to which address the &lt;code>nova-serialproxy&lt;/code> service will connect.&lt;/p>
&lt;p>In other words: a remote client request a serial console will receive
a websocket URL prefixed by &lt;code>base_url&lt;/code>. This URL will connect the
client to the &lt;code>nova-serialproxy&lt;/code> service. The &lt;code>nova-serialproxy&lt;/code>
service will look up the &lt;code>proxyclient_address&lt;/code> associated with the
requested server, and will connect to the appropriate port at that
address.&lt;/p>
&lt;p>Enabling serial console support will result in an entry similar to the
following in the XML description of libvirt guests started by Nova:&lt;/p>
&lt;pre>&lt;code>&amp;lt;console type='tcp'&amp;gt;
&amp;lt;source mode='bind' host='127.0.0.1' service='10000'/&amp;gt;
&amp;lt;protocol type='raw'/&amp;gt;
&amp;lt;target type='serial' port='0'/&amp;gt;
&amp;lt;alias name='serial0'/&amp;gt;
&amp;lt;/console&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="accessing-the-serial-console">Accessing the serial console&lt;/h2>
&lt;p>You can use the &lt;code>nova get-serial-proxy&lt;/code> command to retrieve the
websocket URL for a server&amp;rsquo;s serial console, like this:&lt;/p>
&lt;pre>&lt;code>$ nova get-serial-console my-server
+--------+-----------------------------------------------------------------+
| Type | Url |
+--------+-----------------------------------------------------------------+
| serial | ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d |
+--------+-----------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>Or through the REST API like this:&lt;/p>
&lt;pre>&lt;code>curl -i 'http://127.0.0.1:8774/v2/&amp;lt;tenant_uuid&amp;gt;/servers/&amp;lt;server_uuid&amp;gt;/action' \
-X POST \
-H &amp;quot;Accept: application/json&amp;quot; \
-H &amp;quot;Content-Type: application/json&amp;quot; \
-H &amp;quot;X-Auth-Project-Id: &amp;lt;project_id&amp;gt;&amp;quot; \
-H &amp;quot;X-Auth-Token: &amp;lt;auth_token&amp;gt;&amp;quot; \
-d '{&amp;quot;os-getSerialConsole&amp;quot;: {&amp;quot;type&amp;quot;: &amp;quot;serial&amp;quot;}}'
&lt;/code>&lt;/pre>
&lt;p>But now that you have a websocket URL, what do you do with it? It
turns out that there aren&amp;rsquo;t all that many out-of-the-box tools that
will let you connect interactively to this URL from the command line.
While I&amp;rsquo;m sure that a future version of Horizon will provide a
web-accessible console access mechanism, it is often convenient to
have a command-line tool for this sort of thing because that permits
you to log or otherwise process the output.&lt;/p>
&lt;p>Fortunately, it&amp;rsquo;s not too difficult to write a simple client. The
Python &lt;code>websocket-client&lt;/code> module has the necessary support; given the
above URL, you can open a connection like this:&lt;/p>
&lt;pre>&lt;code>import websocket
ws = websocket.create_connection(
'ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d',
subprotocols=['binary', 'base64'])
&lt;/code>&lt;/pre>
&lt;p>This gets you a &lt;code>WebSocket&lt;/code> object with &lt;code>.send&lt;/code> and &lt;code>.recv&lt;/code> methods
for sending and receiving data (and a &lt;code>.fileno&lt;/code> method for use in
event loops).&lt;/p>
&lt;h2 id="i-was-told-there-would-be-no-programming">I was told there would be no programming&lt;/h2>
&lt;p>If you don&amp;rsquo;t feel like writing your own websocket client, have no
fear! I have put together a simple client called &lt;a href="http://github.com/larsks/novaconsole/">novaconsole&lt;/a>.
Assuming that you have valid credentials in your environment, you can
provide it with a server name or UUID:&lt;/p>
&lt;pre>&lt;code>$ novaconsole my-server
&lt;/code>&lt;/pre>
&lt;p>You can also provide a verbatim websocket URL (in which case you don&amp;rsquo;t
need to bother with OpenStack authentication):&lt;/p>
&lt;pre>&lt;code>$ novaconsole --url ws://127.0.0.1:6083/?token=18510769-71ad-4e5a-8348-4218b5613b3d
&lt;/code>&lt;/pre>
&lt;p>In either case, you will have an interactive session to the specified
serial console. You can exit the session by typing &lt;code>~.&lt;/code> at the
beginning of a line.&lt;/p>
&lt;p>You can only have a single active console connection at a time. Other
connections will block until you disconnect from the active session.&lt;/p>
&lt;h2 id="but-everything-is-not-roses-and-sunshine">But everything is not roses and sunshine&lt;/h2>
&lt;p>One disadvantage to the serial console support is that it &lt;em>replaces&lt;/em>
the console log available via &lt;code>nova console-log&lt;/code>. This means that if,
for example, a server were to encounter problems configuring
networking and emit errors on the console, you would not be able to
see this information unless you happened to be connected to the
console at the time the errors were generated.&lt;/p>
&lt;p>It would be nice to have both mechanisms available &amp;ndash; serial console
support for interactive access, and console logs for retroactive
debugging.&lt;/p></content></item><item><title>Cloud-init and the case of the changing hostname</title><link>https://blog.oddbit.com/post/2014-12-10-cloudinit-and-the-case-of-the-/</link><pubDate>Wed, 10 Dec 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-12-10-cloudinit-and-the-case-of-the-/</guid><description>Setting the stage I ran into a problem earlier this week deploying RDO Icehouse under RHEL 6. My target systems were a set of libvirt guests deployed from the RHEL 6 KVM guest image, which includes cloud-init in order to support automatic configuration in cloud environments. I take advantage of this when using libvirt by attaching a configuration drive so that I can pass in ssh keys and a user-data script.</description><content>&lt;h2 id="setting-the-stage">Setting the stage&lt;/h2>
&lt;p>I ran into a problem earlier this week deploying RDO Icehouse under
RHEL 6. My target systems were a set of libvirt guests deployed from
the RHEL 6 KVM guest image, which includes &lt;a href="https://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> in order to
support automatic configuration in cloud environments. I take
advantage of this when using &lt;code>libvirt&lt;/code> by attaching a configuration
drive so that I can pass in ssh keys and a &lt;code>user-data&lt;/code> script.&lt;/p>
&lt;p>Once the systems were up, I used &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a> to deploy OpenStack
onto a single controller and two compute nodes, and at the conclusion
of the &lt;code>packstack&lt;/code> run everything was functioning correctly. Running
&lt;code>neutron agent-list&lt;/code> showed all agents in good order:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+--------------------+------------+-------+----------------+
| id | agent_type | host | alive | admin_state_up |
+--------------------------------------+--------------------+------------+-------+----------------+
| 0d51d200-d902-4e05-847a-858b69c03088 | DHCP agent | controller | :-) | True |
| 192f76e9-a816-4bd9-8a90-a263a1d54031 | Open vSwitch agent | compute-0 | :-) | True |
| 3d97d7ba-1b1f-43f8-9582-f860fbfe50df | Open vSwitch agent | controller | :-) | True |
| 54d387a6-dca1-4ace-8c1b-7788fb0bc090 | Metadata agent | controller | :-) | True |
| 92fc83bf-0995-43c3-92d1-70002c734604 | L3 agent | controller | :-) | True |
| e06575c2-43b3-4691-80bc-454f501debfe | Open vSwitch agent | compute-1 | :-) | True |
+--------------------------------------+--------------------+------------+-------+----------------+
&lt;/code>&lt;/pre>
&lt;h2 id="a-problem-rears-its-ugly-head">A problem rears its ugly head&lt;/h2>
&lt;p>After rebooting the system, I found that I was missing an expected
Neutron router namespace. Specifically, given:&lt;/p>
&lt;pre>&lt;code># neutron router-list
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| id | name | external_gateway_info |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| e83eec10-0de2-4bfa-8e58-c1bcbe702f51 | router1 | {&amp;quot;network_id&amp;quot;: &amp;quot;b53a9ecd-01fc-4bee-b20d-8fbe0cd2e010&amp;quot;, &amp;quot;enable_snat&amp;quot;: true} |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>I expected to see:&lt;/p>
&lt;pre>&lt;code># ip netns
qrouter-e83eec10-0de2-4bfa-8e58-c1bcbe702f51
&lt;/code>&lt;/pre>
&lt;p>But the &lt;code>qrouter&lt;/code> namespace was missing.&lt;/p>
&lt;p>The output of &lt;code>neutron agent-list&lt;/code> shed some light on the problem:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+--------------------+------------------------+-------+----------------+
| id | agent_type | host | alive | admin_state_up |
+--------------------------------------+--------------------+------------------------+-------+----------------+
| 0832e8f3-61f9-49cf-b49c-886cc94d3d28 | Metadata agent | controller.localdomain | :-) | True |
| 0d51d200-d902-4e05-847a-858b69c03088 | DHCP agent | controller | xxx | True |
| 192f76e9-a816-4bd9-8a90-a263a1d54031 | Open vSwitch agent | compute-0 | :-) | True |
| 3be34828-ca8d-4638-9b3a-4e2f688a9ca9 | L3 agent | controller.localdomain | :-) | True |
| 3d97d7ba-1b1f-43f8-9582-f860fbfe50df | Open vSwitch agent | controller | xxx | True |
| 54d387a6-dca1-4ace-8c1b-7788fb0bc090 | Metadata agent | controller | xxx | True |
| 87b53741-f28b-4582-9ea8-6062ab9962e9 | Open vSwitch agent | controller.localdomain | :-) | True |
| 92fc83bf-0995-43c3-92d1-70002c734604 | L3 agent | controller | xxx | True |
| e06575c2-43b3-4691-80bc-454f501debfe | Open vSwitch agent | compute-1 | :-) | True |
| e327b7f9-c9ce-49f8-89c1-b699d9f7d253 | DHCP agent | controller.localdomain | :-) | True |
+--------------------------------------+--------------------+------------------------+-------+----------------+
&lt;/code>&lt;/pre>
&lt;p>There were two sets of Neutron agents registered using different
hostnames &amp;ndash; one set using the short name of the host, and the other
set using the fully qualified hostname.&lt;/p>
&lt;h2 id="whats-up-with-that">What&amp;rsquo;s up with that?&lt;/h2>
&lt;p>In the &lt;code>cc_set_hostname.py&lt;/code> module, &lt;code>cloud-init&lt;/code> performs the
following operation:&lt;/p>
&lt;pre>&lt;code>(hostname, fqdn) = util.get_hostname_fqdn(cfg, cloud)
try:
log.debug(&amp;quot;Setting the hostname to %s (%s)&amp;quot;, fqdn, hostname)
cloud.distro.set_hostname(hostname, fqdn)
except Exception:
util.logexc(log, &amp;quot;Failed to set the hostname to %s (%s)&amp;quot;, fqdn,
hostname)
raise
&lt;/code>&lt;/pre>
&lt;p>It starts by retrieving the hostname (both the qualified and
unqualified version) from the cloud environment, and then calls
&lt;code>cloud.distro.set_hostname(hostname, fqdn)&lt;/code>. This ends up calling:&lt;/p>
&lt;pre>&lt;code>def set_hostname(self, hostname, fqdn=None):
writeable_hostname = self._select_hostname(hostname, fqdn)
self._write_hostname(writeable_hostname, self.hostname_conf_fn)
self._apply_hostname(hostname)
&lt;/code>&lt;/pre>
&lt;p>Where, on a RHEL system, &lt;code>_select_hostname&lt;/code> is:&lt;/p>
&lt;pre>&lt;code>def _select_hostname(self, hostname, fqdn):
# See: http://bit.ly/TwitgL
# Should be fqdn if we can use it
if fqdn:
return fqdn
return hostname
&lt;/code>&lt;/pre>
&lt;p>So:&lt;/p>
&lt;ul>
&lt;li>&lt;code>cloud-init&lt;/code> sets &lt;code>writeable_hostname&lt;/code> to the fully qualified name
of the system (assuming it is available).&lt;/li>
&lt;li>&lt;code>cloud-init&lt;/code> writes the fully qualified hostname to &lt;code>/etc/sysconfig/network&lt;/code>.&lt;/li>
&lt;li>&lt;code>cloud-init&lt;/code> sets the hostname to the &lt;em>unqualified&lt;/em> hostname&lt;/li>
&lt;/ul>
&lt;p>The result is that your system will probably have a different hostname
after your first reboot, which throws off Neutron.&lt;/p>
&lt;h2 id="and-they-all-lived-happily-ever-after">And they all lived happily ever after?&lt;/h2>
&lt;p>It turns out this bug was reported upstream back in October of 2013 as
&lt;a href="https://bugs.launchpad.net/cloud-init/+bug/1246485">bug 1246485&lt;/a>, and while there are patches available the bug has
been marked as &amp;ldquo;low&amp;rdquo; priority and has been fixed. There are patches
attached to the bug report that purport to fix the problem.&lt;/p></content></item><item><title>Fedora Atomic, OpenStack, and Kubernetes (oh my)</title><link>https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/</link><pubDate>Mon, 24 Nov 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-11-24-fedora-atomic-openstack-and-ku/</guid><description>While experimenting with Fedora Atomic, I was looking for an elegant way to automatically deploy Atomic into an OpenStack environment and then automatically schedule some Docker containers on the Atomic host. This post describes my solution.
Like many other cloud-targeted distributions, Fedora Atomic runs cloud-init when the system boots. We can take advantage of this to configure the system at first boot by providing a user-data blob to Nova when we boot the instance.</description><content>&lt;p>While experimenting with &lt;a href="http://www.projectatomic.io/">Fedora Atomic&lt;/a>, I was looking for an
elegant way to automatically deploy Atomic into an &lt;a href="http://openstack.org/">OpenStack&lt;/a>
environment and then automatically schedule some &lt;a href="http://docker.com/">Docker&lt;/a> containers
on the Atomic host. This post describes my solution.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Like many other cloud-targeted distributions, Fedora Atomic runs
&lt;a href="http://cloudinit.readthedocs.org/">cloud-init&lt;/a> when the system boots. We can take advantage of this
to configure the system at first boot by providing a &lt;code>user-data&lt;/code> blob
to Nova when we boot the instance. A &lt;code>user-data&lt;/code> blob can be as
simple as a shell script, and while we could arguably mash everything
into a single script it wouldn&amp;rsquo;t be particularly maintainable or
flexible in the face of different pod/service/etc descriptions.&lt;/p>
&lt;p>In order to build a more flexible solution, we&amp;rsquo;re going to take
advantage of the following features:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Support for &lt;a href="http://cloudinit.readthedocs.org/en/latest/topics/format.html#mime-multi-part-archive">multipart MIME archives&lt;/a>.&lt;/p>
&lt;p>Cloud-init allows you to pass in multiple files via &lt;code>user-data&lt;/code> by
encoding them as a multipart MIME archive.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Support for a &lt;a href="http://cloudinit.readthedocs.org/en/latest/topics/format.html#part-handler">custom part handler&lt;/a>.&lt;/p>
&lt;p>Cloud-init recognizes a number of specific MIME types (such as
&lt;code>text/cloud-config&lt;/code> or &lt;code>text/x-shellscript&lt;/code>). We can provide a
custom part handler that will be used to handle MIME types not
intrinsincally supported by &lt;code>cloud-init&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="a-custom-part-handler-for-kubernetes-configurations">A custom part handler for Kubernetes configurations&lt;/h2>
&lt;p>I have written a &lt;a href="https://github.com/larsks/atomic-kubernetes-tools/blob/master/kube-part-handler.py">custom part handler&lt;/a> that knows
about the following MIME types:&lt;/p>
&lt;ul>
&lt;li>&lt;code>text/x-kube-pod&lt;/code>&lt;/li>
&lt;li>&lt;code>text/x-kube-service&lt;/code>&lt;/li>
&lt;li>&lt;code>text/x-kube-replica&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>When the part handler is first initialized it will ensure the
Kubernetes is started. If it is provided with a document matching one
of the above MIME types, it will pass it to the appropriate &lt;code>kubecfg&lt;/code>
command to create the objects in Kubernetes.&lt;/p>
&lt;h2 id="creating-multipart-mime-archives">Creating multipart MIME archives&lt;/h2>
&lt;p>I have also created a &lt;a href="https://github.com/larsks/atomic-kubernetes-tools/blob/master/write-mime-multipart.py">modified version&lt;/a> of the standard
&lt;code>write-multipart-mime.py&lt;/code> Python script. This script will inspect the
first lines of files to determine their content type; in addition to
the standard &lt;code>cloud-init&lt;/code> types (like &lt;code>#cloud-config&lt;/code> for a
&lt;code>text/cloud-config&lt;/code> type file), this script recognizes:&lt;/p>
&lt;ul>
&lt;li>&lt;code>#kube-pod&lt;/code> for &lt;code>text/x-kube-pod&lt;/code>&lt;/li>
&lt;li>&lt;code>#kube-service&lt;/code> for &lt;code>text/x-kube-service&lt;/code>&lt;/li>
&lt;li>&lt;code>#kube-replica&lt;/code> for &lt;code>text/x-kube-replca&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>That is, a simple pod description might look something like:&lt;/p>
&lt;pre>&lt;code>#kube-pod
id: dbserver
desiredState:
manifest:
version: v1beta1
id: dbserver
containers:
- image: mysql
name: dbserver
env:
- name: MYSQL_ROOT_PASSWORD
value: secret
&lt;/code>&lt;/pre>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>Assuming that the pod description presented in the previous section is
stored in a file named &lt;code>dbserver.yaml&lt;/code>, we can bundle that file up
with our custom part handler like this:&lt;/p>
&lt;pre>&lt;code>$ write-mime-multipart.py \
kube-part-handler.py dbserver.yaml &amp;gt; userdata
&lt;/code>&lt;/pre>
&lt;p>We would then launch a Nova instance using the &lt;code>nova boot&lt;/code> command,
providing the generated &lt;code>userdata&lt;/code> file as an argument to the
&lt;code>user-data&lt;/code> command:&lt;/p>
&lt;pre>&lt;code>$ nova boot --image fedora-atomic --key-name mykey \
--flavor m1.small --user-data userdata my-atomic-server
&lt;/code>&lt;/pre>
&lt;p>You would obviously need to substitute values for &lt;code>--image&lt;/code> and
&lt;code>--key-name&lt;/code> that are appropriate for your environment.&lt;/p>
&lt;h2 id="details-details">Details, details&lt;/h2>
&lt;p>If you are experimenting with Fedora Atomic 21, you may find out that
the above example doesn&amp;rsquo;t work &amp;ndash; the official &lt;code>mysql&lt;/code> image generates
an selinux error. We can switch selinux to permissive mode by putting
the following into a file called &lt;code>disable-selinux.sh&lt;/code>:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
setenforce 0
sed -i '/^SELINUX=/ s/=.*/=permissive/' /etc/selinux/config
&lt;/code>&lt;/pre>
&lt;p>And then including that in our MIME archive:&lt;/p>
&lt;pre>&lt;code>$ write-mime-multipart.py \
kube-part-handler.py disable-selinux.sh dbserver.yaml &amp;gt; userdata
&lt;/code>&lt;/pre>
&lt;h2 id="a-brief-demonstration">A brief demonstration&lt;/h2>
&lt;p>If we launch an instance as described in the previous section and then
log in, we should find that the pod has already been scheduled:&lt;/p>
&lt;pre>&lt;code># kubecfg list pods
ID Image(s) Host Labels Status
---------- ---------- ---------- ---------- ----------
dbserver mysql / Waiting
&lt;/code>&lt;/pre>
&lt;p>At this point, &lt;code>docker&lt;/code> needs to pull the &lt;code>mysql&lt;/code> image locally, so
this step can take a bit depending on the state of your local internet
connection.&lt;/p>
&lt;p>Running &lt;code>docker ps&lt;/code> at this point will yield:&lt;/p>
&lt;pre>&lt;code># docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
3561e39f198c kubernetes/pause:latest &amp;quot;/pause&amp;quot; 46 seconds ago Up 43 seconds k8s--net.d96a64a9--dbserver.etcd--3d30eac0_-_745c_-_11e4_-_b32a_-_fa163e6e92ce--d872be51
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>pause&lt;/code> image here is a Kubernetes detail that is used to
configure the networking for a pod (in the Kubernetes world, a pod is
a group of linked containers that share a common network namespace).&lt;/p>
&lt;p>After a few minutes, you should eventually see:&lt;/p>
&lt;pre>&lt;code># docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
644c8fc5a79c mysql:latest &amp;quot;/entrypoint.sh mysq 3 minutes ago Up 3 minutes k8s--dbserver.fd48803d--dbserver.etcd--3d30eac0_-_745c_-_11e4_-_b32a_-_fa163e6e92ce--58794467
3561e39f198c kubernetes/pause:latest &amp;quot;/pause&amp;quot; 5 minutes ago Up 5 minutes k8s--net.d96a64a9--dbserver.etcd--3d30eac0_-_745c_-_11e4_-_b32a_-_fa163e6e92ce--d872be51
&lt;/code>&lt;/pre>
&lt;p>And &lt;code>kubecfg&lt;/code> should show the pod as running:&lt;/p>
&lt;pre>&lt;code># kubecfg list pods
ID Image(s) Host Labels Status
---------- ---------- ---------- ---------- ----------
dbserver mysql 127.0.0.1/ Running
&lt;/code>&lt;/pre>
&lt;h2 id="problems-problems">Problems, problems&lt;/h2>
&lt;p>This works and is I think a relatively elegant solution. However,
there are some drawbacks. In particular, the custom part handler
runs fairly early in the &lt;code>cloud-init&lt;/code> process, which means that it
cannot depend on changes implemented by &lt;code>user-data&lt;/code> scripts (because
these run much later).&lt;/p>
&lt;p>A better solution might be to have the custom part handler simply
write the Kubernetes configs into a directory somewhere, and then
install a service that launches after Kubernetes and (a) watches that
directory for files, then (b) passes the configuration to Kubernetes
and deletes (or relocates) the file.&lt;/p></content></item><item><title>Creating a Windows image for OpenStack</title><link>https://blog.oddbit.com/post/2014-11-15-creating-a-windows-image-for-o/</link><pubDate>Sat, 15 Nov 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-11-15-creating-a-windows-image-for-o/</guid><description>If you want to build a Windows image for use in your OpenStack environment, you can follow the example in the official documentation, or you can grab a Windows 2012r2 evaluation pre-built image from the nice folks at CloudBase.
The CloudBase-provided image is built using a set of scripts and configuration files that CloudBase has made available on GitHub.
The CloudBase repository is an excellent source of information, but I wanted to understand the process myself.</description><content>&lt;p>If you want to build a Windows image for use in your OpenStack
environment, you can follow &lt;a href="http://docs.openstack.org/image-guide/content/windows-image.html">the example in the official
documentation&lt;/a>, or you can grab a Windows 2012r2
evaluation &lt;a href="http://www.cloudbase.it/ws2012r2/">pre-built image&lt;/a> from the nice folks at &lt;a href="http://www.cloudbase.it/">CloudBase&lt;/a>.&lt;/p>
&lt;p>The CloudBase-provided image is built using a set of scripts and
configuration files that CloudBase has &lt;a href="https://github.com/cloudbase/windows-openstack-imaging-tools/">made available on
GitHub&lt;/a>.&lt;/p>
&lt;p>The CloudBase repository is an excellent source of information, but I
wanted to understand the process myself. This post describes the
process I went through to establish an automated process for
generating a Windows image suitable for use with OpenStack.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="unattended-windows-installs">Unattended windows installs&lt;/h2>
&lt;p>The Windows installer supports &lt;a href="http://technet.microsoft.com/en-us/library/ff699026.aspx">fully automated installations&lt;/a> through
the use of an answer file, or &amp;ldquo;unattend&amp;rdquo; file, that provides
information to the installer that would otherwise be provided
manually. The installer will look in &lt;a href="http://technet.microsoft.com/en-us/library/cc749415%28v=ws.10%29.aspx">a number of places&lt;/a> to find
this file. For our purposes, the important fact is that the installer
will look for a file named &lt;code>autounattend.xml&lt;/code> in the root of all
available read/write or read-only media. We&amp;rsquo;ll take advantage of this
by creating a file &lt;code>config/autounattend.xml&lt;/code>, and then generating an
ISO image like this:&lt;/p>
&lt;pre>&lt;code>mkisofs -J -r -o config.iso config
&lt;/code>&lt;/pre>
&lt;p>And we&amp;rsquo;ll attach this ISO to a vm later on in order to provide the
answer file to the installer.&lt;/p>
&lt;p>So, what goes into this answer file?&lt;/p>
&lt;p>The answer file is an XML document enclosed in an
&lt;code>&amp;lt;unattend&amp;gt;..&amp;lt;/unattend&amp;gt;&lt;/code> element. In order to provide all the
expected XML namespaces that may be used in the document, you would
typically start with something like this:&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version=&amp;quot;1.0&amp;quot; ?&amp;gt;
&amp;lt;unattend
xmlns=&amp;quot;urn:schemas-microsoft-com:unattend&amp;quot;
xmlns:ms=&amp;quot;urn:schemas-microsoft-com:asm.v3&amp;quot;
xmlns:wcm=&amp;quot;http://schemas.microsoft.com/WMIConfig/2002/State&amp;quot;&amp;gt;
&amp;lt;!-- your content goes here --&amp;gt;
&amp;lt;/unattend&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Inside this &lt;code>&amp;lt;unattend&amp;gt;&lt;/code> element you will put one or more &lt;code>&amp;lt;settings&amp;gt;&lt;/code>
elements, corresponding to the different &lt;a href="http://technet.microsoft.com/en-us/library/cc766245%28v=ws.10%29.aspx">configuration passes&lt;/a> of the
installer:&lt;/p>
&lt;pre>&lt;code>&amp;lt;settings pass=&amp;quot;specialize&amp;quot;&amp;gt;
&amp;lt;/settings&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The available configuration passes are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc749062%28v=ws.10%29.aspx">auditSystem&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc722343%28v=ws.10%29.aspx">auditUser&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc766229%28v=ws.10%29.aspx">generalize&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc749001%28v=ws.10%29.aspx">offlineServicing&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc748990%28v=ws.10%29.aspx">oobeSystem&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc722130%28v=ws.10%29.aspx">specialize&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://technet.microsoft.com/en-us/library/cc766028%28v=ws.10%29.aspx">windowsPE&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Of these, the most interesting for our use will be:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>windowsPE&lt;/code> &amp;ndash; used to install device drivers for use within the
installer environment. We will use this to install the VirtIO
drivers necessary to make VirtIO devices visible to the Windows
installer.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>specialize&lt;/code> &amp;ndash; In this pass, the installer applies machine-specific
configuration. This is typically used to configure networking,
locale settings, and most other things.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>oobeSystem&lt;/code> &amp;ndash; In this pass, the installer configures things that
happen at first boot. We use this to step to install some
additional software and run &lt;a href="http://technet.microsoft.com/en-us/library/cc721940%28v=ws.10%29.aspx">sysprep&lt;/a> in order to prepare the
image for use in OpenStack.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Inside each &lt;code>&amp;lt;settings&amp;gt;&lt;/code> element we will place one or more
&lt;code>&amp;lt;component&amp;gt;&lt;/code> elements that will apply specific pieces of
configuration. For example, the following &lt;code>&amp;lt;component&amp;gt;&lt;/code> configures
language and keyboard settings in the installer:&lt;/p>
&lt;pre>&lt;code>&amp;lt;settings pass=&amp;quot;windowsPE&amp;quot;&amp;gt;
&amp;lt;component name=&amp;quot;Microsoft-Windows-International-Core-WinPE&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;SetupUILanguage&amp;gt;
&amp;lt;UILanguage&amp;gt;en-US&amp;lt;/UILanguage&amp;gt;
&amp;lt;/SetupUILanguage&amp;gt;
&amp;lt;InputLocale&amp;gt;en-US&amp;lt;/InputLocale&amp;gt;
&amp;lt;UILanguage&amp;gt;en-US&amp;lt;/UILanguage&amp;gt;
&amp;lt;SystemLocale&amp;gt;en-US&amp;lt;/SystemLocale&amp;gt;
&amp;lt;UserLocale&amp;gt;en-US&amp;lt;/UserLocale&amp;gt;
&amp;lt;/component&amp;gt;
&amp;lt;/settings&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>&lt;a href="http://technet.microsoft.com/">Technet&lt;/a> provides documentation on the &lt;a href="http://technet.microsoft.com/en-us/library/ff699038.aspx">available components&lt;/a>.&lt;/p>
&lt;h2 id="cloud-init-for-windows">Cloud-init for Windows&lt;/h2>
&lt;p>&lt;a href="http://cloudinit.readthedocs.org/en/latest/">Cloud-init&lt;/a> is a tool that will configure a virtual instance when
it first boots, using metadata provided by the cloud service provider.
For example, when booting a Linux instance under OpenStack,
&lt;code>cloud-init&lt;/code> will contact the OpenStack metadata service at
http://169.254.169.254/ in order to retrieve things like the system
hostname, SSH keys, and so forth.&lt;/p>
&lt;p>While &lt;code>cloud-init&lt;/code> has support for Linux and BSD, it does not support
Windows. The folks at &lt;a href="http://www.cloudbase.it/">Cloudbase&lt;/a> have produced &lt;a href="http://www.cloudbase.it/cloud-init-for-windows-instances/">cloudbase-init&lt;/a>
in order to fill this gap. Once installed, the &lt;code>cloudbase-init&lt;/code> tool
will, upon first booting a system:&lt;/p>
&lt;ul>
&lt;li>Configure the network using information provided in the cloud
metadata&lt;/li>
&lt;li>Set the system hostname&lt;/li>
&lt;li>Create an initial user account (by default &amp;ldquo;Admin&amp;rdquo;) with a randomly
generated password (see below for details)&lt;/li>
&lt;li>Install your public key, if provided&lt;/li>
&lt;li>Execute a script provided via cloud &lt;code>user-data&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="passwords-and-ssh-keys">Passwords and ssh keys&lt;/h3>
&lt;p>While &lt;code>cloudbase-init&lt;/code> will install your SSH public key (by default
into &lt;code>/Users/admin/.ssh/authorized_keys&lt;/code>), Windows does not ship with
an SSH server and cloudbase-init does not install one. So what is it
doing with the public key?&lt;/p>
&lt;p>While you could arrange to install an ssh server that would make use
of the key, &lt;code>cloudbase-init&lt;/code> uses it for a completely unrelated
purpose: encrypting the randomly generated password. This encrypted
password is then passed back to OpenStack, where you can retrieve it
using the &lt;code>nova get-password&lt;/code> command, and decrypt it using the
corresponding SSH private key.&lt;/p>
&lt;p>Running &lt;code>nova get-password myinstance&lt;/code> will return something like:&lt;/p>
&lt;pre>&lt;code>w+In/P6+FeE8nv45oCjc5/Bohq4adqzoycwb9hOy9dlmuYbz0hiV923WW0fL
7hvQcZnWqGY7xLNnbJAeRFiSwv/MWvF3Sq8T0/IWhi6wBhAiVOxM95yjwIit
/L1Fm0TBARjoBuo+xq44YHpep1qzh4frsOo7TxvMHCOtibKTaLyCsioHjRaQ
dHk+uVFM1E0VIXyiqCdj421JoJzg32DqqeQTJJMqT9JiOL3FT26Y4XkVyJvI
vtUCQteIbd4jFtv3wEErJZKHgxHTLEYK+h67nTA4rXpvYVyKw9F8Qwj7JBTj
UJqp1syEqTR5/DUHYS+NoSdONUa+K7hhtSSs0bS1ghQuAdx2ifIA7XQ5eMRS
sXC4JH3d+wwtq4OmYYSOQkjmpKD8s5d4TgtG2dK8/l9B/1HTXa6qqcOw9va7
oUGGws3XuFEVq9DYmQ5NF54N7FU7NVl9UuRW3WTf4Q3q8VwJ4tDrmFSct6oG
2liJ8s7ybbW5PQU/lJe0gGBGGFzo8c+Rur17nsZ01+309JPEUKqUQT/uEg55
ziOo8uAwPvInvPkbxjH5doH79t47Erb3cK44kuqZy7J0RdDPtPr2Jel4NaSt
oCs+P26QF2NVOugsY9O/ugYfZWoEMUZuiwNWCWBqrIohB8JHcItIBQKBdCeY
7ORjotJU+4qAhADgfbkTqwo=
&lt;/code>&lt;/pre>
&lt;p>Providing your secret key as an additional parameter will decrypt the
password:&lt;/p>
&lt;pre>&lt;code>$ nova get-password myinstance ~/.ssh/id_rsa
fjgJmUB7fXF6wo
&lt;/code>&lt;/pre>
&lt;p>With an appropriately configured image, you could connect using an RDP
client and log in as the &amp;ldquo;Admin&amp;rdquo; user using that password.&lt;/p>
&lt;h3 id="passwords-without-ssh-keys">Passwords without ssh keys&lt;/h3>
&lt;p>If you do not provide your instance with an SSH key you will not be
able to retrieve the randomly generated password. However, if you can
get console access to your instance (e.g., via the Horizon dashboard),
you can log in as the &amp;ldquo;Administrator&amp;rdquo; user, at which point you will be
prompted to set an initial password for that account.&lt;/p>
&lt;h3 id="logging">Logging&lt;/h3>
&lt;p>You can find logs for &lt;code>cloudbase-init&lt;/code> in &lt;code>c:\program files (x86)\cloudbase solutions\cloudbase-init\log\cloudbase-init.log&lt;/code>.&lt;/p>
&lt;p>If appropriately configured, &lt;code>cloudbase-init&lt;/code> will also log to the
virtual serial port. This log is available in OpenStack by running
&lt;code>nova console-log &amp;lt;instance&amp;gt;&lt;/code>. For example:&lt;/p>
&lt;pre>&lt;code>$ nova console-log my-windows-server
2014-11-19 04:10:45.887 1272 INFO cloudbaseinit.init [-] Metadata service loaded: 'HttpService'
2014-11-19 04:10:46.339 1272 INFO cloudbaseinit.init [-] Executing plugin 'MTUPlugin'
2014-11-19 04:10:46.371 1272 INFO cloudbaseinit.init [-] Executing plugin 'NTPClientPlugin'
2014-11-19 04:10:46.387 1272 INFO cloudbaseinit.init [-] Executing plugin 'SetHostNamePlugin'
.
.
.
&lt;/code>&lt;/pre>
&lt;h2 id="putting-it-all-together">Putting it all together&lt;/h2>
&lt;p>I have an &lt;a href="https://github.com/larsks/windows-openstack-image/blob/master/install">install script&lt;/a> that drives the process, but it&amp;rsquo;s
ultimately just a wrapper for &lt;code>virt-install&lt;/code> and results in the
following invocation:&lt;/p>
&lt;pre>&lt;code>exec virt-install -n ws2012 -r 2048 \
-w network=default,model=virtio \
--disk path=$TARGET_IMAGE,bus=virtio \
--cdrom $WINDOWS_IMAGE \
--disk path=$VIRTIO_IMAGE,device=cdrom \
--disk path=$CONFIG_IMAGE,device=cdrom \
--os-type windows \
--os-variant win2k8 \
--vnc \
--console pty
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>TARGET_IMAGE&lt;/code> is the name of a pre-existing &lt;code>qcow2&lt;/code> image onto
which we will install Windows, &lt;code>WINDOWS_IMAGE&lt;/code> is the path to an ISO
containing Windows Server 2012r2, &lt;code>VIRTIO_IMAGE&lt;/code> is the path to an ISO
containing VirtIO drivers for Windows (available from the &lt;a href="https://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/bin/">Fedora
project&lt;/a>), and &lt;code>CONFIG_IMAGE&lt;/code> is a path to the ISO containing our
&lt;code>autounattend.xml&lt;/code> file.&lt;/p>
&lt;p>The fully commented &lt;a href="https://github.com/larsks/windows-openstack-image/blob/master/config/autounattend.xml">autounattend.xml&lt;/a> file, along with the script
mentioned above, are available in my &lt;a href="https://github.com/larsks/windows-openstack-image/">windows-openstack-image&lt;/a>
repository on GitHub.&lt;/p>
&lt;h2 id="the-answer-file-in-detail">The answer file in detail&lt;/h2>
&lt;h3 id="windowspe">windowsPE&lt;/h3>
&lt;p>In the &lt;a href="http://technet.microsoft.com/en-us/library/cc766028%28v=ws.10%29.aspx">windowsPE&lt;/a> phase, we start by configuring the installer locale
settings:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-International-Core-WinPE&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;SetupUILanguage&amp;gt;
&amp;lt;UILanguage&amp;gt;en-US&amp;lt;/UILanguage&amp;gt;
&amp;lt;/SetupUILanguage&amp;gt;
&amp;lt;InputLocale&amp;gt;en-US&amp;lt;/InputLocale&amp;gt;
&amp;lt;UILanguage&amp;gt;en-US&amp;lt;/UILanguage&amp;gt;
&amp;lt;SystemLocale&amp;gt;en-US&amp;lt;/SystemLocale&amp;gt;
&amp;lt;UserLocale&amp;gt;en-US&amp;lt;/UserLocale&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And installing the VirtIO drviers using the &lt;a href="http://technet.microsoft.com/en-us/library/ff715623.aspx">Microsoft-Windows-PnpCustomizationsWinPE&lt;/a> component:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-PnpCustomizationsWinPE&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot; language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot; processorArchitecture=&amp;quot;amd64&amp;quot;&amp;gt;
&amp;lt;DriverPaths&amp;gt;
&amp;lt;PathAndCredentials wcm:action=&amp;quot;add&amp;quot; wcm:keyValue=&amp;quot;1&amp;quot;&amp;gt;
&amp;lt;Path&amp;gt;d:\win8\amd64&amp;lt;/Path&amp;gt;
&amp;lt;/PathAndCredentials&amp;gt;
&amp;lt;/DriverPaths&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>This assumes that the VirtIO image is mounted as drive &lt;code>d:&lt;/code>.&lt;/p>
&lt;p>With the drivers installed, we can then call the
&lt;a href="http://technet.microsoft.com/en-us/library/ff715827.aspx">Microsoft-Windows-Setup&lt;/a> component to configure the disks and
install Windows. We start by configuring the product key:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-Setup&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;&amp;gt;
&amp;lt;UserData&amp;gt;
&amp;lt;AcceptEula&amp;gt;true&amp;lt;/AcceptEula&amp;gt;
&amp;lt;ProductKey&amp;gt;
&amp;lt;WillShowUI&amp;gt;OnError&amp;lt;/WillShowUI&amp;gt;
&amp;lt;Key&amp;gt;INSERT-PRODUCT-KEY-HERE&amp;lt;/Key&amp;gt;
&amp;lt;/ProductKey&amp;gt;
&amp;lt;/UserData&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And then configure the disk with a single partition (that will grow to
fill all the available space) which we then format with NTFS:&lt;/p>
&lt;pre>&lt;code> &amp;lt;DiskConfiguration&amp;gt;
&amp;lt;WillShowUI&amp;gt;OnError&amp;lt;/WillShowUI&amp;gt;
&amp;lt;Disk wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;DiskID&amp;gt;0&amp;lt;/DiskID&amp;gt;
&amp;lt;WillWipeDisk&amp;gt;true&amp;lt;/WillWipeDisk&amp;gt;
&amp;lt;CreatePartitions&amp;gt;
&amp;lt;CreatePartition wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;Order&amp;gt;1&amp;lt;/Order&amp;gt;
&amp;lt;Extend&amp;gt;true&amp;lt;/Extend&amp;gt;
&amp;lt;Type&amp;gt;Primary&amp;lt;/Type&amp;gt;
&amp;lt;/CreatePartition&amp;gt;
&amp;lt;/CreatePartitions&amp;gt;
&amp;lt;ModifyPartitions&amp;gt;
&amp;lt;ModifyPartition wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;Format&amp;gt;NTFS&amp;lt;/Format&amp;gt;
&amp;lt;Order&amp;gt;1&amp;lt;/Order&amp;gt;
&amp;lt;PartitionID&amp;gt;1&amp;lt;/PartitionID&amp;gt;
&amp;lt;Label&amp;gt;System&amp;lt;/Label&amp;gt;
&amp;lt;/ModifyPartition&amp;gt;
&amp;lt;/ModifyPartitions&amp;gt;
&amp;lt;/Disk&amp;gt;
&amp;lt;/DiskConfiguration&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We provide information about what to install:&lt;/p>
&lt;pre>&lt;code> &amp;lt;ImageInstall&amp;gt;
&amp;lt;OSImage&amp;gt;
&amp;lt;WillShowUI&amp;gt;Never&amp;lt;/WillShowUI&amp;gt;
&amp;lt;InstallFrom&amp;gt;
&amp;lt;MetaData&amp;gt;
&amp;lt;Key&amp;gt;/IMAGE/Name&amp;lt;/Key&amp;gt;
&amp;lt;Value&amp;gt;Windows Server 2012 R2 SERVERSTANDARDCORE&amp;lt;/Value&amp;gt;
&amp;lt;/MetaData&amp;gt;
&amp;lt;/InstallFrom&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And where we would like it installed:&lt;/p>
&lt;pre>&lt;code> &amp;lt;InstallTo&amp;gt;
&amp;lt;DiskID&amp;gt;0&amp;lt;/DiskID&amp;gt;
&amp;lt;PartitionID&amp;gt;1&amp;lt;/PartitionID&amp;gt;
&amp;lt;/InstallTo&amp;gt;
&amp;lt;/OSImage&amp;gt;
&amp;lt;/ImageInstall&amp;gt;
&lt;/code>&lt;/pre>
&lt;h3 id="specialize">specialize&lt;/h3>
&lt;p>In the &lt;a href="http://technet.microsoft.com/en-us/library/cc722130%28v=ws.10%29.aspx">specialize&lt;/a> phase, we start by setting the system name to a
randomly generated value using the &lt;a href="http://technet.microsoft.com/en-us/library/ff715801.aspx">Microsoft-Windows-Shell-Setup&lt;/a>
component:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-Shell-Setup&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot; language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot; processorArchitecture=&amp;quot;amd64&amp;quot;&amp;gt;
&amp;lt;ComputerName&amp;gt;*&amp;lt;/ComputerName&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We enable remote desktop because in an OpenStack environment this will
probably be the preferred mechanism with which to connect to the host
(but see &lt;a href="http://www.cloudbase.it/windows-without-passwords-in-openstack/">this document&lt;/a> for an alternative mechanism).&lt;/p>
&lt;p>First, we need to permit terminal server connections:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-TerminalServices-LocalSessionManager&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;fDenyTSConnections&amp;gt;false&amp;lt;/fDenyTSConnections&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And we do not want to require network-level authentication prior to
connecting:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-TerminalServices-RDP-WinStationExtensions&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;UserAuthentication&amp;gt;0&amp;lt;/UserAuthentication&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We will also need to open the necessary firewall group:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Networking-MPSSVC-Svc&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot;
versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;FirewallGroups&amp;gt;
&amp;lt;FirewallGroup wcm:action=&amp;quot;add&amp;quot; wcm:keyValue=&amp;quot;RemoteDesktop&amp;quot;&amp;gt;
&amp;lt;Active&amp;gt;true&amp;lt;/Active&amp;gt;
&amp;lt;Profile&amp;gt;all&amp;lt;/Profile&amp;gt;
&amp;lt;Group&amp;gt;@FirewallAPI.dll,-28752&amp;lt;/Group&amp;gt;
&amp;lt;/FirewallGroup&amp;gt;
&amp;lt;/FirewallGroups&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Finally, we use the &lt;a href="http://technet.microsoft.com/en-us/library/ff716283.aspx">Microsoft-Windows-Deployment&lt;/a> component to configure the Windows firewall to permit ICMP traffic:&lt;/p>
&lt;pre>&lt;code>&amp;lt;component name=&amp;quot;Microsoft-Windows-Deployment&amp;quot;
processorArchitecture=&amp;quot;amd64&amp;quot;
publicKeyToken=&amp;quot;31bf3856ad364e35&amp;quot;
language=&amp;quot;neutral&amp;quot; versionScope=&amp;quot;nonSxS&amp;quot;&amp;gt;
&amp;lt;RunSynchronous&amp;gt;
&amp;lt;RunSynchronousCommand wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;Order&amp;gt;3&amp;lt;/Order&amp;gt;
&amp;lt;Path&amp;gt;netsh advfirewall firewall add rule name=ICMP protocol=icmpv4 dir=in action=allow&amp;lt;/Path&amp;gt;
&amp;lt;/RunSynchronousCommand&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And to download the &lt;code>cloudbase-init&lt;/code> installer and make it available
for later steps:&lt;/p>
&lt;pre>&lt;code> &amp;lt;RunSynchronousCommand wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;Order&amp;gt;5&amp;lt;/Order&amp;gt;
&amp;lt;Path&amp;gt;powershell -NoLogo -Command &amp;quot;(new-object System.Net.WebClient).DownloadFile('https://www.cloudbase.it/downloads/CloudbaseInitSetup_Beta_x64.msi', 'c:\Windows\Temp\cloudbase.msi')&amp;quot;&amp;lt;/Path&amp;gt;
&amp;lt;/RunSynchronousCommand&amp;gt;
&amp;lt;/RunSynchronous&amp;gt;
&amp;lt;/component&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re using &lt;a href="http://technet.microsoft.com/en-us/scriptcenter/powershell.aspx">Powershell&lt;/a> here because it has convenient methods
available for downloading URLs to local files. This is roughly
equivalent to using &lt;code>curl&lt;/code> on a Linux system.&lt;/p>
&lt;h3 id="oobesystem">oobeSystem&lt;/h3>
&lt;p>In the &lt;a href="http://technet.microsoft.com/en-us/library/cc748990%28v=ws.10%29.aspx">oobeSystem&lt;/a> phase, we configure an automatic login for the
Administrator user:&lt;/p>
&lt;pre>&lt;code> &amp;lt;UserAccounts&amp;gt;
&amp;lt;AdministratorPassword&amp;gt;
&amp;lt;Value&amp;gt;Passw0rd&amp;lt;/Value&amp;gt;
&amp;lt;PlainText&amp;gt;true&amp;lt;/PlainText&amp;gt;
&amp;lt;/AdministratorPassword&amp;gt;
&amp;lt;/UserAccounts&amp;gt;
&amp;lt;AutoLogon&amp;gt;
&amp;lt;Password&amp;gt;
&amp;lt;Value&amp;gt;Passw0rd&amp;lt;/Value&amp;gt;
&amp;lt;PlainText&amp;gt;true&amp;lt;/PlainText&amp;gt;
&amp;lt;/Password&amp;gt;
&amp;lt;Enabled&amp;gt;true&amp;lt;/Enabled&amp;gt;
&amp;lt;LogonCount&amp;gt;50&amp;lt;/LogonCount&amp;gt;
&amp;lt;Username&amp;gt;Administrator&amp;lt;/Username&amp;gt;
&amp;lt;/AutoLogon&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>This automatic login only happens once, because we configure
&lt;code>FirstLogonCommands&lt;/code> that will first install &lt;code>cloudbase-init&lt;/code>:&lt;/p>
&lt;pre>&lt;code> &amp;lt;FirstLogonCommands&amp;gt;
&amp;lt;SynchronousCommand wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;CommandLine&amp;gt;msiexec /i c:\windows\temp\cloudbase.msi /qb /l*v c:\windows\temp\cloudbase.log LOGGINGSERIALPORTNAME=COM1&amp;lt;/CommandLine&amp;gt;
&amp;lt;Order&amp;gt;1&amp;lt;/Order&amp;gt;
&amp;lt;/SynchronousCommand&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And will then run &lt;code>sysprep&lt;/code> to generalize the system (which will,
among other things, lose the administrator password):&lt;/p>
&lt;pre>&lt;code> &amp;lt;SynchronousCommand wcm:action=&amp;quot;add&amp;quot;&amp;gt;
&amp;lt;CommandLine&amp;gt;c:\windows\system32\sysprep\sysprep /generalize /oobe /shutdown&amp;lt;/CommandLine&amp;gt;
&amp;lt;Order&amp;gt;2&amp;lt;/Order&amp;gt;
&amp;lt;/SynchronousCommand&amp;gt;
&amp;lt;/FirstLogonCommands&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>The system will shut down when &lt;code>sysprep&lt;/code> is complete, leaving you with a
Windows image suitable for uploading into OpenStack:&lt;/p>
&lt;pre>&lt;code>glance image-create --name ws2012 \
--disk-format qcow2 \
--container-format bare \
--file ws2012.qcow2
&lt;/code>&lt;/pre>
&lt;h2 id="troubleshooting">Troubleshooting&lt;/h2>
&lt;p>If you run into problems with an unattended Windows installation:&lt;/p>
&lt;p>During the first stage of the installer, you can look in the
&lt;code>x:\windows\panther&lt;/code> directory for &lt;code>setupact.log&lt;/code> and &lt;code>setuperr.log&lt;/code>,
which will have information about the early install process. The &lt;code>x:&lt;/code>
drive is temporary, and files here will be discarded when the system
reboots.&lt;/p>
&lt;p>Subsequent installer stages will log to
&lt;code>c:\windows\panther\&lt;/code>.&lt;/p>
&lt;p>If you are unfamiliar with Windows, the &lt;code>type&lt;/code> command can be used
very much like the &lt;code>cat&lt;/code> command on Linux, and the &lt;code>more&lt;/code> command
provides paging as you would expect. The &lt;code>notepad&lt;/code> command will open
a GUI text editor/viewer.&lt;/p>
&lt;p>You can emulate the &lt;code>tail&lt;/code> command using &lt;code>powershell&lt;/code>; to see the last
10 lines of a file:&lt;/p>
&lt;pre>&lt;code>C:\&amp;gt; powershell -command &amp;quot;Get-Content setupact.log -Tail 10&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Technet has a &lt;a href="http://technet.microsoft.com/en-us/library/hh825073.aspx">Deployment Troubleshooting and Log Files&lt;/a>
document that discusses in more detail what is logged and where to
find it.&lt;/p></content></item><item><title>Integrating custom code with Nova using hooks</title><link>https://blog.oddbit.com/post/2014-09-27-integrating-custom-code-with-n/</link><pubDate>Sat, 27 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-27-integrating-custom-code-with-n/</guid><description>Would you like to run some custom Python code when Nova creates and destroys virtual instances on your compute hosts? This is possible using Nova&amp;rsquo;s support for hooks, but the existing documentation is somewhat short on examples, so I&amp;rsquo;ve spent some time trying to get things working.
The demo_nova_hooks repository contains a working example of the techniques discussed in this article.
What&amp;rsquo;s a hook? A Nova &amp;ldquo;hook&amp;rdquo; is a mechanism that allows you to attach a class of your own design to a particular function or method call in Nova.</description><content>&lt;p>Would you like to run some custom Python code when Nova creates and
destroys virtual instances on your compute hosts? This is possible
using Nova&amp;rsquo;s support for &lt;a href="http://docs.openstack.org/developer/nova/devref/hooks.html">hooks&lt;/a>, but the existing documentation is
somewhat short on examples, so I&amp;rsquo;ve spent some time trying to get
things working.&lt;/p>
&lt;p>The &lt;a href="https://github.com/larsks/demo_nova_hooks">demo_nova_hooks&lt;/a> repository contains a working example of the
techniques discussed in this article.&lt;/p>
&lt;h2 id="whats-a-hook">What&amp;rsquo;s a hook?&lt;/h2>
&lt;p>A Nova &amp;ldquo;hook&amp;rdquo; is a mechanism that allows you to attach a class of your
own design to a particular function or method call in Nova. Your
class should define a &lt;code>pre&lt;/code> method (that will be called before the
method is called) and &lt;code>post&lt;/code> function (that will be called after the
method is called):&lt;/p>
&lt;pre>&lt;code>class YourHookClass(object):
def pre(self, *args, **kwargs):
....
def post(self, rv, *args, **kwargs):
....
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>pre&lt;/code> method will be called with the positional and keyword
arguments being passed to the hooked function. The &lt;code>post&lt;/code> method
receives the return value of the called method in addition to the
positional and keyword arguments.&lt;/p>
&lt;p>You connect your code to available hooks using &lt;a href="https://pythonhosted.org/setuptools/setuptools.html">Setuptools entry
points&lt;/a>. For example, assuming that the above code lived in
module named &lt;code>your_package.hooks&lt;/code>, you might have the following in the
corresponding &lt;code>setup.py&lt;/code> file:&lt;/p>
&lt;pre>&lt;code>entry_points = {
'nova.hooks': [
'create_instance=your_package.hooks:YourHookClass',
]
},
&lt;/code>&lt;/pre>
&lt;h2 id="what-hooks-are-available">What hooks are available?&lt;/h2>
&lt;p>The Nova code (as of &lt;a href="https://github.com/openstack/nova/commit/81b1babcd9699118f57d5055ff9045e275b536b5">81b1bab&lt;/a>) defines three hooks:&lt;/p>
&lt;ul>
&lt;li>&lt;code>create_instance&lt;/code>&lt;/li>
&lt;li>&lt;code>delete_instances&lt;/code>&lt;/li>
&lt;li>&lt;code>instance_network_info&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="create_instance">create_instance&lt;/h3>
&lt;p>The &lt;code>create_instance&lt;/code> hook is attached to the Nova API &lt;code>create&lt;/code>
function, and will receive the following arguments:&lt;/p>
&lt;pre>&lt;code>def create(self, context, instance_type,
image_href, kernel_id=None, ramdisk_id=None,
min_count=None, max_count=None,
display_name=None, display_description=None,
key_name=None, key_data=None, security_group=None,
availability_zone=None, user_data=None, metadata=None,
injected_files=None, admin_password=None,
block_device_mapping=None, access_ip_v4=None,
access_ip_v6=None, requested_networks=None, config_drive=None,
auto_disk_config=None, scheduler_hints=None, legacy_bdm=True,
shutdown_terminate=False, check_server_group_quota=False):
&lt;/code>&lt;/pre>
&lt;p>When called, &lt;code>self&lt;/code> is a &lt;code>nova.compute.api.API&lt;/code> object, &lt;code>context&lt;/code> is a
&lt;code>nova.context.RequestContext&lt;/code> object, &lt;code>instance_type&lt;/code> is a dictionary
containing information about the selected flavor, and &lt;code>image_href&lt;/code> is
an image UUID.&lt;/p>
&lt;p>During my testing, the &lt;code>instance_type&lt;/code> dictionary looked like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>{'created_at': None,
'deleted': 0L,
'deleted_at': None,
'disabled': False,
'ephemeral_gb': 0L,
'extra_specs': {},
'flavorid': u'2',
'id': 5L,
'is_public': True,
'memory_mb': 2048L,
'name': u'm1.small',
'root_gb': 20L,
'rxtx_factor': 1.0,
'swap': 0L,
'updated_at': None,
'vcpu_weight': None,
'vcpus': 1L}
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;corresponding to the &lt;code>m1.small&lt;/code> flavor on my system.&lt;/p>
&lt;h3 id="delete_instance">delete_instance&lt;/h3>
&lt;p>The &lt;code>delete_instance&lt;/code> hook is attached to the &lt;code>_delete_instance&lt;/code>
method in the &lt;code>nova.compute.manager.ComputeManager&lt;/code> class, which is
called whenever Nova needs to delete an instance. The hook will
receive the following arguments:&lt;/p>
&lt;pre>&lt;code>def _delete_instance(self, context, instance, bdms, quotas):
&lt;/code>&lt;/pre>
&lt;p>Where:&lt;/p>
&lt;ul>
&lt;li>&lt;code>self&lt;/code> is a &lt;code>nova.compute.manager.ComputeManager&lt;/code> object,&lt;/li>
&lt;li>&lt;code>context&lt;/code> is a &lt;code>nova.context.RequestContext&lt;/code>,&lt;/li>
&lt;li>&lt;code>instance&lt;/code> is a &lt;code>nova.objects.instance.Instance&lt;/code> object&lt;/li>
&lt;li>&lt;code>bdms&lt;/code> is a &lt;code>nova.objects.block_device.BlockDeviceMappingList&lt;/code>
object, and&lt;/li>
&lt;li>&lt;code>quotas&lt;/code> is a &lt;code>nova.objects.quotas.Quotas&lt;/code> object&lt;/li>
&lt;/ul>
&lt;h3 id="instance_network_info">instance_network_info&lt;/h3>
&lt;p>The &lt;code>instance_network_info&lt;/code> hook is attached to the
&lt;code>update_instance_cache_with_nw_info&lt;/code> function in
&lt;code>nova.network.base_api.py&lt;/code>. The hook will receive the following
arguments:&lt;/p>
&lt;pre>&lt;code>def update_instance_cache_with_nw_info(impl, context, instance,
nw_info=None, update_cells=True):
&lt;/code>&lt;/pre>
&lt;p>I am not running Nova Network in my environment, so I have not
examined this hook in any additional detail.&lt;/p>
&lt;h2 id="a-working-example">A working example&lt;/h2>
&lt;p>The &lt;a href="https://github.com/larsks/demo_nova_hooks">demo_nova_hooks&lt;/a> repository implements simple logging-only
implementations of &lt;code>create_instance&lt;/code> and &lt;code>delete_instance&lt;/code> hooks. You
can install this code, restart Nova services, boot an instances, and
verify that the code has executed by looking at the logs generated in
&lt;code>/var/log/nova&lt;/code>.&lt;/p></content></item><item><title>Heat Hangout</title><link>https://blog.oddbit.com/post/2014-09-05-heat-hangout/</link><pubDate>Fri, 05 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-05-heat-hangout/</guid><description>I ran a Google Hangout this morning on Deploying with Heat. You can find the slides for the presentation on line here, and the Heat templates (as well as slide sources) are available on github.
If you have any questions about the presentation, please feel free to ping me on irc (larsks).</description><content>&lt;p>I ran a Google Hangout this morning on &lt;a href="https://plus.google.com/events/c9u4sjn7ksb8jrmma7vd25aok94">Deploying with Heat&lt;/a>. You
can find the slides for the presentation on line &lt;a href="http://oddbit.com/rdo-hangout-heat-intro/#/">here&lt;/a>, and the
Heat templates (as well as slide sources) are available &lt;a href="https://github.com/larsks/rdo-hangout-heat-intro/">on
github&lt;/a>.&lt;/p>
&lt;p>If you have any questions about the presentation, please feel free to
ping me on irc (&lt;code>larsks&lt;/code>).&lt;/p>
&lt;!-- raw HTML omitted --></content></item><item><title>Visualizing Heat stacks</title><link>https://blog.oddbit.com/post/2014-09-02-visualizing-heat-stacks/</link><pubDate>Tue, 02 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-02-visualizing-heat-stacks/</guid><description>I spent some time today learning about Heat autoscaling groups, which are incredibly nifty but a little opaque from the Heat command line, since commands such as heat resource-list don&amp;rsquo;t recurse into nested stacks. It is possible to introspect these resources (you can pass the physical resource id of a nested stack to heat resource-list, for example)&amp;hellip;
&amp;hellip;but I really like visualizing things, so I wrote a quick hack called dotstack that will generate dot language output from a Heat stack.</description><content>&lt;p>I spent some time today learning about Heat &lt;a href="https://wiki.openstack.org/wiki/Heat/AutoScaling">autoscaling groups&lt;/a>,
which are incredibly nifty but a little opaque from the Heat command
line, since commands such as &lt;code>heat resource-list&lt;/code> don&amp;rsquo;t recurse into
nested stacks. It is possible to introspect these resources (you can
pass the physical resource id of a nested stack to &lt;code>heat resource-list&lt;/code>, for example)&amp;hellip;&lt;/p>
&lt;p>&amp;hellip;but I really like visualizing things, so I wrote a quick hack
called &lt;a href="http://github.com/larsks/dotstack">dotstack&lt;/a> that will generate &lt;a href="http://en.wikipedia.org/wiki/DOT_(graph_description_language)">dot&lt;/a> language output from a
Heat stack. You can process this with &lt;a href="http://www.graphviz.org/">Graphviz&lt;/a> to produce output
like this, in which graph nodes are automatically colorized by
resource type:&lt;/p>
&lt;figure class="left" >
&lt;img src="sample.svg" />
&lt;/figure>
&lt;p>Or like this, in which each node contains information about its
resource type and physical resource id:&lt;/p>
&lt;figure class="left" >
&lt;img src="sample-detailed.svg" />
&lt;/figure>
&lt;p>The source code is available on &lt;a href="http://github.com/larsks/dotstack">github&lt;/a>.&lt;/p></content></item><item><title>Docker plugin bugs</title><link>https://blog.oddbit.com/post/2014-09-01-docker-plugin-bugs/</link><pubDate>Mon, 01 Sep 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-09-01-docker-plugin-bugs/</guid><description>This is a companion to my article on the Docker plugin for Heat.
While writing that article, I encountered a number of bugs in the Docker plugin and elsewhere. I&amp;rsquo;ve submitted patches for most of the issues I encountered:
Bugs in the Heat plugin https://bugs.launchpad.net/heat/+bug/1364017
docker plugin fails to delete a container resource in CREATE_FAILED state.
https://bugs.launchpad.net/heat/+bug/1364041
docker plugin volumes_from parameter should be a list.
https://bugs.launchpad.net/heat/+bug/1364039
docker plugin volumes_from parameter results in an error</description><content>&lt;p>This is a companion to my &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/">article on the Docker plugin for Heat&lt;/a>.&lt;/p>
&lt;p>While writing that article, I encountered a number of bugs in the
Docker plugin and elsewhere. I&amp;rsquo;ve submitted patches for most of the
issues I encountered:&lt;/p>
&lt;h2 id="bugs-in-the-heat-plugin">Bugs in the Heat plugin&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364017">https://bugs.launchpad.net/heat/+bug/1364017&lt;/a>&lt;/p>
&lt;p>docker plugin fails to delete a container resource in
&lt;code>CREATE_FAILED&lt;/code> state.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364041">https://bugs.launchpad.net/heat/+bug/1364041&lt;/a>&lt;/p>
&lt;p>docker plugin &lt;code>volumes_from&lt;/code> parameter should be a list.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364039">https://bugs.launchpad.net/heat/+bug/1364039&lt;/a>&lt;/p>
&lt;p>docker plugin &lt;code>volumes_from&lt;/code> parameter results in an error&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://bugs.launchpad.net/heat/+bug/1364019">https://bugs.launchpad.net/heat/+bug/1364019&lt;/a>&lt;/p>
&lt;p>docker plugin does not actually remove containers on delete&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugs-in-docker-python-module">Bugs in docker Python module&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/docker/docker-py/pull/310">https://github.com/docker/docker-py/pull/310&lt;/a>&lt;/p>
&lt;p>allow ports to be specified as &lt;code>port/proto&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ul></content></item><item><title>Annotated documentation for DockerInc::Docker::Container</title><link>https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/</guid><description>This is a companion to my article on the Docker plugin for Heat.
DockerInc::Docker::Container Properties cmd : List
Command to run after spawning the container.
Optional property.
Example:
cmd: [ 'thttpd', '-C', '/etc/thttpd.conf', '-D', '-c', '*.cgi'] dns : List
Set custom DNS servers.
Example:
dns: - 8.8.8.8 - 8.8.4.4 docker_endopint : String
Docker daemon endpoint. By default the local Docker daemon will be used.
Example:
docker_endpoint: tcp://192.168.1.100:2375 env : String</description><content>&lt;p>This is a companion to my &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/">article on the Docker plugin for Heat&lt;/a>.&lt;/p>
&lt;h2 id="dockerincdockercontainer">DockerInc::Docker::Container&lt;/h2>
&lt;h3 id="properties">Properties&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;code>cmd&lt;/code> : List&lt;/p>
&lt;p>Command to run after spawning the container.&lt;/p>
&lt;p>Optional property.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> cmd: [ 'thttpd', '-C', '/etc/thttpd.conf', '-D', '-c', '*.cgi']
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>dns&lt;/code> : List&lt;/p>
&lt;p>Set custom DNS servers.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> dns:
- 8.8.8.8
- 8.8.4.4
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>docker_endopint&lt;/code> : String&lt;/p>
&lt;p>Docker daemon endpoint. By default the local Docker daemon will
be used.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> docker_endpoint: tcp://192.168.1.100:2375
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>env&lt;/code> : String&lt;/p>
&lt;p>Set environment variables.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> env:
- MYSQL_ROOT_PASSWORD=secret
- &amp;quot;ANOTHER_VARIABLE=something long with spaces&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>hostname&lt;/code> : String&lt;/p>
&lt;p>Hostname of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> hostname: mywebserver
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>image&lt;/code> : String&lt;/p>
&lt;p>Image name to boot.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> image: mysql
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>links&lt;/code> : Mapping&lt;/p>
&lt;p>Links to other containers.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> links:
name_in_this_container: name_of_that_container
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>memory&lt;/code> : Number&lt;/p>
&lt;p>Memory limit in bytes.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> # 512 MB
memory: 536870912
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>name&lt;/code> : String&lt;/p>
&lt;p>Name of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> name: dbserver
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>open_stdin&lt;/code> : Boolean&lt;/p>
&lt;p>True to open &lt;code>stdin&lt;/code>.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> open_stdin: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>port_bindings&lt;/code> : Map&lt;/p>
&lt;p>TCP/UDP port bindings.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> # bind port 8080 in the container to port 80 on the host
port_bindings:
8080: 80
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>port_specs&lt;/code> : List&lt;/p>
&lt;p>List of TCP/UDP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> port_specs:
- 80
- 53/udp
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>privileged&lt;/code> : Boolean&lt;/p>
&lt;p>Enable extended privileges.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> privileged: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>stdin_once&lt;/code> : Boolean&lt;/p>
&lt;p>If &lt;code>true&lt;/code>, close &lt;code>stdin&lt;/code> after the one attached client disconnects.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> stdin_once: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>tty&lt;/code> : Boolean&lt;/p>
&lt;p>Allocate a pseudo-tty if &lt;code>true&lt;/code>.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> tty: true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>user&lt;/code> : String&lt;/p>
&lt;p>Username or UID for running the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> username: apache
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>volumes&lt;/code> : Map&lt;/p>
&lt;p>Create a bind mount.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> volumes:
/var/tmp/data_on_host: /srv/data_in_container
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>volumes_from&lt;/code> : String&lt;/p>
&lt;p>&lt;em>This option is broken in the current version of the Docker
plugin.&lt;/em>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="attributes">Attributes&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;code>info&lt;/code> : Map&lt;/p>
&lt;p>Information about the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> info:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;info&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> {
&amp;quot;HostsPath&amp;quot;: &amp;quot;/var/lib/docker/containers/d6d84d1bbf2984fa3e04cea36c8d10d27d318b6d96b57c41fca2cbc1da23bf71/hosts&amp;quot;,
&amp;quot;Created&amp;quot;: &amp;quot;2014-09-01T14:21:02.7577874Z&amp;quot;,
&amp;quot;Image&amp;quot;: &amp;quot;a950533b3019d8f6dfdcb8fdc42ef810b930356619b3e4786d4f2acec514238d&amp;quot;,
&amp;quot;Args&amp;quot;: [
&amp;quot;mysqld&amp;quot;,
&amp;quot;--datadir=/var/lib/mysql&amp;quot;,
&amp;quot;--user=mysql&amp;quot;
],
&amp;quot;Driver&amp;quot;: &amp;quot;devicemapper&amp;quot;,
&amp;quot;HostConfig&amp;quot;: {
&amp;quot;CapDrop&amp;quot;: null,
&amp;quot;PortBindings&amp;quot;: {
&amp;quot;3306/tcp&amp;quot;: [
{
&amp;quot;HostPort&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;HostIp&amp;quot;: &amp;quot;&amp;quot;
}
]
},
&amp;quot;NetworkMode&amp;quot;: &amp;quot;&amp;quot;,
.
.
.
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>logs&lt;/code> : String&lt;/p>
&lt;p>Logs from the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> logs:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;logs&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>logs_head&lt;/code> : String&lt;/p>
&lt;p>Most recent log line from the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> logs:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;logs_head&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;2014-09-01 14:21:04 0 [Warning] TIMESTAMP with implicit DEFAULT
value is deprecated. Please use --explicit_defaults_for_timestamp
server option (see documentation for more details).&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_gateway&lt;/code> : String&lt;/p>
&lt;p>IP address of the network gateway for the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_gateway:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_gateway&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;172.17.42.1&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_info&lt;/code> : Map&lt;/p>
&lt;p>Information about the network configuration of the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_info:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_info&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> {
&amp;quot;Bridge&amp;quot;: &amp;quot;docker0&amp;quot;,
&amp;quot;TcpPorts&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;PortMapping&amp;quot;: null,
&amp;quot;IPPrefixLen&amp;quot;: 16,
&amp;quot;UdpPorts&amp;quot;: &amp;quot;&amp;quot;,
&amp;quot;IPAddress&amp;quot;: &amp;quot;172.17.0.10&amp;quot;,
&amp;quot;Gateway&amp;quot;: &amp;quot;172.17.42.1&amp;quot;,
&amp;quot;Ports&amp;quot;: {
&amp;quot;3306/tcp&amp;quot;: [
{
&amp;quot;HostPort&amp;quot;: &amp;quot;3306&amp;quot;,
&amp;quot;HostIp&amp;quot;: &amp;quot;0.0.0.0&amp;quot;
}
]
}
}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_ip&lt;/code> : String&lt;/p>
&lt;p>IP address assigned to the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_ip:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_ip&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;172.17.0.10&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_tcp_ports&lt;/code> : String&lt;/p>
&lt;p>A comma delimited list of TCP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_tcp_ports:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_tcp_ports&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;8443,8080&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;code>network_udp_ports&lt;/code> : String&lt;/p>
&lt;p>A comma delimited list of TCP ports exposed by the container.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;pre>&lt;code> network_udp_ports:
value: {get_attr: [&amp;quot;docker_dbserver&amp;quot;, &amp;quot;network_udp_ports&amp;quot;]}
&lt;/code>&lt;/pre>
&lt;p>Output:&lt;/p>
&lt;pre>&lt;code> &amp;quot;8443,8080&amp;quot;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul></content></item><item><title>Docker plugin for OpenStack Heat</title><link>https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/</guid><description>I have been looking at both Docker and OpenStack recently. In my last post I talked a little about the Docker driver for Nova; in this post I&amp;rsquo;ll be taking an in-depth look at the Docker plugin for Heat, which has been available since the Icehouse release but is surprisingly under-documented.
The release announcement on the Docker blog includes an example Heat template, but it is unfortunately grossly inaccurate and has led many people astray.</description><content>&lt;p>I have been looking at both Docker and OpenStack recently. In my &lt;a href="https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/">last
post&lt;/a> I talked a little about the &lt;a href="https://github.com/stackforge/nova-docker">Docker driver for Nova&lt;/a>; in
this post I&amp;rsquo;ll be taking an in-depth look at the Docker plugin for
Heat, which has been available &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">since the Icehouse release&lt;/a> but is
surprisingly under-documented.&lt;/p>
&lt;p>The &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">release announcement&lt;/a> on the Docker blog includes an
example Heat template, but it is unfortunately grossly inaccurate and
has led many people astray. In particular:&lt;/p>
&lt;ul>
&lt;li>It purports to but does not actually install Docker, due to a basic
&lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a> syntax error, and&lt;/li>
&lt;li>Even if you were to fix that problem, the lack of synchronization
between the two resources in the template would mean that you would
never be able to successfully launch a container.&lt;/li>
&lt;/ul>
&lt;p>In this post, I will present a fully functional example that will work
with the Icehouse release of Heat. We will install the Docker plugin
for Heat, then write a template that will (a) launch a Fedora 20
server and automatically install Docker, and then (b) use the Docker
plugin to launch some containers on that server.&lt;/p>
&lt;p>The &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a> referenced in this article can be found on GitHub:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/larsks/heat-docker-example">https://github.com/larsks/heat-docker-example&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="installing-the-docker-plugin">Installing the Docker plugin&lt;/h2>
&lt;p>The first thing we need to do is install the Docker plugin. I am
running &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> packages for Icehouse locally, which do not include
the Docker plugin. We&amp;rsquo;r going to install the plugin from the Heat
sources.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Download the Heat repository:&lt;/p>
&lt;pre>&lt;code> $ git clone https://github.com/openstack/heat.git
Cloning into 'heat'...
remote: Counting objects: 50382, done.
remote: Compressing objects: 100% (22/22), done.
remote: Total 50382 (delta 7), reused 1 (delta 0)
Receiving objects: 100% (50382/50382), 19.84 MiB | 1.81 MiB/s, done.
Resolving deltas: 100% (34117/34117), done.
Checking connectivity... done.
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>This will result in a directory called &lt;code>heat&lt;/code> in your current
working directory. Change into this directory:&lt;/p>
&lt;pre>&lt;code> $ cd heat
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Patch the Docker plugin.&lt;/p>
&lt;p>You have now checked out the &lt;code>master&lt;/code> branch of the Heat
repository; this is the most recent code committed to the project.
At this point we could check out the &lt;code>stable/icehouse&lt;/code> branch of
the repository to get the version of the plugin released at the
same time as the version of Heat that we&amp;rsquo;re running, but we would
find that the Docker plugin was, at that point in time, somewhat
crippled; in particular:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>It does not support mapping container ports to host ports, so
there is no easy way to expose container services for external
access, and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>It does not know how to automatically &lt;code>pull&lt;/code> missing images, so
you must arrange to run &lt;code>docker pull&lt;/code> a priori for each image you
plan to use in your Heat template.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>That would make us sad, so instead we&amp;rsquo;re going to use the plugin
from the &lt;code>master&lt;/code> branch, which only requires a trivial change in
order to work with the Icehouse release of Heat.&lt;/p>
&lt;p>Look at the file
&lt;code>contrib/heat_docker/heat_docker/resources/docker_container.py&lt;/code>.
Locate the following line:&lt;/p>
&lt;pre>&lt;code> attributes_schema = {
&lt;/code>&lt;/pre>
&lt;p>Add a line immediately before that so that the file look like
this:&lt;/p>
&lt;pre>&lt;code> attributes.Schema = lambda x: x
attributes_schema = {
&lt;/code>&lt;/pre>
&lt;p>If you&amp;rsquo;re curious, here is what we accomplished with that
additional line:&lt;/p>
&lt;p>The code following that point contains multiple stanzas of the
form:&lt;/p>
&lt;pre>&lt;code> INFO: attributes.Schema(
_('Container info.')
),
&lt;/code>&lt;/pre>
&lt;p>In Icehouse, the &lt;code>heat.engine.attributes&lt;/code> module does not have a
&lt;code>Schema&lt;/code> class so this fails. Our patch above adds a module
member named &lt;code>Schema&lt;/code> that simply returns it&amp;rsquo;s arguments (that
is, it is an identity function).&lt;/p>
&lt;p>(&lt;strong>NB&lt;/strong>: At the time this was written, Heat&amp;rsquo;s &lt;code>master&lt;/code> branch was
at &lt;code>a767880&lt;/code>.)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Install the Docker plugin into your Heat plugin directory, which
on my system is &lt;code>/usr/lib/heat&lt;/code> (you can set this explicitly using
the &lt;code>plugin_dirs&lt;/code> directive in &lt;code>/etc/heat/heat.conf&lt;/code>):&lt;/p>
&lt;pre>&lt;code> $ rsync -a --exclude=tests/ contrib/heat_docker/heat_docker \
/usr/lib/heat
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re excluding the &lt;code>tests&lt;/code> directory here because it has
additional prerequisites that aren&amp;rsquo;t operationally necessary but
that will prevent Heat from starting up if they are missing.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Restart your &lt;code>heat-engine&lt;/code> service. On Fedora, that would be:&lt;/p>
&lt;pre>&lt;code> # systemctl restart openstack-heat-engine
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>Verify that the new &lt;code>DockerInc::Docker::Container&lt;/code> resource is
available:&lt;/p>
&lt;pre>&lt;code> $ heat resource-type-list | grep Docker
| DockerInc::Docker::Container |
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="templates-installing-docker">Templates: Installing docker&lt;/h2>
&lt;p>We would like our template to automatically install Docker on a Nova
server. The example in the &lt;a href="https://blog.docker.com/2014/03/docker-will-be-in-openstack-icehouse/">Docker blog&lt;/a> mentioned earlier
attempts to do this by setting the &lt;code>user_data&lt;/code> parameter of a
&lt;code>OS::Nova::Server&lt;/code> resource like this:&lt;/p>
&lt;pre>&lt;code>user_data: #include https://get.docker.io
&lt;/code>&lt;/pre>
&lt;p>Unfortunately, an unquoted &lt;code>#&lt;/code> introduces a comment in &lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a>, so
this is completely ignored. It would be written more correctly like
this (the &lt;code>|&lt;/code> introduces a block of literal text):&lt;/p>
&lt;pre>&lt;code>user_data: |
#include https://get.docker.io
&lt;/code>&lt;/pre>
&lt;p>Or possibly like this, although this would restrict you to a single
line and thus wouldn&amp;rsquo;t be used much in practice:&lt;/p>
&lt;pre>&lt;code>user_data: &amp;quot;#include https://get.docker.io&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And, all other things being correct, this would install Docker on a
system&amp;hellip;but would not necessarily start it, nor would it configure
Docker to listen on a TCP socket. On my Fedora system, I ended up
creating the following &lt;code>user_data&lt;/code> script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
yum -y upgrade
# I have occasionally seen 'yum install' fail with errors
# trying to contact mirrors. Because it can be a pain to
# delete and re-create the stack, just loop here until it
# succeeds.
while :; do
yum -y install docker-io
[ -x /usr/bin/docker ] &amp;amp;&amp;amp; break
sleep 5
done
# Add a tcp socket for docker
cat &amp;gt; /etc/systemd/system/docker-tcp.socket &amp;lt;&amp;lt;EOF
[Unit]
Description=Docker remote access socket
[Socket]
ListenStream=2375
BindIPv6Only=both
Service=docker.service
[Install]
WantedBy=sockets.target
EOF
# Start and enable the docker service.
for sock in docker.socket docker-tcp.socket; do
systemctl start $sock
systemctl enable $sock
done
&lt;/code>&lt;/pre>
&lt;p>This takes care of making sure our packages are current, installing
Docker, and arranging for it to listen on a tcp socket. For that last
bit, we&amp;rsquo;re creating a new &lt;code>systemd&lt;/code> socket file
(&lt;code>/etc/systemd/system/docker-tcp.socket&lt;/code>), which means that &lt;code>systemd&lt;/code>
will actually open the socket for listening and start &lt;code>docker&lt;/code> if
necessary when a client connects.&lt;/p>
&lt;h2 id="templates-synchronizing-resources">Templates: Synchronizing resources&lt;/h2>
&lt;p>In our Heat template, we are starting a Nova server that will run
Docker, and then we are instantiating one or more Docker containers
that will run on this server. This means that timing is suddenly very
important. If we use the &lt;code>user_data&lt;/code> script as presented in the
previous section, we would probably end up with an error like this in
our &lt;code>heat-engine.log&lt;/code>:&lt;/p>
&lt;pre>&lt;code>2014-08-29 17:10:37.598 15525 TRACE heat.engine.resource ConnectionError:
HTTPConnectionPool(host='192.168.200.11', port=2375): Max retries exceeded
with url: /v1.12/containers/create (Caused by &amp;lt;class 'socket.error'&amp;gt;:
[Errno 113] EHOSTUNREACH)
&lt;/code>&lt;/pre>
&lt;p>This happens because it takes &lt;em>time&lt;/em> to install packages. Absent any
dependencies, Heat creates resources in parallel, so Heat is happily
trying to spawn our Docker containers when our server is still
fetching the Docker package.&lt;/p>
&lt;p>Heat does have a &lt;code>depends_on&lt;/code> property that can be applied to
resources. For example, if we have:&lt;/p>
&lt;pre>&lt;code>docker_server:
type: &amp;quot;OS::Nova::Server&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>We can make a Docker container depend on that resource:&lt;/p>
&lt;pre>&lt;code>docker_container_mysql:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
depends_on:
- docker_server
&lt;/code>&lt;/pre>
&lt;p>Looks good, but this does not, in fact, help us. From Heat&amp;rsquo;s
perspective, the dependency is satisfied as soon as the Nova server
&lt;em>boots&lt;/em>, so really we&amp;rsquo;re back where we started.&lt;/p>
&lt;p>The Heat solution to this is the &lt;code>AWS::CloudFormation::WaitCondition&lt;/code>
resource (and its boon companion, the and
&lt;code>AWS::CloudFormation::WaitConditionHandle&lt;/code> resource). A
&lt;code>WaitCondition&lt;/code> is a resource this is not &amp;ldquo;created&amp;rdquo; until it has
received an external signal. We define a wait condition like this:&lt;/p>
&lt;pre>&lt;code>docker_wait_handle:
type: &amp;quot;AWS::CloudFormation::WaitConditionHandle&amp;quot;
docker_wait_condition:
type: &amp;quot;AWS::CloudFormation::WaitCondition&amp;quot;
depends_on:
- docker_server
properties:
Handle:
get_resource: docker_wait_handle
Timeout: &amp;quot;6000&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And then we make our container depend on the wait condition:&lt;/p>
&lt;pre>&lt;code>docker_container_mysql:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
depends_on:
- docker_wait_condition
&lt;/code>&lt;/pre>
&lt;p>With this in place, Heat will not attempt to create the Docker
container until we signal the wait condition resource. In order to do
that, we need to modify our &lt;code>user_data&lt;/code> script to embed the
notification URL generated by heat. We&amp;rsquo;ll use both the &lt;code>get_resource&lt;/code>
and &lt;code>str_replace&lt;/code> &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#intrinsic-functions">intrinsic function&lt;/a> in order to generate the appropriate
script:&lt;/p>
&lt;pre>&lt;code> user_data:
# We're using Heat's 'str_replace' function in order to
# substitute into this script the Heat-generated URL for
# signaling the docker_wait_condition resource.
str_replace:
template: |
#!/bin/sh
yum -y upgrade
# I have occasionally seen 'yum install' fail with errors
# trying to contact mirrors. Because it can be a pain to
# delete and re-create the stack, just loop here until it
# succeeds.
while :; do
yum -y install docker-io
[ -x /usr/bin/docker ] &amp;amp;&amp;amp; break
sleep 5
done
# Add a tcp socket for docker
cat &amp;gt; /etc/systemd/system/docker-tcp.socket &amp;lt;&amp;lt;EOF
[Unit]
Description=Docker remote access socket
[Socket]
ListenStream=2375
BindIPv6Only=both
Service=docker.service
[Install]
WantedBy=sockets.target
EOF
# Start and enable the docker service.
for sock in docker.socket docker-tcp.socket; do
systemctl start $sock
systemctl enable $sock
done
# Signal heat that we are finished settings things up.
cfn-signal -e0 --data 'OK' -r 'Setup complete' '$WAIT_HANDLE'
params:
&amp;quot;$WAIT_HANDLE&amp;quot;:
get_resource: docker_wait_handle
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>str_replace&lt;/code> function probably deserves a closer look; the
general format is:&lt;/p>
&lt;pre>&lt;code>str_replace:
template:
params:
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>template&lt;/code> is text content containing 0 or more things to be
replaced, and &lt;code>params&lt;/code> is a list of tokens to search for and replace
in the &lt;code>template&lt;/code>.&lt;/p>
&lt;p>We use &lt;code>str_replace&lt;/code> to substitute the token &lt;code>$WAIT_HANDLE&lt;/code> with the
result of calling &lt;code>get_resource&lt;/code> on our &lt;code>docker_wait_handle&lt;/code> resource.
This results in a URL that contains an EC2-style signed URL that will
deliver the necessary notification to Heat. In this example we&amp;rsquo;re
using the &lt;code>cfn-signal&lt;/code> tool, which is included in the Fedora cloud
images, but you could accomplish the same thing with &lt;code>curl&lt;/code>:&lt;/p>
&lt;pre>&lt;code>curl -X PUT -H 'Content-Type: application/json' \
--data-binary '{&amp;quot;Status&amp;quot;: &amp;quot;SUCCESS&amp;quot;,
&amp;quot;Reason&amp;quot;: &amp;quot;Setup complete&amp;quot;,
&amp;quot;Data&amp;quot;: &amp;quot;OK&amp;quot;, &amp;quot;UniqueId&amp;quot;: &amp;quot;00000&amp;quot;}' \
&amp;quot;$WAIT_HANDLE&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>You need to have correctly configured Heat in order for this to work;
I&amp;rsquo;ve written a short &lt;a href="https://blog.oddbit.com/post/2014-08-30-using-wait-conditions-with-hea/">companion article&lt;/a> that contains a checklist
and pointers to additional documentation to help work around some
common issues.&lt;/p>
&lt;h2 id="templates-defining-docker-containers">Templates: Defining Docker containers&lt;/h2>
&lt;p>&lt;strong>UPDATE&lt;/strong>: I have generated some &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-contain-doc/">annotated documentation for the
Docker plugin&lt;/a>.&lt;/p>
&lt;p>Now that we have arranged for Heat to wait for the server to finish
configuration before starting Docker contains, how do we create a
container? As Scott Lowe noticed in his &lt;a href="http://blog.scottlowe.org/2014/08/22/a-heat-template-for-docker-containers/">blog post about Heat and
Docker&lt;/a>, there is very little documentation available out there
for the Docker plugin (something I am trying to remedy with this blog
post!). Things are not quite as bleak as you might think, because
Heat resources are to a certain extent self-documenting. If you run:&lt;/p>
&lt;pre>&lt;code>$ heat resource-template DockerInc::Docker::Container
&lt;/code>&lt;/pre>
&lt;p>You will get a complete description of the attributes and properties
available in the named resource. The &lt;code>parameters&lt;/code> section is probably
the most descriptive:&lt;/p>
&lt;pre>&lt;code>parameters:
cmd:
Default: []
Description: Command to run after spawning the container.
Type: CommaDelimitedList
dns: {Description: Set custom dns servers., Type: CommaDelimitedList}
docker_endpoint: {Description: Docker daemon endpoint (by default the local docker
daemon will be used)., Type: String}
env: {Description: Set environment variables., Type: CommaDelimitedList}
hostname: {Default: '', Description: Hostname of the container., Type: String}
image: {Description: Image name., Type: String}
links: {Description: Links to other containers., Type: Json}
memory: {Default: 0, Description: Memory limit (Bytes)., Type: Number}
name: {Description: Name of the container., Type: String}
open_stdin:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Open stdin.
Type: String
port_bindings: {Description: TCP/UDP ports bindings., Type: Json}
port_specs: {Description: TCP/UDP ports mapping., Type: CommaDelimitedList}
privileged:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Enable extended privileges.
Type: String
stdin_once:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: If true, close stdin after the 1 attached client disconnects.
Type: String
tty:
AllowedValues: ['True', 'true', 'False', 'false']
Default: false
Description: Allocate a pseudo-tty.
Type: String
user: {Default: '', Description: Username or UID., Type: String}
volumes:
Default: {}
Description: Create a bind mount.
Type: Json
volumes_from: {Default: '', Description: Mount all specified volumes., Type: String}
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>port_specs&lt;/code> and &lt;code>port_bindings&lt;/code> parameters require a little
additional explanation.&lt;/p>
&lt;p>The &lt;code>port_specs&lt;/code> parameter is a list of (TCP) ports that will be
&amp;ldquo;exposed&amp;rdquo; by the container (similar to the &lt;code>EXPOSE&lt;/code> directive in a
Dockerfile). This corresponds to the &lt;code>PortSpecs&lt;/code> argument in the the
&lt;a href="https://docs.docker.com/reference/api/docker_remote_api_v1.14/#create-a-container">/containers/create&lt;/a> call of the &lt;a href="https://docs.docker.com/reference/api/docker_remote_api/">Docker remote API&lt;/a>.
For example:&lt;/p>
&lt;pre>&lt;code>port_specs:
- 3306
- 53/udp
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>port_bindings&lt;/code> parameter is a mapping that allows you to bind
host ports to ports in the container, similar to the &lt;code>-p&lt;/code> argument to
&lt;code>docker run&lt;/code>. This corresponds to the
&lt;a href="https://docs.docker.com/reference/api/docker_remote_api_v1.14/#start-a-container">/containers/(id)/start&lt;/a> call in the &lt;a href="https://docs.docker.com/reference/api/docker_remote_api/">Docker remote API&lt;/a>.
In the mappings, the key (left-hand side) is the &lt;em>container&lt;/em> port, and
the value (right-hand side) is the &lt;em>host&lt;/em> port.&lt;/p>
&lt;p>For example, to bind container port 3306 to host port 3306:&lt;/p>
&lt;pre>&lt;code>port_bindings:
3306: 3306
&lt;/code>&lt;/pre>
&lt;p>To bind port 9090 in a container to port 80 on the host:&lt;/p>
&lt;pre>&lt;code>port_bindings:
9090: 80
&lt;/code>&lt;/pre>
&lt;p>And in theory, this should also work for UDP ports (but in practice
there is an issue between the Docker plugin and the &lt;code>docker-py&lt;/code> Python
module which makes it impossible to expose UDP ports via &lt;code>port_specs&lt;/code>;
this is fixed in
&lt;a href="https://github.com/docker/docker-py/pull/310" class="pull-request">#310&lt;/a>
on GitHub).&lt;/p>
&lt;pre>&lt;code>port_bindings:
53/udp: 5300
&lt;/code>&lt;/pre>
&lt;p>With all of this in mind, we can create a container resource
definition:&lt;/p>
&lt;pre>&lt;code>docker_dbserver:
type: &amp;quot;DockerInc::Docker::Container&amp;quot;
# here's where we set the dependency on the WaitCondition
# resource we mentioned earlier.
depends_on:
- docker_wait_condition
properties:
docker_endpoint:
str_replace:
template: &amp;quot;tcp://$HOST:2375&amp;quot;
params:
&amp;quot;$HOST&amp;quot;:
get_attr:
- docker_server_floating
- floating_ip_address
image: mysql
env:
# The official MySQL docker image expect the database root
# password to be provided in the MYSQL_ROOT_PASSWORD
# environment variable.
- str_replace:
template: MYSQL_ROOT_PASSWORD=$PASSWORD
params:
&amp;quot;$PASSWORD&amp;quot;:
get_param:
mysql_root_password
port_specs:
- 3306
port_bindings:
3306: 3306
&lt;/code>&lt;/pre>
&lt;p>Take a close look at how we&amp;rsquo;re setting the &lt;code>docker_endpoint&lt;/code> property:&lt;/p>
&lt;pre>&lt;code>docker_endpoint:
str_replace:
template: &amp;quot;tcp://$HOST:2375&amp;quot;
params:
&amp;quot;$HOST&amp;quot;:
get_attr:
- docker_server_floating
- floating_ip_address
&lt;/code>&lt;/pre>
&lt;p>This uses the &lt;code>get_attr&lt;/code> function to get the &lt;code>floating_ip_address&lt;/code>
attribute from the &lt;code>docker_server_floating&lt;/code> resource, which you can
find in the &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a>. We take the return value from that
function and use &lt;code>str_replace&lt;/code> to substitute that into the
&lt;code>docker_endpoint&lt;/code> URL.&lt;/p>
&lt;h2 id="the-pudding">The pudding&lt;/h2>
&lt;p>Using the &lt;a href="https://github.com/larsks/heat-docker-example">complete template&lt;/a> with an appropriate local environment
file, I can launch this stack by runnign:&lt;/p>
&lt;pre>&lt;code>$ heat stack-create -f docker-server.yml -e local.env docker
&lt;/code>&lt;/pre>
&lt;p>And after a while, I can run&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
&lt;/code>&lt;/pre>
&lt;p>And see that the stack has been created successfully:&lt;/p>
&lt;pre>&lt;code>+--------------------------------------+------------+-----------------+----------------------+
| id | stack_name | stack_status | creation_time |
+--------------------------------------+------------+-----------------+----------------------+
| c0fd793e-a1f7-4b35-afa9-12ba1005925a | docker | CREATE_COMPLETE | 2014-08-31T03:01:14Z |
+--------------------------------------+------------+-----------------+----------------------+
&lt;/code>&lt;/pre>
&lt;p>And I can ask for status information on the individual resources in
the stack:&lt;/p>
&lt;pre>&lt;code>$ heat resource-list docker
+------------------------+------------------------------------------+-----------------+
| resource_name | resource_type | resource_status |
+------------------------+------------------------------------------+-----------------+
| fixed_network | OS::Neutron::Net | CREATE_COMPLETE |
| secgroup_db | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| secgroup_docker | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| secgroup_webserver | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| docker_wait_handle | AWS::CloudFormation::WaitConditionHandle | CREATE_COMPLETE |
| extrouter | OS::Neutron::Router | CREATE_COMPLETE |
| fixed_subnet | OS::Neutron::Subnet | CREATE_COMPLETE |
| secgroup_common | OS::Neutron::SecurityGroup | CREATE_COMPLETE |
| docker_server_eth0 | OS::Neutron::Port | CREATE_COMPLETE |
| extrouter_inside | OS::Neutron::RouterInterface | CREATE_COMPLETE |
| docker_server | OS::Nova::Server | CREATE_COMPLETE |
| docker_server_floating | OS::Neutron::FloatingIP | CREATE_COMPLETE |
| docker_wait_condition | AWS::CloudFormation::WaitCondition | CREATE_COMPLETE |
| docker_webserver | DockerInc::Docker::Container | CREATE_COMPLETE |
| docker_dbserver | DockerInc::Docker::Container | CREATE_COMPLETE |
+------------------------+------------------------------------------+-----------------+
&lt;/code>&lt;/pre>
&lt;p>I can run &lt;code>nova list&lt;/code> and see information about my running Nova
server:&lt;/p>
&lt;pre>&lt;code>+--------...+-----------------...+------------------------------------------------------------+
| ID ...| Name ...| Networks |
+--------...+-----------------...+------------------------------------------------------------+
| 301c5ec...| docker-docker_se...| docker-fixed_network-whp3fxhohkxk=10.0.0.2, 192.168.200.46 |
+--------...+-----------------...+------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>I can point a Docker client at the remote address and see the running
containers:&lt;/p>
&lt;pre>&lt;code>$ docker-1.2 -H tcp://192.168.200.46:2375 ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
f2388c871b20 mysql:5 /entrypoint.sh mysql 5 minutes ago Up 5 minutes 0.0.0.0:3306-&amp;gt;3306/tcp grave_almeida
9596cbe51291 larsks/simpleweb:latest /bin/sh -c '/usr/sbi 11 minutes ago Up 11 minutes 0.0.0.0:80-&amp;gt;80/tcp hungry_tesla
&lt;/code>&lt;/pre>
&lt;p>And I can point a &lt;code>mysql&lt;/code> client at the remote address and access the
database server:&lt;/p>
&lt;pre>&lt;code>$ mysql -h 192.168.200.46 -u root -psecret mysql
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
[...]
MySQL [mysql]&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="when-things-go-wrong">When things go wrong&lt;/h2>
&lt;p>Your &lt;code>heat-engine&lt;/code> log, generally &lt;code>/var/log/heat/engine.log&lt;/code>, is going
to be your best source of information if things go wrong. The &lt;code>heat stack-show&lt;/code> command will generally provide useful fault information if
your stack ends up in the &lt;code>CREATE_FAILED&lt;/code> (or &lt;code>DELETE_FAILED&lt;/code>) state.&lt;/p></content></item><item><title>Using wait conditions with Heat</title><link>https://blog.oddbit.com/post/2014-08-30-using-wait-conditions-with-hea/</link><pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-30-using-wait-conditions-with-hea/</guid><description>This post accompanies my article on the Docker plugin for Heat.
In order for WaitCondition resources to operate correctly in Heat, you will need to make sure that that you have:
Created the necessary Heat domain and administrative user in Keystone, Configured appropriate values in heat.conf for stack_user_domain, stack_domain_admin, and stack_domain_admin_password. Configured an appropriate value in heat.conf for heat_waitcondition_server_url. On a single-system install this will often be pointed by default at 127.</description><content>&lt;p>This post accompanies my &lt;a href="https://blog.oddbit.com/post/2014-08-30-docker-plugin-for-openstack-he/">article on the Docker plugin for
Heat&lt;/a>.&lt;/p>
&lt;p>In order for &lt;code>WaitCondition&lt;/code> resources to operate correctly in Heat, you
will need to make sure that that you have:&lt;/p>
&lt;ul>
&lt;li>Created the necessary Heat domain and administrative user in
Keystone,&lt;/li>
&lt;li>Configured appropriate values in &lt;code>heat.conf&lt;/code> for
&lt;code>stack_user_domain&lt;/code>, &lt;code>stack_domain_admin&lt;/code>, and
&lt;code>stack_domain_admin_password&lt;/code>.&lt;/li>
&lt;li>Configured an appropriate value in &lt;code>heat.conf&lt;/code> for
&lt;code>heat_waitcondition_server_url&lt;/code>. On a single-system install this
will often be pointed by default at &lt;code>127.0.0.1&lt;/code>, which, hopefully for
obvious reasons, won&amp;rsquo;t be of any use to your Nova servers.&lt;/li>
&lt;li>Enabled the &lt;code>heat-api-cfn&lt;/code> service,&lt;/li>
&lt;li>Configured your firewall to permit access to the CFN service (which
runs on port 8000).&lt;/li>
&lt;/ul>
&lt;p>Steve Hardy has a blog post on &lt;a href="http://hardysteven.blogspot.co.uk/2014/04/heat-auth-model-updates-part-2-stack.html">stack domain users&lt;/a> that goes into
detail on configuring authentication for Heat and Keystone.&lt;/p></content></item><item><title>nova-docker and environment variables</title><link>https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/</link><pubDate>Thu, 28 Aug 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-08-28-novadocker-and-environment-var/</guid><description>I&amp;rsquo;ve been playing with Docker a bit recently, and decided to take a look at the nova-docker driver for OpenStack.
The nova-docker driver lets Nova, the OpenStack Compute service, spawn Docker containers instead of hypervisor-based servers. For certain workloads, this leads to better resource utilization than you would get with a hypervisor-based solution, while at the same time givin you better support for multi-tenancy and flexible networking than you get with Docker by itself.</description><content>&lt;p>I&amp;rsquo;ve been playing with &lt;a href="https://docker.com/">Docker&lt;/a> a bit recently, and decided to take
a look at the &lt;a href="https://github.com/stackforge/nova-docker">nova-docker&lt;/a> driver for &lt;a href="http://openstack.org/">OpenStack&lt;/a>.&lt;/p>
&lt;p>The &lt;code>nova-docker&lt;/code> driver lets Nova, the OpenStack Compute service,
spawn Docker containers instead of hypervisor-based servers. For
certain workloads, this leads to better resource utilization than you
would get with a hypervisor-based solution, while at the same time
givin you better support for multi-tenancy and flexible networking
than you get with Docker by itself.&lt;/p>
&lt;p>The &lt;a href="https://wiki.openstack.org/wiki/Docker">Docker driver wiki&lt;/a> was mostly sufficient for getting the
&lt;code>nova-docker&lt;/code> driver installed in my existing OpenStack deployment,
although I did make a few &lt;a href="https://wiki.openstack.org/w/index.php?title=Docker&amp;amp;diff=61664&amp;amp;oldid=58546">small changes&lt;/a> to the wiki to reflect
some missing steps. Other than that, the installation was relatively
simple and I was soon able to spin up Docker containers using &lt;code>nova boot ...&lt;/code>&lt;/p>
&lt;p>The one problem I encountered is that it is not possible to pass
environment variable to Docker containers via the &lt;code>nova-docker&lt;/code>
driver. Many existing images (such as the &lt;a href="https://registry.hub.docker.com/_/mysql/">official MySQL image&lt;/a>)
expect configuration information to be passed in using environment
variables; for example, the &lt;code>mysql&lt;/code> image expects to be started like
this:&lt;/p>
&lt;pre>&lt;code>docker run --name some-mysql \
-e MYSQL_ROOT_PASSWORD=mysecretpassword -d mysql
&lt;/code>&lt;/pre>
&lt;p>I have proposed a &lt;a href="https://review.openstack.org/#/c/117583/">patch&lt;/a> to the &lt;code>nova-docker&lt;/code> driver that permits
one to provide environment variables via the Nova metadata service.
With this patch in place, I would start the &lt;code>mysql&lt;/code> container like
this:&lt;/p>
&lt;pre>&lt;code>nova boot --image mysql --flavor m1.small \
--meta ENV_MYSQL_ROOT_PASSWORD=mysecretpassword \
some-mysql
&lt;/code>&lt;/pre>
&lt;p>That is, the driver looks for metadata items that begin with &lt;code>ENV_&lt;/code>
and transforms these into Docker environment variables after stripping
&lt;code>ENV_&lt;/code> from the name.&lt;/p>
&lt;p>While this patch works great in my testing environment, it&amp;rsquo;s unlikely
to get accepted. Generally, the metadata provided by Nova belongs to
the tenant and is not meant to be operationally significant to the
compute driver itself.&lt;/p>
&lt;p>It sounds as if there is a lot of work going on right now regarding
container support in OpenStack, so it is very likely that a better
solution will show up in the near future.&lt;/p>
&lt;p>In the absence of that support, I hope others find this patch helpful.&lt;/p></content></item><item><title>Booting an instance with multiple fixed addresses</title><link>https://blog.oddbit.com/post/2014-05-28-booting-an-instance-with-multi/</link><pubDate>Wed, 28 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-28-booting-an-instance-with-multi/</guid><description>This article expands on my answer to Add multiple specific IPs to instance, a question posted to ask.openstack.org.
In order to serve out SSL services from an OpenStack instance, you will generally want one local ip address for each SSL virtual host you support. It is possible to create an instance with multiple fixed addresses, but there are a few complications to watch out for.
Assumptions This article assumes that the following resources exist:</description><content>&lt;p>This article expands on my answer to &lt;a href="https://ask.openstack.org/en/question/30690/add-multiple-specific-ips-to-instance/">Add multiple specific IPs to
instance&lt;/a>, a question posted to &lt;a href="https://ask.openstack.org/">ask.openstack.org&lt;/a>.&lt;/p>
&lt;p>In order to serve out SSL services from an OpenStack instance, you
will generally want one local ip address for each SSL virtual host you
support. It is possible to create an instance with multiple fixed
addresses, but there are a few complications to watch out for.&lt;/p>
&lt;h1 id="assumptions">Assumptions&lt;/h1>
&lt;p>This article assumes that the following resources exist:&lt;/p>
&lt;ul>
&lt;li>a private network &lt;code>net0&lt;/code>.&lt;/li>
&lt;li>a private network &lt;code>net0-subnet0&lt;/code>, associated with &lt;code>net0&lt;/code>, assigned
the range &lt;code>10.0.0.0/24&lt;/code>.&lt;/li>
&lt;li>a public network &lt;code>external&lt;/code> assigned the range &lt;code>192.168.200.0/24&lt;/code>.&lt;/li>
&lt;li>an image named &lt;code>fedora-20-x86_64&lt;/code>, with hopefully self-evident
contents.&lt;/li>
&lt;/ul>
&lt;h1 id="creating-a-port">Creating a port&lt;/h1>
&lt;p>Start by creating a port in Neutron:&lt;/p>
&lt;pre>&lt;code>$ neutron port-create net0 \
--fixed-ip subnet_id=net0-subnet0 \
--fixed-ip subnet_id=net0-subnet0
&lt;/code>&lt;/pre>
&lt;p>This will create a neutron port to which have been allocated to fixed
ip addresses from &lt;code>net0-subnet0&lt;/code>:&lt;/p>
&lt;pre>&lt;code>+-----------------------+----------------------------------------------------------------------------------+
| Field | Value |
+-----------------------+----------------------------------------------------------------------------------+
| admin_state_up | True |
| allowed_address_pairs | |
| binding:vnic_type | normal |
| device_id | |
| device_owner | |
| fixed_ips | {&amp;quot;subnet_id&amp;quot;: &amp;quot;f8ca90fd-cb82-4218-9627-6fa66e4c9c3c&amp;quot;, &amp;quot;ip_address&amp;quot;: &amp;quot;10.0.0.18&amp;quot;} |
| | {&amp;quot;subnet_id&amp;quot;: &amp;quot;f8ca90fd-cb82-4218-9627-6fa66e4c9c3c&amp;quot;, &amp;quot;ip_address&amp;quot;: &amp;quot;10.0.0.19&amp;quot;} |
| id | 3c564dd5-fd45-4f61-88df-715f71667b3b |
| mac_address | fa:16:3e:e1:15:7f |
| name | |
| network_id | bb4e5e37-74e1-41bd-880e-b59e94236c5e |
| security_groups | 52f7a87c-380f-4a07-a6ff-d64be495f25b |
| status | DOWN |
| tenant_id | 4dfe8e38f68449b6a0c9cd73037726f7 |
+-----------------------+----------------------------------------------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>If you want, you can specify an explicit set of addresses rather than
having neutron allocate them for you:&lt;/p>
&lt;pre>&lt;code>$ neutron port-create net0 \
--fixed-ip subnet_id=net0-subnet0,ip_address=10.0.0.18 \
--fixed-ip subnet_id=net0-subnet0,ip_address=10.0.0.19
&lt;/code>&lt;/pre>
&lt;h1 id="boot-an-instance">Boot an instance&lt;/h1>
&lt;p>You can boot an instance using this port using the &lt;code>port-id=...&lt;/code>
parameter to the &lt;code>--nic&lt;/code> option:&lt;/p>
&lt;pre>&lt;code>$ nova boot \
--nic port-id=3c564dd5-fd45-4f61-88df-715f71667b3b \
--flavor m1.tiny \
--image fedora-20-x86_64 \
--key-name lars test0
&lt;/code>&lt;/pre>
&lt;p>This is where the first complication arises: the instance will boot
and receive a DHCP lease for one of the fixed addresses you created,
but you don&amp;rsquo;t know which one. This isn&amp;rsquo;t an insurmountable problem;
you can assign floating ips to each one and then try logging in to
both and see which works.&lt;/p>
&lt;p>Rather than playing network roulette, you can pass in a script via the
&lt;code>--user-data&lt;/code> option that will take care of configuring the network
correctly. For example, something like this:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
cat &amp;gt; /etc/sysconfig/network-scripts/ifcfg-eth0 &amp;lt;&amp;lt;EOF
DEVICE=eth0
BOOTPROTO=none
IPADDR=10.0.0.18
NETMASK=255.255.255.0
GATEWAY=10.0.0.1
ONBOOT=yes
EOF
cat &amp;gt; /etc/sysconfig/network-scripts/ifcfg-eth0:0 &amp;lt;&amp;lt;EOF
DEVICE=eth0:0
BOOTPROTO=none
IPADDR=10.0.0.19
NETMASK=255.255.255.0
GATEWAY=10.0.0.1
ONBOOT=yes
EOF
ifdown eth0
ifup eth0
ifup eth0:0
&lt;/code>&lt;/pre>
&lt;p>And boot the instance like this:&lt;/p>
&lt;pre>&lt;code>$ nova boot --nic port-id=3c564dd5-fd45-4f61-88df-715f71667b3b \
--flavor m1.tiny --image fedora-20-x86_64 --key-name lars \
--user-data userdata.txt test0
&lt;/code>&lt;/pre>
&lt;p>Assuming that your image uses &lt;a href="http://cloudinit.readthedocs.org/en/latest/">cloud-init&lt;/a> or something similar, it
should execute the &lt;code>user-data&lt;/code> script at boot and set up the
persistent network configuration.&lt;/p>
&lt;p>At this stage, you can verify that both addresses have been assigned
by using the &lt;code>ip netns&lt;/code> command to run &lt;code>ping&lt;/code> inside an appropriate
namespace. Something like:&lt;/p>
&lt;pre>&lt;code>$ sudo ip netns exec qdhcp-bb4e5e37-74e1-41bd-880e-b59e94236c5e ping -c1 10.0.0.18
PING 10.0.0.18 (10.0.0.18) 56(84) bytes of data.
64 bytes from 10.0.0.18: icmp_seq=1 ttl=64 time=1.60 ms
--- 10.0.0.18 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.606/1.606/1.606/0.000 ms
$ sudo ip netns exec qdhcp-bb4e5e37-74e1-41bd-880e-b59e94236c5e ping -c1 10.0.0.19
PING 10.0.0.19 (10.0.0.19) 56(84) bytes of data.
64 bytes from 10.0.0.19: icmp_seq=1 ttl=64 time=1.60 ms
--- 10.0.0.19 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 1.701/1.701/1.701/0.000 ms
&lt;/code>&lt;/pre>
&lt;p>This assumes that the UUID of the &lt;code>net0&lt;/code> network is &lt;code>bb4e5e37-74e1-41bd-880e-b59e94236c5e&lt;/code>. On your system, the namespace will be something different.&lt;/p>
&lt;h1 id="assign-floating-ips">Assign floating ips&lt;/h1>
&lt;p>Assign a floating ip address to each of the fixed addresses. You will
need to use the &lt;code>--fixed-address&lt;/code> option to &lt;code>nova add-floating-ip&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova add-floating-ip --fixed-address 10.0.0.19 test0 192.168.200.6
$ nova add-floating-ip --fixed-address 10.0.0.18 test0 192.168.200.4
&lt;/code>&lt;/pre>
&lt;p>With these changes in place, the system is accessible via either
address:&lt;/p>
&lt;pre>&lt;code>$ ssh fedora@192.168.200.4 uptime
14:51:52 up 4 min, 0 users, load average: 0.00, 0.02, 0.02
$ ssh fedora@192.168.200.6 uptime
14:51:54 up 4 min, 0 users, load average: 0.00, 0.02, 0.02
&lt;/code>&lt;/pre>
&lt;p>And looking at the network configuration on the system, we can see
that both addresses have been assigned to &lt;code>eth0&lt;/code> as expected:&lt;/p>
&lt;pre>&lt;code>$ ssh fedora@192.168.200.4 /sbin/ip a
[...]
2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
link/ether fa:16:3e:bf:f9:6a brd ff:ff:ff:ff:ff:ff
inet 10.0.0.18/24 brd 10.0.0.255 scope global eth0
valid_lft forever preferred_lft forever
inet 10.0.0.19/24 brd 10.0.0.255 scope global secondary eth0:0
valid_lft forever ...
&lt;/code>&lt;/pre></content></item><item><title>Multiple external networks with a single L3 agent</title><link>https://blog.oddbit.com/post/2014-05-28-multiple-external-networks-wit/</link><pubDate>Wed, 28 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-28-multiple-external-networks-wit/</guid><description>In the old days (so, like, last year), Neutron supported a single external network per L3 agent. You would run something like this&amp;hellip;
$ neutron net-create external --router:external=true &amp;hellip;and neutron would map this to the bridge defined in external_network_bridge in /etc/neutron/l3_agent.ini. If you wanted to support more than a single external network, you would need to run multiple L3 agents, each with a unique value for external_network_bridge.
There is now a better option available.</description><content>&lt;p>In the old days (so, like, last year), Neutron supported a single
external network per L3 agent. You would run something like this&amp;hellip;&lt;/p>
&lt;pre>&lt;code>$ neutron net-create external --router:external=true
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and neutron would map this to the bridge defined in
&lt;code>external_network_bridge&lt;/code> in &lt;code>/etc/neutron/l3_agent.ini&lt;/code>. If you
wanted to support more than a single external network, you would need
to run multiple L3 agents, each with a unique value for
&lt;code>external_network_bridge&lt;/code>.&lt;/p>
&lt;p>There is now a better option available.&lt;/p>
&lt;h2 id="assumptions">Assumptions&lt;/h2>
&lt;p>In this post, I&amp;rsquo;m assuming:&lt;/p>
&lt;ul>
&lt;li>You&amp;rsquo;re using the ML2 plugin for Neutron.&lt;/li>
&lt;li>You&amp;rsquo;re using the Open vSwitch mechanism driver for the ML2 plugin&lt;/li>
&lt;li>You have &lt;code>eth1&lt;/code> and &lt;code>eth2&lt;/code> connected directly to networks that you
would like to make available as external networks in OpenStack.&lt;/li>
&lt;/ul>
&lt;h2 id="create-your-bridges">Create your bridges&lt;/h2>
&lt;p>For each external network you wish to support, create a new OVS
bridge. For example, assuming that we want to make a network attached
to &lt;code>eth1&lt;/code> and a network attached to &lt;code>eth2&lt;/code> available to tenants:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-br br-eth1
# ovs-vsctl add-port br-eth1 eth1
# ovs-vsctl add-br br-eth2
# ovs-vsctl add-port br-eth2 eth2
&lt;/code>&lt;/pre>
&lt;p>Realistically, you would accomplish this via your system&amp;rsquo;s native
network configuration mechanism, but I&amp;rsquo;m going to gloss over that
detail for now.&lt;/p>
&lt;h2 id="configure-the-l3-agent">Configure the L3 Agent&lt;/h2>
&lt;p>Start with the following comment in &lt;code>l3_agent.ini&lt;/code>:&lt;/p>
&lt;pre>&lt;code># When external_network_bridge is set, each L3 agent can be associated
# with no more than one external network. This value should be set to the UUID
# of that external network. To allow L3 agent support multiple external
# networks, both the external_network_bridge and gateway_external_network_id
# must be left empty.
&lt;/code>&lt;/pre>
&lt;p>Following those instructions, make sure that both
&lt;code>external_network_bridge&lt;/code> and &lt;code>gateway_external_network_id&lt;/code> are unset
in &lt;code>l3_agent.ini&lt;/code>.&lt;/p>
&lt;h2 id="configure-the-ml2-plugin">Configure the ML2 Plugin&lt;/h2>
&lt;p>We are creating &amp;ldquo;flat&amp;rdquo; networks in this example, so we need to make
sure that we can create flat networks. Make sure that the
&lt;code>type_drivers&lt;/code> parameter of the &lt;code>[ml2]&lt;/code> section of your plugin
configuration includes &lt;code>flat&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[ml2]
type_drivers = local,flat,gre,vxlan
&lt;/code>&lt;/pre>
&lt;p>In the &lt;code>[ml2_type_flat]&lt;/code> section, need to create a list of physical
network names that can be used to create flat networks. If you want
all physical networks to be available for flat networks, you can use
&lt;code>*&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[ml2_type_flat]
flat_networks = *
&lt;/code>&lt;/pre>
&lt;p>Both of these changes probably go in &lt;code>/etc/neutron/plugin.ini&lt;/code>, but
&lt;em>may&lt;/em> going elsewhere depending on how your system is configured.&lt;/p>
&lt;h2 id="configure-the-open-vswitch-agent">Configure the Open vSwitch Agent&lt;/h2>
&lt;p>For each bridge, you will need to add entries to both the
&lt;code>network_vlan_ranges&lt;/code> and &lt;code>bridge_mappings&lt;/code> parameters of the &lt;code>[ovs]&lt;/code>
section of your plugin configuration. For the purposes of this post,
that means:&lt;/p>
&lt;pre>&lt;code>[ovs]
network_vlan_ranges = physnet1,physnet2
bridge_mappings = physnet1:br-eth1,physnet2:br-eth2
&lt;/code>&lt;/pre>
&lt;p>This will probably go in &lt;code>/etc/neutron/plugin.ini&lt;/code>. Specifically, it
needs to go wherever your &lt;code>neutron-openvswitch-agent&lt;/code> process is
looking for configuration information. So you if you see this:&lt;/p>
&lt;pre>&lt;code>$ ps -fe | grep openvswitch-agent
neutron 12529 1 0 09:50 ? 00:00:08 /usr/bin/python /usr/bin/neutron-openvswitch-agent --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini --log-file /var/log/neutron/openvswitch-agent.log
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;then you would make the changes to &lt;code>/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini&lt;/code>.&lt;/p>
&lt;h2 id="restart-neutron">Restart Neutron&lt;/h2>
&lt;p>You will need to restart both the l3 agent and the openvswitch agent.
If you&amp;rsquo;re on a recent Fedora/RHEL/CentOS, you can restart all Neutron
services like this:&lt;/p>
&lt;pre>&lt;code># openstack-service restart neutron
&lt;/code>&lt;/pre>
&lt;h2 id="inspect-your-open-vswitch-configuration">Inspect your Open vSwitch Configuration&lt;/h2>
&lt;p>As root, run &lt;code>ovs-vsctl show&lt;/code>. You should see something like this:&lt;/p>
&lt;pre>&lt;code>f4a4312b-307e-4c3c-b728-9434000a34ff
Bridge br-int
Port br-int
Interface br-int
type: internal
Port &amp;quot;int-br-eth2&amp;quot;
Interface &amp;quot;int-br-eth2&amp;quot;
Port int-br-ex
Interface int-br-ex
Port &amp;quot;int-br-eth1&amp;quot;
Interface &amp;quot;int-br-eth1&amp;quot;
Bridge &amp;quot;br-eth2&amp;quot;
Port &amp;quot;br-eth2&amp;quot;
Interface &amp;quot;br-eth2&amp;quot;
type: internal
Port &amp;quot;phy-br-eth2&amp;quot;
Interface &amp;quot;phy-br-eth2&amp;quot;
Port &amp;quot;eth2&amp;quot;
Interface &amp;quot;eth2&amp;quot;
Bridge &amp;quot;br-eth1&amp;quot;
Port &amp;quot;br-eth1&amp;quot;
Interface &amp;quot;br-eth1&amp;quot;
type: internal
Port &amp;quot;phy-br-eth1&amp;quot;
Interface &amp;quot;phy-br-eth1&amp;quot;
Port &amp;quot;eth1&amp;quot;
Interface &amp;quot;eth1&amp;quot;
ovs_version: &amp;quot;2.0.1&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Here you can see the OVS bridge &lt;code>br-eth1&lt;/code> and &lt;code>br-eth2&lt;/code>, each with the
appropriate associated physical interface and links to the integration
bridge, &lt;code>br-int&lt;/code>.&lt;/p>
&lt;h2 id="create-your-external-networks">Create your external networks&lt;/h2>
&lt;p>With admin credentials, use the &lt;code>net-create&lt;/code> and &lt;code>subnet-create&lt;/code>
commands to create the appropiate networks:&lt;/p>
&lt;pre>&lt;code>$ neutron net-create external1 -- --router:external=true \
--provider:network_type=flat \
--provider:physical_network=physnet1
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | True |
| id | 23f4b5f6-14fd-4bab-a8b0-445257bbc0d1 |
| name | external1 |
| provider:network_type | flat |
| provider:physical_network | physnet1 |
| provider:segmentation_id | |
| router:external | True |
| shared | False |
| status | ACTIVE |
| subnets | |
| tenant_id | 6f736b1361b74789a81d4d53d88be3c5 |
+---------------------------+--------------------------------------+
$ neutron subnet-create --disable-dhcp external1 10.1.0.0/24
+------------------+--------------------------------------------+
| Field | Value |
+------------------+--------------------------------------------+
| allocation_pools | {&amp;quot;start&amp;quot;: &amp;quot;10.1.0.2&amp;quot;, &amp;quot;end&amp;quot;: &amp;quot;10.1.0.254&amp;quot;} |
| cidr | 10.1.0.0/24 |
| dns_nameservers | |
| enable_dhcp | False |
| gateway_ip | 10.1.0.1 |
| host_routes | |
| id | 363ba289-a989-4acb-ac3b-ffaeb90796fc |
| ip_version | 4 |
| name | |
| network_id | 23f4b5f6-14fd-4bab-a8b0-445257bbc0d1 |
| tenant_id | 6f736b1361b74789a81d4d53d88be3c5 |
+------------------+--------------------------------------------+
$ neutron net-create external2 -- --router:external=true \
--provider:network_type=flat \
--provider:physical_network=physnet2
+---------------------------+--------------------------------------+
| Field | Value |
+---------------------------+--------------------------------------+
| admin_state_up | True |
| id | 762be5de-31a2-46b8-925c-0967871f8181 |
| name | external2 |
| provider:network_type | flat |
| provider:physical_network | physnet2 |
| provider:segmentation_id | |
| router:external | True |
| shared | False |
| status | ACTIVE |
| subnets | |
| tenant_id | 6f736b1361b74789a81d4d53d88be3c5 |
+---------------------------+--------------------------------------+
$ neutron subnet-create --disable-dhcp external2 10.2.0.0/24
+------------------+--------------------------------------------+
| Field | Value |
+------------------+--------------------------------------------+
| allocation_pools | {&amp;quot;start&amp;quot;: &amp;quot;10.2.0.2&amp;quot;, &amp;quot;end&amp;quot;: &amp;quot;10.2.0.254&amp;quot;} |
| cidr | 10.2.0.0/24 |
| dns_nameservers | |
| enable_dhcp | False |
| gateway_ip | 10.2.0.1 |
| host_routes | |
| id | edffc5c6-0e16-4da0-8eba-9d79ab9fd2fe |
| ip_version | 4 |
| name | |
| network_id | 762be5de-31a2-46b8-925c-0967871f8181 |
| tenant_id | 6f736b1361b74789a81d4d53d88be3c5 |
+------------------+--------------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>This assumes that &lt;code>eth1&lt;/code> is connected to a network using
&lt;code>10.1.0.0/24&lt;/code> and &lt;code>eth2&lt;/code> is connected to a network using
&lt;code>10.2.0.0/24&lt;/code>, and that each network has a gateway sitting at the
corresponding &lt;code>.1&lt;/code> address.&lt;/p>
&lt;p>And you&amp;rsquo;re all set!&lt;/p></content></item><item><title>Video: Configuring OpenStack's external bridge on a single-interface system</title><link>https://blog.oddbit.com/post/2014-05-27-configuring-openstacks-externa/</link><pubDate>Tue, 27 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-27-configuring-openstacks-externa/</guid><description>I&amp;rsquo;ve just put a video on Youtube that looks at the steps required to set up the external bridge (br-ex) on a single-interface system:</description><content>&lt;p>I&amp;rsquo;ve just put a video on Youtube that looks at the steps required to
set up the external bridge (&lt;code>br-ex&lt;/code>) on a single-interface system:&lt;/p>
&lt;!-- raw HTML omitted --></content></item><item><title>Open vSwitch and persistent MAC addresses</title><link>https://blog.oddbit.com/post/2014-05-23-open-vswitch-and-persistent-ma/</link><pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-23-open-vswitch-and-persistent-ma/</guid><description>Normally I like to post solutions, but today&amp;rsquo;s post is about a vexing problem to which I have not been able to find a solution.
This started as a simple attempt to set up external connectivity on an all-in-one Icehouse install deployed on an OpenStack instance. I wanted to add eth0 to br-ex in order to model a typical method for providing external connectivity, but I ran into a very odd problem: the system would boot and work fine for a few seconds, but would then promptly lose network connectivity.</description><content>&lt;p>Normally I like to post solutions, but today&amp;rsquo;s post is about a
vexing problem to which I have not been able to find a solution.&lt;/p>
&lt;p>This started as a simple attempt to set up external connectivity on
an all-in-one Icehouse install deployed on an OpenStack instance. I
wanted to add &lt;code>eth0&lt;/code> to &lt;code>br-ex&lt;/code> in order to model a typical method for
providing external connectivity, but I ran into a very odd problem:
the system would boot and work fine for a few seconds, but would then
promptly lose network connectivity.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The immediate cause was that the MAC address on &lt;code>br-ex&lt;/code> was changing.
I was setting the MAC explicitly in the configuration file:&lt;/p>
&lt;pre>&lt;code># cat ifcfg-br-ex
DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
ONBOOT=yes
OVSBOOTPROTO=dhcp
OVSDHCPINTERFACES=eth0
MACADDR=fa:16:3e:ef:91:ec
&lt;/code>&lt;/pre>
&lt;p>This was required in this case in order to make the MAC-address
filters on the host happy. When booting an instance, Neutron sets up
a rule like this:&lt;/p>
&lt;pre>&lt;code>-A neutron-openvswi-s55439d7d-a -s 10.0.0.8/32 -m mac --mac-source FA:16:3E:EF:91:EC -j RETURN
-A neutron-openvswi-s55439d7d-a -j DROP
&lt;/code>&lt;/pre>
&lt;p>But things quickly got weird. Some testing demonstrated that the MAC
address was changing when starting &lt;code>neutron-openvswitch-agent&lt;/code>, but a
thorough inspection of the code didn&amp;rsquo;t yield any obvious culprits for
this behavior.&lt;/p>
&lt;p>I liberally sprinkled the agent with the following (incrementing the
argument to &lt;code>echo&lt;/code> each time to uniquely identify each message):&lt;/p>
&lt;pre>&lt;code>os.system('echo 1 &amp;gt;&amp;gt; /tmp/ovs.log; ip link show dev br-ex &amp;gt;&amp;gt; /tmp/ovs.log')
&lt;/code>&lt;/pre>
&lt;p>It turns out that the MAC address on &lt;code>br-ex&lt;/code> was changing&amp;hellip;when
Neutron was deleting a port on &lt;code>br-int&lt;/code>. Specifically, at &lt;a href="https://github.com/openstack/neutron/blob/423ca756af10e10398636d6d34a7594a4fd4bc87/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py#L909">this
line&lt;/a> in &lt;code>ovs_neutron_agent.py&lt;/code>:&lt;/p>
&lt;pre>&lt;code>self.int_br.delete_port(int_veth_name)
&lt;/code>&lt;/pre>
&lt;p>After some additional testing, it turns out that just about &lt;em>any&lt;/em> OVS
operation causes an explicit MAC address to disappear. For example,
create a new OVS bridge:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-br br-test0
# ip link show dev br-test0
9: br-test0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether ba:cb:48:b9:6a:43 brd ff:ff:ff:ff:ff:ff
&lt;/code>&lt;/pre>
&lt;p>Then set the MAC address:&lt;/p>
&lt;pre>&lt;code># ip link set br-test0 addr c0:ff:ee:ee:ff:0c
# ip link show br-test0
8: br-test0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether c0:ff:ee:ee:ff:0c brd ff:ff:ff:ff:ff:ff
&lt;/code>&lt;/pre>
&lt;p>Now create a new bridge:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-br br-test1
&lt;/code>&lt;/pre>
&lt;p>And inspect the MAC address on the first bridge:&lt;/p>
&lt;pre>&lt;code># ip link show dev br-test0
9: br-test0: &amp;lt;BROADCAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN
link/ether ba:cb:48:b9:6a:43 brd ff:ff:ff:ff:ff:ff
&lt;/code>&lt;/pre>
&lt;p>In other words, creating a new bridge caused the MAC address on
&lt;code>br-ex&lt;/code> to revert. Other operations (e.g., deleting a port on an
unrelated switch) will cause the same behavior.&lt;/p>
&lt;p>I&amp;rsquo;ve seen this behavior on both versions &lt;code>1.11.0&lt;/code> and &lt;code>2.0.1&lt;/code>.&lt;/p>
&lt;p>So far everyone I&amp;rsquo;ve asked about this behavior has been stumped. If I
am able to figure out what&amp;rsquo;s going on I will update this post. Thanks
for reading!&lt;/p></content></item><item><title>Solved: Open vSwitch and persistent MAC addresses</title><link>https://blog.oddbit.com/post/2014-05-23-solved-open-vswitch-and-persis/</link><pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-23-solved-open-vswitch-and-persis/</guid><description>In my previous post I discussed a problem I was having setting a persistent MAC address on an OVS bridge device. It looks like the short answer is, &amp;ldquo;don&amp;rsquo;t use ip link set ...&amp;rdquo; for this purpose.
You can set the bridge MAC address via ovs-vsctl like this:
ovs-vsctl set bridge br-ex other-config:hwaddr=$MACADDR So I&amp;rsquo;ve updated my ifconfig-br-ex to look like this:
DEVICE=br-ex DEVICETYPE=ovs TYPE=OVSBridge ONBOOT=yes OVSBOOTPROTO=dhcp OVSDHCPINTERFACES=eth0 MACADDR=fa:16:3e:ef:91:ec OVS_EXTRA=&amp;quot;set bridge br-ex other-config:hwaddr=$MACADDR&amp;quot; The OVS_EXTRA parameter gets passed to the add-br call like this:</description><content>&lt;p>In my &lt;a href="https://blog.oddbit.com/2014/05/23/open-vswitch-and-persistent-ma/">previous post&lt;/a> I discussed a problem I was having setting a
persistent MAC address on an OVS bridge device. It looks like the
short answer is, &amp;ldquo;don&amp;rsquo;t use &lt;code>ip link set ...&lt;/code>&amp;rdquo; for this purpose.&lt;/p>
&lt;p>You can set the bridge MAC address via &lt;code>ovs-vsctl&lt;/code> like this:&lt;/p>
&lt;pre>&lt;code>ovs-vsctl set bridge br-ex other-config:hwaddr=$MACADDR
&lt;/code>&lt;/pre>
&lt;p>So I&amp;rsquo;ve updated my &lt;code>ifconfig-br-ex&lt;/code> to look like this:&lt;/p>
&lt;pre>&lt;code>DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
ONBOOT=yes
OVSBOOTPROTO=dhcp
OVSDHCPINTERFACES=eth0
MACADDR=fa:16:3e:ef:91:ec
OVS_EXTRA=&amp;quot;set bridge br-ex other-config:hwaddr=$MACADDR&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>OVS_EXTRA&lt;/code> parameter gets passed to the &lt;code>add-br&lt;/code> call like this:&lt;/p>
&lt;pre>&lt;code>ovs-vsctl --may-exist add-br br-ex -- set bridge br-ex other-config:hwaddr=$MACADDR
&lt;/code>&lt;/pre>
&lt;p>And unlike using &lt;code>ip link set&lt;/code>, this seems to stick.&lt;/p></content></item><item><title>Fedora and OVS Bridge Interfaces</title><link>https://blog.oddbit.com/post/2014-05-20-fedora-and-ovs-bridge-interfac/</link><pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-20-fedora-and-ovs-bridge-interfac/</guid><description>I run OpenStack on my laptop, and I&amp;rsquo;ve been chasing down a pernicious problem with OVS bridge interfaces under both F19 and F20. My OpenStack environment relies on an OVS bridge device named br-ex for external connectivity and for making services available to OpenStack instances, but after rebooting, br-ex was consistently unconfigured, which caused a variety of problems.
This is the network configuration file for br-ex on my system:
DEVICE=br-ex DEVICETYPE=ovs TYPE=OVSBridge BOOTPROT=static IPADDR=192.</description><content>&lt;p>I run OpenStack on my laptop, and I&amp;rsquo;ve been chasing down a pernicious
problem with OVS bridge interfaces under both F19 and F20. My
OpenStack environment relies on an OVS bridge device named &lt;code>br-ex&lt;/code> for
external connectivity and for making services available to OpenStack
instances, but after rebooting, &lt;code>br-ex&lt;/code> was consistently unconfigured,
which caused a variety of problems.&lt;/p>
&lt;p>This is the network configuration file for &lt;code>br-ex&lt;/code> on my system:&lt;/p>
&lt;pre>&lt;code>DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROT=static
IPADDR=192.168.200.1
NETMASK=255.255.255.0
ONBOOT=yes
NM_CONTROLLED=no
ZONE=openstack
&lt;/code>&lt;/pre>
&lt;p>Running &lt;code>ifup br-ex&lt;/code> would also fail to configure the interface, but
running &lt;code>ifdown br-ex; ifup br-ex&lt;/code> would configure things
appropriately.&lt;/p>
&lt;p>I finally got fed up with this behavior and spent some time chasing
down the problem, and this is what I found:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Calling &lt;code>ifup br-ex&lt;/code> passes control to
&lt;code>/etc/sysconfig/network-scripts/ifup-ovs&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>ifup-ovs&lt;/code> calls the &lt;code>check_device_down&lt;/code> function from
&lt;code>network-functions&lt;/code>, which looks like:&lt;/p>
&lt;pre>&lt;code> check_device_down ()
{
[ ! -d /sys/class/net/$1 ] &amp;amp;&amp;amp; return 0
if LC_ALL=C ip -o link show dev $1 2&amp;gt;/dev/null | grep -q &amp;quot;,UP&amp;quot; ; then
return 1
else
return 0
fi
}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>This returns failure (=1) if the interface flags contain &lt;code>,UP&lt;/code>.
Unfortunately, since information about this device is stored
persistently in &lt;code>ovsdb&lt;/code>, the device is already &lt;code>UP&lt;/code> when &lt;code>ifup&lt;/code> is
called, which causes &lt;code>ifup-ovs&lt;/code> to skip further device
configuration. The logic that calls &lt;code>check_device_down&lt;/code> looks like
this:&lt;/p>
&lt;pre>&lt;code>if check_device_down &amp;quot;${DEVICE}&amp;quot;; then
ovs-vsctl -t ${TIMEOUT} -- --may-exist add-br &amp;quot;$DEVICE&amp;quot; $OVS_OPTIONS \
${OVS_EXTRA+-- $OVS_EXTRA} \
${STP+-- set bridge &amp;quot;$DEVICE&amp;quot; stp_enable=&amp;quot;${STP}&amp;quot;}
else
OVSBRIDGECONFIGURED=&amp;quot;yes&amp;quot;
fi
&lt;/code>&lt;/pre>
&lt;p>This sets &lt;code>OVSBRIDGECONFIGURED&lt;/code> if it believes the device is &lt;code>UP&lt;/code>,
which causes &lt;code>ifup-ovs&lt;/code> to skip the call to &lt;code>ifup-eth&lt;/code> to configure
the interface:&lt;/p>
&lt;pre>&lt;code>if [ &amp;quot;${OVSBOOTPROTO}&amp;quot; != &amp;quot;dhcp&amp;quot; ] &amp;amp;&amp;amp; [ -z &amp;quot;${OVSINTF}&amp;quot; ] &amp;amp;&amp;amp; \
[ &amp;quot;${OVSBRIDGECONFIGURED}&amp;quot; != &amp;quot;yes&amp;quot; ]; then
${OTHERSCRIPT} ${CONFIG}
fi
&lt;/code>&lt;/pre>
&lt;p>I have found that the simplest solution to this problem is to disable
the logic that sets &lt;code>OVSBRIDGECONFIGURED&lt;/code>, by changing this:&lt;/p>
&lt;pre>&lt;code>else
OVSBRIDGECONFIGURED=&amp;quot;yes&amp;quot;
fi
&lt;/code>&lt;/pre>
&lt;p>To this:&lt;/p>
&lt;pre>&lt;code>else
: OVSBRIDGECONFIGURED=&amp;quot;yes&amp;quot;
fi
&lt;/code>&lt;/pre>
&lt;p>With this change in place, &lt;code>br-ex&lt;/code> is correctly configured after a
reboot.&lt;/p></content></item><item><title>Firewalld, NetworkManager, and OpenStack</title><link>https://blog.oddbit.com/post/2014-05-20-firewalld-and-openstack/</link><pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-20-firewalld-and-openstack/</guid><description>These are my notes on making OpenStack play well with firewalld and NetworkManager.
NetworkManager By default, NetworkManager attempts to start a DHCP client on every new available interface. Since booting a single instance in OpenStack can result in the creation of several virtual interfaces, this results in a lot of:
May 19 11:58:24 pk115wp-lkellogg NetworkManager[1357]: &amp;lt;info&amp;gt; Activation (qvb512640bd-ee) starting connection 'Wired connection 2' You can disable this behavior by adding the following to /etc/NetworkManager/NetworkManager.</description><content>&lt;p>These are my notes on making OpenStack play well with &lt;a href="https://fedoraproject.org/wiki/FirewallD">firewalld&lt;/a>
and &lt;a href="https://wiki.gnome.org/Projects/NetworkManager">NetworkManager&lt;/a>.&lt;/p>
&lt;h2 id="networkmanager">NetworkManager&lt;/h2>
&lt;p>By default, NetworkManager attempts to start a DHCP client on every
new available interface. Since booting a single instance in OpenStack
can result in the creation of several virtual interfaces, this results
in a lot of:&lt;/p>
&lt;pre>&lt;code>May 19 11:58:24 pk115wp-lkellogg NetworkManager[1357]: &amp;lt;info&amp;gt;
Activation (qvb512640bd-ee) starting connection 'Wired connection 2'
&lt;/code>&lt;/pre>
&lt;p>You can disable this behavior by adding the following to
&lt;code>/etc/NetworkManager/NetworkManager.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[main]
no-auto-default=*
&lt;/code>&lt;/pre>
&lt;p>From &lt;code>NetworkManager.conf(5)&lt;/code>:&lt;/p>
&lt;blockquote>
&lt;p>Comma-separated list of devices for which NetworkManager shouldn&amp;rsquo;t
create default wired connection (Auto eth0). By default,
NetworkManager creates a temporary wired connection for any
Ethernet device that is managed and doesn&amp;rsquo;t have a connection
configured. List a device in this option to inhibit creating the
default connection for the device. May have the special value * to
apply to all devices.&lt;/p>
&lt;/blockquote>
&lt;h2 id="firewalld">FirewallD&lt;/h2>
&lt;p>&lt;a href="https://fedoraproject.org/wiki/FirewallD">FirewallD&lt;/a> is the firewall manager recently introduced in Fedora
(and soon to be appearing in RHEL 7).&lt;/p>
&lt;p>I start by creating a new zone named &lt;code>openstack&lt;/code> by creating the file
&lt;code>/etc/firewalld/zones/openstack.xml&lt;/code> with the following content:&lt;/p>
&lt;pre>&lt;code>&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;zone&amp;gt;
&amp;lt;short&amp;gt;OpenStack&amp;lt;/short&amp;gt;
&amp;lt;description&amp;gt;For OpenStack services&amp;lt;/description&amp;gt;
&amp;lt;/zone&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>After populating this file, you need to run &lt;code>firewall-cmd --reload&lt;/code>
to make the zone available. Note that if you&amp;rsquo;re already running
OpenStack this will hose any rules set up by Neutron or Nova, so
you&amp;rsquo;ll probably want to restart those services:&lt;/p>
&lt;pre>&lt;code># openstack-service restart nova neutron
&lt;/code>&lt;/pre>
&lt;p>I then add &lt;code>br-ex&lt;/code> to this zone, where &lt;code>br-ex&lt;/code> is the OVS bridge my
OpenStack environment uses for external connectivity:&lt;/p>
&lt;pre>&lt;code># echo ZONE=openstack &amp;gt;&amp;gt; /etc/sysconfig/network-scripts/ifcfg-br-ex
&lt;/code>&lt;/pre>
&lt;p>I run a &lt;code>dnsmasq&lt;/code> instance on my laptop to which I expect OpenStack
instances to connect, so I need to add the &lt;code>dns&lt;/code> service to this zone:&lt;/p>
&lt;pre>&lt;code># firewall-cmd --zone openstack --add-service dns
# firewall-cmd --zone openstack --add-service dns --permanent
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m running &lt;code>firewall-cmd&lt;/code> twice here: the first time modifies the
currently running configuration, while the second makes the change
persistent across reboots.&lt;/p>
&lt;p>On my laptop, I handle external connectivity through NAT rather than
placing floating ips on a &amp;ldquo;real&amp;rdquo; network. To make this work, I add my
ethernet and wireless interfaces to the &lt;code>external&lt;/code> zone, which already
has ip masquerading enabled, by adding a &lt;code>ZONE&lt;/code> directive to the
appropriate interface configuration file:&lt;/p>
&lt;pre>&lt;code># echo ZONE=external &amp;gt;&amp;gt; /etc/sysconfig/network-scripts/ifcfg-em1
&lt;/code>&lt;/pre>
&lt;p>After a reboot, things look like this:&lt;/p>
&lt;pre>&lt;code># firewall-cmd --get-active-zones
openstack
interfaces: br-ex
external
interfaces: em1
public
interfaces: int-br-ex phy-br-ex qvb58cc67ca-06 qvo58cc67ca-06
# firewall-cmd --zone openstack --list-services
dns
&lt;/code>&lt;/pre></content></item><item><title>Flat networks with ML2 and OpenVSwitch</title><link>https://blog.oddbit.com/post/2014-05-19-flat-networks-with-ml-and-open/</link><pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-05-19-flat-networks-with-ml-and-open/</guid><description>Due to an unfortunate incident involving sleep mode and an overheated backpack I had the &amp;ldquo;opportunity&amp;rdquo; to rebuild my laptop. Since this meant reinstalling OpenStack I used this as an excuse to finally move to the ML2 network plugin for Neutron.
I was attempting to add an external network using the normal incantation:
neutron net-create external -- --router:external=true \ --provider:network_type=flat \ --provider:physical_network=physnet1 While this command completed successfully, I was left without any connectivity between br-int and br-ex, despite having in my /etc/neutron/plugins/ml2/ml2_conf.</description><content>&lt;p>Due to an unfortunate incident involving sleep mode and an overheated
backpack I had the &amp;ldquo;opportunity&amp;rdquo; to rebuild my laptop. Since this meant
reinstalling OpenStack I used this as an excuse to finally move to the ML2
network plugin for Neutron.&lt;/p>
&lt;p>I was attempting to add an external network using the normal incantation:&lt;/p>
&lt;pre>&lt;code>neutron net-create external -- --router:external=true \
--provider:network_type=flat \
--provider:physical_network=physnet1
&lt;/code>&lt;/pre>
&lt;p>While this command completed successfully, I was left without any
connectivity between &lt;code>br-int&lt;/code> and &lt;code>br-ex&lt;/code>, despite having in my
&lt;code>/etc/neutron/plugins/ml2/ml2_conf.ini&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[ml2_type_flat]
flat_networks = *
[ovs]
network_vlan_ranges = physnet1
bridge_mappings = physnet1:br-ex
&lt;/code>&lt;/pre>
&lt;p>The reason this is failing is very simple, but not terribly clear from
the existing documentation. This is how the &lt;code>neutron-server&lt;/code> process
is running:&lt;/p>
&lt;pre>&lt;code>/usr/bin/python /usr/bin/neutron-server \
--config-file /usr/share/neutron/neutron-dist.conf \
--config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/plugin.ini \
--log-file /var/log/neutron/server.log
&lt;/code>&lt;/pre>
&lt;p>This is how the &lt;code>neutron-openvswitch-agent&lt;/code> process is running:&lt;/p>
&lt;pre>&lt;code>/usr/bin/python /usr/bin/neutron-openvswitch-agent \
--config-file /usr/share/neutron/neutron-dist.conf \
--config-file /etc/neutron/neutron.conf \
--config-file /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini \
--log-file /var/log/neutron/openvswitch-agent.log
&lt;/code>&lt;/pre>
&lt;p>Note in particular that &lt;code>neutron-server&lt;/code> is looking at
&lt;code>/etc/neutron/plugin.ini&lt;/code>, which is a symlink to
&lt;code>/etc/neutron/plugins/ml2/ml2_conf.ini&lt;/code>, while
&lt;code>neutron-openvswitch-agent&lt;/code> is looking explicitly at
&lt;code>/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini&lt;/code>. The
physical network configuration needs to go into the
&lt;code>ovs_neutron_plugin.ini&lt;/code> configuration file.&lt;/p></content></item><item><title>Multinode OpenStack with Packstack</title><link>https://blog.oddbit.com/post/2014-02-27-multinode/</link><pubDate>Thu, 27 Feb 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-02-27-multinode/</guid><description>I was the presenter for this morning&amp;rsquo;s RDO hangout, where I ran through a simple demonstration of setting up a multinode OpenStack deployment using packstack.
The slides are online here.
Here&amp;rsquo;s the video (also available on the event page):</description><content>&lt;p>I was the presenter for this morning&amp;rsquo;s &lt;a href="http://openstack.redhat.com/">RDO&lt;/a> hangout, where I ran
through a simple demonstration of setting up a multinode OpenStack
deployment using &lt;a href="https://wiki.openstack.org/wiki/Packstack">packstack&lt;/a>.&lt;/p>
&lt;p>The slides are online &lt;a href="http://goo.gl/Yvmd0P">here&lt;/a>.&lt;/p>
&lt;p>Here&amp;rsquo;s the video (also available on the &lt;a href="https://plus.google.com/events/cm9ff549vmsim737lj7hopk4gao">event page&lt;/a>):&lt;/p>
&lt;!-- raw HTML omitted --></content></item><item><title>Show OVS external-ids</title><link>https://blog.oddbit.com/post/2014-01-19-show-ovs-externalids/</link><pubDate>Sun, 19 Jan 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-01-19-show-ovs-externalids/</guid><description>This is just here as a reminder for me:
An OVS interface has a variety of attributes associated with it, including an external-id field that can be used to associate resources outside of OpenVSwitch with the interface. You can view this field with the following command:
$ ovs-vsctl --columns=name,external-ids list Interface Which on my system, with a single virtual instance, looks like this:
# ovs-vsctl --columns=name,external-ids list Interface . . .</description><content>&lt;p>This is just here as a reminder for me:&lt;/p>
&lt;p>An OVS interface has a variety of attributes associated with it, including an
&lt;code>external-id&lt;/code> field that can be used to associate resources outside of
OpenVSwitch with the interface. You can view this field with the following
command:&lt;/p>
&lt;pre>&lt;code>$ ovs-vsctl --columns=name,external-ids list Interface
&lt;/code>&lt;/pre>
&lt;p>Which on my system, with a single virtual instance, looks like this:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl --columns=name,external-ids list Interface
.
.
.
name : &amp;quot;qvo519d7cc4-75&amp;quot;
external_ids : {attached-mac=&amp;quot;fa:16:3e:f7:75:b0&amp;quot;, iface-id=&amp;quot;519d7cc4-7593-4944-af7b-4056436f2d66&amp;quot;, iface-status=active, vm-uuid=&amp;quot;0330b084-03db-4d42-a231-2cd6ad89515b&amp;quot;}
.
.
.
&lt;/code>&lt;/pre>
&lt;p>Note the information contained here:&lt;/p>
&lt;ul>
&lt;li>&lt;code>attached-mac&lt;/code> is the MAC address of the device attached to this interface.&lt;/li>
&lt;li>&lt;code>vm-uuid&lt;/code> is the libvirt UUID for the instance attached to this interface&amp;hellip;&lt;/li>
&lt;li>&amp;hellip;which also happens to be the Nova UUID for the instance.&lt;/li>
&lt;/ul>
&lt;p>So we can pass that UUID to &lt;code>virsh dumpxml&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ virsh dumpxml 0330b084-03db-4d42-a231-2cd6ad89515b
&amp;lt;domain type='kvm' id='150'&amp;gt;
&amp;lt;name&amp;gt;instance-0000009c&amp;lt;/name&amp;gt;
&amp;lt;uuid&amp;gt;0330b084-03db-4d42-a231-2cd6ad89515b&amp;lt;/uuid&amp;gt;
&amp;lt;memory unit='KiB'&amp;gt;6144000&amp;lt;/memory&amp;gt;
&amp;lt;currentMemory unit='KiB'&amp;gt;6144000&amp;lt;/currentMemory&amp;gt;
&amp;lt;vcpu placement='static'&amp;gt;1&amp;lt;/vcpu&amp;gt;
.
.
.
&lt;/code>&lt;/pre>
&lt;p>Or to &lt;code>nova show&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova show 0330b084-03db-4d42-a231-2cd6ad89515b
+--------------------------------------+----------------------------------------------------------+
| Property | Value |
+--------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig | MANUAL |
| OS-EXT-AZ:availability_zone | nova |
.
.
.
&lt;/code>&lt;/pre></content></item><item><title>Stupid OpenStack Tricks</title><link>https://blog.oddbit.com/post/2014-01-16-stupid-openstack-tricks/</link><pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-01-16-stupid-openstack-tricks/</guid><description>I work with several different OpenStack installations. I usually work on the command line, sourcing in an appropriate stackrc with credentials as necessary, but occasionally I want to use the dashboard for something.
For all of the deployments with which I work, the keystone endpoint is on the same host as the dashboard. So rather than trying to remember which dashboard url I want for the environment I&amp;rsquo;m currently using on the command line, I put together this shell script:</description><content>&lt;p>I work with several different OpenStack installations. I usually work
on the command line, sourcing in an appropriate &lt;code>stackrc&lt;/code> with
credentials as necessary, but occasionally I want to use the dashboard
for something.&lt;/p>
&lt;p>For all of the deployments with which I work, the keystone endpoint is
on the same host as the dashboard. So rather than trying to remember
which dashboard url I want for the environment I&amp;rsquo;m currently using on
the command line, I put together this shell script:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
url=${OS_AUTH_URL%:*}/
exec xdg-open $url
&lt;/code>&lt;/pre>
&lt;p>This takes the value of your &lt;code>OS_AUTH_URL&lt;/code> environment variable,
strips off everything after the port specification, and passes that to
your default browser.&lt;/p></content></item><item><title>Direct access to Nova metadata</title><link>https://blog.oddbit.com/post/2014-01-14-direct-access-to-nova-metadata/</link><pubDate>Tue, 14 Jan 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-01-14-direct-access-to-nova-metadata/</guid><description>When you boot a virtual instance under OpenStack, your instance has access to certain instance metadata via the Nova metadata service, which is canonically available at http://169.254.169.254/.
In an environment running Neutron, a request from your instance must traverse a number of steps:
From the instance to a router, Through a NAT rule in the router namespace, To an instance of the neutron-ns-metadata-proxy, To the actual Nova metadata service When there are problem accessing the metadata, it can be helpful to verify that the metadata service itself is configured correctly and returning meaningful information.</description><content>&lt;p>When you boot a virtual instance under &lt;a href="http://www.openstack.org/">OpenStack&lt;/a>, your instance
has access to certain &lt;a href="http://docs.openstack.org/admin-guide-cloud/content//section_metadata-service.html">instance metadata&lt;/a> via the Nova metadata service,
which is canonically available at &lt;a href="http://169.254.169.254/">http://169.254.169.254/&lt;/a>.&lt;/p>
&lt;p>In an environment running &lt;a href="https://wiki.openstack.org/wiki/Neutron">Neutron&lt;/a>, a request from your instance
must traverse a number of steps:&lt;/p>
&lt;ul>
&lt;li>From the instance to a router,&lt;/li>
&lt;li>Through a NAT rule in the router namespace,&lt;/li>
&lt;li>To an instance of the neutron-ns-metadata-proxy,&lt;/li>
&lt;li>To the actual Nova metadata service&lt;/li>
&lt;/ul>
&lt;p>When there are problem accessing the metadata, it can be helpful to
verify that the metadata service itself is configured correctly and
returning meaningful information.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Naively trying to contact the Nova metadata service listening on port
8775 will, not unexpectedly, fail:&lt;/p>
&lt;pre>&lt;code>$ curl http://localhost:8775/latest/meta-data/
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;400 Bad Request&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;400 Bad Request&amp;lt;/h1&amp;gt;
X-Instance-ID header is missing from request.&amp;lt;br /&amp;gt;&amp;lt;br /&amp;gt;
&amp;lt;/body&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>You can grab the UUID of a running instance with &lt;code>nova list&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ nova list
+--------------------------------------+-------...
| ID | Name ...
+--------------------------------------+-------...
| 32d0524b-314d-4594-b3a3-607e3f2354f8 | test0 ...
+--------------------------------------+-------...
&lt;/code>&lt;/pre>
&lt;p>You can retry your request with an appropraite &lt;code>X-Instance-ID&lt;/code> header
(&lt;code>-H 'x-instance-id: 32d0524b-314d-4594-b3a3-607e3f2354f8'&lt;/code>), but
ultimately (after also adding the tenant id), you&amp;rsquo;ll find that you
need to add an &lt;code>x-instance-id-signature&lt;/code> header. If you investigate
the &lt;a href="https://github.com/openstack/nova/blob/master/nova/api/metadata/handler.py">Nova source code&lt;/a>, you&amp;rsquo;ll find that this header is calculated
via an HMAC over the instance ID and a shared secret:&lt;/p>
&lt;pre>&lt;code>expected_signature = hmac.new(
CONF.neutron_metadata_proxy_shared_secret,
instance_id,
hashlib.sha256).hexdigest()
&lt;/code>&lt;/pre>
&lt;p>You can get the shared secret from &lt;code>/etc/nova/nova.conf&lt;/code>:&lt;/p>
&lt;pre>&lt;code># grep shared_secret /etc/nova/nova.conf
neutron_metadata_proxy_shared_secret=deadbeef2eb84d8d
&lt;/code>&lt;/pre>
&lt;p>And insert that into the previous Python code:&lt;/p>
&lt;pre>&lt;code>Python 2.7.5 (default, Nov 12 2013, 16:18:42)
[GCC 4.8.2 20131017 (Red Hat 4.8.2-1)] on linux2
Type &amp;quot;help&amp;quot;, &amp;quot;copyright&amp;quot;, &amp;quot;credits&amp;quot; or &amp;quot;license&amp;quot; for more information.
&amp;gt;&amp;gt;&amp;gt; import hmac
&amp;gt;&amp;gt;&amp;gt; import hashlib
&amp;gt;&amp;gt;&amp;gt; hmac.new('deadbeef2eb84d8d',
&amp;gt;&amp;gt;&amp;gt; '32d0524b-314d-4594-b3a3-607e3f2354f8',
&amp;gt;&amp;gt;&amp;gt; hashlib.sha256).hexdigest()
'6bcbe3885ae7efc49cef35b438efe29c95501f4a720a0c53ed000d8fcf04a605'
&amp;gt;&amp;gt;&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And now make a request directly to the metadata service:&lt;/p>
&lt;pre>&lt;code>$ curl \
-H 'x-instance-id: 32d0524b-314d-4594-b3a3-607e3f2354f8' \
-H 'x-tenant-id: 28a490a0f8b28800181ce490a74df8d2' \
-H 'x-instance-id-signature: 6bcbe3885ae7efc49cef35b438efe29c95501f4a720a0c53ed000d8fcf04a605' \
http://localhost:8775/latest/meta-data
ami-id
ami-launch-index
ami-manifest-path
block-device-mapping/
hostname
instance-action
instance-id
instance-type
kernel-id
local-hostname
local-ipv4
placement/
public-hostname
public-ipv4
public-keys/
ramdisk-id
reservation-id
&lt;/code>&lt;/pre>
&lt;p>And you&amp;rsquo;re done!&lt;/p></content></item><item><title>RDO Bug Triage</title><link>https://blog.oddbit.com/post/2014-01-13-rdo-bug-triage/</link><pubDate>Mon, 13 Jan 2014 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2014-01-13-rdo-bug-triage/</guid><description>This Wednesday, January 15, at 14:00 UTC (that&amp;rsquo;s 9AM US/Eastern, or date -d &amp;quot;14:00 UTC&amp;quot; in your local timezone) I will be helping out with the RDO bug triage day. We&amp;rsquo;ll be trying to validate all the untriaged bugs opened against RDO.
Feel free to drop by on #rdo and help out or ask questions.</description><content>&lt;p>This Wednesday, January 15, at 14:00 UTC (that&amp;rsquo;s 9AM US/Eastern, or
&lt;code>date -d &amp;quot;14:00 UTC&amp;quot;&lt;/code> in your local timezone) I will be helping out
with the
&lt;a href="http://openstack.redhat.com/">RDO&lt;/a> &lt;a href="http://openstack.redhat.com/RDO-BugTriage">bug triage day&lt;/a>. We&amp;rsquo;ll be trying to validate all the
&lt;a href="http://goo.gl/NqW2LN">untriaged bugs&lt;/a> opened against RDO.&lt;/p>
&lt;p>Feel free to drop by on &lt;code>#rdo&lt;/code> and help out or ask questions.&lt;/p></content></item><item><title>Visualizing Neutron Networking with GraphViz</title><link>https://blog.oddbit.com/post/2013-12-23-visualizing-network-with-graph/</link><pubDate>Mon, 23 Dec 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-12-23-visualizing-network-with-graph/</guid><description>I&amp;rsquo;ve put together a few tools to help gather information about your Neutron and network configuration and visualize it in different ways. All of these tools are available as part of my neutron-diag repository on GitHub.
In this post I&amp;rsquo;m going to look at a tool that will help you visualize the connectivity of network devices on your system.
mk-network-dot There are a lot of devices involved in your Neutron network configuration.</description><content>&lt;p>I&amp;rsquo;ve put together a few tools to help gather information about your
Neutron and network configuration and visualize it in different ways.
All of these tools are available as part of my &lt;a href="http://github.com/larsks/neutron-diag/">neutron-diag&lt;/a>
repository on GitHub.&lt;/p>
&lt;p>In this post I&amp;rsquo;m going to look at a tool that will help you visualize
the connectivity of network devices on your system.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="mk-network-dot">mk-network-dot&lt;/h2>
&lt;p>There are a lot of devices involved in your Neutron network
configuration. Information originating in one of your instances has
two traverse &lt;em>at least&lt;/em> seven network devices before seeing the light
of day. Understanding how everything connects is critical if you&amp;rsquo;re
trying to debug problems in your envionment.&lt;/p>
&lt;p>The &lt;code>mk-network-dot&lt;/code> tool interrogates your system for information
about network devices and generates &lt;a href="http://en.wikipedia.org/wiki/DOT_%28graph_description_language%29">dot format&lt;/a> output showing how
everything connects. You can use &lt;a href="http://www.graphviz.org/">GraphViz&lt;/a> to render this into a
variety of output formats. The script must be run as &lt;code>root&lt;/code>, so I
usually do something like this:&lt;/p>
&lt;pre>&lt;code>sudo sh mk-network-dot | dot -Tsvg -o network.svg
&lt;/code>&lt;/pre>
&lt;p>The &lt;em>dot&lt;/em> language is a language designed for describing graphs, and
the syntax looks something like this:&lt;/p>
&lt;pre>&lt;code>digraph example {
A -&amp;gt; B
A -&amp;gt; C
C -&amp;gt; D
B -&amp;gt; D
}
&lt;/code>&lt;/pre>
&lt;p>Which would produce output like this:&lt;/p>
&lt;p>&lt;img src="dot-example.svg" alt="Dot output example">&lt;/p>
&lt;p>When run on my laptop, with a simple all-in-one configuration and five
instances across two networks, the result of running &lt;code>mk-network-dot&lt;/code>
looks like this:&lt;/p>
&lt;figure class="left" >
&lt;img src="network.svg" />
&lt;/figure>
&lt;p>There are a few caveats with this tool:&lt;/p>
&lt;ul>
&lt;li>As of this writing, it doesn&amp;rsquo;t know about either bond interfaces or
VLAN interfaces.&lt;/li>
&lt;li>It&amp;rsquo;s had only limited testing.&lt;/li>
&lt;/ul>
&lt;p>If you try this out and something doesn&amp;rsquo;t work as you expect, please
open a new issues on the &lt;a href="https://github.com/larsks/neutron-diag/issues">GitHub issues page&lt;/a>.&lt;/p></content></item><item><title>An introduction to OpenStack Heat</title><link>https://blog.oddbit.com/post/2013-12-06-an-introduction-to-openstack-h/</link><pubDate>Fri, 06 Dec 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-12-06-an-introduction-to-openstack-h/</guid><description>Heat is a template-based orchestration mechanism for use with OpenStack. With Heat, you can deploy collections of resources &amp;ndash; networks, servers, storage, and more &amp;ndash; all from a single, parameterized template.
In this article I will introduce Heat templates and the heat command line client.
Writing templates Because Heat began life as an analog of AWS CloudFormation, it supports the template formats used by the CloudFormation (CFN) tools. It also supports its own native template format, called HOT (&amp;ldquo;Heat Orchestration Templates&amp;rdquo;).</description><content>&lt;p>&lt;a href="https://wiki.openstack.org/wiki/Heat">Heat&lt;/a> is a template-based orchestration mechanism for use with
OpenStack. With Heat, you can deploy collections of resources &amp;ndash;
networks, servers, storage, and more &amp;ndash; all from a single,
parameterized template.&lt;/p>
&lt;p>In this article I will introduce Heat templates and the &lt;code>heat&lt;/code> command
line client.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="writing-templates">Writing templates&lt;/h2>
&lt;p>Because Heat began life as an analog of AWS &lt;a href="http://aws.amazon.com/cloudformation/">CloudFormation&lt;/a>, it
supports the template formats used by the CloudFormation (CFN) tools.
It also supports its own native template format, called HOT (&amp;ldquo;Heat
Orchestration Templates&amp;rdquo;). In this article I will be using the HOT
template syntax, which is fully specified on &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html">the OpenStack
website&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>NB: Heat is under active development, and there are a variety of
discussions going on right now regarding the HOT specification. I
will try to keep this post up-to-date as the spec evolves.&lt;/p>
&lt;/blockquote>
&lt;p>A HOT template is written using &lt;a href="http://en.wikipedia.org/wiki/YAML">YAML&lt;/a> syntax and has three major
sections:&lt;/p>
&lt;ul>
&lt;li>&lt;em>parameters&lt;/em> &amp;ndash; these are input parameters that you provide when you
deploy from the template.&lt;/li>
&lt;li>&lt;em>resources&lt;/em> &amp;ndash; these are things created by the template.&lt;/li>
&lt;li>&lt;em>outputs&lt;/em> &amp;ndash; these are output parameters generated by Heat and
available to you via the API.&lt;/li>
&lt;/ul>
&lt;h3 id="parameters">Parameters&lt;/h3>
&lt;p>The &lt;code>parameters&lt;/code> section defines the list of available parameters.
For each parameter, you define a data type, an optional default value
(that will be used if you do not otherwise specify a value for the
parameter), an optional description, constraints to validate the data,
and so forth. The definition from &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html">the spec&lt;/a> looks like this:&lt;/p>
&lt;pre>&lt;code>parameters:
&amp;lt;param name&amp;gt;:
type: &amp;lt;string | number | json | comma_delimited_list&amp;gt;
description: &amp;lt;description of the parameter&amp;gt;
default: &amp;lt;default value for parameter&amp;gt;
hidden: &amp;lt;true | false&amp;gt;
constraints:
&amp;lt;parameter constraints&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>A simple example might look like this:&lt;/p>
&lt;pre>&lt;code>parameters:
flavor:
type: string
default: m1.small
constraints:
- allowed_values: [m1.nano, m1.tiny, m1.small, m1.large]
description: Value must be one of 'm1.tiny', 'm1.small' or 'm1.large'
&lt;/code>&lt;/pre>
&lt;p>This defines one parameter named &lt;code>flavor&lt;/code> with a default value of
&lt;code>m1.small&lt;/code>. Any value passed in when you deploy from this template
must match of one the values in the &lt;code>allowed_values&lt;/code> constraint.&lt;/p>
&lt;h3 id="resources">Resources&lt;/h3>
&lt;p>The &lt;code>resources&lt;/code> section of your template defines the items that will
be created by Heat when you deploy from your template. This may
include storage, networks, ports, routers, security groups, firewall
rules, or any other of the &lt;a href="http://docs.openstack.org/developer/heat/template_guide/openstack.html">many available resources&lt;/a>.&lt;/p>
&lt;p>The definition of this section from &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html">the spec&lt;/a> looks like
this:&lt;/p>
&lt;pre>&lt;code>resources:
&amp;lt;resource ID&amp;gt;:
type: &amp;lt;resource type&amp;gt;
properties:
&amp;lt;property name&amp;gt;: &amp;lt;property value&amp;gt;
# more resource specific metadata
&lt;/code>&lt;/pre>
&lt;p>Here&amp;rsquo;s a simple example that would create a single server:&lt;/p>
&lt;pre>&lt;code>resources:
instance0:
type: OS::Nova::Server
properties:
name: instance0
image: cirros
flavor: m1.tiny
key_name: mykey
&lt;/code>&lt;/pre>
&lt;p>The complete list of resources and their available properties can be
found &lt;a href="http://docs.openstack.org/developer/heat/template_guide/openstack.html">in the documentation&lt;/a>.&lt;/p>
&lt;p>You&amp;rsquo;ll notice that the above example is static: it will always result
in an instance using the &lt;code>cirros&lt;/code> image and the &lt;code>m1.tiny&lt;/code> flavor.
This isn&amp;rsquo;t terribly useful, so let&amp;rsquo;s redefine this example assuming
that we have available the &lt;code>parameter&lt;/code> section from the previous
example:&lt;/p>
&lt;pre>&lt;code>resources:
instance0:
type: OS::Nova::Server
properties:
name: instance0
image: cirros
flavor: {get_param: flavor}
key_name: mykey
&lt;/code>&lt;/pre>
&lt;p>Here we are using the &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#get-param">get_param&lt;/a> &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#intrinsic-functions">intrinsic function&lt;/a> to
retrieve an insert the value of the &lt;code>flavor&lt;/code> parameter.&lt;/p>
&lt;h3 id="outputs">Outputs&lt;/h3>
&lt;p>The &lt;code>outputs&lt;/code> section of your template defines parameters that will be
available to you (via the API or command line client) after your stack
has been deployed. This may include things like this ip addresses
assigned to your instances. The &lt;code>outputs&lt;/code> section definition is:&lt;/p>
&lt;pre>&lt;code>outputs:
&amp;lt;parameter name&amp;gt;:
description: &amp;lt;description&amp;gt;
value: &amp;lt;parameter value&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>In order to make the &lt;code>outputs&lt;/code> section useful, we&amp;rsquo;ll need another
template function, &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#get-attr">get_attr&lt;/a>. Where &lt;code>get_param&lt;/code> accesses values from
your &lt;code>parameters&lt;/code> section, &lt;code>get_attr&lt;/code> accesses attributes of your
resources. For example:&lt;/p>
&lt;pre>&lt;code>outputs:
instance_ip:
value: {get_attr: [instance0, first_address]}
&lt;/code>&lt;/pre>
&lt;p>You will again want to refer to the &lt;a href="http://docs.openstack.org/developer/heat/template_guide/openstack.html">list of resource types&lt;/a>
for a list of available attributes.&lt;/p>
&lt;h3 id="putting-it-all-together">Putting it all together&lt;/h3>
&lt;p>Using the above information, let&amp;rsquo;s put together a slightly more
complete template. This example will:&lt;/p>
&lt;ul>
&lt;li>Deploy a single instance&lt;/li>
&lt;li>Assign it a floating ip address&lt;/li>
&lt;li>Ensure ssh access via an ssh key published to Nova&lt;/li>
&lt;/ul>
&lt;p>We&amp;rsquo;ll get the flavor name, image name, key name, and network
information from user-provided parameters.&lt;/p>
&lt;p>Since this is a complete example, we need to add the
&lt;code>heat_template_version&lt;/code> key to our template:&lt;/p>
&lt;pre>&lt;code>heat_template_version: 2013-05-23
&lt;/code>&lt;/pre>
&lt;p>And a description provides useful documentation:&lt;/p>
&lt;pre>&lt;code>description: &amp;gt;
A simple HOT template for demonstrating Heat.
&lt;/code>&lt;/pre>
&lt;p>We define parameters for the key name, flavor, and image, as well as
network ids for address provisioning:&lt;/p>
&lt;pre>&lt;code>parameters:
key_name:
type: string
default: lars
description: Name of an existing key pair to use for the instance
flavor:
type: string
description: Instance type for the instance to be created
default: m1.small
constraints:
- allowed_values: [m1.nano, m1.tiny, m1.small, m1.large]
description: Value must be one of 'm1.tiny', 'm1.small' or 'm1.large'
image:
type: string
default: cirros
description: ID or name of the image to use for the instance
private_net_id:
type: string
description: Private network id
private_subnet_id:
type: string
description: Private subnet id
public_net_id:
type: string
description: Public network id
&lt;/code>&lt;/pre>
&lt;p>In the &lt;code>resources&lt;/code> section, we define a single instance of
&lt;code>OS::Nova::Server&lt;/code>, attach to it an instance of &lt;code>OS::Neutron::Port&lt;/code>,
and attach to that port an instance of &lt;code>OS::Neutron::FloatingIP&lt;/code>.
Note that use of the &lt;a href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#get-resource">get_resource&lt;/a> function here to refer to a
resource defined elsewhere in the template:&lt;/p>
&lt;pre>&lt;code>resources:
instance0:
type: OS::Nova::Server
properties:
name: instance0
image: { get_param: image }
flavor: { get_param: flavor }
key_name: { get_param: key_name }
networks:
- port: { get_resource: instance0_port0 }
instance0_port0:
type: OS::Neutron::Port
properties:
network_id: { get_param: private_net_id }
security_groups:
- default
fixed_ips:
- subnet_id: { get_param: private_subnet_id }
instance0_public:
type: OS::Neutron::FloatingIP
properties:
floating_network_id: { get_param: public_net_id }
port_id: { get_resource: instance0_port0 }
&lt;/code>&lt;/pre>
&lt;p>As outputs we provide the fixed and floating ip addresses assigned to
our instance:&lt;/p>
&lt;pre>&lt;code>outputs:
instance0_private_ip:
description: IP address of instance0 in private network
value: { get_attr: [ instance0, first_address ] }
instance0_public_ip:
description: Floating IP address of instance0 in public network
value: { get_attr: [ instance0_public, floating_ip_address ] }
&lt;/code>&lt;/pre>
&lt;h2 id="filling-in-the-blanks">Filling in the blanks&lt;/h2>
&lt;p>Now that we have a complete template, what do we do with it?&lt;/p>
&lt;p>When you deploy from a template, you need to provide values for any
parameters required by the template (and you may also want to override
default values). You can do this using the &lt;code>-P&lt;/code> (aka &lt;code>--parameter&lt;/code>)
command line option, which takes a semicolon-delimited list of
&lt;code>name=value&lt;/code> pairs:&lt;/p>
&lt;pre>&lt;code>heat stack-create -P 'param1=value1;param2=value2' ...
&lt;/code>&lt;/pre>
&lt;p>While this works, it&amp;rsquo;s not terribly useful, especially as the
parameter list grows long. You can also provide parameters via an
&lt;a href="https://wiki.openstack.org/wiki/Heat/Environments">environment file&lt;/a>, which a YAML configuration file containing a
&lt;code>parameters&lt;/code> key (you can use environment files for other things, too,
but here we&amp;rsquo;re going to focus on their use for template parameters).
A sample file, equivalent to arguments to &lt;code>-P&lt;/code> in the above command
line, might look like:&lt;/p>
&lt;pre>&lt;code>parameters:
param1: value1
param2: value2
&lt;/code>&lt;/pre>
&lt;p>An environment file appropriate to our example template from the
previous section might look like this:&lt;/p>
&lt;pre>&lt;code>parameters:
image: fedora-19-x86_64
flavor: m1.small
private_net_id: 99ab8ebf-ad2f-4a4b-9890-fee37cea4254
private_subnet_id: ed8ad5f5-4c47-4204-9ca3-1b3bc4de286d
public_net_id: 7e687cc3-8155-4ec2-bd11-ba741ecbf4f0
&lt;/code>&lt;/pre>
&lt;p>You would, of course, need to replace the network ids with ones
appropriate to your environment.&lt;/p>
&lt;h2 id="command-line-client">Command line client&lt;/h2>
&lt;p>With the template in a file called &lt;code>template.yml&lt;/code> and the parameters
in a file called &lt;code>environment.yml&lt;/code>, we could deploy an instance like
this:&lt;/p>
&lt;pre>&lt;code>heat stack-create -f template.yml \
-e environment.yml mystack
&lt;/code>&lt;/pre>
&lt;p>This would, assuming no errors, create a stack called &lt;code>mystack&lt;/code>. You
can view the status of your stacks with the &lt;code>stack-list&lt;/code> subcommand:&lt;/p>
&lt;pre>&lt;code>$ heat stack-list
+-------+------------+-----------------+----------------------+
| id | stack_name | stack_status | creation_time |
+-------+------------+-----------------+----------------------+
| 0...6 | mystack | CREATE_COMPLETE | 2013-12-06T21:37:32Z |
+-------+------------+-----------------+----------------------+
&lt;/code>&lt;/pre>
&lt;p>You can view detailed information about your stack &amp;ndash; including the
values of your outputs &amp;ndash; using the &lt;code>stack-show&lt;/code> subcommand:&lt;/p>
&lt;pre>&lt;code>$ heat stack-show mystack
&lt;/code>&lt;/pre>
&lt;p>(The output is a little too verbose to include here.)&lt;/p>
&lt;p>If you want more convenient access to the values of your outputs,
you&amp;rsquo;re going to have to either make direct use of the Heat &lt;a href="http://api.openstack.org/api-ref-orchestration.html">REST
API&lt;/a>, or wait for my &lt;a href="https://review.openstack.org/#/c/60591/">proposed change&lt;/a> to the
&lt;a href="https://launchpad.net/python-heatclient">python-heatclient&lt;/a> package, which will add the &lt;code>output-list&lt;/code> and
&lt;code>output-get&lt;/code> subcommands:&lt;/p>
&lt;pre>&lt;code>$ heat output-list mystack
instance0_private_ip
instance0_public_ip
$ heat output-get mystack instance0_public_ip
192.168.122.203
&lt;/code>&lt;/pre>
&lt;h2 id="working-with-neutron">Working with Neutron&lt;/h2>
&lt;p>If you are using Heat in an environment that uses &lt;a href="https://wiki.openstack.org/wiki/Neutron">Neutron&lt;/a> for
networking you may need to take a few additional steps. By default,
your virtual instances will not be associated with &lt;em>any&lt;/em> security
groups, which means that they will have neither outbound or inbound
network connectivity. This is in contrast to instances started using
the &lt;code>nova boot&lt;/code> command, which will automatically be members of the
&lt;code>default&lt;/code> security group.&lt;/p>
&lt;p>In order to provide your instances with appropriate network
connectivity, you will need to associate each &lt;code>OS::Neutron::Port&lt;/code>
resource in your template with one or more security groups. For
example, the following configuration snippet would create a port
&lt;code>instance0_port0&lt;/code> and assign it to the &lt;code>default&lt;/code> and &lt;code>webserver&lt;/code>
security groups:&lt;/p>
&lt;pre>&lt;code>instance0_port0:
type: OS::Neutron::Port
properties:
network_id: { get_param: private_net_id }
security_groups:
- default
- webserver
fixed_ips:
- subnet_id: { get_param: private_subnet_id }
&lt;/code>&lt;/pre>
&lt;p>For this to work, you will need to be running a (very) recent version
of Heat. Until commit &lt;a href="https://github.com/openstack/heat/commit/902154c">902154c&lt;/a>, Heat was unable to look up Neutron security
groups by name. It worked fine if you specified security groups by
UUID:&lt;/p>
&lt;pre>&lt;code>security_groups:
- 4296f3ff-9dc0-4b0b-a633-c30eacc8493d
- 8c49cd42-7c42-4a1f-af1d-492a0687fc12
&lt;/code>&lt;/pre>
&lt;h2 id="see-also">See also&lt;/h2>
&lt;ul>
&lt;li>The Heat project maintains a &lt;a href="https://github.com/openstack/heat-templates">repository of example
templates&lt;/a>.&lt;/li>
&lt;/ul></content></item><item><title>A unified CLI for OpenStack</title><link>https://blog.oddbit.com/post/2013-11-22-a-unified-cli-for-op/</link><pubDate>Fri, 22 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-22-a-unified-cli-for-op/</guid><description>The python-openstackclient project, by Dean Troyer and others, is a new command line tool to replace the existing command line clients (including commands such as nova, keystone, cinder, etc).
This tool solves two problems I&amp;rsquo;ve encountered in the past:
Command line options between different command line clients are sometimes inconsistent.
The output from the legacy command line tools is not designed to be machine parse-able (and yet people do it anyway).</description><content>&lt;p>The &lt;a href="https://github.com/openstack/python-openstackclient">python-openstackclient&lt;/a> project, by &lt;a href="https://github.com/dtroyer">Dean Troyer&lt;/a> and
others, is a new command line tool to replace the existing command
line clients (including commands such as &lt;code>nova&lt;/code>, &lt;code>keystone&lt;/code>, &lt;code>cinder&lt;/code>,
etc).&lt;/p>
&lt;p>This tool solves two problems I&amp;rsquo;ve encountered in the past:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Command line options between different command line clients are
sometimes inconsistent.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The output from the legacy command line tools is not designed to be
machine parse-able (and yet people &lt;a href="https://github.com/openstack/python-openstackclient">do it anyway&lt;/a>).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The new &lt;code>openstack&lt;/code> CLI framework is implement using the &lt;a href="https://github.com/dreamhost/cliff">cliff&lt;/a>
module for Python, which will help enforce a consistent interface to
the various subcommands (because common options can be shared, and
just having everything in the same codebase will help tremendously).
Cliff also provides flexible table formatters. It includes a number
of useful formatters out of the box, and can be extended via
setuptools entry points.&lt;/p>
&lt;p>The &lt;code>csv&lt;/code> formatter can be used to produce machine parse-able output
from list commands. For example:&lt;/p>
&lt;pre>&lt;code>$ openstack -q endpoint list -f csv --quote none
ID,Region,Service Name,Service Type
ba686936d31846f5b226539dba285654,RegionOne,quantum,network
161684fd123740138c8806267c489766,RegionOne,cinder,volume
b2019dbef5f34d1bb809e8e399369782,RegionOne,keystone,identity
4b5dd8c6b961442ba13d6b9d317d718a,RegionOne,swift_s3,s3
ac766707ffa3437eaaeaafa3c3eace08,RegionOne,swift,object-store
e3f7bd37b51341bbaa77f81ba39a3bf2,RegionOne,glance,image
6821fad71a914636af6e98775e52e1ec,RegionOne,nova_ec2,ec2
3b2a90e9f85a468988af763c707961d7,RegionOne,nova,compute
&lt;/code>&lt;/pre>
&lt;p>For &amp;ldquo;show&amp;rdquo; commands, the &lt;code>shell&lt;/code> formatter produces output in
&lt;code>name=value&lt;/code> format, like this:&lt;/p>
&lt;pre>&lt;code>$ openstack -q endpoint show image -f shell --all
adminurl=&amp;quot;http://192.168.122.110:9292&amp;quot;
id=&amp;quot;e3f7bd37b51341bbaa77f81ba39a3bf2&amp;quot;
internalurl=&amp;quot;http://192.168.122.110:9292&amp;quot;
publicurl=&amp;quot;http://192.168.122.110:9292&amp;quot;
region=&amp;quot;RegionOne&amp;quot;
service_id=&amp;quot;14a1479f77274dd485e9fb52af2e1721&amp;quot;
service_name=&amp;quot;glance&amp;quot;
service_type=&amp;quot;image&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>This output could easily be sourced into a shell script.&lt;/p></content></item><item><title>json-tools: cli for generating and filtering json</title><link>https://blog.oddbit.com/post/2013-11-17-json-tools/</link><pubDate>Sun, 17 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-17-json-tools/</guid><description>Interacting with JSON-based APIs from the command line can be difficult, and OpenStack is filled with REST APIs that consume or produce JSON. I&amp;rsquo;ve just put pair of tools for generating and filtering JSON on the command line, called collectively json-tools.
Both make use of the Python dpath module to populate or filter JSON objects.
The jsong command generates JSON on stdout. You provide /-delimited paths on the command line to represent the JSON structure.</description><content>&lt;p>Interacting with JSON-based APIs from the command line can be
difficult, and OpenStack is filled with REST APIs that consume or
produce JSON. I&amp;rsquo;ve just put pair of tools for generating and
filtering JSON on the command line, called collectively
&lt;a href="http://github.com/larsks/json-tools/">json-tools&lt;/a>.&lt;/p>
&lt;p>Both make use of the Python &lt;a href="https://github.com/akesterson/dpath-python">dpath&lt;/a> module to populate or filter
JSON objects.&lt;/p>
&lt;p>The &lt;code>jsong&lt;/code> command generates JSON on &lt;code>stdout&lt;/code>. You provide &lt;code>/&lt;/code>-delimited paths
on the command line to represent the JSON structure. For example, if
you run:&lt;/p>
&lt;pre>&lt;code>$ jsong auth/passwordCredentials/username=admin \
auth/passwordCredentials/password=secret
&lt;/code>&lt;/pre>
&lt;p>You get:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;auth&amp;quot;: {
&amp;quot;passwordCredentials&amp;quot;: {
&amp;quot;username&amp;quot;: &amp;quot;admin&amp;quot;,
&amp;quot;password&amp;quot;: &amp;quot;secret&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>jsonx&lt;/code> command accepts JSON on &lt;code>stdin&lt;/code> and selects subtrees or
values for output on &lt;code>stdout&lt;/code>. Given the above output, you could
extract the password with:&lt;/p>
&lt;pre>&lt;code>jsonx -v auth/passwordCredentials/password
&lt;/code>&lt;/pre>
&lt;p>Which would give you:&lt;/p>
&lt;pre>&lt;code>secret
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>-v&lt;/code> flag here indicates that you only want the values of matched
paths; without the &lt;code>-v&lt;/code> the output would have been:&lt;/p>
&lt;pre>&lt;code>auth/passwordCredentials/password secret
&lt;/code>&lt;/pre>
&lt;p>There are more examples &amp;ndash; including some use of the OpenStack APIs &amp;ndash;
in the &lt;a href="https://github.com/larsks/json-tools/blob/master/README.md">README&lt;/a> document.&lt;/p></content></item><item><title>Quantum in Too Much Detail</title><link>https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/</link><pubDate>Thu, 14 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-14-quantum-in-too-much-detail/</guid><description>I originally posted this article on the RDO website.
The players This document describes the architecture that results from a particular OpenStack configuration, specifically:
Quantum networking using GRE tunnels; A dedicated network controller; A single instance running on a compute host Much of the document will be relevant to other configurations, but details will vary based on your choice of layer 2 connectivity, number of running instances, and so forth.</description><content>&lt;blockquote>
&lt;p>I originally posted this article on
the &lt;a href="http://openstack.redhat.com/Networking_in_too_much_detail">RDO&lt;/a>
website.&lt;/p>
&lt;/blockquote>
&lt;h1 id="the-players">The players&lt;/h1>
&lt;p>This document describes the architecture that results from a
particular OpenStack configuration, specifically:&lt;/p>
&lt;ul>
&lt;li>Quantum networking using GRE tunnels;&lt;/li>
&lt;li>A dedicated network controller;&lt;/li>
&lt;li>A single instance running on a compute host&lt;/li>
&lt;/ul>
&lt;p>Much of the document will be relevant to other configurations, but
details will vary based on your choice of layer 2 connectivity, number
of running instances, and so forth.&lt;/p>
&lt;p>The examples in this document were generated on a system with Quantum
networking but will generally match what you see under Neutron as
well, if you replace &lt;code>quantum&lt;/code> by &lt;code>neutron&lt;/code> in names. The OVS flow
rules under Neutron are somewhat more complex and I will cover those
in another post.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h1 id="the-lay-of-the-land">The lay of the land&lt;/h1>
&lt;p>This is a simplified architecture diagram of network connectivity in a
quantum/neutron managed world:&lt;/p>
&lt;figure class="left" >
&lt;img src="quantum-gre.svg" />
&lt;/figure>
&lt;p>Section names in this document include
parenthetical references to the nodes on the map relevant to that
particular section.&lt;/p>
&lt;h1 id="compute-host-instance-networking-abc">Compute host: instance networking (A,B,C)&lt;/h1>
&lt;p>An outbound packet starts on &lt;code>eth0&lt;/code> of the virtual instance, which is
connected to a &lt;code>tap&lt;/code> device on the host, &lt;code>tap7c7ae61e-05&lt;/code>. This &lt;code>tap&lt;/code>
device is attached to a Linux bridge device, &lt;code>qbr7c7ae61e-05&lt;/code>. What is
this bridge device for? From the &lt;a href="http://docs.openstack.org/network-admin/admin/content/under_the_hood_openvswitch.html">OpenStack Networking Administration
Guide&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Ideally, the TAP device vnet0 would be connected directly to the
integration bridge, br-int. Unfortunately, this isn&amp;rsquo;t possible because
of how OpenStack security groups are currently implemented. OpenStack
uses iptables rules on the TAP devices such as vnet0 to implement
security groups, and Open vSwitch is not compatible with iptables
rules that are applied directly on TAP devices that are connected to
an Open vSwitch port.&lt;/p>
&lt;/blockquote>
&lt;p>Because this bridge device exists primarily to support firewall rules,
I&amp;rsquo;m going to refer to it as the &amp;ldquo;firewall bridge&amp;rdquo;.&lt;/p>
&lt;p>If you examine the firewall rules on your compute host, you will find
that there are several rules associated with this &lt;code>tap&lt;/code> device:&lt;/p>
&lt;pre>&lt;code># iptables -S | grep tap7c7ae61e-05
-A quantum-openvswi-FORWARD -m physdev --physdev-out tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-sg-chain
-A quantum-openvswi-FORWARD -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-sg-chain
-A quantum-openvswi-INPUT -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-o7c7ae61e-0
-A quantum-openvswi-sg-chain -m physdev --physdev-out tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-i7c7ae61e-0
-A quantum-openvswi-sg-chain -m physdev --physdev-in tap7c7ae61e-05 --physdev-is-bridged -j quantum-openvswi-o7c7ae61e-0
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>quantum-openvswi-sg-chain&lt;/code> is where &lt;code>neutron&lt;/code>-managed security
groups are realized. The &lt;code>quantum-openvswi-o7c7ae61e-0&lt;/code> chain
controls outbound traffic FROM the instance, and by default looks like
this:&lt;/p>
&lt;pre>&lt;code>-A quantum-openvswi-o7c7ae61e-0 -m mac ! --mac-source FA:16:3E:03:00:E7 -j DROP
-A quantum-openvswi-o7c7ae61e-0 -p udp -m udp --sport 68 --dport 67 -j RETURN
-A quantum-openvswi-o7c7ae61e-0 ! -s 10.1.0.2/32 -j DROP
-A quantum-openvswi-o7c7ae61e-0 -p udp -m udp --sport 67 --dport 68 -j DROP
-A quantum-openvswi-o7c7ae61e-0 -m state --state INVALID -j DROP
-A quantum-openvswi-o7c7ae61e-0 -m state --state RELATED,ESTABLISHED -j RETURN
-A quantum-openvswi-o7c7ae61e-0 -j RETURN
-A quantum-openvswi-o7c7ae61e-0 -j quantum-openvswi-sg-fallback
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>quantum-openvswi-i7c7ae61e-0&lt;/code> chain controls inbound traffic TO
the instance. After opening up port 22 in the default security group:&lt;/p>
&lt;pre>&lt;code># neutron security-group-rule-create --protocol tcp \
--port-range-min 22 --port-range-max 22 --direction ingress default
&lt;/code>&lt;/pre>
&lt;p>The rules look like this:&lt;/p>
&lt;pre>&lt;code>-A quantum-openvswi-i7c7ae61e-0 -m state --state INVALID -j DROP
-A quantum-openvswi-i7c7ae61e-0 -m state --state RELATED,ESTABLISHED -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -p icmp -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -p tcp -m tcp --dport 22 -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -p tcp -m tcp --dport 80 -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -s 10.1.0.3/32 -p udp -m udp --sport 67 --dport 68 -j RETURN
-A quantum-openvswi-i7c7ae61e-0 -j quantum-openvswi-sg-fallback
&lt;/code>&lt;/pre>
&lt;p>A second interface attached to the bridge, &lt;code>qvb7c7ae61e-05&lt;/code>, attaches
the firewall bridge to the integration bridge, typically named
&lt;code>br-int&lt;/code>.&lt;/p>
&lt;h1 id="compute-host-integration-bridge-de">Compute host: integration bridge (D,E)&lt;/h1>
&lt;p>The integration bridge, &lt;code>br-int&lt;/code>, performs VLAN tagging and un-tagging
for traffic coming from and to your instances. At this moment,
&lt;code>br-int&lt;/code> looks something like this:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl show
Bridge br-int
Port &amp;quot;qvo7c7ae61e-05&amp;quot;
tag: 1
Interface &amp;quot;qvo7c7ae61e-05&amp;quot;
Port patch-tun
Interface patch-tun
type: patch
options: {peer=patch-int}
Port br-int
Interface br-int
type: internal
&lt;/code>&lt;/pre>
&lt;p>The interface &lt;code>qvo7c7ae61e-05&lt;/code> is the other end of &lt;code>qvb7c7ae61e-05&lt;/code>,
and carries traffic to and from the firewall bridge. The &lt;code>tag: 1&lt;/code> you
see in the above output integrates that this is an access port
attached to VLAN 1. Untagged outbound traffic from this instance will be
assigned VLAN ID 1, and inbound traffic with VLAN ID 1 will
stripped of it&amp;rsquo;s VLAN tag and sent out this port.&lt;/p>
&lt;p>Each network you create (with &lt;code>neutron net-create&lt;/code>) will be assigned a
different VLAN ID.&lt;/p>
&lt;p>The interface named &lt;code>patch-tun&lt;/code> connects the integration bridge to the
tunnel bridge, &lt;code>br-tun&lt;/code>.&lt;/p>
&lt;h1 id="compute-host-tunnel-bridge-fg">Compute host: tunnel bridge (F,G)&lt;/h1>
&lt;p>The tunnel bridge translates VLAN-tagged traffic from the
integration bridge into &lt;code>GRE&lt;/code> tunnels. The translation between VLAN
IDs and tunnel IDs is performed by OpenFlow rules installed on
&lt;code>br-tun&lt;/code>. Before creating any instances, the flow rules on the bridge
look like this:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=871.283s, table=0, n_packets=4, n_bytes=300, idle_age=862, priority=1 actions=drop
&lt;/code>&lt;/pre>
&lt;p>There is a single rule that causes the bridge to drop all traffic.
Afrer you boot an instance on this compute node, the rules are
modified to look something like:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=422.158s, table=0, n_packets=2, n_bytes=120, idle_age=55, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
cookie=0x0, duration=421.948s, table=0, n_packets=64, n_bytes=8337, idle_age=31, priority=3,tun_id=0x2,dl_dst=fa:16:3e:dd:c1:62 actions=mod_vlan_vid:1,NORMAL
cookie=0x0, duration=422.357s, table=0, n_packets=82, n_bytes=10443, idle_age=31, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
cookie=0x0, duration=1502.657s, table=0, n_packets=8, n_bytes=596, idle_age=423, priority=1 actions=drop
&lt;/code>&lt;/pre>
&lt;p>In general, these rules are responsible for mapping traffic between
VLAN ID 1, used by the integration bridge, and tunnel id 2, used by
the GRE tunnel.&lt;/p>
&lt;p>The first rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=422.158s, table=0, n_packets=2, n_bytes=120, idle_age=55, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches all multicast traffic (see &lt;a href="http://openvswitch.org/cgi-bin/ovsman.cgi?page=utilities%2Fovs-ofctl.8">ovs-ofctl(8)&lt;/a>)
on tunnel id 2 (&lt;code>tun_id=0x2&lt;/code>), tags the ethernet frame with VLAN ID
1 (&lt;code>actions=mod_vlan_vid:1&lt;/code>), and sends it out port 1. We can see
from &lt;code>ovs-ofctl show br-tun&lt;/code> that port 1 is &lt;code>patch-int&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl show br-tun
OFPT_FEATURES_REPLY (xid=0x2): dpid:0000068df4e44a49
n_tables:254, n_buffers:256
capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP
actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC SET_DL_DST SET_NW_SRC SET_NW_DST SET_NW_TOS SET_TP_SRC SET_TP_DST ENQUEUE
1(patch-int): addr:46:3d:59:17:df:62
config: 0
state: 0
speed: 0 Mbps now, 0 Mbps max
2(gre-2): addr:a2:5f:a1:92:29:02
config: 0
state: 0
speed: 0 Mbps now, 0 Mbps max
LOCAL(br-tun): addr:06:8d:f4:e4:4a:49
config: 0
state: 0
speed: 0 Mbps now, 0 Mbps max
OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0
&lt;/code>&lt;/pre>
&lt;p>The next rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=421.948s, table=0, n_packets=64, n_bytes=8337, idle_age=31, priority=3,tun_id=0x2,dl_dst=fa:16:3e:dd:c1:62 actions=mod_vlan_vid:1,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches traffic coming in on tunnel 2 (&lt;code>tun_id=0x2&lt;/code>) with an
ethernet destination of &lt;code>fa:16:3e:dd:c1:62&lt;/code>
(&lt;code>dl_dst=fa:16:3e:dd:c1:62&lt;/code>) and tags the ethernet frame with VLAN
ID 1 (&lt;code>actions=mod_vlan_vid:1&lt;/code>) before sending it out &lt;code>patch-int&lt;/code>.&lt;/p>
&lt;p>The following rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=422.357s, table=0, n_packets=82, n_bytes=10443, idle_age=31, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches traffic coming in on port 1 (&lt;code>in_port=1&lt;/code>) with VLAN ID 1
(&lt;code>dl_vlan=1&lt;/code>) and set the tunnel id to 2 (&lt;code>actions=set_tunnel:0x2&lt;/code>)
before sending it out the GRE tunnel.&lt;/p>
&lt;h1 id="network-host-tunnel-bridge-hi">Network host: tunnel bridge (H,I)&lt;/h1>
&lt;p>Traffic arrives on the network host via the GRE tunnel attached to
&lt;code>br-tun&lt;/code>. This bridge has a flow table very similar to &lt;code>br-tun&lt;/code> on
the compute host:&lt;/p>
&lt;pre>&lt;code># ovs-ofctl dump-flows br-tun
NXST_FLOW reply (xid=0x4):
cookie=0x0, duration=1239.229s, table=0, n_packets=23, n_bytes=4246, idle_age=15, priority=3,tun_id=0x2,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=mod_vlan_vid:1,output:1
cookie=0x0, duration=524.477s, table=0, n_packets=15, n_bytes=3498, idle_age=10, priority=3,tun_id=0x2,dl_dst=fa:16:3e:83:69:cc actions=mod_vlan_vid:1,NORMAL
cookie=0x0, duration=1239.157s, table=0, n_packets=50, n_bytes=4565, idle_age=148, priority=3,tun_id=0x2,dl_dst=fa:16:3e:aa:99:3c actions=mod_vlan_vid:1,NORMAL
cookie=0x0, duration=1239.304s, table=0, n_packets=76, n_bytes=9419, idle_age=10, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
cookie=0x0, duration=1527.016s, table=0, n_packets=12, n_bytes=880, idle_age=527, priority=1 actions=drop
&lt;/code>&lt;/pre>
&lt;p>As on the compute host, the first rule maps multicast traffic on
tunnel ID 2 to VLAN 1.&lt;/p>
&lt;p>The second rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=524.477s, table=0, n_packets=15, n_bytes=3498, idle_age=10, priority=3,tun_id=0x2,dl_dst=fa:16:3e:83:69:cc actions=mod_vlan_vid:1,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches traffic on the tunnel destined for the DHCP server at
&lt;code>fa:16:3e:83:69:cc&lt;/code>. This is a &lt;code>dnsmasq&lt;/code> process running inside a
network namespace, the details of which we will examine shortly.&lt;/p>
&lt;p>The next rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=1239.157s, table=0, n_packets=50, n_bytes=4565, idle_age=148, priority=3,tun_id=0x2,dl_dst=fa:16:3e:aa:99:3c actions=mod_vlan_vid:1,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;matches traffic on tunnel ID 2 destined for the router at &lt;code>fa:16:3e:aa:99:3c&lt;/code>, which is an interface in another network namespace.&lt;/p>
&lt;p>The following rule&amp;hellip;&lt;/p>
&lt;pre>&lt;code> cookie=0x0, duration=1239.304s, table=0, n_packets=76, n_bytes=9419, idle_age=10, priority=4,in_port=1,dl_vlan=1 actions=set_tunnel:0x2,NORMAL
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;simply maps outbound traffic on VLAN ID 1 to tunnel ID 2.&lt;/p>
&lt;h1 id="network-host-integration-bridge-jkm">Network host: integration bridge (J,K,M)&lt;/h1>
&lt;p>The integration bridge on the network controller serves to connect
instances to network services, such as routers and DHCP servers.&lt;/p>
&lt;pre>&lt;code># ovs-vsctl show
.
.
.
Bridge br-int
Port patch-tun
Interface patch-tun
type: patch
options: {peer=patch-int}
Port &amp;quot;tapf14c598d-98&amp;quot;
tag: 1
Interface &amp;quot;tapf14c598d-98&amp;quot;
Port br-int
Interface br-int
type: internal
Port &amp;quot;tapc2d7dd02-56&amp;quot;
tag: 1
Interface &amp;quot;tapc2d7dd02-56&amp;quot;
.
.
.
&lt;/code>&lt;/pre>
&lt;p>It connects to the tunnel bridge, &lt;code>br-tun&lt;/code>, via a patch interface,
&lt;code>patch-tun&lt;/code>.&lt;/p>
&lt;h1 id="network-host-dhcp-server-m">Network host: DHCP server (M)&lt;/h1>
&lt;p>Each network for which DHCP is enabled has a DHCP server running on
the network controller. The DHCP server is an instance of &lt;a href="http://www.thekelleys.org.uk/dnsmasq/doc.html">dnsmasq&lt;/a>
running inside a &lt;em>network namespace&lt;/em>. A &lt;em>network namespace&lt;/em> is a
Linux kernel facility that allows groups of processes to have a
network stack (interfaces, routing tables, iptables rules) distinct
from that of the host.&lt;/p>
&lt;p>You can see a list of network namespace with the &lt;code>ip netns&lt;/code> command,
which in our configuration will look something like this:&lt;/p>
&lt;pre>&lt;code># ip netns
qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264
qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f
&lt;/code>&lt;/pre>
&lt;p>The first of these (&lt;code>qdhcp...&lt;/code>) is the DHCP server namespace for our private
subnet, while the second (&lt;code>qrouter...&lt;/code>) is the router.&lt;/p>
&lt;p>You can run a command inside a network namespace using the &lt;code>ip netns exec&lt;/code> command. For example, to see the interface configuration inside
the DHCP server namespace (&lt;code>lo&lt;/code> removed for brevity):&lt;/p>
&lt;pre>&lt;code># ip netns exec qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264 ip addr
71: tapf14c598d-98: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
link/ether fa:16:3e:10:2f:03 brd ff:ff:ff:ff:ff:ff
inet 10.1.0.3/24 brd 10.1.0.255 scope global ns-f14c598d-98
inet6 fe80::f816:3eff:fe10:2f03/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>Note the MAC address on interface &lt;code>tapf14c598d-98&lt;/code>; this matches the MAC address in the flow rule we saw on the tunnel bridge.&lt;/p>
&lt;p>You can find the &lt;code>dnsmasq&lt;/code> process associated with this namespace by
search the output of &lt;code>ps&lt;/code> for the id (the number after &lt;code>qdhcp-&lt;/code> in the
namespace name):&lt;/p>
&lt;pre>&lt;code># ps -fe | grep 88b1609c-68e0-49ca-a658-f1edff54a264
nobody 23195 1 0 Oct26 ? 00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=ns-f14c598d-98 --except-interface=lo --pid-file=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/pid --dhcp-hostsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/host --dhcp-optsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/opts --dhcp-script=/usr/bin/quantum-dhcp-agent-dnsmasq-lease-update --leasefile-ro --dhcp-range=tag0,10.1.0.0,static,120s --conf-file= --domain=openstacklocal
root 23196 23195 0 Oct26 ? 00:00:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=ns-f14c598d-98 --except-interface=lo --pid-file=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/pid --dhcp-hostsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/host --dhcp-optsfile=/var/lib/quantum/dhcp/88b1609c-68e0-49ca-a658-f1edff54a264/opts --dhcp-script=/usr/bin/quantum-dhcp-agent-dnsmasq-lease-update --leasefile-ro --dhcp-range=tag0,10.1.0.0,static,120s --conf-file= --domain=openstacklocal
&lt;/code>&lt;/pre>
&lt;h1 id="network-host-router-kl">Network host: Router (K,L)&lt;/h1>
&lt;p>A router is a network namespace with a set of routing tables
and iptables rules that performs the routing between subnets. Recall
that we saw two network namespaces in our configuration:&lt;/p>
&lt;pre>&lt;code># ip netns
qdhcp-88b1609c-68e0-49ca-a658-f1edff54a264
qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f
&lt;/code>&lt;/pre>
&lt;p>Using the &lt;code>ip netns exec&lt;/code> command, we can inspect the interfaces
associated with the router (&lt;code>lo&lt;/code> removed for brevity):&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f ip addr
66: qg-d48b49e0-aa: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
link/ether fa:16:3e:5c:a2:ac brd ff:ff:ff:ff:ff:ff
inet 172.24.4.227/28 brd 172.24.4.239 scope global qg-d48b49e0-aa
inet 172.24.4.228/32 brd 172.24.4.228 scope global qg-d48b49e0-aa
inet6 fe80::f816:3eff:fe5c:a2ac/64 scope link
valid_lft forever preferred_lft forever
68: qr-c2d7dd02-56: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
link/ether fa:16:3e:ea:64:6e brd ff:ff:ff:ff:ff:ff
inet 10.1.0.1/24 brd 10.1.0.255 scope global qr-c2d7dd02-56
inet6 fe80::f816:3eff:feea:646e/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>
&lt;p>The first interface, &lt;code>qg-d48b49e0-aa&lt;/code>, connects the router to the
gateway set by the &lt;code>router-gateway-set&lt;/code> command. The second
interface, &lt;code>qr-c2d7dd02-56&lt;/code>, is what connects the router to the
integration bridge.&lt;/p>
&lt;p>Looking at the routing tables inside the router, we see that there is
a default gateway pointing to the &lt;code>.1&lt;/code> address of our external
network, and the expected network routes for directly attached
networks:&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f ip route
172.24.4.224/28 dev qg-d48b49e0-aa proto kernel scope link src 172.24.4.227
10.1.0.0/24 dev qr-c2d7dd02-56 proto kernel scope link src 10.1.0.1
default via 172.24.4.225 dev qg-d48b49e0-aa
&lt;/code>&lt;/pre>
&lt;p>The netfilter &lt;code>nat&lt;/code> table inside the router namespace is responsible
for associating floating IP addresses with your instances. For
example, after associating the address &lt;code>172.24.4.228&lt;/code> with our
instance, the &lt;code>nat&lt;/code> table looks like this:&lt;/p>
&lt;pre>&lt;code># ip netns exec qrouter-2d214fde-293c-4d64-8062-797f80ae2d8f iptables -t nat -S
-P PREROUTING ACCEPT
-P POSTROUTING ACCEPT
-P OUTPUT ACCEPT
-N quantum-l3-agent-OUTPUT
-N quantum-l3-agent-POSTROUTING
-N quantum-l3-agent-PREROUTING
-N quantum-l3-agent-float-snat
-N quantum-l3-agent-snat
-N quantum-postrouting-bottom
-A PREROUTING -j quantum-l3-agent-PREROUTING
-A POSTROUTING -j quantum-l3-agent-POSTROUTING
-A POSTROUTING -j quantum-postrouting-bottom
-A OUTPUT -j quantum-l3-agent-OUTPUT
-A quantum-l3-agent-OUTPUT -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2
-A quantum-l3-agent-POSTROUTING ! -i qg-d48b49e0-aa ! -o qg-d48b49e0-aa -m conntrack ! --ctstate DNAT -j ACCEPT
-A quantum-l3-agent-PREROUTING -d 169.254.169.254/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 9697
-A quantum-l3-agent-PREROUTING -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2
-A quantum-l3-agent-float-snat -s 10.1.0.2/32 -j SNAT --to-source 172.24.4.228
-A quantum-l3-agent-snat -j quantum-l3-agent-float-snat
-A quantum-l3-agent-snat -s 10.1.0.0/24 -j SNAT --to-source 172.24.4.227
-A quantum-postrouting-bottom -j quantum-l3-agent-snat
&lt;/code>&lt;/pre>
&lt;p>There are &lt;code>SNAT&lt;/code> and &lt;code>DNAT&lt;/code> rules to map traffic between the floating
address, &lt;code>172.24.4.228&lt;/code>, and the private address &lt;code>10.1.0.2&lt;/code>:&lt;/p>
&lt;pre>&lt;code>-A quantum-l3-agent-OUTPUT -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2
-A quantum-l3-agent-PREROUTING -d 172.24.4.228/32 -j DNAT --to-destination 10.1.0.2
-A quantum-l3-agent-float-snat -s 10.1.0.2/32 -j SNAT --to-source 172.24.4.228
&lt;/code>&lt;/pre>
&lt;p>When you associate a floating ip address with an instance, similar
rules will be created in this table.&lt;/p>
&lt;p>There is also an &lt;code>SNAT&lt;/code> rule that NATs all outbound traffic from our
private network to &lt;code>172.24.4.227&lt;/code>:&lt;/p>
&lt;pre>&lt;code>-A quantum-l3-agent-snat -s 10.1.0.0/24 -j SNAT --to-source 172.24.4.227
&lt;/code>&lt;/pre>
&lt;p>This permits instances to have outbound connectivity even without a
public ip address.&lt;/p>
&lt;h1 id="network-host-external-traffic-l">Network host: External traffic (L)&lt;/h1>
&lt;p>&amp;ldquo;External&amp;rdquo; traffic flows through &lt;code>br-ex&lt;/code> via the &lt;code>qg-d48b49e0-aa&lt;/code>
interface in the router name space.&lt;/p>
&lt;pre>&lt;code>Bridge br-ex
Port &amp;quot;qg-d48b49e0-aa&amp;quot;
Interface &amp;quot;qg-d48b49e0-aa&amp;quot;
Port br-ex
Interface br-ex
type: internal
&lt;/code>&lt;/pre>
&lt;p>What happens when traffic gets this far depends on your local
configuration.&lt;/p>
&lt;h2 id="nat-to-host-address">NAT to host address&lt;/h2>
&lt;p>If you assign the gateway address for your public network to &lt;code>br-ex&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ip addr add 172.24.4.225/28 dev br-ex
&lt;/code>&lt;/pre>
&lt;p>Then you can create forwarding and NAT rules that will cause
&amp;ldquo;external&amp;rdquo; traffic from your instances to get rewritten to your
network controller&amp;rsquo;s ip address and sent out on the network:&lt;/p>
&lt;pre>&lt;code># iptables -A FORWARD -d 172.24.4.224/28 -j ACCEPT
# iptables -A FORWARD -s 172.24.4.224/28 -j ACCEPT
# iptables -t nat -I POSTROUTING 1 -s 172.24.4.224/28 -j MASQUERADE
&lt;/code>&lt;/pre>
&lt;h2 id="direct-network-connection">Direct network connection&lt;/h2>
&lt;p>If you have an external router that will act as a gateway for your
public network, you can add an interface on that network to the
bridge. For example, assuming that &lt;code>eth2&lt;/code> was on the same network as
&lt;code>172.24.4.225&lt;/code>:&lt;/p>
&lt;pre>&lt;code># ovs-vsctl add-port br-ex eth2
&lt;/code>&lt;/pre></content></item><item><title>A random collection of OpenStack Tools</title><link>https://blog.oddbit.com/post/2013-11-12-a-random-collection/</link><pubDate>Tue, 12 Nov 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-11-12-a-random-collection/</guid><description>I&amp;rsquo;ve been working with OpenStack a lot recently, and I&amp;rsquo;ve ended up with a small collection of utilities that make my life easier. On the odd chance that they&amp;rsquo;ll make your life easier, too, I thought I&amp;rsquo;d hilight them here.
Crux Crux is a tool for provisioning tenants, users, and roles in keystone. Instead of a sequence of keystone command, you can provision new tenants, users, and roles with a single comand.</description><content>&lt;p>I&amp;rsquo;ve been working with &lt;a href="http://openstack.org/">OpenStack&lt;/a> a lot recently, and I&amp;rsquo;ve ended up with a small collection of utilities that make my life easier. On the odd chance that they&amp;rsquo;ll make your life easier, too, I thought I&amp;rsquo;d hilight them here.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="crux">Crux&lt;/h2>
&lt;p>&lt;a href="http://github.com/larsks/crux">Crux&lt;/a> is a tool for provisioning tenants, users, and roles in keystone. Instead of a sequence of keystone command, you can provision new tenants, users, and roles with a single comand.&lt;/p>
&lt;p>For example, to create user &lt;code>demo&lt;/code> in the &lt;code>demo&lt;/code> tenant with password secret:&lt;/p>
&lt;pre>&lt;code># crux --user demo:demo::secret
2013-10-21 crux WARNING creating tenant demo
2013-10-21 crux WARNING creating user demo with password secret
&lt;/code>&lt;/pre>
&lt;p>Crux is in general idempotent; if we were to run the same command a second time we woudl see:&lt;/p>
&lt;pre>&lt;code># crux --user demo:demo::secret
2013-10-21 crux WARNING set password for user demo to secret
&lt;/code>&lt;/pre>
&lt;p>Crux can also take input from a configuration file, so you can quickly set up a complex set of tenants and users.&lt;/p>
&lt;h2 id="sqlcli">Sqlcli&lt;/h2>
&lt;p>&lt;a href="http://github.com/larsks/sqlcli">Sqlcli&lt;/a> uses &lt;a href="http://www.sqlalchemy.org/">SQLAlchemy&lt;/a> to run SQL queries against a variety of backends specified by SQL connection URLs. I wrote this to perform queries and simple maintenance against the various databases used by OpenStack. For example, you can get a list of networks from the Neutron database with a command like this:&lt;/p>
&lt;pre>&lt;code># sqlcli -f /etc/neutron/plugin.ini -i DATABASE/sql_connection \
'select name,cidr from subnets'
&lt;/code>&lt;/pre>
&lt;p>And get output like this:&lt;/p>
&lt;pre>&lt;code>public,172.24.4.224/28
net0-subnet0,10.0.0.0/24
&lt;/code>&lt;/pre>
&lt;p>You can add the &lt;code>--pretty&lt;/code> flag and get output like this:&lt;/p>
&lt;pre>&lt;code>+--------------+-----------------+
| name | cidr |
+--------------+-----------------+
| public | 172.24.4.224/28 |
| net0-subnet0 | 10.0.0.0/24 |
+--------------+-----------------+
&lt;/code>&lt;/pre>
&lt;h2 id="openstack-service">openstack-service&lt;/h2>
&lt;p>The &lt;a href="http://github.com/larsks/osctl">openstack-service&lt;/a> command is a convenience tool for managing OpenStack services. It lets you start/stop or query the status of groups of related services. For example, if you want to see the status of all your Cinder services, you can run:&lt;/p>
&lt;pre>&lt;code># openstack-service status cinder
&lt;/code>&lt;/pre>
&lt;p>And get:&lt;/p>
&lt;pre>&lt;code>openstack-cinder-api (pid 11644) is running...
openstack-cinder-scheduler (pid 11790) is running...
openstack-cinder-volume (pid 11712) is running...
&lt;/code>&lt;/pre>
&lt;p>You can stop all Cinder and Glance services like this:&lt;/p>
&lt;pre>&lt;code># openstack-service stop cinder glance
Stopping openstack-cinder-api: [ OK ]
Stopping openstack-cinder-scheduler: [ OK ]
Stopping openstack-cinder-volume: [ OK ]
Stopping openstack-glance-api: [ OK ]
Stopping openstack-glance-registry: [ OK ]
&lt;/code>&lt;/pre>
&lt;p>And start them back up again the same way:&lt;/p>
&lt;pre>&lt;code># openstack-service start cinder glance
Starting openstack-cinder-api: [ OK ]
Starting openstack-cinder-scheduler: [ OK ]
Starting openstack-cinder-volume: [ OK ]
Starting openstack-glance-api: [ OK ]
Starting openstack-glance-registry: [ OK ]
&lt;/code>&lt;/pre>
&lt;p>Without any additional arguments &lt;code>openstack-service stop&lt;/code> will stop all OpenStack services on the current host.&lt;/p></content></item><item><title>Why does the Neutron documentation recommend three interfaces?</title><link>https://blog.oddbit.com/post/2013-10-28-why-does-the-neutron/</link><pubDate>Mon, 28 Oct 2013 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2013-10-28-why-does-the-neutron/</guid><description>The documentation for configuring Neutron recommends that a network controller has three physical interfaces:
Before you start, set up a machine to be a dedicated network node. Dedicated network nodes should have the following NICs: the management NIC (called MGMT_INTERFACE), the data NIC (called DATA_INTERFACE), and the external NIC (called EXTERNAL_INTERFACE).
People occasionally ask, &amp;ldquo;why three interfaces? What if I only have two?&amp;rdquo;, so I wanted to provide an extended answer that might help people understand what the interfaces are for and what trade-offs are involved in using fewer interfaces.</description><content>&lt;p>The &lt;a href="http://docs.openstack.org/havana/install-guide/install/yum/content/neutron-install.dedicated-network-node.html">documentation for configuring Neutron&lt;/a> recommends
that a network controller has three physical interfaces:&lt;/p>
&lt;blockquote>
&lt;p>Before you start, set up a machine to be a dedicated network node.
Dedicated network nodes should have the following NICs: the
management NIC (called MGMT_INTERFACE), the data NIC (called
DATA_INTERFACE), and the external NIC (called EXTERNAL_INTERFACE).&lt;/p>
&lt;/blockquote>
&lt;p>People occasionally ask, &amp;ldquo;why three interfaces? What if I only have
two?&amp;rdquo;, so I wanted to provide an extended answer that might help
people understand what the interfaces are for and what trade-offs are
involved in using fewer interfaces.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>The &lt;code>MGMT_INTERFACE&lt;/code> is used for communication between nodes. This
can include traffic from services to the messaging server (&lt;code>qpid&lt;/code>,
&lt;code>rabbitmq&lt;/code>, etc), traffic between nova and neutron, connections to the
database, and other traffic used to manage your OpenStack environment.&lt;/p>
&lt;p>The &lt;code>DATA_INTERFACE&lt;/code> is used for instance traffic&amp;hellip;that is, traffic
generated by or inbound to instances running in your OpenStack
environment. If you are using GRE or VXLAN tunnels your tunnel
endpoints will be associated with this interface.&lt;/p>
&lt;p>The &lt;code>EXTERNAL_INTERFACE&lt;/code> is used to provide public access to your
instances. The network attached to this interface is generally open
to external traffic, and ip addresses are managed by the floating-ip
functionality in Neutron or Nova.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You want your &lt;code>MGMT_INTERFACE&lt;/code> seperate from your &lt;code>DATA_INTERFACE&lt;/code>
in order to avoid accidentally granting management access to your
OpenStack hosts to your tenants. A typical OpenStack environment
may not use authentication in all cases, and a tenant host with
access to the management network could intentionally or accidentally
cause problems.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You want your &lt;code>EXTERNAL_INTERFACE&lt;/code> separate from your
&lt;code>DATA_INTERFACE&lt;/code> because your network controller &lt;em>must&lt;/em> be acting as
a router between these two interfaces in order for the netfilter
&lt;code>PREROUTING&lt;/code> and &lt;code>POSTROUTING&lt;/code> rules to activate. These rules are
used to map floating ip addresses to internal addresses via &lt;code>SNAT&lt;/code>
and &lt;code>DNAT&lt;/code> rules, which only work packets traverse the &lt;code>FORWARD&lt;/code>
chain.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You want your &lt;code>MGMT_INTERFACE&lt;/code> separate from your
&lt;code>EXTERNAL_INTERFACE&lt;/code> because they have dramatically different access
requirements. Your &lt;code>MGMT_INTERFACE&lt;/code> should typically only be
available to other hosts in your OpenStack deployment, while your
&lt;code>EXTERNAL_INTERFACE&lt;/code> will generally require much broader access.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If you are deploying a proof-of-concept (POC) deployment to which you
are not actually providing public access, you can elect to not have an
&lt;code>EXTERNAL_INTERFACE&lt;/code>. Rather than adding this device to &lt;code>br-ex&lt;/code>, you
will set up outbound NAT rules so that &amp;ldquo;external&amp;rdquo; traffic from your
instances will masquerade using the primary ip address of your network
controller.&lt;/p></content></item><item><title>Automatic configuration of Windows instances in OpenStack, part 1</title><link>https://blog.oddbit.com/post/2012-11-04-openstack-windows-config-part1/</link><pubDate>Sun, 04 Nov 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-11-04-openstack-windows-config-part1/</guid><description>This is the first of two articles in which I discuss my work in getting some Windows instances up and running in our OpenStack environment. This article is primarily about problems I encountered along the way.
Motivations Like many organizations, we have a mix of Linux and Windows in our environment. Some folks in my group felt that it would be nice to let our Windows admins take advantage of OpenStack for prototyping and sandboxing in the same ways our Linux admins can use it.</description><content>&lt;p>This is the first of two articles in which I discuss my work in
getting some Windows instances up and running in our &lt;a href="http://www.openstack.org/">OpenStack&lt;/a>
environment. This article is primarily about problems I encountered
along the way.&lt;/p>
&lt;h2 id="motivations">Motivations&lt;/h2>
&lt;p>Like many organizations, we have a mix of Linux and Windows in our
environment. Some folks in my group felt that it would be nice to let
our Windows admins take advantage of OpenStack for prototyping and
sandboxing in the same ways our Linux admins can use it.&lt;/p>
&lt;p>While it is trivial to get Linux instances running in
OpenStack (there are downloadable images from several distributions that
will magically configure themselves on first boot), getting Windows
systems set up is a little trickier. There are no pre-configured
images to download, and it looks as if there aren&amp;rsquo;t that many people
trying to run Windows under OpenStack right now so there is a lot less
common experience to reference.&lt;/p>
&lt;h2 id="like-the-cool-kids-do-it">Like the cool kids do it&lt;/h2>
&lt;p>My first approach to this situation was to set up our Windows
instances to act just like our Linux instances:&lt;/p>
&lt;ul>
&lt;li>Install &lt;a href="http://cygwin.com/">Cygwin&lt;/a>.&lt;/li>
&lt;li>Run an SSH server.&lt;/li>
&lt;li>Have the system pull down an SSH public key on first boot and use
this for administrative access.&lt;/li>
&lt;/ul>
&lt;p>This worked reasonably well, but many people felt that this wasn&amp;rsquo;t a
great solution because it wouldn&amp;rsquo;t feel natural to a typical Windows
administrator. It also required a full Cygwin install to drive
things, which isn&amp;rsquo;t terrible but still feels like a pretty big hammer.&lt;/p>
&lt;p>As an alternative, we decided we needed some way to either (a) allow
the user to pass a password into the instance environment, or (b)
provide some way for the instance to communicate a generated password
back to the user.&lt;/p>
&lt;h2 id="how-about-user-data">How about user-data?&lt;/h2>
&lt;p>One of my colleagues suggested that we could allow people to pass an
administrative password into the environment via the &lt;code>user-data&lt;/code>
attribute available from the &lt;a href="http://docs.openstack.org/trunk/openstack-compute/admin/content/metadata-service.html">metadata service&lt;/a>. While this sounds
like a reasonable idea at first, it has one major flaw: data from the
metadata service is available to anyone on the system who is able to
retrieve a URL. This would make it trivial for anyone on the instance
to retrieve the administrator password.&lt;/p>
&lt;h2 id="how-about-adminpass">How about adminPass?&lt;/h2>
&lt;p>When you boot an instance using the nova command line tools&amp;hellip;&lt;/p>
&lt;pre>&lt;code>nova boot ...
&lt;/code>&lt;/pre>
&lt;p>You get back a chunk of metadata, including an &lt;code>adminPass&lt;/code> key, which
is a password randomly generated by OpenStack and availble during the
instance provisioning process:&lt;/p>
&lt;pre>&lt;code>+------------------------+--------------------------------------+
| Property | Value |
+------------------------+--------------------------------------+
...
| adminPass | RBiWrSNYqK5R |
...
+------------------------+--------------------------------------+
&lt;/code>&lt;/pre>
&lt;p>This would be an ideal solution, if only I were able to figure out how
OpenStack made this value available to the instance. After asking
around on &lt;a href="http://wiki.openstack.org/UsingIRC">#openstack&lt;/a> it turns
out that not many people were even aware this feature exists, so
information was hard to come by. I ran across some &lt;a href="http://docs.openstack.org/trunk/openstack-compute/admin/content/hypervisor-configuration-basics.html">documentation&lt;/a>
that mentioned the &lt;code>libvirt_inject_password&lt;/code> option in &lt;code>nova.conf&lt;/code>
with the following description:&lt;/p>
&lt;blockquote>
&lt;p>(BoolOpt) Inject the admin password at boot time, without an agent.&lt;/p>
&lt;/blockquote>
&lt;p>&amp;hellip;but that still didn&amp;rsquo;t actually explain how it worked, so I went
diving through the code. The &lt;code>libvirt_inject_password&lt;/code> option appears
in only a single file, &lt;code>nova/virt/libvirt/connection.py&lt;/code>, so I knew
where to start. This led me to the &lt;code>_create_image&lt;/code> method, which
grabs the &lt;code>admin_pass&lt;/code> generated by OpenStack:&lt;/p>
&lt;pre>&lt;code>if FLAGS.libvirt_inject_password:
admin_pass = instance.get('admin_pass')
else:
admin_pass = None
&lt;/code>&lt;/pre>
&lt;p>And then passes it to the &lt;code>inject_data&lt;/code> method:&lt;/p>
&lt;pre>&lt;code>disk.inject_data(injection_path,
key, net, metadata, admin_pass, files,
partition=target_partition,
use_cow=FLAGS.use_cow_images,
config_drive=config_drive)
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>inject_data&lt;/code> method comes from &lt;code>nova/virt/disk/api.py&lt;/code>, which is
where things get interesting: it turns out that the injection
mechanism works by:&lt;/p>
&lt;ul>
&lt;li>Mounting the root filesystem,&lt;/li>
&lt;li>Copying out &lt;code>/etc/passwd&lt;/code> and &lt;code>/etc/shadow&lt;/code>,&lt;/li>
&lt;li>Modifying them, and&lt;/li>
&lt;li>Copying them back.&lt;/li>
&lt;/ul>
&lt;p>Like this:&lt;/p>
&lt;pre>&lt;code>passwd_path = _join_and_check_path_within_fs(fs, 'etc', 'passwd')
shadow_path = _join_and_check_path_within_fs(fs, 'etc', 'shadow')
utils.execute('cp', passwd_path, tmp_passwd, run_as_root=True)
utils.execute('cp', shadow_path, tmp_shadow, run_as_root=True)
_set_passwd(admin_user, admin_passwd, tmp_passwd, tmp_shadow)
utils.execute('cp', tmp_passwd, passwd_path, run_as_root=True)
os.unlink(tmp_passwd)
utils.execute('cp', tmp_shadow, shadow_path, run_as_root=True)
os.unlink(tmp_shadow)
&lt;/code>&lt;/pre>
&lt;p>Do you see a problem here, given that I&amp;rsquo;m working with a Windows
instance? First, it&amp;rsquo;s possible that the host will be unable to mount
the NTFS filesystem, and secondly, there are no &lt;code>passwd&lt;/code> or &lt;code>shadow&lt;/code>
files of any use on the target.&lt;/p>
&lt;p>You can pass &lt;code>--config-drive=True&lt;/code> to &lt;code>nova boot&lt;/code> and it will use a
configuration drive (a whole-disk FAT filesystem) for configuration
data (and make this available as a block device when the system
boots), but this fails, hard: most of the code treats this as being
identical to the original root filesystem, so it still tries to
perform the modifications to &lt;code>/etc/passwd&lt;/code> and &lt;code>/etc/shadow&lt;/code> which, of
course, don&amp;rsquo;t exist.&lt;/p>
&lt;p>I whipped some quick
&lt;a href="https://github.com/seas-computing/nova/commits/lars/admin_pass">patches&lt;/a>
that would write the configuration data (such as &lt;code>admin_pass&lt;/code>) to
simple files at the root of the configuration drive&amp;hellip;but then I ran
into a new problem:&lt;/p>
&lt;p>Windows doesn&amp;rsquo;t know how to deal with whole-disk filesystems (nor,
apparently, do many &lt;a href="http://serverfault.com/questions/444446/mounting-whole-disk-filesystems-in-windows-2008/444448#comment481758_444448">windows
admins&lt;/a>).
In the absence of a partition map, Windows assumes that the device is
empty.&lt;/p>
&lt;p>Oops. At this point it was obvious I was treading on ground best left
undisturbed.&lt;/p></content></item><item><title>Chasing OpenStack idle connection timeouts</title><link>https://blog.oddbit.com/post/2012-07-30-openstack-idle-connection-time/</link><pubDate>Mon, 30 Jul 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-07-30-openstack-idle-connection-time/</guid><description>The original problem I&amp;rsquo;ve recently spent some time working on an OpenStack deployment. I ran into a problem in which the compute service would frequently stop communicating with the AMQP message broker (qpidd).
In order to gather some data on the problem, I ran the following simple test:
Wait n minutes Run nova boot ... to create an instance Wait a minute and see if the new instance becomes ACTIVE If it works, delete the instance, set n = 2n and repeat This demonstrated that communication was failing after about an hour, which correlates rather nicely with the idle connection timeout on the firewall.</description><content>&lt;h2 id="the-original-problem">The original problem&lt;/h2>
&lt;p>I&amp;rsquo;ve recently spent some time working on an OpenStack deployment. I ran into a
problem in which the &lt;a href="http://docs.openstack.org/trunk/openstack-compute/starter/content/Compute_Worker_nova-compute_-d1e232.html">compute service&lt;/a> would frequently stop communicating
with the &lt;a href="http://www.amqp.org/">AMQP&lt;/a> message broker (&lt;code>qpidd&lt;/code>).&lt;/p>
&lt;p>In order to gather some data on the problem, I ran the following simple test:&lt;/p>
&lt;ul>
&lt;li>Wait &lt;code>n&lt;/code> minutes&lt;/li>
&lt;li>Run &lt;code>nova boot ...&lt;/code> to create an instance&lt;/li>
&lt;li>Wait a minute and see if the new instance becomes &lt;code>ACTIVE&lt;/code>&lt;/li>
&lt;li>If it works, delete the instance, set &lt;code>n&lt;/code> = &lt;code>2n&lt;/code> and repeat&lt;/li>
&lt;/ul>
&lt;p>This demonstrated that communication was failing after about an hour, which
correlates rather nicely with the idle connection timeout on the firewall.&lt;/p>
&lt;p>I wanted to continue working with our OpenStack environment while testing
different solutions to this problem, so I put an additional interface on the
controller (the system running the AMQ message broker, &lt;code>qpidd&lt;/code>, as well as
&lt;code>nova-api&lt;/code>, &lt;code>nova-scheduler&lt;/code>, etc) that was on the same network as our
&lt;code>nova-compute&lt;/code> hosts. This would allow the compute service to communicate with
the message broker without traversing the firewall infrastructure.&lt;/p>
&lt;p>As a workaround it worked fine, but it introduced a &lt;em>new&lt;/em> problem that sent us
down a bit of a rabbit hole.&lt;/p>
&lt;h2 id="the-new-problem">The new problem&lt;/h2>
&lt;p>With the compute hosts talking happily to the controller, I started looking at
the connection timeout settings in the firewall. As a first step I cranked the
default connection timeout up to two hours and repeated our earlier test&amp;hellip;only
to find that connections were now failing in a matter of minutes!&lt;/p>
&lt;p>So, what happened?&lt;/p>
&lt;p>By adding an interface on a shared network, I created an asymmetric route
between the two hosts &amp;ndash; that is, the network path taking by packets from the
compute host to the controller was different from the network path taken by
packets in the other direction.&lt;/p>
&lt;p>In the most common configuration, Linux (and other operating systems) only have
a single routing decision to make:&lt;/p>
&lt;ul>
&lt;li>Am I communicating with a host on a directly attached network?&lt;/li>
&lt;/ul>
&lt;p>If the answer is &amp;ldquo;yes&amp;rdquo;, a packet will be routed directly to the destination
host, otherwise it will be routed via the default gateway (and transit the
campus routing/firewall infrastructure).&lt;/p>
&lt;p>On the compute host, with its single interface, the decision is simple. Since
the canonical address of the controller is not on the same network, packets
will be routed via the default gateway. On the controller, the situation is
different. While the packet came in on the canonical interface, the kernel will
realize that the request comes from a host on a network to which there is a
more specific route than the default gateway: the new network interface on the
same network as the compute host. This means that reply packets will be routed
directly.&lt;/p>
&lt;p>Asymmetric routing is not, by itself, a problem. However, throw in a stateful
firewall and you now have a recipe for dropped connections. The firewall
appliances in use at my office maintain a table of established TCP connections.
This is used to reduce the processing necessary for packets associated with
established connections. From the &lt;a href="http://www.cisco.com/en/US/docs/security/asdm/6_2/user/guide/protect.html#wp1291963">Cisco documentation&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>By default, all traffic that goes through the security appliance is inspected
using the Adaptive Security Algorithm and is either allowed through or
dropped based on the security policy. The security appliance maximizes the
firewall performance by checking the state of each packet (is this a new
connection or an established connection?) and assigning it to either the
session management path (a new connection SYN packet), the fast path (an
established connection), or the control plane path (advanced inspection).&lt;/p>
&lt;/blockquote>
&lt;p>In order for two systems to successfully established a TCP connection, they
must complete a &lt;a href="http://en.wikipedia.org/wiki/Transmission_Control_Protocol#Connection_establishment">three-way handshake&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>The initiating systems sends a &lt;code>SYN&lt;/code> packet.&lt;/li>
&lt;li>The receiving system sends &lt;code>SYN-ACK&lt;/code> packet.&lt;/li>
&lt;li>The initiating system sends an &lt;code>ACK&lt;/code> packet.&lt;/li>
&lt;/ul>
&lt;p>The routing structure introduced by our interface change meant that while the
initial &lt;code>SYN&lt;/code> packet was traversing the firewall, the subsequent &lt;code>SYN-ACK&lt;/code>
packet was being routed directly, which means that from the point of view of
the firewall the connection was never successfully established&amp;hellip;and after 20
seconds (the default &amp;ldquo;embryonic connection timeout&amp;rdquo;) the connection gets
dropped by the firewall. The diagram below illustrates exactly what was
happening:&lt;/p>
&lt;p>&lt;img src="asymmetric-routing.png" alt="assymetric routing">&lt;/p>
&lt;p>There are various ways of correcting this situation:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You could use the advanced &lt;a href="http://www.policyrouting.org/PolicyRoutingBook/ONLINE/TOC.html">policy routing&lt;/a> features available in Linux
to set up a routing policy that would route replies out the same interface
on which a packet was received, thus returning to a more typical symmetric
routing model.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You could use the &lt;a href="http://www.cisco.com/en/US/docs/security/asdm/6_2/user/guide/protect.html#wp1291963">tcp state bypass&lt;/a> feature available in the Cisco
firewall appliance to exempt AMQ packets from the normal TCP state
processing.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;m not going to look at either of these solution in detail, since this whole
issue was secondary to the initial idle connection timeout problem, which has a
different set of solutions.&lt;/p>
&lt;h2 id="dealing-with-connection-timeouts">Dealing with connection timeouts&lt;/h2>
&lt;p>Returning to the original problem, what am I to do about the idle connection
timeouts?&lt;/p>
&lt;h3 id="disable-idle-connection-timeouts-on-the-firewall">Disable idle connection timeouts on the firewall&lt;/h3>
&lt;p>Once can disable idle connection timeouts on the firewall, either globally &amp;ndash;
which would be a bad idea &amp;ndash; or for certain traffic classes. For example, &amp;ldquo;all
traffic to or from TCP port 5672&amp;rdquo;. This can be done by adding a rule to the
default global policy:&lt;/p>
&lt;pre>&lt;code>class-map amq
description Disable connection timeouts for AMQ connections (for OpenStack)
match port tcp eq 5672
policy-map global_policy
class amq
set connection random-sequence-number disable
set connection timeout embryonic 0:00:20 half-closed 0:00:20 tcp 0:00:00
&lt;/code>&lt;/pre>
&lt;p>While this works fine, it makes successful deployment of OpenStack dependent on
a specific firewall configuration.&lt;/p>
&lt;h3 id="enable-linux-kernel-keepalive-support-for-tcp-connections">Enable Linux kernel keepalive support for TCP connections&lt;/h3>
&lt;p>The Linux kernel supports a &lt;a href="http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/">keepalive&lt;/a> feature intended to deal with this
exact situation. After a connection has been idle for a certain amount of time
(&lt;code>net.ipv4.tcp_keepalive_time&lt;/code>), the kernel will send zero-length packets every
&lt;code>net.ipv4.tcp_keepalive_intvl&lt;/code> seconds in order to keep the connection active.
The kernel defaults to an initial interval of 7200 seconds (aka two hours),
which is longer than the default idle connection timeout on our Cisco
firewalls, but this value is easily tuneable via the
&lt;code>net.ipv4.tcp_keepalive_time&lt;/code> sysctl value.&lt;/p>
&lt;p>This sounds like a great solution, until you pay close attention to the &lt;code>tcp&lt;/code>
man page (or the &lt;code>HOWTO&lt;/code> document):&lt;/p>
&lt;blockquote>
&lt;p>Keep-alives are only sent when the &lt;code>SO_KEEPALIVE&lt;/code> socket option is enabled.&lt;/p>
&lt;/blockquote>
&lt;p>If your application doesn&amp;rsquo;t already set &lt;code>SO_KEEPALIVE&lt;/code> (or give you an option
for doing do), you&amp;rsquo;re mostly out of luck. While it would certainly be possible
to modify either the OpenStack source or the QPID source to set the appropriate
option on AMQ sockets, I don&amp;rsquo;t really want to put myself in the position of
having to maintain this sort of one-off patch.&lt;/p>
&lt;p>But all is not lost! It is possible to override functions in dynamic
executables using a mechanism called &lt;a href="http://www.jayconrod.com/cgi/view_post.py?23">function interposition&lt;/a>. Create a
library that implements the function you want to override, and then preload it
when running an application via the &lt;code>LD_PRELOAD&lt;/code> environment variable (or
&lt;code>/etc/ld.so.preload&lt;/code>, if you want it to affect everything).&lt;/p>
&lt;p>It can be tricky to correctly implement function interposition, so I&amp;rsquo;m
fortunate that the &lt;a href="http://libkeepalive.sourceforge.net">libkeepalive&lt;/a> project has already taken care of this. By
installing &lt;code>libkeepalive&lt;/code> and adding &lt;code>libkeepalive.so&lt;/code> to &lt;code>/etc/ld.so.preload&lt;/code>,
it is possible to have the &lt;code>SO_KEEPALIVE&lt;/code> option set by default on all sockets.
&lt;code>libkeepalive&lt;/code> implements a wrapper to the &lt;code>socket&lt;/code> system call that calls
&lt;code>setsockopt&lt;/code> with the &lt;code>SO_KEEPALIVE&lt;/code> option for all TCP sockets.&lt;/p>
&lt;p>Here&amp;rsquo;s what setting up a listening socket with [netcat][] looks like before
installing &lt;code>libkeepalive&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ strace -e trace=setsockopt nc -l 8080
setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
&lt;/code>&lt;/pre>
&lt;p>And here&amp;rsquo;s what things look like after adding &lt;code>libkeepalive.so&lt;/code> to
&lt;code>/etc/ld.so.preload&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ strace -e trace=setsockopt nc -l 8080
setsockopt(3, SOL_SOCKET, SO_KEEPALIVE, [1], 4) = 0
setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
&lt;/code>&lt;/pre>
&lt;h3 id="enable-application-level-keepalive">Enable application level keepalive&lt;/h3>
&lt;p>Many applications implement their own keepalive mechanism. For example,
&lt;a href="http://openssh.org/">OpenSSH&lt;/a> provides the &lt;a href="http://dan.hersam.com/2007/03/05/how-to-avoid-ssh-timeouts/">ClientAliveInterval&lt;/a> configuration setting to
control how often keepalive packets are sent by the server to a connected
client. When this option is available it&amp;rsquo;s probably the best choice, since it
has been designed with the particular application in mind.&lt;/p>
&lt;p>OpenStack in theory provides the &lt;a href="http://docs.openstack.org/essex/openstack-compute/admin/content/configuration-qpid.html">qpid_heartbeat&lt;/a> setting, which is meant
to provide a heartbeat for AMQ connections to the &lt;code>qpidd&lt;/code> process. According to
the documentation, the default behavior of QPID clients in the OpenStack
framework is to send heartbeat packets every five seconds.&lt;/p>
&lt;p>When first testing this option it was obvious that things weren&amp;rsquo;t working as
documented. Querying the connection table on the firewall would often should
connections with more than five seconds of idle time:&lt;/p>
&lt;pre>&lt;code>% show conn lport 5672
[...]
TCP ...:630 10.243.16.151:39881 ...:621 openstack-dev-2:5672 idle 0:34:02 Bytes 5218662 FLAGS - UBOI
[...]
&lt;/code>&lt;/pre>
&lt;p>And of course if the &lt;code>qpid_heartbeat&lt;/code> option had been working correctly I would
not have experienced the idle connection timeout issue in the first place.&lt;/p>
&lt;p>A &lt;a href="https://lists.launchpad.net/openstack/msg15191.html">post to the OpenStack mailing list&lt;/a> led to the source of the problem: a
typo in the &lt;code>impl_qpid&lt;/code> Python module:&lt;/p>
&lt;pre>&lt;code>diff --git a/nova/rpc/impl_qpid.py b/nova/rpc/impl_qpid.py
index 289f21b..e19079e 100644
--- a/nova/rpc/impl_qpid.py
+++ b/nova/rpc/impl_qpid.py
@@ -317,7 +317,7 @@ class Connection(object):
FLAGS.qpid_reconnect_interval_min)
if FLAGS.qpid_reconnect_interval:
self.connection.reconnect_interval = FLAGS.qpid_reconnect_interval
- self.connection.hearbeat = FLAGS.qpid_heartbeat
+ self.connection.heartbeat = FLAGS.qpid_heartbeat
self.connection.protocol = FLAGS.qpid_protocol
self.connection.tcp_nodelay = FLAGS.qpid_tcp_nodelay
&lt;/code>&lt;/pre>
&lt;p>If it&amp;rsquo;s not obvious, &lt;code>heartbeat&lt;/code> was mispelled &lt;code>hearbeat&lt;/code> in the above block of
code. Putting this change into production has completely resolved the idle
connection timeout problem.&lt;/p></content></item></channel></rss>