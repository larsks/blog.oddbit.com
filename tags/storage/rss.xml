<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>storage on blog.oddbit.com</title><link>https://blog.oddbit.com/tags/storage/</link><description>Recent content in storage on blog.oddbit.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Lars Kellogg-Stedman</copyright><lastBuildDate>Mon, 23 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.oddbit.com/tags/storage/rss.xml" rel="self" type="application/rss+xml"/><item><title>Connecting OpenShift to an External Ceph Cluster</title><link>https://blog.oddbit.com/post/2021-08-23-external-ocs/</link><pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2021-08-23-external-ocs/</guid><description>Red Hat&amp;rsquo;s OpenShift Data Foundation (formerly &amp;ldquo;OpenShift Container Storage&amp;rdquo;, or &amp;ldquo;OCS&amp;rdquo;) allows you to either (a) automatically set up a Ceph cluster as an application running on your OpenShift cluster, or (b) connect your OpenShift cluster to an externally managed Ceph cluster. While setting up Ceph as an OpenShift application is a relatively polished experienced, connecting to an external cluster still has some rough edges.
NB I am not a Ceph expert.</description><content>&lt;p>Red Hat&amp;rsquo;s &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation">OpenShift Data Foundation&lt;/a> (formerly &amp;ldquo;OpenShift
Container Storage&amp;rdquo;, or &amp;ldquo;OCS&amp;rdquo;) allows you to either (a) automatically
set up a Ceph cluster as an application running on your OpenShift
cluster, or (b) connect your OpenShift cluster to an externally
managed Ceph cluster. While setting up Ceph as an OpenShift
application is a relatively polished experienced, connecting to an
external cluster still has some rough edges.&lt;/p>
&lt;p>&lt;strong>NB&lt;/strong> I am not a Ceph expert. If you read this and think I&amp;rsquo;ve made a
mistake with respect to permissions or anything else, please feel free
to leave a comment and I will update the article as necessary. In
particular, I think it may be possible to further restrict the &lt;code>mgr&lt;/code>
permissions shown in this article and I&amp;rsquo;m interested in feedback on
that topic.&lt;/p>
&lt;h2 id="installing-ocs">Installing OCS&lt;/h2>
&lt;p>Regardless of which option you choose, you start by installing the
&amp;ldquo;OpenShift Container Storage&amp;rdquo; operator (the name change apparently
hasn&amp;rsquo;t made it to the Operator Hub yet). When you select &amp;ldquo;external
mode&amp;rdquo;, you will be given the opportunity to download a Python script
that you are expected to run on your Ceph cluster. This script will
create some Ceph authentication principals and will emit a block of
JSON data that gets pasted into the OpenShift UI to configure the
external StorageCluster resource.&lt;/p>
&lt;p>The script has a single required option, &lt;code>--rbd-data-pool-name&lt;/code>, that
you use to provide the name of an existing pool. If you run the script
with only that option, it will create the following ceph principals
and associated capabilities:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>client.csi-rbd-provisioner&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mgr = &amp;#34;allow rw&amp;#34;
caps mon = &amp;#34;profile rbd&amp;#34;
caps osd = &amp;#34;profile rbd&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>&lt;code>client.csi-rbd-node&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mon = &amp;#34;profile rbd&amp;#34;
caps osd = &amp;#34;profile rbd&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>&lt;code>client.healthchecker&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mgr = &amp;#34;allow command config&amp;#34;
caps mon = &amp;#34;allow r, allow command quorum_status, allow command version&amp;#34;
caps osd = &amp;#34;allow rwx pool=default.rgw.meta, allow r pool=.rgw.root, allow rw pool=default.rgw.control, allow rx pool=default.rgw.log, allow x pool=default.rgw.buckets.index&amp;#34;
&lt;/code>&lt;/pre>&lt;p>This account is used to verify the health of the ceph cluster.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If you also provide the &lt;code>--cephfs-filesystem-name&lt;/code> option, the script
will also create:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>client.csi-cephfs-provisioner&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mgr = &amp;#34;allow rw&amp;#34;
caps mon = &amp;#34;allow r&amp;#34;
caps osd = &amp;#34;allow rw tag cephfs metadata=*&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>&lt;code>client.csi-cephfs-node&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>caps mds = &amp;#34;allow rw&amp;#34;
caps mgr = &amp;#34;allow rw&amp;#34;
caps mon = &amp;#34;allow r&amp;#34;
caps osd = &amp;#34;allow rw tag cephfs *=*&amp;#34;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>If you specify &lt;code>--rgw-endpoint&lt;/code>, the script will create a RGW user
named &lt;code>rgw-admin-ops-user&lt;/code>with administrative access to the default
RGW pool.&lt;/p>
&lt;h2 id="so-whats-the-problem">So what&amp;rsquo;s the problem?&lt;/h2>
&lt;p>The above principals and permissions are fine if you&amp;rsquo;ve created an
external Ceph cluster explicitly for the purpose of supporting a
single OpenShift cluster.&lt;/p>
&lt;p>In an environment where a single Ceph cluster is providing storage to
multiple OpenShift clusters, and &lt;em>especially&lt;/em> in an environment where
administration of the Ceph and OpenShift environments are managed by
different groups, the process, principals, and permissions create a
number of problems.&lt;/p>
&lt;p>The first and foremost is that the script provided by OCS both (a)
gathers information about the Ceph environment, and (b) &lt;em>makes changes
to that environment&lt;/em>. If you are installing OCS on OpenShift and want
to connect to a Ceph cluster over which you do not have administrative
control, you may find yourself stymied when the storage administrators
refuse to run your random Python script on the Ceph cluster.&lt;/p>
&lt;p>Ideally, the script would be read-only, and instead of &lt;em>making&lt;/em>
changes to the Ceph cluster it would only &lt;em>validate&lt;/em> the cluster
configuration, and inform the administrator of what changes were
necessary. There should be complete documentation that describes the
necessary configuration scripts so that a Ceph cluster can be
configured correctly without running &lt;em>any&lt;/em> script, and OCS should
provide something more granular than &amp;ldquo;drop a blob of JSON here&amp;rdquo; for
providing the necessary configuration to OpenShift.&lt;/p>
&lt;p>The second major problem is that while the script creates several
principals, it only allows you to set the name of one of them. The
script has a &lt;code>--run-as-user&lt;/code> option, which at first sounds promising,
but ultimately is of questionable use: it only allows you set the Ceph
principal used for cluster health checks.&lt;/p>
&lt;p>There is no provision in the script to create separate principals for
each OpenShift cluster.&lt;/p>
&lt;p>Lastly, the permissions granted to the principals are too broad. For
example, the &lt;code>csi-rbd-node&lt;/code> principal has access to &lt;em>all&lt;/em> RBD pools on
the cluster.&lt;/p>
&lt;h2 id="how-can-we-work-around-it">How can we work around it?&lt;/h2>
&lt;p>If you would like to deploy OCS in an environment where the default
behavior of the configuration script is inappropriate you can work
around this problem by:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Manually generating the necessary principals (with more appropriate
permissions), and&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Manually generating the JSON data for input into OCS&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="create-the-storage">Create the storage&lt;/h3>
&lt;p>I&amp;rsquo;ve adopted the following conventions for naming storage pools and
filesystems:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>All resources are prefixed with the name of the cluster (represented
here by &lt;code>${clustername}&lt;/code>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The RBD pool is named &lt;code>${clustername}-rbd&lt;/code>. I create it like this:&lt;/p>
&lt;pre tabindex="0">&lt;code> ceph osd pool create ${clustername}-rbd
ceph osd pool application enable ${clustername}-rbd rbd
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>The CephFS filesystem (if required) is named
&lt;code>${clustername}-fs&lt;/code>, and I create it like this:&lt;/p>
&lt;pre tabindex="0">&lt;code> ceph fs volume create ${clustername}-fs
&lt;/code>&lt;/pre>&lt;p>In addition to the filesystem, this creates two pools:&lt;/p>
&lt;ul>
&lt;li>&lt;code>cephfs.${clustername}-fs.meta&lt;/code>&lt;/li>
&lt;li>&lt;code>cephfs.${clustername}-fs.data&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="creating-the-principals">Creating the principals&lt;/h3>
&lt;p>Assuming that you have followed the same conventions and have an RBD
pool named &lt;code>${clustername}-rbd&lt;/code> and a CephFS filesystem named
&lt;code>${clustername}-fs&lt;/code>, the following set of &lt;code>ceph auth add&lt;/code> commands
should create an appropriate set of principals (with access limited to
just those resources that belong to the named cluster):&lt;/p>
&lt;pre tabindex="0">&lt;code>ceph auth add client.healthchecker-${clustername} \
mgr &amp;#34;allow command config&amp;#34; \
mon &amp;#34;allow r, allow command quorum_status, allow command version&amp;#34;
ceph auth add client.csi-rbd-provisioner-${clustername} \
mgr &amp;#34;allow rw&amp;#34; \
mon &amp;#34;profile rbd&amp;#34; \
osd &amp;#34;profile rbd pool=${clustername}-rbd&amp;#34;
ceph auth add client.csi-rbd-node-${clustername} \
mon &amp;#34;profile rbd&amp;#34; \
osd &amp;#34;profile rbd pool=${clustername}-rbd&amp;#34;
ceph auth add client.csi-cephfs-provisioner-${clustername} \
mgr &amp;#34;allow rw&amp;#34; \
mds &amp;#34;allow rw fsname=${clustername}-fs&amp;#34; \
mon &amp;#34;allow r fsname=${clustername}-fs&amp;#34; \
osd &amp;#34;allow rw tag cephfs metadata=${clustername}-fs&amp;#34;
ceph auth add client.csi-cephfs-node-${clustername} \
mgr &amp;#34;allow rw&amp;#34; \
mds &amp;#34;allow rw fsname=${clustername}-fs&amp;#34; \
mon &amp;#34;allow r fsname=${clustername}-fs&amp;#34; \
osd &amp;#34;allow rw tag cephfs data=${clustername}-fs&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Note that I&amp;rsquo;ve excluded the RGW permissions here; in our OpenShift
environments, we typically rely on the object storage interface
provided by &lt;a href="https://www.noobaa.io/">Noobaa&lt;/a> so I haven&amp;rsquo;t spent time investigating
permissions on the RGW side.&lt;/p>
&lt;h3 id="create-the-json">Create the JSON&lt;/h3>
&lt;p>The final step is to create the JSON blob that you paste into the OCS
installation UI. I use the following script which calls &lt;code>ceph -s&lt;/code>,
&lt;code>ceph mon dump&lt;/code>, and &lt;code>ceph auth get-key&lt;/code> to get the necessary
information from the cluster:&lt;/p>
&lt;pre tabindex="0">&lt;code>#!/usr/bin/python3
import argparse
import json
import subprocess
from urllib.parse import urlparse
usernames = [
&amp;#39;healthchecker&amp;#39;,
&amp;#39;csi-rbd-node&amp;#39;,
&amp;#39;csi-rbd-provisioner&amp;#39;,
&amp;#39;csi-cephfs-node&amp;#39;,
&amp;#39;csi-cephfs-provisioner&amp;#39;,
]
def parse_args():
p = argparse.ArgumentParser()
p.add_argument(&amp;#39;--use-cephfs&amp;#39;, action=&amp;#39;store_true&amp;#39;, dest=&amp;#39;use_cephfs&amp;#39;)
p.add_argument(&amp;#39;--no-use-cephfs&amp;#39;, action=&amp;#39;store_false&amp;#39;, dest=&amp;#39;use_cephfs&amp;#39;)
p.add_argument(&amp;#39;instance_name&amp;#39;)
p.set_defaults(use_rbd=True, use_cephfs=True)
return p.parse_args()
def main():
args = parse_args()
cluster_status = json.loads(subprocess.check_output([&amp;#39;ceph&amp;#39;, &amp;#39;-s&amp;#39;, &amp;#39;-f&amp;#39;, &amp;#39;json&amp;#39;]))
mon_status = json.loads(subprocess.check_output([&amp;#39;ceph&amp;#39;, &amp;#39;mon&amp;#39;, &amp;#39;dump&amp;#39;, &amp;#39;-f&amp;#39;, &amp;#39;json&amp;#39;]))
users = {}
for username in usernames:
key = subprocess.check_output([&amp;#39;ceph&amp;#39;, &amp;#39;auth&amp;#39;, &amp;#39;get-key&amp;#39;, &amp;#39;client.{}-{}&amp;#39;.format(username, args.instance_name)])
users[username] = {
&amp;#39;name&amp;#39;: &amp;#39;client.{}-{}&amp;#39;.format(username, args.instance_name),
&amp;#39;key&amp;#39;: key.decode(),
}
mon_name = mon_status[&amp;#39;mons&amp;#39;][0][&amp;#39;name&amp;#39;]
mon_ip = [
addr for addr in
mon_status[&amp;#39;mons&amp;#39;][0][&amp;#39;public_addrs&amp;#39;][&amp;#39;addrvec&amp;#39;]
if addr[&amp;#39;type&amp;#39;] == &amp;#39;v1&amp;#39;
][0][&amp;#39;addr&amp;#39;]
prom_url = urlparse(cluster_status[&amp;#39;mgrmap&amp;#39;][&amp;#39;services&amp;#39;][&amp;#39;prometheus&amp;#39;])
prom_ip, prom_port = prom_url.netloc.split(&amp;#39;:&amp;#39;)
output = [
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-mon-endpoints&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;ConfigMap&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;data&amp;#34;: &amp;#34;{}={}&amp;#34;.format(mon_name, mon_ip),
&amp;#34;maxMonId&amp;#34;: &amp;#34;0&amp;#34;,
&amp;#34;mapping&amp;#34;: &amp;#34;{}&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-mon&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;admin-secret&amp;#34;: &amp;#34;admin-secret&amp;#34;,
&amp;#34;fsid&amp;#34;: cluster_status[&amp;#39;fsid&amp;#39;],
&amp;#34;mon-secret&amp;#34;: &amp;#34;mon-secret&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-operator-creds&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;userID&amp;#34;: users[&amp;#39;healthchecker&amp;#39;][&amp;#39;name&amp;#39;],
&amp;#34;userKey&amp;#34;: users[&amp;#39;healthchecker&amp;#39;][&amp;#39;key&amp;#39;],
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;ceph-rbd&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;StorageClass&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;pool&amp;#34;: &amp;#34;{}-rbd&amp;#34;.format(args.instance_name),
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;monitoring-endpoint&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;CephCluster&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;MonitoringEndpoint&amp;#34;: prom_ip,
&amp;#34;MonitoringPort&amp;#34;: prom_port,
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-rbd-node&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;userID&amp;#34;: users[&amp;#39;csi-rbd-node&amp;#39;][&amp;#39;name&amp;#39;].replace(&amp;#39;client.&amp;#39;, &amp;#39;&amp;#39;),
&amp;#34;userKey&amp;#34;: users[&amp;#39;csi-rbd-node&amp;#39;][&amp;#39;key&amp;#39;],
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-rbd-provisioner&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;userID&amp;#34;: users[&amp;#39;csi-rbd-provisioner&amp;#39;][&amp;#39;name&amp;#39;].replace(&amp;#39;client.&amp;#39;, &amp;#39;&amp;#39;),
&amp;#34;userKey&amp;#34;: users[&amp;#39;csi-rbd-provisioner&amp;#39;][&amp;#39;key&amp;#39;],
}
}
]
if args.use_cephfs:
output.extend([
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-cephfs-provisioner&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;adminID&amp;#34;: users[&amp;#39;csi-cephfs-provisioner&amp;#39;][&amp;#39;name&amp;#39;].replace(&amp;#39;client.&amp;#39;, &amp;#39;&amp;#39;),
&amp;#34;adminKey&amp;#34;: users[&amp;#39;csi-cephfs-provisioner&amp;#39;][&amp;#39;key&amp;#39;],
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-cephfs-node&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;adminID&amp;#34;: users[&amp;#39;csi-cephfs-node&amp;#39;][&amp;#39;name&amp;#39;].replace(&amp;#39;client.&amp;#39;, &amp;#39;&amp;#39;),
&amp;#34;adminKey&amp;#34;: users[&amp;#39;csi-cephfs-node&amp;#39;][&amp;#39;key&amp;#39;],
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;cephfs&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;StorageClass&amp;#34;,
&amp;#34;data&amp;#34;: {
&amp;#34;fsName&amp;#34;: &amp;#34;{}-fs&amp;#34;.format(args.instance_name),
&amp;#34;pool&amp;#34;: &amp;#34;cephfs.{}-fs.data&amp;#34;.format(args.instance_name),
}
}
])
print(json.dumps(output, indent=2))
if __name__ == &amp;#39;__main__&amp;#39;:
main()
&lt;/code>&lt;/pre>&lt;p>If you&amp;rsquo;d prefer a strictly manual process, you can fill in the
necessary values yourself. The JSON produced by the above script
looks like the following, which is invalid JSON because I&amp;rsquo;ve use
inline comments to mark all the values which you would need to
provide:&lt;/p>
&lt;pre tabindex="0">&lt;code>[
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-mon-endpoints&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;ConfigMap&amp;#34;,
&amp;#34;data&amp;#34;: {
# The format is &amp;lt;mon_name&amp;gt;=&amp;lt;mon_endpoint&amp;gt;, and you only need to
# provide a single mon address.
&amp;#34;data&amp;#34;: &amp;#34;ceph0=192.168.122.140:6789&amp;#34;,
&amp;#34;maxMonId&amp;#34;: &amp;#34;0&amp;#34;,
&amp;#34;mapping&amp;#34;: &amp;#34;{}&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-mon&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the fsid of your Ceph cluster.
&amp;#34;fsid&amp;#34;: &amp;#34;c9c32c73-dac4-4cc9-8baa-d73b96c135f4&amp;#34;,
# Do **not** fill in these values, they are unnecessary. OCS
# does not require admin access to your Ceph cluster.
&amp;#34;admin-secret&amp;#34;: &amp;#34;admin-secret&amp;#34;,
&amp;#34;mon-secret&amp;#34;: &amp;#34;mon-secret&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-ceph-operator-creds&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key for your healthchecker principal.
# Note that here, unlike elsewhere in this JSON, you must
# provide the &amp;#34;client.&amp;#34; prefix to the principal name.
&amp;#34;userID&amp;#34;: &amp;#34;client.healthchecker-mycluster&amp;#34;,
&amp;#34;userKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;ceph-rbd&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;StorageClass&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name of your RBD pool.
&amp;#34;pool&amp;#34;: &amp;#34;mycluster-rbd&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;monitoring-endpoint&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;CephCluster&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the address and port of the Ceph cluster prometheus
# endpoint.
&amp;#34;MonitoringEndpoint&amp;#34;: &amp;#34;192.168.122.140&amp;#34;,
&amp;#34;MonitoringPort&amp;#34;: &amp;#34;9283&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-rbd-node&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key of the csi-rbd-node principal.
&amp;#34;userID&amp;#34;: &amp;#34;csi-rbd-node-mycluster&amp;#34;,
&amp;#34;userKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-rbd-provisioner&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key of your csi-rbd-provisioner
# principal.
&amp;#34;userID&amp;#34;: &amp;#34;csi-rbd-provisioner-mycluster&amp;#34;,
&amp;#34;userKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-cephfs-provisioner&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key of your csi-cephfs-provisioner
# principal.
&amp;#34;adminID&amp;#34;: &amp;#34;csi-cephfs-provisioner-mycluster&amp;#34;,
&amp;#34;adminKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;rook-csi-cephfs-node&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;Secret&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name and key of your csi-cephfs-node principal.
&amp;#34;adminID&amp;#34;: &amp;#34;csi-cephfs-node-mycluster&amp;#34;,
&amp;#34;adminKey&amp;#34;: &amp;#34;&amp;lt;key&amp;gt;&amp;#34;
}
},
{
&amp;#34;name&amp;#34;: &amp;#34;cephfs&amp;#34;,
&amp;#34;kind&amp;#34;: &amp;#34;StorageClass&amp;#34;,
&amp;#34;data&amp;#34;: {
# Fill in the name of your CephFS filesystem and the name of the
# associated data pool.
&amp;#34;fsName&amp;#34;: &amp;#34;mycluster-fs&amp;#34;,
&amp;#34;pool&amp;#34;: &amp;#34;cephfs.mycluster-fs.data&amp;#34;
}
}
]
&lt;/code>&lt;/pre>&lt;h2 id="associated-bugs">Associated Bugs&lt;/h2>
&lt;p>I&amp;rsquo;ve opened several bug reports to see about adressing some of these
issues:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1996833">#1996833&lt;/a>
&amp;ldquo;ceph-external-cluster-details-exporter.py should have a read-only
mode&amp;rdquo;&lt;/li>
&lt;li>&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1996830">#1996830&lt;/a> &amp;ldquo;OCS
external mode should allow specifying names for all Ceph auth
principals&amp;rdquo;&lt;/li>
&lt;li>&lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1996829">#1996829&lt;/a>
&amp;ldquo;Permissions assigned to ceph auth principals when using external
storage are too broad&amp;rdquo;&lt;/li>
&lt;/ul></content></item><item><title>Fun with devicemapper snapshots</title><link>https://blog.oddbit.com/post/2018-01-25-fun-with-devicemapper-snapshot/</link><pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2018-01-25-fun-with-devicemapper-snapshot/</guid><description>I find myself working with Raspbian disk images fairly often. A typical workflow is:
Download the disk image. Mount the filesystem somewhere to check something. Make some changes or install packages just to check something else. Crap I&amp;rsquo;ve made changes. &amp;hellip;at which point I need to fetch a new copy of the image next time I want to start fresh.
Sure, I could just make a copy of the image and work from there, but what fun is that?</description><content>&lt;p>I find myself working with &lt;a href="https://www.raspberrypi.org/downloads/raspbian/">Raspbian&lt;/a> disk images fairly often. A
typical workflow is:&lt;/p>
&lt;ul>
&lt;li>Download the disk image.&lt;/li>
&lt;li>Mount the filesystem somewhere to check something.&lt;/li>
&lt;li>Make some changes or install packages just to check something else.&lt;/li>
&lt;li>Crap I&amp;rsquo;ve made changes.&lt;/li>
&lt;/ul>
&lt;p>&amp;hellip;at which point I need to fetch a new copy of the image next time I
want to start fresh.&lt;/p>
&lt;p>Sure, I could just make a copy of the image and work from there, but
what fun is that? This seemed like a perfect opportunity to learn more
about the &lt;a href="https://www.kernel.org/doc/Documentation/device-mapper/">device mapper&lt;/a> and in particular how the &lt;a href="https://www.kernel.org/doc/Documentation/device-mapper/snapshot.txt">snapshot&lt;/a>
target works.&lt;/p>
&lt;h2 id="making-sure-we-have-a-block-device">Making sure we have a block device&lt;/h2>
&lt;p>The device mapper only operates on block devices, so the first thing
we need to do is to make the source image available as a block device.
We can do that with the &lt;a href="http://manpages.ubuntu.com/manpages/xenial/man8/losetup.8.html">losetup&lt;/a> command, which maps a file to a
virtual block device (or &lt;em>loop&lt;/em> device).&lt;/p>
&lt;p>I run something like this:&lt;/p>
&lt;pre>&lt;code>losetup --find --show --read-only 2017-11-29-raspbian-stretch-lite.img
&lt;/code>&lt;/pre>
&lt;p>This will find the first available block device, and then use it make
my disk image available in read-only mode. Those of you who are
familiar with &lt;code>losetup&lt;/code> may be thinking, &amp;ldquo;you know, &lt;code>losetup&lt;/code> knows
how to handle partitioned devices&amp;rdquo;, but I am ignoring that for the
purpose of using device mapper to solve things.&lt;/p>
&lt;h2 id="mapping-a-partition">Mapping a partition&lt;/h2>
&lt;p>We&amp;rsquo;ve just mapped the entire disk image to a block device. We can&amp;rsquo;t
use this directly because the image has multiple partitions:&lt;/p>
&lt;pre>&lt;code># sfdisk -l /dev/loop0
Disk /dev/loop0: 1.7 GiB, 1858076672 bytes, 3629056 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x37665771
Device Boot Start End Sectors Size Id Type
/dev/loop0p1 8192 93236 85045 41.5M c W95 FAT32 (LBA)
/dev/loop0p2 94208 3629055 3534848 1.7G 83 Linux
&lt;/code>&lt;/pre>
&lt;p>We want to expose partition 2, which contains the root filesystem. We
can see from the above output that partition 2 starts at sector
&lt;code>94208&lt;/code> and extends for &lt;code>3534848&lt;/code> sectors (where a &lt;em>sector&lt;/em> is for our
purposes 512 bytes). If you need to get at this information
programatically, &lt;code>sfdisk&lt;/code> has &lt;code>--json&lt;/code> option that can be useful; for
example:&lt;/p>
&lt;pre>&lt;code># p_start=$(sfdisk --json /dev/loop0 |
jq &amp;quot;.partitiontable.partitions[1].start&amp;quot;)
# echo $p_start
94208
&lt;/code>&lt;/pre>
&lt;p>We want to expose this partition as a distinct block device. We&amp;rsquo;re
going to do this by creating a device mapper &lt;code>linear&lt;/code> target. To
create device mapper devices, we use the &lt;code>dmsetup&lt;/code> command; the basic
syntax is:&lt;/p>
&lt;pre>&lt;code>dmsetup create &amp;lt;name&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>By default, this expects to read a table describing the device on
&lt;code>stdin&lt;/code>, although it is also possible to specify one on the command
line. A table consists of one or more lines of the format:&lt;/p>
&lt;pre>&lt;code>&amp;lt;base&amp;gt; &amp;lt;length&amp;gt; &amp;lt;target&amp;gt; [&amp;lt;options&amp;gt;]
&lt;/code>&lt;/pre>
&lt;p>Where &lt;code>&amp;lt;base&amp;gt;&lt;/code> is the starting offset in sectors for this particular
segment, &lt;code>&amp;lt;length&amp;gt;&lt;/code> is the length, &lt;code>&amp;lt;target&amp;gt;&lt;/code> is the target type
(linear, snapshot, zero, etc), and the option are specific to the
particular target in use.&lt;/p>
&lt;p>To create a device exposing partition 2 of our image, we run:&lt;/p>
&lt;pre>&lt;code># dmsetup create base &amp;lt;&amp;lt;EOF
0 3534848 linear /dev/loop0 94208
EOF
&lt;/code>&lt;/pre>
&lt;p>This creates a device named &lt;code>/dev/mapper/base&lt;/code>. Sectors &lt;code>0&lt;/code> through
&lt;code>3534848&lt;/code> of this device will be provided by &lt;code>/dev/loop0&lt;/code>, starting at
offset &lt;code>94208&lt;/code>. At this point, we can actually mount the filesystem:&lt;/p>
&lt;pre>&lt;code># mount -o ro /dev/mapper/base /mnt
# ls /mnt
bin dev home lost+found mnt proc run srv tmp var
boot etc lib media opt root sbin sys usr
# umount /mnt
&lt;/code>&lt;/pre>
&lt;p>But wait, there&amp;rsquo;s a problem! These disk images usually have very
little free space. We&amp;rsquo;re going to want to extend the length of our
base device by some amount so that we have room for new packages and
so forth. Fortunately, since our goal is that all writes are going to
a snapshot, we don&amp;rsquo;t need to use &lt;em>real&lt;/em> space. We can add another
segment to our table that uses the &lt;a href="https://www.kernel.org/doc/Documentation/device-mapper/zero.txt">zero&lt;/a> target.&lt;/p>
&lt;p>Let&amp;rsquo;s first get rid of the device we just created:&lt;/p>
&lt;pre>&lt;code># dmsetup remove base
&lt;/code>&lt;/pre>
&lt;p>And create a new one:&lt;/p>
&lt;pre>&lt;code># dmsetup create base &amp;lt;&amp;lt;EOF
0 3534848 linear /dev/loop0 94208
3534848 6950912 zero
EOF
&lt;/code>&lt;/pre>
&lt;p>This extends our &lt;code>base&lt;/code> device out to 5G (or a total of &lt;code>10485760&lt;/code>
sectors), although attempting to read from anything beyond sector
&lt;code>3534848&lt;/code> will return zeros, and writes will be discarded. But that&amp;rsquo;s
okay, because the space available for writes is going to come from a
COW (&amp;ldquo;copy-on-write&amp;rdquo;) device associated with our snapshot: in other
words, the capacity of our snapshot will be linked to size of our COW
device, rather than the size of the underlying base image.&lt;/p>
&lt;h2 id="creating-a-snapshot">Creating a snapshot&lt;/h2>
&lt;p>Now that we&amp;rsquo;ve sorted out our base image it&amp;rsquo;s time to create the
snapshot device. According to &lt;a href="https://www.kernel.org/doc/Documentation/device-mapper/snapshot.txt">the documentation&lt;/a>, the
table entry for a snapshot looks like:&lt;/p>
&lt;pre>&lt;code>snapshot &amp;lt;origin&amp;gt; &amp;lt;COW device&amp;gt; &amp;lt;persistent?&amp;gt; &amp;lt;chunksize&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>We have our &lt;code>&amp;lt;origin&amp;gt;&lt;/code> (that&amp;rsquo;s the base image we created in the
previous step), but what are we going to use as our &lt;code>&amp;lt;COW device&amp;gt;&lt;/code>?
This is a chunk of storage that will receive any writes to the
snapshot device. This could be any block device (another loop device,
an LVM volume, a spare disk partition), but for my purposes it seemed
convenient to use a RAM disk, since I had no need for persistent
changes. We can use the &lt;a href="https://www.kernel.org/doc/Documentation/blockdev/zram.txt">zram&lt;/a> kernel module for that. Let&amp;rsquo;s start
by loading the module:&lt;/p>
&lt;pre>&lt;code># modprobe zram
&lt;/code>&lt;/pre>
&lt;p>Without any additional parameters this will create a single RAM disk,
&lt;code>/dev/zram0&lt;/code>. Initially, it&amp;rsquo;s not very big:&lt;/p>
&lt;pre>&lt;code># blockdev --getsz /dev/zram0
0
&lt;/code>&lt;/pre>
&lt;p>But we can change that using the &lt;code>sysfs&lt;/code> interface provided in
&lt;code>/sys/block/zram0/&lt;/code>. The &lt;code>disksize&lt;/code> option controls the size of the
disk. Let&amp;rsquo;s say we want to handle up to 512M of writes; that means we
need to write the value &lt;code>512M&lt;/code> to &lt;code>/sys/block/zram0/disksize&lt;/code>:&lt;/p>
&lt;pre>&lt;code>echo 512M &amp;gt; /sys/block/zram0/disksize
&lt;/code>&lt;/pre>
&lt;p>And now:&lt;/p>
&lt;pre>&lt;code># blockdev --getsz /dev/zram0
1048576
&lt;/code>&lt;/pre>
&lt;p>We now have all the requirements to create our snapshot device:&lt;/p>
&lt;pre>&lt;code>dmsetup create snap &amp;lt;&amp;lt;EOF
0 10485760 snapshot /dev/mapper/base /dev/zram0 N 16
EOF
&lt;/code>&lt;/pre>
&lt;p>This creates a device named &lt;code>/dev/mapper/snap&lt;/code>. It is a 5G block
device backed by &lt;code>/dev/mapper/base&lt;/code>, with changes written to
&lt;code>/dev/zram0&lt;/code>. We can mount it:&lt;/p>
&lt;pre>&lt;code># mount /dev/mapper/snap /mnt
# df -h /mnt
Filesystem Size Used Avail Use% Mounted on
/dev/mapper/snap 1.7G 943M 623M 61% /mnt
&lt;/code>&lt;/pre>
&lt;p>And we can resize it:&lt;/p>
&lt;pre>&lt;code># resize2fs !$
resize2fs /dev/mapper/snap
resize2fs 1.43.3 (04-Sep-2016)
Filesystem at /dev/mapper/snap is mounted on /mnt; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/mapper/snap is now 1310720 (4k) blocks long.
# df -h /mnt
Filesystem Size Used Avail Use% Mounted on
/dev/mapper/snap 4.9G 944M 3.8G 20% /mnt
&lt;/code>&lt;/pre>
&lt;p>You&amp;rsquo;ll note here that it looks like we have a 5G device, because
that&amp;rsquo;s the size of our base image. Because we&amp;rsquo;ve only allocated
&lt;code>512M&lt;/code> to our COW device, we can actually only handle up to 512M of
writes before we invalidate the snapshot.&lt;/p>
&lt;p>We can inspect the amount of our COW device that has been consumed by
changes by using &lt;code>dmsetup status&lt;/code>:&lt;/p>
&lt;pre>&lt;code># dmsetup status snap
0 10485760 snapshot 107392/1048576 0
&lt;/code>&lt;/pre>
&lt;p>This tells us that &lt;code>107392&lt;/code> sectors of &lt;code>1048576&lt;/code> total have been
consumed so far (in other words, about 54M out of 512M). We can get
similar information from the perspective of the &lt;code>zram&lt;/code> module using
&lt;code>zramctl&lt;/code>:&lt;/p>
&lt;pre>&lt;code># zramctl
NAME ALGORITHM DISKSIZE DATA COMPR TOTAL STREAMS MOUNTPOINT
/dev/zram0 lzo 512M 52.4M 34K 72K 4
&lt;/code>&lt;/pre>
&lt;p>This information is also available in &lt;code>/sys/block/zram0/mm_stat&lt;/code>, but
without any labels.&lt;/p></content></item><item><title>Growing a filesystem on a virtual disk</title><link>https://blog.oddbit.com/post/2012-10-24-resizing-virtual-disk/</link><pubDate>Wed, 24 Oct 2012 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2012-10-24-resizing-virtual-disk/</guid><description>Occasionally we will deploy a virtual instance into our KVM infrastructure and realize after the fact that we need more local disk space available. This is the process we use to expand the disk image. This process assumes the following:
You&amp;rsquo;re using legacy disk partitions. The process for LVM is similar and I will describe that in another post (it&amp;rsquo;s generally identical except for an additional pvresize thrown in and lvextend in place of resize2fs).</description><content>&lt;p>Occasionally we will deploy a virtual instance into our KVM
infrastructure and realize after the fact that we need more local disk
space available. This is the process we use to expand the disk image.
This process assumes the following:&lt;/p>
&lt;ul>
&lt;li>You&amp;rsquo;re using legacy disk partitions. The process for LVM is similar
and I will describe that in another post (it&amp;rsquo;s generally identical
except for an additional &lt;code>pvresize&lt;/code> thrown in and &lt;code>lvextend&lt;/code> in
place of &lt;code>resize2fs&lt;/code>).&lt;/li>
&lt;li>The partition you need to resize is the last partition on the disk.&lt;/li>
&lt;/ul>
&lt;p>This process will work with either a &lt;code>qcow2&lt;/code> or &lt;code>raw&lt;/code> disk image. For
&lt;code>raw&lt;/code> images you can also run &lt;code>fdisk&lt;/code> on the host, potentially saving
yourself a reboot, but that&amp;rsquo;s less convenient for &lt;code>qcow2&lt;/code> format
images.&lt;/p>
&lt;hr>
&lt;p>We start with a 5.5G root filesystem with 4.4G free:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda2 5.5G 864M 4.4G 17% /
&lt;/code>&lt;/pre>
&lt;p>We need to shut down the system to grow the underlying image:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# poweroff
&lt;/code>&lt;/pre>
&lt;p>On the host, we use the &lt;code>qemu-img resize&lt;/code> command to grow the image.
First we need the path to the underlying disk image:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system dumpxml lars-test-0 | grep file
&amp;lt;disk type='file' device='disk'&amp;gt;
&amp;lt;source file='/var/lib/libvirt/images/lars-test-0-1.img'/&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>And now we increase the image size by 10G:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ sudo qemu-img resize /var/lib/libvirt/images/lars-test-0.img +10G
Image resized.
&lt;/code>&lt;/pre>
&lt;p>Now reboot the instance:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system start lars-test-0
&lt;/code>&lt;/pre>
&lt;p>And login in on the console:&lt;/p>
&lt;pre>&lt;code>[lars@madhatter blog]$ virsh -c qemu:///system console lars-test-0
Connected to domain lars-test-0
Escape character is ^]
Fedora release 17 (Beefy Miracle)
Kernel 3.6.2-4.fc17.x86_64 on an x86_64 (ttyS0)
localhost login: root
Password:
&lt;/code>&lt;/pre>
&lt;p>We&amp;rsquo;re going to use &lt;code>fdisk&lt;/code> to modify the partition layout. Run
&lt;code>fdisk&lt;/code> on the system disk:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# fdisk /dev/vda
Welcome to fdisk (util-linux 2.21.2).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.
&lt;/code>&lt;/pre>
&lt;p>Print out the existing partition table and verify that you really are
going to be modifying the final partition:&lt;/p>
&lt;pre>&lt;code>Command (m for help): p
Disk /dev/vda: 19.3 GB, 19327352832 bytes
16 heads, 63 sectors/track, 37449 cylinders, total 37748736 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00007d9f
Device Boot Start End Blocks Id System
/dev/vda1 * 2048 1026047 512000 83 Linux
/dev/vda2 1026048 5154815 2064384 82 Linux swap / Solaris
/dev/vda3 5154816 16777215 5811200 83 Linux
&lt;/code>&lt;/pre>
&lt;p>Delete and recreate the final partition, in this case &lt;code>/dev/vda3&lt;/code>&amp;hellip;&lt;/p>
&lt;pre>&lt;code>Command (m for help): d
Partition number (1-4): 3
Partition 3 is deleted
&lt;/code>&lt;/pre>
&lt;p>&amp;hellip;and create a new partition, accepting all the defaults. This will
create a new partition starting in the same place and extending to the
end of the disk:&lt;/p>
&lt;pre>&lt;code>Command (m for help): n
Partition type:
p primary (2 primary, 0 extended, 2 free)
e extended
Select (default p): p
Partition number (1-4, default 3): 3
First sector (5154816-37748735, default 5154816):
Using default value 5154816
Last sector, +sectors or +size{K,M,G} (5154816-37748735, default 37748735):
Using default value 37748735
Partition 3 of type Linux and of size 15.6 GiB is set
&lt;/code>&lt;/pre>
&lt;p>You can print out the new partition table to see that indeed
&lt;code>/dev/vda3&lt;/code> is now larger:&lt;/p>
&lt;pre>&lt;code>Command (m for help): p
Disk /dev/vda: 19.3 GB, 19327352832 bytes
16 heads, 63 sectors/track, 37449 cylinders, total 37748736 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00007d9f
Device Boot Start End Blocks Id System
/dev/vda1 * 2048 1026047 512000 83 Linux
/dev/vda2 1026048 5154815 2064384 82 Linux swap / Solaris
/dev/vda3 5154816 37748735 16296960 83 Linux
&lt;/code>&lt;/pre>
&lt;p>Write the changes to disk:&lt;/p>
&lt;pre>&lt;code>Command (m for help): w
The partition table has been altered!
Calling ioctl() to re-read partition table.
WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>Note the warning!&lt;/strong> The kernel has cached a copy of the old
partition table. We need to reboot the system before our changes are
visible! So we reboot the system:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# reboot
&lt;/code>&lt;/pre>
&lt;p>And log back in. Run &lt;code>df&lt;/code> to see the current size of the root
filesystem:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda3 5.5G 864M 4.4G 17% /
&lt;/code>&lt;/pre>
&lt;p>Now run &lt;code>resize2fs&lt;/code> to resize the root filesystem so that it expands
to fill our extended &lt;code>/dev/vda3&lt;/code>:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# resize2fs /dev/vda3
resize2fs 1.42.3 (14-May-2012)
Filesystem at /dev/vda3 is mounted on /; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vda3 is now 4074240 blocks long.
&lt;/code>&lt;/pre>
&lt;p>Run &lt;code>df&lt;/code> again to see that we now have additional space available:&lt;/p>
&lt;pre>&lt;code>[root@localhost ~]# df -h /
Filesystem Size Used Avail Use% Mounted on
/dev/vda3 16G 867M 14G 6% /
[root@localhost ~]#
&lt;/code>&lt;/pre></content></item><item><title>Installing CrashPlan under FreeBSD 8</title><link>https://blog.oddbit.com/post/2011-05-22-installing-crashplan-under-fre/</link><pubDate>Sun, 22 May 2011 00:00:00 +0000</pubDate><guid>https://blog.oddbit.com/post/2011-05-22-installing-crashplan-under-fre/</guid><description>This articles describes how I got CrashPlan running on my FreeBSD 8(-STABLE) system. These instructions by Kim Scarborough were my starting point, but as these were for FreeBSD 7 there were some additional steps necessary to get things working.
Install Java I had originally thought that it might be possible to run the CrashPlan client &amp;ldquo;natively&amp;rdquo; under FreeBSD. CrashPlan is a Java application, so this seemed like a possible solution. Unfortunately, Java under FreeBSD 8 seems to be a lost cause.</description><content>&lt;p>This articles describes how I got &lt;a href="http://crashplan.com/">CrashPlan&lt;/a> running on my FreeBSD 8(-STABLE) system. &lt;a href="http://kim.scarborough.chicago.il.us/do/nerd/tips/crashplan">These instructions&lt;/a> by Kim Scarborough were my starting point, but as these were for FreeBSD 7 there were some additional steps necessary to get things working.&lt;/p>
&lt;h1 id="install-java">Install Java&lt;/h1>
&lt;p>I had originally thought that it might be possible to run the CrashPlan client &amp;ldquo;natively&amp;rdquo; under FreeBSD. CrashPlan is a Java application, so this seemed like a possible solution. Unfortunately, Java under FreeBSD 8 seems to be a lost cause. I finally gave up and just installed Java under Linux.&lt;/p>
&lt;h2 id="set-up-your-linux-compatability-environment">Set up your Linux compatability environment&lt;/h2>
&lt;p>The simplest way to do this is to follow the instructions in the &lt;a href="http://www.freebsd.org/doc/handbook/linuxemu-lbc-install.html">FreeBSD Handbook&lt;/a>. This will get you a Fedora 10 based Linux userspace, which should be more than sufficient. I&amp;rsquo;m using a CentOS 5.6 userspace, but for what we&amp;rsquo;re doing it shouldn&amp;rsquo;t matter, modulo some minor differences in paths.&lt;/p>
&lt;p>Note that Linux software running in this environment will have a modified view of your filesystem. In particular, /etc will map to /compat/linux/etc, and ZFS filesystems with non-default mountpoints seem to behave oddly (they are accessible, but not necessarily visible before you access them). This may require some workarounds in CrashPlan, depending on what you&amp;rsquo;re trying to back up.&lt;/p>
&lt;h2 id="install-java-jre">Install Java JRE&lt;/h2>
&lt;p>I installed a compatible Java environment from the CentOS package repository:&lt;/p>
&lt;pre>&lt;code># chroot /compat/linux bash
bash-3.2# yum install java-1.6.0-openjdk
bash-3.2# exit
&lt;/code>&lt;/pre>
&lt;h1 id="install-crashplan">Install CrashPlan&lt;/h1>
&lt;h2 id="install-the-crashplan-software">Install the CrashPlan software&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Download &lt;a href="http://www.crashplan.com/consumer/download.html?os=Linux">CrashPlan for Linux&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Unpack the archive (named something like CrashPlan_3.0.3_Linux.tgz)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Change to the CrashPlan-install directory.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Run the following commands:&lt;/p>
&lt;h1 id="export-pathcompatlinuxusrlibjvmjre-160-openjdkbinpath">export PATH=/compat/linux/usr/lib/jvm/jre-1.6.0-openjdk/bin:$PATH&lt;/h1>
&lt;h1 id="compatlinuxbinbash-installsh">/compat/linux/bin/bash install.sh&lt;/h1>
&lt;/li>
&lt;li>
&lt;p>Install CrashPlan into /usr/local. When prompted for where to locate init scripts (&amp;ldquo;What directory contains your SYSV init scripts?&amp;rdquo; and &amp;ldquo;What directory contains your runlevel init links?&amp;rdquo;), enter /tmp (because the installed init scripts aren&amp;rsquo;t ideal for your FreeBSD environment &amp;ndash; we&amp;rsquo;ll install our own later on).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="fix-java">Fix Java&lt;/h2>
&lt;p>The Linux runtime provided by the FreeBSD Linux compatability layer does not include all of the features of recent Linux kernels. In particular, it is missing the epoll* syscalls, which will cause Java to die with a &lt;em>Function not implemented&lt;/em> error. The workaround for this is documented in the &lt;a href="http://wiki.freebsd.org/linux-kernel">linux-kernel&lt;/a> page on the &lt;a href="http://wiki.freebsd.org/">FreeBSD wiki&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>If you run an application in the linux java which wants to use the linux epoll functions (you should see &amp;ldquo;not implemented&amp;rdquo; messages in dmesg), you can start java with the argument -Djava.nio.channels.spi.SelectorProvider=sun.nio.ch.PollSelectorProvider&lt;/p>
&lt;/blockquote>
&lt;h2 id="install-an-rc-script">Install an rc script&lt;/h2>
&lt;p>Place the following script into /usr/local/etc/rc.d/crashplan:&lt;/p>
&lt;pre>&lt;code>#!/bin/sh
#
# PROVIDE: crashplan
# REQUIRE: NETWORKING
# KEYWORD: shutdown
. /etc/rc.subr
name=&amp;quot;crashplan&amp;quot;
rcvar=`set_rcvar`
start_cmd=crashplan_start
stop_cmd=crashplan_stop
crashplan_start () {
/compat/linux/bin/bash /usr/local/crashplan/bin/CrashPlanEngine start
}
crashplan_stop () {
/compat/linux/bin/bash /usr/local/crashplan/bin/CrashPlanEngine stop
}
load_rc_config $name
run_rc_command &amp;quot;$1&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>And then add:&lt;/p>
&lt;pre>&lt;code>crashplan_enable=&amp;quot;YES&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>To /etc/rc.conf (or /etc/rc.conf.local).&lt;/p>
&lt;h2 id="start-crashplan">Start CrashPlan&lt;/h2>
&lt;p>Run:&lt;/p>
&lt;pre>&lt;code>/usr/local/etc/rc.d/crashplan start
&lt;/code>&lt;/pre>
&lt;p>Wait a moment, then run:&lt;/p>
&lt;pre>&lt;code>/compat/linux/bin/bash /usr/local/crashplan/bin/CrashPlanEngine status
&lt;/code>&lt;/pre>
&lt;p>This should verify that CrashPlan is running.&lt;/p>
&lt;h1 id="connect-crashplan-client">Connect CrashPlan client&lt;/h1>
&lt;p>Follow the instructions provided by CrashPlan for &lt;a href="http://stgsupport.crashplan.com/doku.php/how_to/configure_a_headless_client">connecting to a headless CrashPlan desktop&lt;/a>.&lt;/p></content></item></channel></rss>